[
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "My papers",
    "section": "",
    "text": "Here’s a selection of my published papers:\n\nModeling Racial/ethnic Differences in COVID-19 Incidence with Covariates Subject to Non-random Missingness (Trangucci, Chen, and Zelner 2023)\nRacial Disparities in Coronavirus Disease 2019 (COVID-19) Mortality Are Driven by Unequal Infection Risks (Zelner et al. 2021)\nQuantifying Observed Prior Impact (Jones, Trangucci, and Chen 2021)\nModeling Spatial Risk of Diarrheal Disease Associated with Household Proximity to Untreated Wastewater Used for Irrigation in the Mezquital Valley, Mexico (Contreras Jesse D. et al. 2020)\nBayesian Hierarchical Weighting Adjustment and Survey Inference. (Si 2020)\nEffects of Sequential Influenza A(H1N1)Pdm09 Vaccination on Antibody Waning (Zelner et al. 2019)"
  },
  {
    "objectID": "papers.html#preprints",
    "href": "papers.html#preprints",
    "title": "My papers",
    "section": "Preprints",
    "text": "Preprints\n\nIdentified vaccine efficacy for binary post-infection outcomes under misclassification without monotonicity (Trangucci, Chen, and Zelner 2022)\nBayesian Methods for Modeling Cumulative Exposure to Extensive Environmental Health Hazards (Submitted) (Trangucci et al. 2024)"
  },
  {
    "objectID": "papers.html#conference-papers",
    "href": "papers.html#conference-papers",
    "title": "My papers",
    "section": "Conference papers",
    "text": "Conference papers\n\nHierarchical Gaussian Processes in Stan (Trangucci 2017)\nPrior formulation for Gaussian process hyperparameters. (Trangucci, Betancourt, and Vehtari 2016)"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "STAT 625 - Winter 2024\n\n\n\n\n\nSTAT 625 - Winter 2025\nSTAT 599 - Winter 2025\n\n\n\n\n\nSTAT 625 - Winter 2026\nSTAT 599 - Winter 2026"
  },
  {
    "objectID": "posts/heckman-in-stan.html",
    "href": "posts/heckman-in-stan.html",
    "title": "Heckman models in Stan",
    "section": "",
    "text": "This is cmdstanr version 0.8.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/robertntrangucci/.cmdstan/cmdstan-2.36.0\n\n\n- CmdStan version: 2.36.0\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\nI took a class in the first year of my Master’s program about missing data from Ben Goodrich. The class was all about what to do when you encountered some sort of missing observations when analyzing a dataset. Missingness can arise for many reasons when analyzing data: it can arise from people declining to participate in a survey, survey respondents refusing to answer a question, from patients who drop out of a randomized study, or from the fact that an outcome of interest is only observable for a subset of patients who experience (or don’t experience) an intermediate event.\nWhen dealing with missing data, it is crucial that we understand why an observation, be it an outcome, a covariate, or a unit, is missing. If the\nThe Heckman selection model was one of the first examples we were shown in class of a missing data problem where the probability that an individual had a missing observation depended on the value of the missing observation (Heckman 1979). These are the hardest problems to address in missing data because\nA standard survey example would be that people with very high and those with very low incomes may decline to report income on a survey more than those with moderate incomes. In the vaccine example, the selection effect we are concerned with is that people with weaker immune systems may become infected even with a vaccine and thus have higher viral loads. A naive comparison of viral loads between vaccinated and unvaccinated people might show that the vaccine increases viral loads in those who are vaccinated (Hudgens, Hoering, and Self 2003).\nTo make things a little more concrete, suppose we’re interested in learning about the how vaccines moderate post-infection viral loads. The precise quantity we’re interested in is the change in post-infection viral load for people who would be infected with the pathogen no matter their vaccination status. These unlucky people are referred to as the ``Always-Infected’’ group. We might be tempted to get a rough estimate of this quantity by comparing viral loads in infected people in the vaccinated group vs. those in the unvaccinated group. The problem with this analysis is that if the vaccine has any causal effect on infection the individual characteristics of the two groups, infected-vaccinated people and infected-unvaccinated people, may differ. In this problem missingness arises in two forms. The first is that we observe only one outcome for each participant in our trial: the outcome corresponding to the treatment arm assignment. This is what Holland called the fundamental problem of causal inference. The second is more unique to this setting: we observe viral loads only in those who are infected.\nLet \\(Z_i\\) be the vaccine treatment, with \\(Z_i = 1\\) indicating treatment and \\(Z_i = 0\\) indicating placebo. For the ease of exposition, let’s suppose that treatment is randomized. We don’t need more selection bias (though selection into treatment could very well be a source of additional selection bias). Let \\(S_i(z)\\) be the infection status of individual \\(i\\) with vaccination status \\(z\\), and let \\(Y_i(z)\\) be the viral load of individual \\(i\\). If \\(S_i(z) = 0\\), we set \\(Y_i(z) = *\\).\nSuppose we also observe the study site, denoted as \\(R_i\\). This is the hospital or health clinic that enrolled patient \\(i\\) into the study. This study site is assumed to impact that probability of infection, but it is assumed to be independent of post-infection viral load. The standard term for a variable like \\(R_i\\) is an instrumental variable.\nThe model we’ll simulate data from is the following: \\[\n\\begin{aligned}\n\\log Y_i(z) & = \\mu_Y(z) + \\sigma_z\\, U_i(z) \\\\\n\\tilde{S}_i(z) \\mid R_i & = \\mu_S(R_i) + z \\,\\gamma_S + W_i \\\\\nS_i(z) & = 1(\\tilde{S}_i(z) > 0) \\\\\n(U_i(0),U_i(1),W_i) & \\sim \\text{Normal}(\\mathbf{0}, \\boldsymbol{\\Omega})\n\\end{aligned}\n\\] The individuals who are always infected are identified as those with \\(\\{S_i(1) = 1,S_i(0) = 1\\}\\). One key assumption of this model is that these individuals can be identified with certainty. This is also known as a assumption. To see why, let’s see what our model implies about the always-infected event: \\[\n\\begin{aligned}\n\\{S_i(1) = 1, S_i(0) = 1\\} & = \\{\\tilde{S}_i(1) > 0, \\tilde{S}_i(0) > 0\\} \\\\\n& = \\{\\mu_S(R_i) + \\gamma_S > -W_i, \\mu_S(R_i) > -W_i\\} \\\\\n& = \\{\\min(\\mu_S(R_i) + \\gamma_S,\\mu_S(R_i))  > -W_i\\}\n\\end{aligned}\n\\] The event that \\(S_i(z) = 1\\) is equivalent to \\[\n\\{\\mu_S(R_i) + \\gamma_S > -W_i\\}\n\\] Thus, if \\(\\gamma_S\\) is less than zero, as we would hope in a vaccine efficacy trial, then those who are infected in the vaccine arm are also those who would be infected in the placebo arm. The monotonicity assumption is a consequence of two assumptions of the model: - there is one error term for the probit regression - there is no individual heterogeneity in the coefficient \\(\\gamma_S\\).\nThe likelihood for the model is fairly straightforward: For individuals who remain uninfected, the likelihood term corresponds to \\(P(\\tilde{S}_i(z) \\leq 0)\\), which corresponds to the univariate standard normal CDF evaluated at \\(-\\mu_S(z,r)\\), or: \\[\nP(\\mu_S(z,r) + W_i \\leq 0) = \\Phi(-\\mu_S(z,r))\n\\]\nThe likelihood for an individual \\(i\\) who is infected corresponds to observing \\(\\log Y_i(z) = y_i\\) and \\(\\tilde{S}_i(z) > 0 \\mid \\log Y_i(z) = y_i, R_i = r_i\\): \\[\n\\begin{aligned}\nL_i(\\theta) & = \\frac{1}{\\sqrt{2\\pi} \\sigma_z}\\exp\\left(-\\frac{1}{2\\sigma_z^2}(y_i - \\mu_Y(z_i))^2\\right) \\\\\n& \\Phi\\left(\\frac{\\mu_S(z_i) + \\frac{\\rho_{z_i}}{\\sigma_z}(y_i - \\mu_Y(z_i))}{\\sqrt{1 - \\rho_{z_i}^2}}\\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(\\rho_z = \\text{Cor}(W_i,U_i(z))\\).\nTo get a sense for what this model implies about the treatment effect, let’s examine the conditional expectation of \\(Y_i(z)\\) given \\(\\tilde{S}_i(z) > 0\\) when a patient is vaccinated vs. when the patient is unvaccinated:\n\\[\n\\tau^\\text{naive} = \\mathbb{E} \\left[Y_i(1) \\mid \\tilde{S}_i(1) > 0 \\right] - \\mathbb{E} \\left[Y_i(0) \\mid \\tilde{S}_i(0) > 0 \\right]\n\\] This is what we’ll call the naive estimand, because it conditions on a post-treatment outcome, namely infection, which can induce selection bias, as we’ll see.\nThe expectation \\(\\mathbb{E} \\left[Y_i(z) \\mid \\tilde{S}_i(z) > 0 \\right]\\) is the following:\n\\[\n\\begin{aligned}\n\\frac{(2 \\pi\\sigma_z^2)^{-1/2}}{\\Phi(\\mu_S(z_i))}\\int_{-\\infty}^{\\infty}  \\exp\\left(y-\\frac{1}{2\\sigma_z^2}(y - \\mu_Y(z_i))^2\\right)\n\\Phi\\left(\\frac{\\mu_S(z_i) + \\frac{\\rho_{z_i}}{\\sigma_z}(y - \\mu_Y(z_i))}{\\sqrt{1 - \\rho_{z_i}^2}}\\right) dy\n\\end{aligned}\n\\] To my knowledge, this doesn’t have a closed form expression because of the exponentiation of \\(y\\), though there might be a clever way to evaluate this integral. We can, however, use R’s 1-d numerical integration routine to approximate this expectation.\nThe code below evaluates \\(\\tau^\\text{naive}\\), as well as\n\ncond_exp <- function(y, mu_y, mu_s, sd_y, rho) {\n  exp(y + dnorm(y, mu_y, sd_y, log = TRUE) + pnorm((mu_s + rho / sd_y * (y - mu_y))/sqrt(1 - rho^2),log.p = TRUE))\n}\ntau_naive <- function(rho_0, rho_1, sd_y, mu_y, mu_s, s_eff, y_eff) {\n  I1 <- integrate(cond_exp, -Inf, Inf, \n                  mu_y = mu_y + y_eff, \n                  mu_s = mu_s + s_eff, \n                  sd_y = sd_y, \n                  rho = rho_1, \n                  abs.tol = 0L, \n                  rel.tol = .Machine$double.eps^0.85\n  )\n  stopifnot(I1$subdivisions > 3)\n  D1 <- pnorm(mu_s + s_eff)\n  I2 <- integrate(cond_exp, -Inf, Inf, \n                  mu_y = mu_y, \n                  mu_s = mu_s, \n                  sd_y = sd_y, \n                  rho = rho_0, \n                  abs.tol = 0L, \n                  rel.tol = .Machine$double.eps^0.85)\n  stopifnot(I2$subdivisions > 3)\n  D2 <- pnorm(mu_s)\n  return(I1$value / D1 - I2$value / D2)\n}\ntrue_eff <- function(sd_y, mu_y, y_eff) {\n  mu_0 <- exp(mu_y + sd_y^2 / 2)\n  mu_1 <- exp(mu_y + y_eff + sd_y^2 / 2)\n  return(mu_1 - mu_0)\n}\ndelta_s <- function(mu_z, s_eff) {\n  p_0 <- pnorm(mu_z)\n  p_1 <- pnorm(mu_z + s_eff)\n  return(p_1 - p_0)\n}\n\nFirst we’ll explore a scenario where there is high positive correlation between the errors in the viral load equation and the latent variable related to infection. We set \\(\\rho_1 = \\rho_0 = 0.9\\), \\(\\sigma_0 = \\sigma_1 = 0.5\\), \\(\\mu_Y(0) = 10\\) and \\(\\mu_S(0) = -1.5\\). Let the true effect of the vaccine on mean log-viral load be \\(\\delta_y = -0.25\\).\nWe will plot how the naive estimand of viral load effect changes with the change in infection probability.\n\neffs <- seq(-3, 3, by = 0.01)\np_s <- sapply(effs, \\(x) delta_s(-0.5, x)) \nestimand <- sapply(effs, \\(x) f_naive_est(rho = 0.9, sd_y = 0.5, mu_y = 10, mu_s = -1.5, s_eff = x, y_eff = -0.25))\np_change_in_sign <- p_s[which.min(abs(estimand))]\nplot(p_s, \n     estimand,\n     main = \"Naive effect estimand vs. infection effect\",\n     ylab = bquote(E(y ~ \"|\" ~ S == 1, Z == 1) - E(y ~ \"|\" ~ S == 1, Z == 0)),\n     xlab = bquote(P(S == 1 ~ \"|\" ~ Z == 1) - P(S == 1 ~ \"|\" ~ Z == 0)),\n     type=\"l\")\nabline(v = p_change_in_sign, col = \"red\")\nabline(h = 0, col = \"red\")\n\n\n\n\nThis graph shows that as the vaccine becomes more effective at preventing infection, the apparent benefit of the vaccine on viral load decreases. In fact, at around a -0.18 decrease in the probability of infection, the naive estimand shows that the vaccine increases viral load, which is not the case.\nIf we fix a scenario and\n\neffs <- seq(-3, 3, by = 0.01)\np_s <- sapply(effs, \\(x) delta_s(-0.5, x)) \nestimand <- sapply(effs, \\(x) f_naive_est(rho = 0.9, sd_y = 0.5, mu_y = 10, mu_s = -1.5, s_eff = x, y_eff = -0.25))\np_change_in_sign <- p_s[which.min(abs(estimand))]\nplot(p_s, \n     estimand,\n     main = \"Naive effect estimand vs. infection effect\",\n     ylab = bquote(E(y ~ \"|\" ~ S == 1, Z == 1) - E(y ~ \"|\" ~ S == 1, Z == 0)),\n     xlab = bquote(P(S == 1 ~ \"|\" ~ Z == 1) - P(S == 1 ~ \"|\" ~ Z == 0)),\n     type=\"l\")\nabline(v = p_change_in_sign, col = \"red\")\nabline(h = 0, col = \"red\")\n\n\n\n\nNote that \\(\\rho_{01} = \\text{Cor}(U_i(0),U_i(1))\\) does not enter into the likelihood, which makes sense because we’ll never observe both \\(S_i(0)\\) and \\(S_i(1)\\) for the same individual. This does not mean that we won’t have any information about \\(\\rho_{01}\\); because \\(\\boldsymbol{\\Omega}\\) must be positive definite, the \\(\\det \\boldsymbol{\\Omega} > 0\\). This means that what we do learn about \\(\\rho_0\\) and \\(\\rho_1\\) will constrain the domain of \\(\\rho_{01}\\) so that \\(\\det \\boldsymbol{\\Omega} > 0\\). This translates to the following bounds on \\(\\rho_{01}\\) \\[\n\\rho_{01} \\in \\left(\\rho_0\\rho_1 - \\sqrt{1 - \\rho_0^2 - \\rho_1^2 + \\rho_0^2\\rho_1^2},\\, \\rho_0\\rho_1 + \\sqrt{1 - \\rho_0^2 - \\rho_1^2 + \\rho_0^2\\rho_1^2}\\right)\n\\] If we incorporate prior information, the asymptotic posterior for \\(\\rho_{01}\\) will take on the shape of the conditional prior for \\(\\rho_{01}\\) given the values for \\(\\rho_0\\) and \\(\\rho_1\\).\nGiven our focus on comparing the impact of vaccination on viral load, the target causal estimand is a measure of the average decrease (hopefully) in viral load caused by vaccination in individuals who are always infected. Mathematically, this is: \\[\\mathbb{E}[Y_i(1) - Y_i(0) \\mid S_i(1) = S_i(0) = 1].\\] This estimand is somewhat odd in that we can never observe both infection outcomes, so we can never calculate this estimand exactly from observed data without extra assumptions. I explained above why we can’t equate this target estimand with the ``naive’’ version, the difference in viral load between vaccinated and unvaccinated people: \\[\n\\mathbb{E}[Y_i(1) \\mid S_i(1) = 1] - \\mathbb{E}[Y_i(0) \\mid S_i(0) = 1].\n\\] Logically, it makes sense, because the naive estimand is a mixture over several groups, instead of just the always-infected group, but we’ll simulate some data to get a better sense of how different the two estimands can be.\nIn our simulated example, we’ll suppose we have a randomized vaccine trial with 20,000 participants spread evenly across 10 study sites. We’ll set \\(\\rho_0\\) and \\(\\rho_1\\) to \\(0.75\\), which equates to strong residual correlation between infection risk and viral load; that might be reasonable if there is an unobserved factor impacting both infection and viral load. Maybe this is prior exposure to the virus plus genetic factors related to immune system health.\nThe covariates we’re including in the infection risk equation, \\(\\mu_S(z)\\) are age and study site, while the only covariates we’re including in the viral load model are categorical age predictors.\n\nset.seed(12222)\nn <- 20000\n\nsigma_0 <- 0.7\nsigma_1 <- 0.7\nsigma_W <- 1\nrho_0 <- 0.9\nrho_1 <- 0.9\nlb <- rho_0*rho_1 - sqrt(1 - rho_0^2 - rho_1^2 + rho_0^2*rho_1^2)\nub <- rho_0*rho_1 + sqrt(1 - rho_0^2 - rho_1^2 + rho_0^2*rho_1^2)\nrho_01 <- 0.7\ndiag_sigs <- diag(c(sigma_0,sigma_1,sigma_W))\nOmega <- matrix(c(1,rho_01,rho_0,rho_01,1,rho_1,rho_0,rho_1,1),3,3)\nSigma <- diag_sigs %*% Omega  %*% diag_sigs\nU0_U1_W <- mvrnorm(n = n, mu = c(0, 0, 0), Sigma = Sigma)\n\nThe plot of \\(U_0\\) vs. \\(W\\) gives a sense of how related the two error terms are:\n\n\n\n\n\nThe covariates we will include in the model are age, which we treat as a categorical variable, the study site, which is also treated as a categorical variable, and the binary treatment variable, which is assumed to be balanced within study sites.\n\nN_age <- 4\nN_sites <- 10\nage_vec <- sample(N_age, n, TRUE) |> factor()\nsite_vec <- rep(1:N_sites,each=n / N_sites) |> factor()\ntreat <- rep(c(rep(0,n/20),rep(1,n/20)),10)\n\nWe’ll create the model matrices from these variables, along with the coefficients for each model equation:\n\nX_S <- model.matrix(~ age_vec + site_vec)\nX_Y <- model.matrix(~ age_vec)\nd_S <- ncol(X_S)\nd_Y <- ncol(X_Y)\ngamma_Y <- -0.25\ngamma_S <- -0.85\n\nbeta_S <- c(-1.5, 0.1, 0.2, 0.3, rnorm(N_sites - 1) * 0.1)\nbeta_Y <- c(10.3, 0.5, 0.5, 0.5)\n\nS_0  <- (X_S %*% beta_S + U0_U1_W[,3]) >= 0\nS_1  <- (X_S %*% beta_S + gamma_S + U0_U1_W[,3]) >= 0\nS <- cbind(as.integer(S_0), as.integer(S_1))\nY_0  <- X_Y %*% beta_Y + U0_U1_W[,1]\nY_1  <- X_Y %*% beta_Y + gamma_Y + U0_U1_W[,2]\nY <- cbind(ifelse(S_0 == 1, Y_0, NA),\n           ifelse(S_1 == 1, Y_1, NA))\n\nsel <- as.vector(rbind(1-treat, treat))\nS_o <- as.vector(t(S))[sel == 1]\nY_o <- as.vector(t(Y))[sel == 1]\n\nidx_S_o <- which(S_o == 1)\nidx_AI <- which(S_0 == 1 & S_1 == 1)\n\nY_o_sel <- Y_o[idx_S_o]\nX_Y_sel <- X_Y[idx_S_o, ]\n\n\nnaive_estimand <- mean(exp(Y_o_sel[treat[idx_S_o] == 1])) -\n                  mean(exp(Y_o_sel[treat[idx_S_o] == 0]))\n\nY_estimand <- mean(exp(Y_1[idx_AI])) -\n               mean(exp(Y_0[idx_AI]))\n\nn_s_0 <- sum(treat[idx_S_o] == 1)\nn_s_1 <- sum(treat[idx_S_o] == 0)\n\nn_AI <- length(idx_AI)\n\nAfter all is said and done, the finite-sample estimator for the population estimand is -63,883, with a standard error of 6888.1207534 while the naive estimator for the population estimand is 41,600 with a standard error of 8335.141496. Note that these standard errors are understated a bit because the selection indicators are treated as fixed. In reality, both the selection and the outcome are random, so a more complete standard error calculation would account for the randomness in both sets of outcomesboth sets of outcomes.\nThis means that the naive estimator understates the benefit of the vaccine for people who are always infected by about 165%.\nThe approximate standard error for the naive estimator is 6888.1207534, while the approximate standard error .\nLet’s build the Stan model block by block. First, the data block needs to take in the sizes and types of all the matrices and vectors that we’ll need to fit the model.\n\nData block\n\n\ndata {\n  int<lower=1> N;     \n  int<lower=1> N_neg; \n  int<lower=1> N_pos; \n  int<lower=1> D_S;   \n  int<lower=1> D_Y;   \n  vector[N_pos] Y;    \n  array[N] int<lower=0,upper=1> S;  \n  array[N] int<lower=0,upper=1> Z;  \n  matrix[N_pos,D_Y] X_Y; \n  matrix[N,D_S] X_S;  \n}\n\nWe need the number of observations, N, the number of negative and positive cases (N_neg and N_pos). We also need the dimensions of the predictors for the \\(S\\) equation and the \\(Y\\) equation, D_S and D_Y. The observed viral loads are collected into a length-N_pos vector Y, while the observed infection status and treatment assignment are collected into length-N binary integer arrays S and Z. Finally, we have the predictors for the \\(Y\\) equation, X_Y, which is an N_pos by D_Y matrix, and the predictors for the \\(S\\) equation, which is an N by D_S matrix.\n\nTransformed data block\n\nThe transformed data block is needed to collect the indices, measured from \\(1,\\dots,N\\), of the negative cases (\\(S_i = 0\\)) and positive cases (\\(S_i = 1\\)). The reason we need this information is because the likelihood contribution to the infection status probit model, shown in Equation 1, requires the value of the log-viral load, so we’ll need to match the infection status outcomes to the viral load outcomes. We could do this matching in the R code outside the Stan model, but I prefer to do it within the Stan code so everything is in one place. We’ll collect the indices of the positive cases in n_pos, which is an array of length N_pos and holds integers between 1 and N, while negative case indices are collected in n_neg, defined similarly.\nThe other piece of information the infection status likelihood will need for positive cases is the value \\(\\rho_z\\), which depends on the individual treatment assignment. This information is collected in the vector Z_idx.\n\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\n\n\nParameters block\n\nThe parameter block is where we define which unknown parameters we need to estimate with our model. Our unknown parameters is the set: - \\(\\boldsymbol{\\Omega}\\): correlation matrix between the error terms for the regressions - \\(\\sigma_0\\): error standard deviation for log-viral load regression in placebo group - \\(\\sigma_1\\): error standard deviation for log-viral load regression in treatment group - \\(\\beta_y\\): regression coefficients for the log-viral load regression - \\(\\beta_s\\) - \\(\\gamma_s\\) _y)$, the correlation matrix for the errors. In our case, we have a\n\nparameters {\n  cholesky_factor_corr[3] L_Omega;\n  vector<lower=0>[2] sd_Y;\n  vector[D_Y] b_Y;\n  vector[D_S] b_S;\n  real gamma_S;\n  real gamma_Y;\n}\n\nThe Stan model is written like so:\n\ndata {\n  int<lower=1> N;     \n  int<lower=1> N_neg; \n  int<lower=1> N_pos; \n  int<lower=1> D_S;   \n  int<lower=1> D_Y;   \n  vector[N_pos] Y;    \n  array[N] int<lower=0,upper=1> S;  \n  array[N] int<lower=0,upper=1> Z;  \n  matrix[N_pos,D_Y] X_Y; \n  matrix[N,D_S] X_S;  \n}\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\nparameters {\n  cholesky_factor_corr[3] L_Omega;\n  vector<lower=0>[2] sd_Y;\n  vector[D_Y] b_Y;\n  vector[D_S] b_S;\n  real gamma_S;\n  real gamma_Y;\n}\ntransformed parameters {\n  matrix[3,3] Omega = L_Omega * L_Omega';\n}\nmodel {\n  vector[N] mu_S = X_S * b_S + to_vector(Z) * gamma_S;\n  vector[N_pos] mu_Y = X_Y * b_Y + to_vector(Z[n_pos]) * gamma_Y;\n  array[2] real rho = {Omega[1,3], Omega[2,3]};\n\n  b_Y ~ normal(0, 1);\n  b_S ~ normal(0, 1);\n  sd_Y ~ normal(0, 2);\n  gamma_S ~ normal(0, 5);\n  gamma_Y ~ normal(0, 5);\n\n  for (n in 1:N_neg) {\n    target += log(Phi(-mu_S[n_neg[n]]));\n  }\n  for (n in 1:N_pos) {\n    int treat_idx = Z_idx[n_pos[n]];\n    target += log(Phi(sqrt((1 - rho[treat_idx]) * (1 + rho[treat_idx]))^(-1)*(mu_S[n_pos[n]]\n                               + rho[treat_idx] / sd_Y[treat_idx]\n                               * (Y[n] - mu_Y[n]))));\n    Y[n] ~ normal(mu_Y[n], sd_Y[treat_idx]);\n  }\n}\ngenerated quantities {\n  array[2] real rho = {Omega[1,3], Omega[2,3]};\n  real Y_estimand = 0;\n  {\n    vector[N] mu_S = X_S * b_S;\n    matrix[3,3] Sigma = quad_form_diag(Omega, append_row(sd_Y, [1]'));\n    int tot_11 = 0;\n    for (n in 1:N) {\n      vector[3] errors = multi_normal_rng(rep_vector(0,3), Sigma);\n      if (errors[3] > -mu_S[n] - gamma_S && errors[3] > -mu_S[n]) {\n        real mu_Y_cf = X_S[n,1:D_Y] * b_Y;\n        real Y_0 = errors[1] + mu_Y_cf;\n        real Y_1 = errors[2] + mu_Y_cf + gamma_Y;\n        Y_estimand += Y_1 -  Y_0;\n        tot_11 += 1;\n      }\n    }\n    Y_estimand /= tot_11;\n  }\n}\n\n\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\n\nSome comments on the Stan code are in order. The data block is pretty straightforward, though it’s worth keeping in mind that we’ll need to keep track of three dimensions: the total number of data points, the number of infected and uninfected patients. This is because we’ll only have access to viral loads in the infected patients.\nLet’s generate some data to see how the errors change how the .\nFirst, let’s see whether the\n\n\n\n\nReferences\n\nHeckman, James J. 1979. “Sample Selection Bias as a Specification Error.” Econometrica 47 (1): 153. https://doi.org/10.2307/1912352.\n\n\nHudgens, Michael G., Antje Hoering, and Steven G. Self. 2003. “On the Analysis of Viral Load Endpoints in HIV Vaccine Trials.” Statistics in Medicine 22 (14): 2281–98. https://doi.org/10.1002/sim.1394."
  },
  {
    "objectID": "posts/beta-processes-in-stan.html",
    "href": "posts/beta-processes-in-stan.html",
    "title": "Gamma and Beta Processes in Stan",
    "section": "",
    "text": "rdirichlet <- function(n, alpha) {\n  d <- length(alpha)\n  rates <- rep(1, d)\n  gammas <- replicate(n = n,\n                      rgamma(d, shape = alpha,\n                             rate = rates)\n                      )\n  draws <- sweep(gammas, MARGIN = 2, STATS = colSums(gammas), FUN = \"/\")\n  return(t(draws))\n}\n\nI taught the PhD-level survival analysis course in the 2024 Winter quarter at Oregon State (website). The prior years’ courses in survival analysis were exclusively focused on Frequentist nonparametric and semiparametrci methods for inference: Kaplan-Meier, Cox proportional hazards model, etc. I wanted to see if I could bring a bit more Bayesian inference into the course. Ultimately, I didn’t include any Bayesian nonparametrics, but it gave me the opportunity to try some simple nonparametric methods in Stan (Stan is always my first choice to develop bespoke models, it’s second nature at this point).\nMy starting point to learn more about Bayesian nonparametric survival analysis modeling was our textbook for the course Klein and Moeschberger (Klein and Moeschberger 2003). They review several nonparameteric Bayesian models for survival analysis in 6.4: Dirichlet processes, and beta processes. Both methods put a nonparametric prior over the survival function, or \\(S(t)\\), but they do so in different ways.\nThese methods are used to model time-to-event data. Let’s call the time-to-event random variable \\(T_i\\) for an individual \\(i\\). The survival function for \\(T_i\\) is the complement of the cumulative distribution function, \\(P(T_i \\leq t)\\):\n\\[\nS(t) = P(T_i > t) = 1 - P(T_i \\leq t)\n\\]\nTime-to-event data is complicated by the fact that we often can’t observe all times to failure, but instead we observe only a subset of these failures corresponding to those times that occur prior to a censoring time. In the simplest case, called Type I censoring, we observe only \\(T_i\\) that are less than a constant \\(C\\).\nThe standard set-up for survival analysis data is to define two new random variables for each observation \\(i\\):\n\\[\nX_i = 1(T_i \\leq C) T_i + C 1(T_i > C), \\Delta_i = 1(T_i \\leq C).\n\\]\nNow we have a single time-to-event random variable \\(X_i\\) which is equal to \\(T_i\\) if \\(T_i\\) occurs prior to the censoring time \\(C\\). In this case, \\(\\Delta_i = 1\\). If \\(T_i\\) occurs after the censoring time, we don’t observe \\(T_i\\) but rather we observe only the censoring time. This sort of censoring set up occurs if we enroll patients in a randomized controlled trial, administer a treatment, and follow each patient for, say, 10 months. If the event of interest occurs within the 10-month window, we will have observed \\(T_i\\), otherwise, our only knowledge of the event time is that it is greater than 10-months.\nBack to our modeling strategy to learn about \\(S(t)\\) from a set of data, \\(\\{(X_i, \\Delta_i), i.= 1, \\dots, n\\}\\). One strategy to model the survival function nonparametrically is to define a partition of the positive real line, \\(\\{t_j, j = 0, \\dots, J\\}\\) with \\(t_0 = 0, t_J = \\infty\\)to model increments of the survival function \\(S(t_j) - S(t_{j-1})\\) over this partition. These increments will be between \\((0,1)\\) and they will sum to \\(1\\). A natural distribution for these random variables is a Dirichlet distribution, which respects the natural constraints of the increments process:\n\\[\n(S(t_0) - S(t_1), \\dots, S(t_j) - S(t_{j-1}), S(t_{J-1}) - S(t_{J})) \\sim \\text{Dirichlet}(c\\,\\boldsymbol{\\alpha}).\n\\]\nTypically, \\(\\boldsymbol{\\alpha}\\) is chosen so that it too is a function of time \\(t\\) and that it corresponds to a known parametric survival function, like say the exponential function: \\(S(t) = \\exp(-\\lambda t)\\).\nWhen we combine the prior over the survival function with counts of failures within an interval\n, but the function that we’re modeling is different between the models.\nThe Dirichlet process directly models the unknown survival function, while the beta process models the unknown cumulative hazard function.\nAnother popular choice for Bayesian nonparametric survival analysis, not covered by KM for some reason, is a gamma process. My first choice was a gamma process.\nOne of the proposed methods is the Beta process, which is rigorously developed in (Hjort 1990).\n\n\n\n\nReferences\n\nHjort, Nils Lid. 1990. “Nonparametric Bayes Estimators Based on Beta Processes in Models for Life History Data.” The Annals of Statistics 18 (3). https://doi.org/10.1214/aos/1176347749.\n\n\nKlein, John P., and Melvin L. Moeschberger. 2003. Survival Analysis: Techniques for Censored and Truncated Data. 2nd ed. Statistics for Biology and Health. New York: Springer."
  },
  {
    "objectID": "st_625_w_24.html",
    "href": "st_625_w_24.html",
    "title": "ST 625 - W 24",
    "section": "",
    "text": "Required: Survival Analysis: Techniques for censored and truncated data, Klein and Moeschberger\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nNotes"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rob Trangucci",
    "section": "",
    "text": "I am an assistant professor of Statistics at Oregon State University. My research focuses on novel statistical methodology in missing data analysis and causal inference for problems in epidemiology, designing Bayesian methods for survey inference, and creating tools to quantify how priors impact posterior inferences. Before I returned to academia to pursue a doctorate in statistics, I worked as a data scientist for a fintech startup, a statistical consultant for a Big Five publisher, and a core developer for the Stan statistical modeling and inference platform (https://mc-stan.org/). I earned a PhD in Statistics from the University of Michigan, an M.A. in Quantitative Methods in the Social Sciences from Columbia University, and a B.A. in Physics from Bucknell University."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Rob Trangucci",
    "section": "Interests",
    "text": "Interests\n\nCausal inference for vaccine efficacy\nMissing data\nPrincipal stratification\nPrior influence\nMultilevel regression and poststratification (MRP)\nBayesian inference"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rob Trangucci",
    "section": "Education",
    "text": "Education\n\nPhD in Statistics, 2023\n\nUniversity of Michigan\n\nMA in Quantitative Methods in the Social Science, 2014\n\nColumbia University\n\nBA in Physics, 2009\n\nBucknell University"
  },
  {
    "objectID": "st_625_w_25.html",
    "href": "st_625_w_25.html",
    "title": "ST 625 - W 25",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nNotes\n\n\n\n\n\nHW 1\nHW 2\nHW 3\nHW 4\nHW 5\nHW 6"
  },
  {
    "objectID": "st_599_w_25.html",
    "href": "st_599_w_25.html",
    "title": "ST 599 - W 25: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are identifiable for a given data generating process.\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes\nLecture 3 notes\nLecture 4 notes\nLecture 5 notes\nLecture 6 notes\nLecture 7 notes\nLecture 8 notes\nLecture 9 notes\nLecture 10 notes\nLecture 11 notes\nLecture 12 notes\nLecture 13 notes\nLecture 14 notes\nLecture 15 notes\nLecture 16 notes\n\n\n\n\n\nHW 1\nHW 2\nHW 3"
  },
  {
    "objectID": "courses.html#academic-courses",
    "href": "courses.html#academic-courses",
    "title": "Courses",
    "section": "",
    "text": "STAT 625 - Winter 2024\n\n\n\n\n\nSTAT 625 - Winter 2025\nSTAT 599 - Winter 2025\n\n\n\n\n\nSTAT 625 - Winter 2026\nSTAT 599 - Winter 2026"
  },
  {
    "objectID": "st_599_w_26.html",
    "href": "st_599_w_26.html",
    "title": "ST 599 - W 26: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are appropriately estimated under missingness for a given missing data technique (complete case analysis, multiple imputation, data augmentation, etc.).\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReading\nLO\nAssignments\n\n\n\n\n1/6/2026\nMissingness mechanisms and patterns\nLR 1.1-1.4\n1, 2\nNA\n\n\n1/8/2026\nMissing data techniques Complete case analysis  Weighting\nLR Ch. 3\n1, 2\nNA\n\n\n1/13/2026\nSingle imputation techniques\nLR Ch. 4\n1, 2\nNA\n\n\n1/15/2026\nMultiple imputation techniques\nLR Ch. 5\n1, 2\nNA\n\n\n1/20/2026\nLikelihood theory\nLR Ch. 6\n1, 5\nNA\n\n\n1/22/2026\nLikelihood theory\nLR Ch. 6\n1, 5\n1\n\n\n\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes\nLecture 3 notes\nLecture 4 notes\nLecture 5 notes\nLecture 6 notes\nLecture 7 notes\nLecture 8 notes\nLecture 9 notes\nLecture 10 notes\n\n\n\n\n\nMCMC intro and diagnostics\nHamiltonian Monte Carlo\n\n\n\n\n\nHW 1\nHW 2"
  },
  {
    "objectID": "st_599_w_26.html#syllabus",
    "href": "st_599_w_26.html#syllabus",
    "title": "ST 599 - W 26: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are appropriately estimated under missingness for a given missing data technique (complete case analysis, multiple imputation, data augmentation, etc.).\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReading\nLO\nAssignments\n\n\n\n\n1/6/2026\nMissingness mechanisms and patterns\nLR 1.1-1.4\n1, 2\nNA\n\n\n1/8/2026\nMissing data techniques Complete case analysis  Weighting\nLR Ch. 3\n1, 2\nNA\n\n\n1/13/2026\nSingle imputation techniques\nLR Ch. 4\n1, 2\nNA\n\n\n1/15/2026\nMultiple imputation techniques\nLR Ch. 5\n1, 2\nNA\n\n\n1/20/2026\nLikelihood theory\nLR Ch. 6\n1, 5\nNA\n\n\n1/22/2026\nLikelihood theory\nLR Ch. 6\n1, 5\n1\n\n\n\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes\nLecture 3 notes\nLecture 4 notes\nLecture 5 notes\nLecture 6 notes\nLecture 7 notes\nLecture 8 notes\nLecture 9 notes\nLecture 10 notes\n\n\n\n\n\nMCMC intro and diagnostics\nHamiltonian Monte Carlo\n\n\n\n\n\nHW 1\nHW 2"
  },
  {
    "objectID": "st_625_w_26.html",
    "href": "st_625_w_26.html",
    "title": "ST 625 - W 26",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9\nLecture 10\nLecture 11\nLecture 12\n\n\n\n\n\nMaps between Weibull parameterizations\nHW 1\nHW 2\nHW 3\nHW 4"
  },
  {
    "objectID": "st_625_w_26.html#syllabus",
    "href": "st_625_w_26.html#syllabus",
    "title": "ST 625 - W 26",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\nLecture 9\nLecture 10\nLecture 11\nLecture 12\n\n\n\n\n\nMaps between Weibull parameterizations\nHW 1\nHW 2\nHW 3\nHW 4"
  },
  {
    "objectID": "papers.html#published-papers",
    "href": "papers.html#published-papers",
    "title": "My papers",
    "section": "",
    "text": "Here’s a selection of my published papers:\n\nModeling Racial/ethnic Differences in COVID-19 Incidence with Covariates Subject to Non-random Missingness (Trangucci, Chen, and Zelner 2023)\nRacial Disparities in Coronavirus Disease 2019 (COVID-19) Mortality Are Driven by Unequal Infection Risks (Zelner et al. 2021)\nQuantifying Observed Prior Impact (Jones, Trangucci, and Chen 2021)\nModeling Spatial Risk of Diarrheal Disease Associated with Household Proximity to Untreated Wastewater Used for Irrigation in the Mezquital Valley, Mexico (Contreras Jesse D. et al. 2020)\nBayesian Hierarchical Weighting Adjustment and Survey Inference. (Si 2020)\nEffects of Sequential Influenza A(H1N1)Pdm09 Vaccination on Antibody Waning (Zelner et al. 2019)"
  },
  {
    "objectID": "survival-material/lecture-1.html",
    "href": "survival-material/lecture-1.html",
    "title": "Lecture 1",
    "section": "",
    "text": "This introduction is based in part on Klein, Moeschberger, et al. (2003), and in part on Aalen, Borgan, and Gjessing (2008) plus Fleming and Harrington (2005).\nSurvival analysis is the modeling and analysis of time-to-event data; this means we will be studying how to model nonnegative random variables (time will always be measured in such a way so that the observations are nonnegative). Think about a clinical trial for a new COVID vaccine and how you might model the length of time between study entry and infection in each arm of the trial. Let \\(X_i\\) be the time from trial entry to infection for the \\(i\\)-th participant. These sorts of trials are typically run until a prespecified number of people have become infected. Let \\(n\\) be the total number of participants in the trial and let \\(r\\) be the prespecified number of infections. Let \\(T_i\\) be the observed infection time for the \\(i\\)-th participant. This means that for \\(r\\) participants, \\(T_i = X_i\\), but for \\(n-r\\) participants we know only that the time-to-infection is larger than the observed time. Let \\(C_i\\) denote the time from study entry for participant \\(i\\) to study end. Then \\(T_i = \\min (X_i, C_i)\\), and let \\(\\delta_i = \\ind{T_i = X_i}\\). The density of \\(T_i\\) is related to the joint probability for \\(X_i\\) and \\(C_i\\), which is indexed by a possibly infinite dimensional parameter \\(\\theta\\): \\(P_{\\theta}(X_i &gt; t, C_i &gt; c)\\). When \\(\\delta_i = 1\\), and \\(T_i = X_i\\), the likelihood of the observation is \\[\n\\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_i &gt; u, C_i &gt; t)\\rp\\right\\rvert_{u = t},\n\\] while the likelihood for \\(\\delta_i = 0\\) is \\[\n\\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_i &gt; t, C_i &gt; u)\\rp\\right\\rvert_{u = t},\n\\] Then \\(T_i = C_i\\) for the other \\(n-r\\) participants. Under the null hypothesis that the vaccine has no effect, the population distribution function for all \\(n\\) participants for \\(X_i, C_i\\) is \\(P_{\\theta}(X_1 &gt; x, C_1 &gt; c)\\) (i.e. the distribution for survival times in the treatment group and the placebo group is the same). Then the joint density for the observed infection times is as follows: \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\theta) & = n! \\prod_{i=1}^r \\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_1 &gt; u, C_1 &gt; t_{(i)})\\rp\\right\\rvert_{u = t_{(i)}} \\\\\n& \\times \\prod_{i=r+1}^n \\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_1 &gt; t_{(i)}, C_1 &gt; u)\\rp\\right\\rvert_{u = t_{(i)}},\n\\end{align*}\\] where \\(t_{(i)}\\) is the \\(i\\)-th order statistic of the set \\(\\{t_1, \\dots, t_n\\}\\). Note that this is different from most other data analysis where missing observations are not expected to occur with much frequency. On the contrary, in survival analysis, missingness, both truncation and censoring are expected to occur with nearly every dataset, so much of our time will be spent ensuring our methods work when data arise with these peculiarities.\n\n\nNow suppose that \\(X_1 \\indy C_1\\), and that \\(\\theta\\) partitions into \\(\\eta\\) and \\(\\phi\\), such that \\[\nP_{\\theta}(X_1 &gt; x, C_1 &gt; c) = P_{\\eta}(X_1 &gt; x)P_{\\phi}(C_1 &gt; c).\n\\] Then we can rewrite the joint observational density for \\(T_i\\) as: \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\theta) & = n! \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)}) \\\\\n& \\times \\lp \\prod_{i=1}^r P_{\\phi}(C_1 &gt; t_{(i)}) \\rp \\prod_{i=r+1}^n f_C(t_{(i)} ;\\, \\phi).\n\\end{align*}\\] If we are only interested about inference about \\(\\eta\\), the parameters that govern the distribution of the true time-to-infection random variables, we can ignore the the distribution for the censoring random variables \\(C_1\\), and maximize the likelihood because, in \\(\\eta\\): \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\eta) \\propto \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)})  \n\\end{align*}\\] We will talk in more detail about censoring in the coming lectures.\n\n\n\nAalen, Borgan, and Gjessing (2008) notes that we cannot even compute a simple mean in this situation, so something like a t-test will be useless. As an aside, let’s try to compute a mean from the data above. Let \\(\\bar{T} = \\frac{1}{n} \\sum_{i=1}^n T_i\\). We can show that \\(\\lim_{n \\to \\infty} \\bar{T} \\leq \\Exp{X_i}\\) with probability \\(1\\).\n\nProof. Let \\(T_i = X_i \\ind{X_i \\leq C_i} + C_i \\ind{X_i &gt; C_i}\\). Then by the SLLN \\(\\bar{T} \\overset{\\text{a.s.}}{\\to} \\Exp{T_i}\\). \\[\\begin{align*}\n\\Exp{T_i} & = \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{C_i \\ind{X_i &gt; C_i}} \\\\\n& \\leq \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{X_i \\ind{X_i &gt; C_i}} = \\Exp{X_i}\n\\end{align*}\\]\n\n\n\n\nHow can we compute the mean time to infection then? One way to estimate the mean time to infection is to first estimate the function \\(S_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i &gt; t)\\), which is also known as the survival function. Recall this fact about non-negative random variables \\(X_i \\geq 0\\) w.p. 1: \\[\\begin{align*}\n\\Exp{X_i} = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt    \n\\end{align*}\\] This follows from an application of Fubini’s theorem applied to the integral: \\[\\begin{align*}\n\\Exp{X_i} & = \\int_0^\\infty u dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dt \\, dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dP_{X_i}(u ;\\, \\theta) dt \\\\\n& = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt\n\\end{align*}\\]\n\n\nLet \\(F_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i \\leq t)\\). Then because the survival function is defined as \\(S_{X_i}(t ;\\, \\theta) = 1 - F_{X_i}(t ;\\, \\theta)\\) (also known as the complementary CDF) the survival function inherits its properties from the CDF. The survival function:\n\n\\(S_{X_i}(t ;\\, \\theta)\\) is a nonincreasing function\n\\(S_{X_i}(0 ;\\, \\theta) = 1\\)\n\\(\\lim_{t\\to\\infty} S_{X_i}(t ;\\, \\theta) = 0\\)\nHas lefthand limits: \\(\\lim_{s \\nearrow t} S_{X_i}(s ;\\, \\theta) = S_{X_i}(t-;\\, \\theta).\\)\nIs right continuous: \\(\\lim_{s \\searrow t} S_{X_i}(s;\\, \\theta) = S_{X_i}(t;\\, \\theta).\\)\n\nAn example of a discrete survival function is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Example plot of a survival function for a discrete survival time, bounded between \\([0,10]\\)"
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-indycensor",
    "href": "survival-material/lecture-1.html#sec-indycensor",
    "title": "Lecture 1",
    "section": "",
    "text": "Now suppose that \\(X_1 \\indy C_1\\), and that \\(\\theta\\) partitions into \\(\\eta\\) and \\(\\phi\\), such that \\[\nP_{\\theta}(X_1 &gt; x, C_1 &gt; c) = P_{\\eta}(X_1 &gt; x)P_{\\phi}(C_1 &gt; c).\n\\] Then we can rewrite the joint observational density for \\(T_i\\) as: \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\theta) & = n! \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)}) \\\\\n& \\times \\lp \\prod_{i=1}^r P_{\\phi}(C_1 &gt; t_{(i)}) \\rp \\prod_{i=r+1}^n f_C(t_{(i)} ;\\, \\phi).\n\\end{align*}\\] If we are only interested about inference about \\(\\eta\\), the parameters that govern the distribution of the true time-to-infection random variables, we can ignore the the distribution for the censoring random variables \\(C_1\\), and maximize the likelihood because, in \\(\\eta\\): \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\eta) \\propto \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)})  \n\\end{align*}\\] We will talk in more detail about censoring in the coming lectures."
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-prop-surv-fun",
    "href": "survival-material/lecture-1.html#sec-prop-surv-fun",
    "title": "Lecture 1",
    "section": "3.1 Properties of the survival function",
    "text": "3.1 Properties of the survival function\nLet \\(F_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i \\leq t)\\). Then because the survival function is defined as \\(S_{X_i}(t ;\\, \\theta) = 1 - F_{X_i}(t ;\\, \\theta)\\) (also known as the complementary CDF) the survival function inherits its properties from the CDF. The survival function:\n\n\\(S_{X_i}(t ;\\, \\theta)\\) is a nonincreasing function\n\\(S_{X_i}(0 ;\\, \\theta) = 1\\)\n\\(\\lim_{t\\to\\infty} S_{X_i}(t ;\\, \\theta) = 0\\)\nHas lefthand limits: \\(\\lim_{s \\nearrow t} S_{X_i}(s ;\\, \\theta) = S_{X_i}(t-;\\, \\theta).\\)\nIs right continuous: \\(\\lim_{s \\searrow t} S_{X_i}(s;\\, \\theta) = S_{X_i}(t;\\, \\theta).\\)\n\nAn example of a discrete survival function is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Example plot of a survival function for a discrete survival time, bounded between \\([0,10]\\)"
  },
  {
    "objectID": "survival-material/lecture-2.html",
    "href": "survival-material/lecture-2.html",
    "title": "Lecture 2",
    "section": "",
    "text": "Another way to characterize the random variable \\(X_i\\) is the hazard function, which is typically denoted as \\(\\lambda(t)\\) or \\(h(t)\\) and is defined as \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X_i &lt; t + \\Delta t \\mid X_i \\geq t}{\\theta} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t }{\\theta}}\n\\end{align*}\\] First, note that we can define \\(\\Prob{X_i \\geq t}{\\theta}\\) in terms of the survival function as: \\[\n\\Prob{X_i \\geq t}{\\theta} = \\lim_{s\\nearrow t} S_{X_i}(s ;\\, \\theta).\n\\] Using the notation introduced in the last lecture, we can write this as \\[\n\\Prob{X_i \\geq t}{\\theta} = S_{X_i}(t- ;\\, \\theta).\n\\] Of course, when \\(X_i\\) is absolutely continuous,\\(S_{X_i}(t- ;\\, \\theta) = S_{X_i}(t ;\\, \\theta)\\), but when \\(X_i\\) is discrete, or mixed discrete and continuous, as noted above, it is not true in general that the survival function is left-continuous.\nA few things to note about \\(\\lambda_{X_i}(t ;\\, \\theta)\\): when \\(X_i\\) is an absolutely continuous random variable, which occurs when we’re considering survival in continuous time, we can write this in terms of the probability density function \\(f_{X_i}(t ;\\, \\theta)\\) and the cumulative distribution function \\(F_{X_i}(t ;\\, \\theta)\\): \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{F_{X_i}(t + \\Delta t;\\, \\theta) - F_{X_i}(t;\\, \\theta)}{\\Delta t} \\times \\frac{1}{1 - F_{X_i}(t;\\, \\theta)} \\\\\n& = \\frac{f_{X_i}(t;\\, \\theta)}{1 - F_{X_i}(t;\\, \\theta)}.\n\\end{align*}\\] Let’s examine how the survival function and the hazard function fit together. \\[\n\\lambda_{X_i}(t) = \\frac{f_{X_i}(t ;\\, \\theta)}{S_{X_i}(t- ;\\, \\theta)}.\n\\] Note that we can write the hazard function in terms of the survival function instead of the density, when \\(X_i\\) is absolutely continuous: \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& =  \\frac{1}{S_{X_i}(t ;\\, \\theta)} \\times \\lim_{\\Delta t \\searrow 0}\\frac{S_{X_i}(t ;\\, \\theta)-S_{X_i}(t + \\Delta t ;\\, \\theta) }{\\Delta t} \\\\\n& = \\frac{1}{S_{X_i}(t ;\\, \\theta)} \\times -\\frac{d}{dt} S_{X_i}(t ;\\, \\theta).\n\\end{align*}\\] This implies that \\[\n\\lambda_{X_i}(t) = -\\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta).\n\\] If we integrate both sides, we get another important identity in survival analysis: \\[\\begin{align}\n   \\int_{0}^u \\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta) dt & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\\\\n   \\log S_{X_i}(u ;\\, \\theta) - \\log S_{X_i}(0 ;\\, \\theta) & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\quad \\text{note}\\,\\,\\, S_{X_i}(0 ;\\, \\theta) = 1\\\\\n   S_{X_i}(u ;\\, \\theta) & = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp \\label{eq:exp-hazard}\n\\end{align}\\]\n\n\nThe relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-2.html#properties-of-the-hazard-function",
    "href": "survival-material/lecture-2.html#properties-of-the-hazard-function",
    "title": "Lecture 2",
    "section": "",
    "text": "The relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-1.html#properties-of-the-hazard-function",
    "href": "survival-material/lecture-1.html#properties-of-the-hazard-function",
    "title": "Lecture 1",
    "section": "4.1 Properties of the hazard function",
    "text": "4.1 Properties of the hazard function\nThe relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-mttf",
    "href": "survival-material/lecture-1.html#sec-mttf",
    "title": "Lecture 1",
    "section": "",
    "text": "Aalen, Borgan, and Gjessing (2008) notes that we cannot even compute a simple mean in this situation, so something like a t-test will be useless. As an aside, let’s try to compute a mean from the data above. Let \\(\\bar{T} = \\frac{1}{n} \\sum_{i=1}^n T_i\\). We can show that \\(\\lim_{n \\to \\infty} \\bar{T} \\leq \\Exp{X_i}\\) with probability \\(1\\).\n\nProof. Let \\(T_i = X_i \\ind{X_i \\leq C_i} + C_i \\ind{X_i &gt; C_i}\\). Then by the SLLN \\(\\bar{T} \\overset{\\text{a.s.}}{\\to} \\Exp{T_i}\\). \\[\\begin{align*}\n\\Exp{T_i} & = \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{C_i \\ind{X_i &gt; C_i}} \\\\\n& \\leq \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{X_i \\ind{X_i &gt; C_i}} = \\Exp{X_i}\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-survfun",
    "href": "survival-material/lecture-1.html#sec-survfun",
    "title": "Lecture 1",
    "section": "",
    "text": "How can we compute the mean time to infection then? One way to estimate the mean time to infection is to first estimate the function \\(S_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i &gt; t)\\), which is also known as the survival function. Recall this fact about non-negative random variables \\(X_i \\geq 0\\) w.p. 1: \\[\\begin{align*}\n\\Exp{X_i} = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt    \n\\end{align*}\\] This follows from an application of Fubini’s theorem applied to the integral: \\[\\begin{align*}\n\\Exp{X_i} & = \\int_0^\\infty u dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dt \\, dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dP_{X_i}(u ;\\, \\theta) dt \\\\\n& = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt\n\\end{align*}\\]\n\n\nLet \\(F_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i \\leq t)\\). Then because the survival function is defined as \\(S_{X_i}(t ;\\, \\theta) = 1 - F_{X_i}(t ;\\, \\theta)\\) (also known as the complementary CDF) the survival function inherits its properties from the CDF. The survival function:\n\n\\(S_{X_i}(t ;\\, \\theta)\\) is a nonincreasing function\n\\(S_{X_i}(0 ;\\, \\theta) = 1\\)\n\\(\\lim_{t\\to\\infty} S_{X_i}(t ;\\, \\theta) = 0\\)\nHas lefthand limits: \\(\\lim_{s \\nearrow t} S_{X_i}(s ;\\, \\theta) = S_{X_i}(t-;\\, \\theta).\\)\nIs right continuous: \\(\\lim_{s \\searrow t} S_{X_i}(s;\\, \\theta) = S_{X_i}(t;\\, \\theta).\\)\n\nAn example of a discrete survival function is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Example plot of a survival function for a discrete survival time, bounded between \\([0,10]\\)"
  },
  {
    "objectID": "survival-material/lecture-1.html#hazard-function",
    "href": "survival-material/lecture-1.html#hazard-function",
    "title": "Lecture 1",
    "section": "",
    "text": "Another way to characterize the random variable \\(X_i\\) is the hazard function, which is typically denoted as \\(\\lambda(t)\\) or \\(h(t)\\) and is defined as \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X_i &lt; t + \\Delta t \\mid X_i \\geq t}{\\theta} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t }{\\theta}}\n\\end{align*}\\] First, note that we can define \\(\\Prob{X_i \\geq t}{\\theta}\\) in terms of the survival function as: \\[\n\\Prob{X_i \\geq t}{\\theta} = \\lim_{s\\nearrow t} S_{X_i}(s ;\\, \\theta).\n\\] Using the notation introduced in Section 1.3.1, we can write this as \\[\n\\Prob{X_i \\geq t}{\\theta} = S_{X_i}(t- ;\\, \\theta).\n\\] Of course, when \\(X_i\\) is absolutely continuous,\\(S_{X_i}(t- ;\\, \\theta) = S_{X_i}(t ;\\, \\theta)\\), but when \\(X_i\\) is discrete, or mixed discrete and continuous, as noted above, it is not true in general that the survival function is left-continuous.\nA few things to note about \\(\\lambda_{X_i}(t ;\\, \\theta)\\): when \\(X_i\\) is an absolutely continuous random variable, which occurs when we’re considering survival in continuous time, we can write this in terms of the probability density function \\(f_{X_i}(t ;\\, \\theta)\\) and the cumulative distribution function \\(F_{X_i}(t ;\\, \\theta)\\): \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{F_{X_i}(t + \\Delta t;\\, \\theta) - F_{X_i}(t;\\, \\theta)}{\\Delta t} \\times \\frac{1}{1 - F_{X_i}(t;\\, \\theta)} \\\\\n& = \\frac{f_{X_i}(t;\\, \\theta)}{1 - F_{X_i}(t;\\, \\theta)}.\n\\end{align*}\\] Let’s examine how the survival function and the hazard function fit together. \\[\n\\lambda_{X_i}(t) = \\frac{f_{X_i}(t ;\\, \\theta)}{S_{X_i}(t- ;\\, \\theta)}.\n\\] Note that we can write the hazard function in terms of the survival function instead of the density, when \\(X_i\\) is absolutely continuous: \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{S_{X_i}(t ;\\, \\theta)-S_{X_i}(t + \\Delta t ;\\, \\theta) }{\\Delta t} \\times \\frac{1}{S_{X_i}(t ;\\, \\theta)} \\\\\n& = -\\frac{d}{dt} S_{X_i}(t ;\\, \\theta)/S_{X_i}(t ;\\, \\theta).\n\\end{align*}\\] This implies that \\[\n\\lambda_{X_i}(t) = -\\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta).\n\\] If we integrate both sides, we get another important identity in survival analysis: \\[\\begin{align}\n   \\int_{0}^u \\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta) dt & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\\\\n   \\log S_{X_i}(u ;\\, \\theta) - \\log S_{X_i}(0 ;\\, \\theta) & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\quad \\text{note}\\,\\,\\, S_{X_i}(0 ;\\, \\theta) = 1\\\\\n   S_{X_i}(u ;\\, \\theta) & = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp \\label{eq:exp-hazard}\n\\end{align}\\]\n\n\nThe relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-1.html#density-function-for-survival-time",
    "href": "survival-material/lecture-1.html#density-function-for-survival-time",
    "title": "Lecture 1",
    "section": "",
    "text": "Given that we have \\(S_{X_i}(t;\\,\\theta)\\) and \\(\\lambda(t) = \\frac{f_{X_i}(t;\\,\\theta)}{S_{X_i}(t-;\\,\\theta)}\\), we can recover the density, \\(f_{X_i}(t;\\,\\theta)\\) easily: \\[\nf_{X_i}(t;\\,\\theta) = \\lambda_{X_i}(t) S_{X_i}(t-;\\,\\theta)\n\\]"
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-cumu-haz",
    "href": "survival-material/lecture-1.html#sec-cumu-haz",
    "title": "Lecture 1",
    "section": "",
    "text": "One final important quantity that describes a survival distribution is that of , which we’ll denote as \\(\\Lambda_{X_i}(t)\\), though it is also denoted as \\(H(t)\\) in . This is defined as you might expect: \\[\n\\Lambda_{X_i}(t) = \\int_{0}^t \\lambda_{X_i}(u) du.\n\\] It has the important property that for any absolutely continuous failure time \\(X_i\\) with a given cumulative hazard function, the random variable \\(Y_i = \\Lambda_{X_i}(X_i)\\) is exponentially distributed with rate \\(1\\). The derivation is straightforward. Remember that \\(P(X_i &gt; t) = \\exp\\lp-\\Lambda_{X_i}(t)\\rp\\) \\[\\begin{align*}\nP(\\Lambda_{X_i}(X_i) &gt; t) & = P(X_i &gt; \\Lambda_{X_i}^{-1}(t)) \\\\\n& = \\exp\\lp-\\Lambda_{X_i}(\\Lambda_{X_i}^{-1}(t))\\rp \\\\\n& = \\exp\\lp-t\\rp\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-1.html#discrete-survival-time",
    "href": "survival-material/lecture-1.html#discrete-survival-time",
    "title": "Lecture 1",
    "section": "",
    "text": "We’ve been working with continuous survival times until now. If \\(X_i\\) is a discrete random variable with support on \\(\\{t_1, t_2, \\dots\\}\\), we lose some of the tidyness of the previous derivations. We can define the distribution of \\(X_i\\) in terms of the survival function, \\(P_{\\theta}(X_i &gt; t)\\). First let \\(p_j = P_{\\theta}(X_i = t_j)\\), so \\[\nS_{X_i}(t;\\,\\theta) = P_{\\theta}(X_i &gt; t) = \\sum_{j \\mid t_j &gt; t} p_j\n\\] We can also define the hazard function for a discrete random variable: \\[\n\\lambda_{X_i}(t_j) = \\frac{p_j}{S_{X_i}(t_{j-1};\\,\\theta)} = \\frac{p_j}{p_j + p_{j+1} + \\dots}\n\\] Note that \\(p_j = S_{X_i}(t_{j-1};\\,\\theta) - S_{X_i}(t_{j};\\,\\theta)\\), then \\[\n\\lambda_{X_i}(t_j) = 1 - \\frac{S_{X_i}(t_j;\\,\\theta)}{S_{X_i}(t_{j-1};\\,\\theta)}.\n\\] If we let \\(t_0 = 0\\) then \\(S_{X_i}(t_0;\\,\\theta) = 1\\). This allows us to write the survival function in a sort of telescoping product: \\[\\begin{align*}\nP_{\\theta}(X_i &gt; t_j) & = P_{\\theta}(X_i &gt; t_0) \\frac{P_{\\theta}(X_i &gt; t_1)}{P_{\\theta}(X_i &gt; t_0)} \\frac{P_{\\theta}(X_i &gt; t_2)}{P_{\\theta}(X_i &gt; t_1)} \\dots \\frac{P_{\\theta}(X_i &gt; t_j)}{P_{\\theta}(X_i &gt; t_{j-1})} \\\\\n   & =  1 \\frac{S_{X_i}(t_1;\\,\\theta)}{S_{X_i}(t_0;\\,\\theta)}\\frac{S_{X_i}(t_2;\\,\\theta)}{S_{X_i}(t_1;\\,\\theta)}  \\dots \\frac{S_{X_i}(t_j;\\,\\theta)}{S_{X_i}(t_{j-1};\\,\\theta)}\n\\end{align*}\\] This yields another way to write \\(S_{X_i}(t;\\,\\theta)\\): \\[\\begin{align}\\label{eq:discrete-survival}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j \\leq t} (1 - \\lambda_{X_i}(t_j)).\n\\end{align}\\] It turns out that we can write the survival function for continuous random variables in the same way. ## Connection between discrete and continuous survival functions {#sec:disc-continue} Recall the definition of the hazard function: \\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html",
    "title": "Lecture-1",
    "section": "",
    "text": "Defined in the book roughly as missing values that would be meaningful for you analysis if it had been observed\nMissing data is ubiquitous\nI’d bet everyone has analyzed data with missing values\nWhat did you do with the units that had missing data?"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#introduction",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#introduction",
    "title": "Lecture-1",
    "section": "",
    "text": "Defined in the book roughly as missing values that would be meaningful for you analysis if it had been observed\nMissing data is ubiquitous\nI’d bet everyone has analyzed data with missing values\nWhat did you do with the units that had missing data?"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#missing-data-patterns",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#missing-data-patterns",
    "title": "Lecture-1",
    "section": "Missing data patterns",
    "text": "Missing data patterns\n\n\nCourse will focus on rectangular or tabular datasets\nAssume we have \\(p\\) measurements on \\(n\\) units, arranged into matrix \\(\\mathbf{Y}\\)\nWe can create an \\(n \\times p\\) matrix \\(\\mathbf{M}\\), where \\(\\mathbf{M}_{ij} = 1\\) if \\(\\mathbf{Y}_{ij}\\) is missing and \\(\\mathbf{M}_{ij} = 0\\) if \\(\\mathbf{Y}_{ij}\\) is observed\n\nFor the next few slides, we’ll use \\(n=10\\) and \\(p=3\\)"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#univariate-missingness",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#univariate-missingness",
    "title": "Lecture-1",
    "section": "Univariate missingness",
    "text": "Univariate missingness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Graph with M\nplot(1, type = \"n\", xlim = c(0.5, 5.5), ylim = c(0, 1), axes = FALSE, xlab = \"\", ylab = \"\")\n\n# Define the bar positions and heights\nblue_heights &lt;- c(1, 1, 0.4)\nred_heights &lt;- c(0, 0, 0.6)\n\n# Adjusted bar positions for no spacing\nx_positions &lt;- c(1, 1.5, 2)\nbar_width &lt;- 0.5\n\n# Draw the blue bars\nrect(x_positions[1] - bar_width / 2, 0, x_positions[1] + bar_width / 2, blue_heights[1], col = \"#ADD8E6\", border = \"blue\", lwd = 2)\nrect(x_positions[2] - bar_width / 2, red_heights[2], x_positions[2] + bar_width / 2, red_heights[2] + blue_heights[2], col = \"#ADD8E6\", border = \"blue\", lwd = 2)\nrect(x_positions[3] - bar_width / 2, red_heights[3], x_positions[3] + bar_width / 2, red_heights[3] + blue_heights[3], col = \"#ADD8E6\", border = \"blue\", lwd = 2)\n\n# Draw the red bars\nrect(x_positions[3] - bar_width / 2, 0, x_positions[3] + bar_width / 2, red_heights[3], col = \"#FFCCCB\", border = \"red\", lwd = 2)\n\n# Add annotations\ntext(x_positions[2], red_heights[2] - 0.2, \"?\", col = \"red\", cex = 1.5)\ntext(x_positions[3], red_heights[3] - 0.2, \"?\", col = \"red\", cex = 1.5)\n\n# Add category labels\ntext(x_positions[1], par(\"usr\")[4] + 0.05, labels = bquote(Y[1]), xpd = TRUE)\ntext(x_positions[2], par(\"usr\")[4] + 0.05, labels = bquote(Y[2]), xpd = TRUE)\ntext(x_positions[3], par(\"usr\")[4] + 0.05, labels = bquote(Y[3]), xpd = TRUE)\n\n\nx_positions &lt;- c(2.5, 3, 3.5)\nbar_width &lt;- 0.5\n\n# Draw the M columns\nrect(x_positions[1] - bar_width / 2, 0, x_positions[1] + bar_width / 2, 1, col = \"white\", border = \"blue\", lwd = 2)\nrect(x_positions[2] - bar_width / 2, 0, x_positions[2] + bar_width / 2, 1, col = \"white\", border = \"blue\", lwd = 2)\nrect(x_positions[3] - bar_width / 2, 0, x_positions[3] + bar_width / 2, 1, col = \"white\", border = \"blue\", lwd = 2)\ndf &lt;- data.frame(y = seq(0.05,0.95, length.out = 10),\n                 m1 = rep(0,10),\n                 m2 = rep(0,10),\n                 m3 = c(rep(1,6),rep(0,4)))\ntext(rep(x_positions[1],10), df$y, label = df$m1)\ntext(rep(x_positions[2],10), df$y, label = df$m2)\ntext(rep(x_positions[3],6), df$y[1:6], label = df$m3[1:6], col = \"red\")\ntext(rep(x_positions[3],4), df$y[7:10], label = df$m3[7:10])\n\n# Add category labels\ntext(x_positions[1], par(\"usr\")[4] + 0.05, labels = bquote(M[1]), xpd = TRUE)\ntext(x_positions[2], par(\"usr\")[4] + 0.05, labels = bquote(M[2]), xpd = TRUE)\ntext(x_positions[3], par(\"usr\")[4] + 0.05, labels = bquote(M[3]), xpd = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nExample for this sort of missingness would be unit nonresponse on a survey; \\(Y_1, Y_2\\) would be design variables that are known for all potential survey respondenents, while \\(Y_3\\) would be measurement of interest"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#does-missingness-matter",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#does-missingness-matter",
    "title": "Lecture-1",
    "section": "Does missingness matter?",
    "text": "Does missingness matter?\n\n\nThe book’s definition of missing data implies that knowing the missing value would meaningfully change our inferences\nWe can bound this"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html",
    "title": "Missing data lecture 1",
    "section": "",
    "text": "At first glance the title of our textbook, Statistical Analysis with Missing Data, seems redundant. What is statistics but the study of drawing conclusions from limited data? One of the most basic applications of statistics is about how to make inferences about a population quantity from a simple random sample of that population. The measurements from those who were not sampled are, by definition, missing. Because we have a simple random sample from our population, however, we can be sure that the sample mean of the measurements from the sampled units will be an unbiased estimator of the population-level mean. What if some of the sampled units refuse to participate in the survey? Suppose the survey asks about income, and some respondents refuse to report income?\nOur definition of missing data for this course will condition on the sample drawn; in other words, we will focus on values that haven’t been recorded for the sample of data we observe. Suppose we have a sample of \\(n\\) units, on which we have \\(K\\) measurements, collected into a \\(n \\times K\\) matrix \\(Y\\) with elements \\(y_{ij}\\).\nPaired with this matrix of measurements is another \\(n \\times K\\) matrix \\(M\\) with elements \\(m_{ij}\\), called the missingness indicator matrix. This matrix encodes the information about whether the \\((i,j)^\\mathrm{th}\\) element of \\(Y\\) is missing or not. Let \\(m_{ij} = 1\\) if \\(y_{ij}\\) is missing, and \\(0\\) if \\(y_{ij}\\) is observed.\nThroughout the course we’ll keep in mind that we’re never looking to explicitly fill in the missing values with a single ``best” value. Instead, we’re going to consider the distribution of possible values that could be filled in and look at how our estimates change for each filled-in dataset.\n\n\nThe textbook defines missing data roughly as missing values that would be meaningful for your analysis if it had been observed (Roderick JA Little and Rubin 2019). The word ``meaningful” is doing a lot of work here; we’ll need to define meaningful for ourselves. Conceptually, we need to define why data is missing in the first place. For example, let’s say we’re analyzing the results from a longitudinal trial comparing Infliximab for severe Crohn’s disease, which is an autoinflammatory disease and a category of inflammatory bowel disease (IBD) (note this is not IBS, or irritable bowel syndrome, which is fairly common), to a new treatment. The investigators are interested comparing Crohn’s Disease Activity Index (CDAI) between the two arms, which is a measure of the severity of symptoms in Crohn’s patients. Imagine two scenarios: one in which an enrolled patient subsequently drops out of the study after several infusions of the new treatment, and one in which an enrolled patient dies prior to the end of the study. In the first scenario, it makes sense to consider that patient’s measure of CDAI to be missing, whereas in the second scenario, it doesn’t make much sense to think about imputing a CDAI for someone who has died.\nLet’s say we’re in the first scenario, and we’re confronted with some proportion of patients that have dropped out of the study. Dropout is common in longitudinal studies; in one dataset we’ll encounter later investigating the efficacy of a treatment for schizophrenia, about 37% of patients dropped out by the end of the study (Van Der Elst et al. 2024). The default way to deal with missing data in R is to use na.omit. This is also known as complete case analysis (CC analysis). How much would just using the complete cases impact our inferences?\nBeing statisticians, we’ll focus on the bias and variance of our estimates. Let’s make things more concrete. Suppose in our Crohn’s trial the outcome \\(Y_{i}\\) is the change from baseline CDAI to CDAI at the final visit. Assume that all participants have an initial CDAI, so \\(M_i\\) is \\(1\\) if the individual dropped out prior the final visit. As for bias, one can show (read: you’ll show on HW 1) that the following relationship holds: \\[\n\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y} = \\frac{\\text{Cov}(Y, \\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] We can bound the magnitude of this expression by using Cauchy-Schwarz:\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\frac{\\text{SD}(Y) \\text{SD}(\\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] which simplifies to \\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}}\\text{SD}(Y)\n\\] This makes sense; for a given proportion of missing values, the larger the variance of \\(Y\\) the larger the potential bias will be by excluding some of them.\nThis standard deviation is of course not estimable because we don’t have the missing values of \\(Y\\), but for variables with bounded support \\(Y \\in [a, b]\\), we can get a further upper bound on the standard deviation using Popoviciu’s inequality: \\[\n\\text{Var}(Y) \\leq \\frac{(b - a)^2}{4}\n\\] In the next code cell, I’ve written an expression for the upper bound of CDAI.\n\nupper_bound_CDAI &lt;- \n  7 * 20 * 2 +  \n  1 * 30 +\n  7 * 3 * 5 +\n  7 * 4 * 7 +\n  6 * 20 +\n  5 * 10 +\n  (42 - 3) * 6 +\n  50\n\nThe upper bound is approximately 1100 (see (Best 2006) for more details, the only value that may not have a hard upper bound is the number of stools per day, which I’ve set to \\(20\\) above, but may be higher)\nThis is useful in our hypothetical example because the CDAI scale runs from \\([0,1100]\\)\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}} \\times 550\n\\]\n\n\n\n\n\n\n\n\n\nWhile these are worst-case bounds, this shows that even small proportions of missing values can impact inferences if the missingness is correlated with the outcome value.\nTo give a sense of how large a proportion of patient might have missing values, a recent clinical trial evaluating Skyrizi, a popular treatment for Crohn’s, had three treatment groups (1 placebo and two active treatment arms), of which 12%, 8% and 15% had missing values for CDAI at week 52.\nThere is also the variance to consider. Even if the covariance between the missing values is zero, we will lose efficiency by dropping observations that have missing values. In the case where our estimator is a sample mean, and there are \\(n\\) units with \\(m\\) missing values, the variance of the CC estimator will be larger by \\(1 + \\frac{m}{n - m}\\).\nThus, in many cases, even if there are only small proportions of missing values, it can make sense to use partial information from incomplete cases to improve our estimators."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#does-missingness-matter",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#does-missingness-matter",
    "title": "Missing data lecture 1",
    "section": "",
    "text": "The textbook defines missing data roughly as missing values that would be meaningful for your analysis if it had been observed (Roderick JA Little and Rubin 2019). The word ``meaningful” is doing a lot of work here; we’ll need to define meaningful for ourselves. Conceptually, we need to define why data is missing in the first place. For example, let’s say we’re analyzing the results from a longitudinal trial comparing Infliximab for severe Crohn’s disease, which is an autoinflammatory disease and a category of inflammatory bowel disease (IBD) (note this is not IBS, or irritable bowel syndrome, which is fairly common), to a new treatment. The investigators are interested comparing Crohn’s Disease Activity Index (CDAI) between the two arms, which is a measure of the severity of symptoms in Crohn’s patients. Imagine two scenarios: one in which an enrolled patient subsequently drops out of the study after several infusions of the new treatment, and one in which an enrolled patient dies prior to the end of the study. In the first scenario, it makes sense to consider that patient’s measure of CDAI to be missing, whereas in the second scenario, it doesn’t make much sense to think about imputing a CDAI for someone who has died.\nLet’s say we’re in the first scenario, and we’re confronted with some proportion of patients that have dropped out of the study. Dropout is common in longitudinal studies; in one dataset we’ll encounter later investigating the efficacy of a treatment for schizophrenia, about 37% of patients dropped out by the end of the study (Van Der Elst et al. 2024). The default way to deal with missing data in R is to use na.omit. This is also known as complete case analysis (CC analysis). How much would just using the complete cases impact our inferences?\nBeing statisticians, we’ll focus on the bias and variance of our estimates. Let’s make things more concrete. Suppose in our Crohn’s trial the outcome \\(Y_{i}\\) is the change from baseline CDAI to CDAI at the final visit. Assume that all participants have an initial CDAI, so \\(M_i\\) is \\(1\\) if the individual dropped out prior the final visit. As for bias, one can show (read: you’ll show on HW 1) that the following relationship holds: \\[\n\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y} = \\frac{\\text{Cov}(Y, \\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] We can bound the magnitude of this expression by using Cauchy-Schwarz:\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\frac{\\text{SD}(Y) \\text{SD}(\\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] which simplifies to \\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}}\\text{SD}(Y)\n\\] This makes sense; for a given proportion of missing values, the larger the variance of \\(Y\\) the larger the potential bias will be by excluding some of them.\nThis standard deviation is of course not estimable because we don’t have the missing values of \\(Y\\), but for variables with bounded support \\(Y \\in [a, b]\\), we can get a further upper bound on the standard deviation using Popoviciu’s inequality: \\[\n\\text{Var}(Y) \\leq \\frac{(b - a)^2}{4}\n\\] In the next code cell, I’ve written an expression for the upper bound of CDAI.\n\nupper_bound_CDAI &lt;- \n  7 * 20 * 2 +  \n  1 * 30 +\n  7 * 3 * 5 +\n  7 * 4 * 7 +\n  6 * 20 +\n  5 * 10 +\n  (42 - 3) * 6 +\n  50\n\nThe upper bound is approximately 1100 (see (Best 2006) for more details, the only value that may not have a hard upper bound is the number of stools per day, which I’ve set to \\(20\\) above, but may be higher)\nThis is useful in our hypothetical example because the CDAI scale runs from \\([0,1100]\\)\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}} \\times 550\n\\]\n\n\n\n\n\n\n\n\n\nWhile these are worst-case bounds, this shows that even small proportions of missing values can impact inferences if the missingness is correlated with the outcome value.\nTo give a sense of how large a proportion of patient might have missing values, a recent clinical trial evaluating Skyrizi, a popular treatment for Crohn’s, had three treatment groups (1 placebo and two active treatment arms), of which 12%, 8% and 15% had missing values for CDAI at week 52.\nThere is also the variance to consider. Even if the covariance between the missing values is zero, we will lose efficiency by dropping observations that have missing values. In the case where our estimator is a sample mean, and there are \\(n\\) units with \\(m\\) missing values, the variance of the CC estimator will be larger by \\(1 + \\frac{m}{n - m}\\).\nThus, in many cases, even if there are only small proportions of missing values, it can make sense to use partial information from incomplete cases to improve our estimators."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-patterns",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-patterns",
    "title": "Missing data lecture 1",
    "section": "Missingness patterns",
    "text": "Missingness patterns\nConsider three variables, \\(Y_1, Y_2, Y_3\\) that we’ve measured on a sample of \\(n\\) participants. Each variable has an associated binary vector: \\(M_1, M_2, M_3\\). Missingness patterns refer to the observed sample space for the vectors \\([m_{i1}, m_{i2}, m_{i3}]\\). The simplest missingness pattern is where only one of the variables is subject to missingness: \\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,0,1]\\}.\n\\] This is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Univariate missingness pattern (Credit goes in part to ChatGPT who wrote the initial version of this plot)\n\n\n\n\n\nWe could also have multivariate missingness with only two missingness patterns \\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,1,1]\\}\n\\] which is shown in Figure 2:\n\n\n\n\n\n\n\n\nFigure 2: Multivariate missingness pattern\n\n\n\n\n\nWe could have something called monotone missingness, where we can order the missingness matrix such that if \\(M_{i2} = 1\\) then so is \\(M_{i3} = 1\\): \\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,1,1], [0,0,1]\\}\n\\]\n\n\n\n\n\n\n\n\nFigure 3: Monotone missingness pattern\n\n\n\n\n\nThe least restricted missingness pattern is called a general pattern. This would be a case where there is no special structure. Of course, for \\(p\\) variables each subject to missingness, the general missingness has a sample space of size \\(2^p\\)\nIf we consider that only \\(Y_2, Y_3\\) are subject to missingness, then we have the following, depicted in :\n\\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,1,0], [0,0,1], [0,1,1]\\}\n\\]\n\n\n\n\n\n\n\n\nFigure 4: General missingness pattern for two variables\n\n\n\n\n\nNot surprisingly, the general missingness pattern is the most realistic. You might imagine these patterns occurring during a survey. The pattern \\([0,1,1]\\) represents unit nonresponse (where a person who is contacted declines to participate in the survey), while \\([0,1,0], [0,0,1]\\) would be item nonresponse.\nThe reason that categorizing patterns of missingness is useful is because it can suggest different methods for dealing with missing data. It’s also something that is observable; missing values are not observable, but the patterns are. Thus, in the survey unit and item nonresponse, we might consider different strategies for dealing with unit nonresponse and item nonresponse."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-mechanisms",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-mechanisms",
    "title": "Missing data lecture 1",
    "section": "Missingness mechanisms",
    "text": "Missingness mechanisms\nThe most important paper in missing data was published by Don Rubin in 1976 Rubin (1976). Somewhat surprisingly to me, this paper was rejected by many stats journals. Rod Little says that he was assigned to review the paper as a graduate student when it was submitted to Biometrika, and he was convinced the paper was wrong after writing a long review. Luckily Little was overridden by his advisor, David Cox, who thought the paper was right, and decided to accept the paper.\nThe paper was important because it formalized methods of modeling missingness indicators, or the \\(M\\) matrix from above. Prior to this paper, the \\(M\\) matrix was not considered an outcome that could be modeled. Rubin’s paper instead categorized \\(M\\) as a random variable, and determined how the conditional distribution \\(M \\mid Y\\) impacted inferences using only the observed values of \\(Y\\).\nThe various ways in which \\(M\\) can depend on \\(Y\\) is really an investigation of why a value is missing. Is a survey respondent unwilling to report their income because is high? Did the patient drop out of the study because of side-effects of a drug, or because the drug exacerbated their condition? Did a database error lead to the random dropping of records?\nThe crux of missing data analysis hinges in what we’re willing to believe about why data are missing. These beliefs aren’t typically testable, unless we have designed our study to have missingness1.\nAgain, let \\(M\\) be the matrix with \\((i,j)^\\text{th}\\) entry \\(M_{ij}\\). Further, let \\(M_i\\) be the \\(i^\\text{th}\\) row of \\(M\\), let \\(\\tilde{m}_i\\) be a particular realization of \\(M_i\\) and let \\(m_i\\) be a dummy value. Let \\(Y\\) be the \\(n \\times K\\) matrix of measurements of interest (possibly including covariates), with elements \\(Y_{ij}\\). Let \\(Y_i\\) be the \\(i^\\mathrm{th}\\) row of matrix \\(Y\\), while \\(\\tilde{y}_i\\) is a particular realization of \\(Y_i\\) and \\(y_i\\) is a dummy value. We assume for simplicity (and for much of the book) that \\((M_i, Y_i)\\) are independent between rows.\nTo put a finer point on it, there are generally three categories of missingness mechanisms. They each relate to the distribution: \\[\nf_{M_i \\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi),\n\\] where \\(\\phi\\) are the parameters that govern the missingness mechanism.\nIt will be useful in the next subsections to define the following partitions of \\(Y_i\\): Let \\[Y_{(0)i} = (Y_{ij} : M_{ij} = 0)\\] be the vector of components of \\(Y_i\\) that are observed for unit \\(i\\) and let \\[Y_{(1)i} = (Y_{ij} : M_{ij} = 1)\\] denote the vector of \\(Y_i\\) components that are missing for \\(Y_i\\). The corresponding values, \\(\\tilde{y}_{(0)i}, \\tilde{y}_{(1)i}\\). For example: \\[\\tilde{y}_{(0)i} = (\\tilde{y}_{ij} : \\tilde{m}_{ij} = 0)\\] are particular realizations of these variables, while \\(y_{(0)i}, y_{(1)i}\\) will be dummy values. For example: \\[y_{(0)i} = (y_{ij} : m_{ij} = 0)\\]\nWe can see that \\(Y_{(0)i}, Y_{(1)i}, \\tilde{y}_{(0)i}, y_{(1)i}, y_{(0)i}, y_{(1)i}\\) depend on the missingness indicator variables, \\(M_i, \\tilde{m}_i, m_i\\), respectively.\n\nMissing completely at random (MCAR)\nThe simplest mechanism is called missing-completely-at-random (MCAR). This is where the missingness is unrelated to the outcome. Data are said to MCAR if the following holds for all \\(i\\), \\(y_i\\), \\(y^\\star_i\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi) = f_{M\\mid Y}(M_i = \\tilde{m}_i \\mid Y_i = y^*_i, \\phi).\n\\] An imporant clarification is that this NOT a conditional independence assumption. It is an assumption about the evaluation of the conditional mass function \\(f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi)\\) at a specific \\(m_i\\) (Mealli and Rubin 2015). This point is often misunderstood (including by me).\nConditional independence, \\(M_i \\indy Y_i\\) would be characterized as missing-always-completely-at-random (MACAR): Data are said to MACAR if the following holds for all \\(i\\), \\(m_i\\), \\(y_i\\), \\(y^\\star_i\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = m_i \\mid Y_i = y_i, \\phi) = f_{M\\mid Y}(M_i = m_i \\mid Y_i = y^*_i, \\phi).\n\\] The next mechanism is less restrictive than MCAR.\n\n\nMissing at random (MAR)\nMissing at random data are characterized by the following equality for all \\(i\\), \\(y_{(1)i}\\), \\(y^*_{(1)i}\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i} \\phi) = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i} \\phi)  \n\\] Again, as with MCAR, this is a statement about the evaluation of the function \\(f_{M\\mid Y}(m_i \\mid y_i, \\phi)\\). We can define a missing-at-random variant, missing-always-at-random (MAAR) that is equivalent to \\(M \\indy Y_{(1)i} \\mid Y_{(0)i}\\)2.\nThe following example is adapted from Mealli and Rubin (2015): Suppose we’re analyzing data from that Crohn’s disease trial and \\(Y_i\\) has two components: \\(Y_{i1}\\) is CDAI at visit 1 and \\(Y_{i2}\\) is CDAI at visit 2. For patient \\(i\\) suppose that \\(m_i = (0, 1)\\), or that \\(Y_{i2}\\) is missing but \\(Y_{i1}\\) is observed.\nBringing this example into line with our previous notation, \\(Y_{(0)i} \\equiv Y_{i1}\\) and \\(Y_{(1)i} \\equiv Y_{i2}\\). Then \\(\\tilde{y}_{(0)i} \\equiv \\tilde{y}_{i1}\\).\nConsider two scenarios:\n\n\\(Y_{i2}\\) is missing because \\(Y_{i1} &gt; \\phi\\). Given that \\(0^0 = 1\\), in this scenario the missingness mechanism can be translated as: \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{\\tilde{y}_{(0)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{\\tilde{y}_{(0)i} &gt; \\phi})^{1 - m_{i2}}\n\\]\n\\(Y_{i2}\\) is missing because \\(Y_{i2} &gt; \\phi\\). This mechanism is translated as \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{y_{(1)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{y_{(1)i} &gt; \\phi})^{1 - m_{i2}}\n\\] In scenario 1 the data are MAR because the mass function is a function of \\(\\tilde{y}_{(0)i}\\) only, (i.e it depends only on \\(Y_{i1}\\)), while in scenario 2 the data do not satisfy the definition of MAR."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#footnotes",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#footnotes",
    "title": "Missing data lecture 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne way that can happen is in a univariate missingness setting where \\(Y_3\\) respresents a hard-to-measure quantity (say number of REM cycles per night) and \\(Y_1, Y_2\\) are proxies for this quantity. If we randomly select a subset of our participants in which to measure \\(Y_3\\) then we know that missingness \\(M\\) is not related to \\(Y\\) (assuming that none of our selected participants refuse to participate!).↩︎\nMissing always at random (MAAR) Missing always at random data are characterized by the following equality for all \\(i\\), \\(m_i\\) \\(y_{(1)i}\\), \\(y^*_{(1)i}\\): \\[\nf_{M_i\\mid Y_i}(M_i = m_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = f_{M_i\\mid Y_i}(M_i = m_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i}, \\phi)  \n\\] This is a more restrictive assumption than MAR alone, though one could see why MAAR might be invoked for asymptotic arguments (Roderick J. Little 2021).↩︎"
  },
  {
    "objectID": "survival-material/lecture-2.html#connection-between-discrete-and-continuous-survival-functions-secdisc-continue-recall-the-definition-of-the-hazard-function",
    "href": "survival-material/lecture-2.html#connection-between-discrete-and-continuous-survival-functions-secdisc-continue-recall-the-definition-of-the-hazard-function",
    "title": "Lecture 2",
    "section": "4.1 Connection between discrete and continuous survival functions {#sec:disc-continue} Recall the definition of the hazard function:",
    "text": "4.1 Connection between discrete and continuous survival functions {#sec:disc-continue} Recall the definition of the hazard function:\n\\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use Equation 1 to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "survival-material/lecture-2.html#sec-disc-continue",
    "href": "survival-material/lecture-2.html#sec-disc-continue",
    "title": "Lecture 2",
    "section": "4.1 Connection between discrete and continuous survival functions",
    "text": "4.1 Connection between discrete and continuous survival functions\nRecall the definition of the hazard function:\n\\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that \\(\\lambda_{X_i}(t) \\,\\Delta t\\) is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\), \\(t_j = t_{j-1} + \\Delta t\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use Equation 1 to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html",
    "title": "Missing data lecture 2",
    "section": "",
    "text": "Notation\nMeaning\nDomain\n\n\n\n\n\\(Y\\)\n\\(n\\times K\\) matrix, collection of measurements of interest\n\\(\\R^{n \\times K}\\)\n\n\n\\(\\tilde{y}\\)\nRealization of \\(Y\\)\n\\(\\R^{n \\times K}\\)\n\n\n\\(Y_{ij}\\)\nElement of \\(Y\\), random variables\n\\(\\R\\)\n\n\n\\(\\tilde{y}_{ij}\\)\nParticular realization of \\(Y_{ij}\\)\n\\(\\R\\)\n\n\n\\(Y_{i}\\)\n\\(i^\\mathrm{th}\\) row of \\(Y\\), random variables\n\\(\\R^K\\)\n\n\n\\(\\tilde{y}_{i}\\)\nParticular realization of \\(Y_{i}\\)\n\\(\\R^K\\)\n\n\n\\(y_{i}\\)\nArbitrary element of \\(\\R^K\\), for use in density functions related to \\(Y_i\\)\n\\(\\R^K\\)\n\n\n\\(M\\)\n\\(n\\times K\\) binary matrix of missingness indicators\n\\(\\{0,1\\}^{n \\times K}\\)\n\n\n\\(\\tilde{m}\\)\nRealization of \\(M\\)\n\\(\\{0,1\\}^{n \\times K}\\)\n\n\n\\(M_{ij}\\)\nElement of \\(M\\), random variable\n\\(\\{0,1\\}\\)\n\n\n\\(M_{i}\\)\n\\(i^\\mathrm{th}\\) row of \\(M\\), random variables\n\\(\\{0,1\\}^{K}\\)\n\n\n\\(\\tilde{m}_{ij}\\)\nRealization of \\(M_{ij}\\)\n\\(\\{0,1\\}\\)\n\n\n\\(\\tilde{m}_{i}\\)\nRealization of \\(M_{i}\\)\n\\(\\{0,1\\}^K\\)\n\n\n\\(m_{i}\\)\nArbitrary element of \\(\\{0,1\\}^K\\) for use in PMFs for \\(M_i\\)\n\\(\\{0,1\\}^K\\)\n\n\n\\(R_i\\)\nNumber of observed values for row \\(i\\) in \\(Y\\), \\(\\sum_{j=1}^K (1 - M_{ij})\\)\n\\(\\mathbb{N}\\)\n\n\n\\(Y_{(0)i}\\)\nObserved elements of \\(Y_i\\)\n\\(\\R^{R_i}\\)\n\n\n\\(\\tilde{y}_{(0)i}\\)\nRealization of \\(Y_{(0)i}\\)\n\\(\\R^{R_i}\\)\n\n\n\\(y_{(0)i}\\)\nArbitrary element of \\(\\R^{R_i}\\)\n\\(\\R^{R_i}\\)\n\n\n\\(Y_{(1)i}\\)\nUnobserved or missing elements of \\(Y_i\\)\n\\(\\R^{K - R_i}\\)\n\n\n\\(\\tilde{y}_{(1)i}\\)\nRealization of \\(Y_{(1)i}\\)\n\\(\\R^{K - R_i}\\)\n\n\n\\(y_{(1)i}, y^\\star_{(1)i}\\)\nArbitrary elements of \\(\\R^{K - R_i}\\)\n\\(\\R^{K - R_i}\\)"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#footnotes",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#footnotes",
    "title": "Missing data lecture 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne exception is regression with missing predictors; scenarios in which the predictors have MNAR missingness that doesn’t depend on the outcome can be analyzed with CCA.↩︎"
  },
  {
    "objectID": "survival-material/lecture-3.html",
    "href": "survival-material/lecture-3.html",
    "title": "Lecture 3",
    "section": "",
    "text": "Now let’s delve into more detail about censoring, and how the likelihood can be built up from the hazard function and the survival function. define censoring as imprecise knowledge about an event time. If we observe a failure or an event exactly, the observation is not censored, but if we know only that an observation occurred within a range of values, we say the observation is censored. Let \\(X_i\\), as usual, be our failure time, which is not completely observed. Instead if:\n\n\\(X_i \\in [C, \\infty)\\), the observation is right censored\n\\(X_i \\in [0, U)\\), the observation is left censored\n\\(X_i \\in [C, U)\\), the observation is interval censored"
  },
  {
    "objectID": "survival-material/lecture-3.html#properties-of-the-hazard-function",
    "href": "survival-material/lecture-3.html#properties-of-the-hazard-function",
    "title": "Lecture 2",
    "section": "",
    "text": "The relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-3.html#sec-disc-continue",
    "href": "survival-material/lecture-3.html#sec-disc-continue",
    "title": "Lecture 2",
    "section": "4.1 Connection between discrete and continuous survival functions",
    "text": "4.1 Connection between discrete and continuous survival functions\nRecall the definition of the hazard function:\n\\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use Equation 1 to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#missingness-mechanisms",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#missingness-mechanisms",
    "title": "Missing data lecture 2",
    "section": "Missingness mechanisms",
    "text": "Missingness mechanisms\nMechanisms relate to the distribution: \\[\nf_{M_i \\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi),\n\\] where \\(\\phi\\) are the parameters that govern the missingness mechanism.\n\nMissing completely at random (MCAR)\nData are said to be MCAR if the following holds for all \\(i\\), \\(y_i\\), \\(y^\\star_i\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi) = f_{M\\mid Y}(M_i = \\tilde{m}_i \\mid Y_i = y^\\star_i, \\phi).\n\\]\n\n\nMissing at random (MAR)\nMissing at random data are characterized by the following equality for all \\(i\\), \\(y_{(1)i}\\), \\(y^\\star_{(1)i}\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i}, \\phi)  \n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#continuing-with-our-example-from-last-lecture",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#continuing-with-our-example-from-last-lecture",
    "title": "Missing data lecture 2",
    "section": "Continuing with our example from last lecture",
    "text": "Continuing with our example from last lecture\nThe following example is adapted from Mealli and Rubin (2015): Suppose we’re analyzing data from that Crohn’s disease trial and \\(Y_i\\) has two components: \\(Y_{i1}\\) is CDAI at visit 1 and \\(Y_{i2}\\) is CDAI at visit 2. For patient \\(i\\) suppose that \\(m_i = (0, 1)\\), or that \\(Y_{i2}\\) is missing but \\(Y_{i1}\\) is observed.\nBringing this example into line with our previous notation, \\(Y_{(0)i} \\equiv Y_{i1}\\) and \\(Y_{(1)i} \\equiv Y_{i2}\\). Then \\(\\tilde{y}_{(0)i} \\equiv \\tilde{y}_{i1}\\).\nConsider two scenarios:\n\n\\(Y_{i2}\\) is missing because \\(Y_{i1} &gt; \\phi\\). Given that \\(0^0 = 1\\), in this scenario the missingness mechanism can be translated as: \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{\\tilde{y}_{(0)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{\\tilde{y}_{(0)i} &gt; \\phi})^{1 - m_{i2}}\n\\]\n\\(Y_{i2}\\) is missing because \\(Y_{i2} &gt; \\phi\\). This mechanism is translated as \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{y_{(1)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{y_{(1)i} &gt; \\phi})^{1 - m_{i2}}\n\\] In scenario 1 the data are MAR because the mass function is a function of \\(\\tilde{y}_{(0)i}\\) only, (i.e it depends only on \\(Y_{i1}\\)), while in scenario 2 the data do not satisfy the definition of MAR.\n\n\nGeneralizing the example\nLet’s make this example more general. The following is from Little and Rubin (2019, 23). Again consider the bivariate case with \\(y_{i1}, y_{i2}\\). There are 4 possible missing data patterns: \\[\n(m_{i1}, m_{i2}) \\in \\{(0,0), (0,1), (1,0), (1,1)\\}\n\\] We’ll need to define \\(f_{M \\mid Y}(m_{i1} = r, m_{i2} = s \\mid y_{i1}, y_{i2}, \\phi)\\). To simplify the notation, let \\[\ng_{rs}(y_{i1}, y_{i2}, \\phi) = f_{M \\mid Y}(m_{i1} = r, m_{i2} = s \\mid y_{i1}, y_{i2}, \\phi)\n\\] The MAR assumption implies the following: \\[\n\\begin{aligned}\ng_{11}(y_{i1}, y_{i2}, \\phi) & = g_{11}(\\phi) \\\\\ng_{01}(y_{i1}, y_{i2}, \\phi) & = g_{01}(y_{i1}, \\phi) \\\\\ng_{10}(y_{i1}, y_{i2}, \\phi) & = g_{10}(y_{i2}, \\phi) \\\\\ng_{00}(y_{i1}, y_{i2}, \\phi) & = 1 - g_{10}(y_{i2}, \\phi)  - g_{01}(y_{i1}, \\phi) -  g_{11}(\\phi)\n\\end{aligned}\n\\] Thus the probability that \\(y_{ij}\\) is missing can depend only on \\(y_{i(-j)}\\), which is a bit odd.\nLittle and Rubin (2019) proposes the following modification:\n\\[\n\\begin{aligned}\ng_{11}(y_{i1}, y_{i2}, \\phi) & = g_{1+}(y_{i1}, \\phi)g_{+1}(y_{i2}, \\phi) \\\\\ng_{01}(y_{i1}, y_{i2}, \\phi) & = (1 - g_{1+}(y_{i1}, \\phi))g_{+1}(y_{i2}, \\phi) \\\\\ng_{10}(y_{i1}, y_{i2}, \\phi) & = g_{1+}(y_{i1}, \\phi)(1 - g_{+1}(y_{i2}, \\phi)) \\\\\ng_{00}(y_{i1}, y_{i2}, \\phi) & = (1 - g_{1+}(y_{i1}, \\phi))(1 - g_{+1}(y_{i2}, \\phi))\n\\end{aligned}\n\\] While this is maybe more realistic, though it does make an assumption that \\(m_{i1}\\) and \\(m_{i2}\\) are conditionally independent given \\(y_{i1}, y_{i2}\\), it is also hard to estimate, because we won’t observe missing values of \\(y_{i1}\\) and \\(y_{i2}\\).\nThis is a scenario called missing-not-at-random, or MNAR. This is defined in the next subsection."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#univariate-mar-data",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#univariate-mar-data",
    "title": "Missing data lecture 2",
    "section": "Univariate MAR data",
    "text": "Univariate MAR data\nUnivariate MAR data is useful to consider because it elucidates a key assumption that can be used in MAR analyses.\nSuppose we have a single measurement on \\(n\\) individuals, \\(Y_i\\) and \\(r\\) of the individuals in our sample have observations \\(\\tilde{y}_i\\) while \\(n-r\\) individuals have missing values for \\(Y_i\\) (i.e. \\(\\tilde{y}_{(1)i} = \\tilde{y}_{i}\\), \\(\\tilde{y}_{(0)i} = \\emptyset\\)). Suppose we also have iid measurements and an iid missingness process.\nWriting out the missingness mechanism for this scenario gives us: \\[\n\\begin{aligned}\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) & = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i}, \\phi) \\\\\n& = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid \\phi)  \n\\end{aligned}\n\\] This implies something about the relationship between the units the have observed data and those that do not: \\[\n\\begin{aligned}\nf_{Y_i \\mid M_i}(Y_{i} = y_{i} \\mid M_i = \\tilde{m}_i, \\theta) & = \\frac{f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{i} = y_i, \\phi) f_{Y_i}(Y_i = y_i \\mid \\theta)}{f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid \\phi)} \\\\\n& = f_{Y_i}(Y_i = y_i \\mid \\theta)\n\\end{aligned}\n\\] This implies that \\[\n\\begin{aligned}\nf_{Y_i \\mid M_i}(Y_{i} = y_{i} \\mid M_i = 1, \\theta) & = f_{Y_i \\mid M_i}(Y_{i} = y_{i} \\mid M_i = 0, \\theta)\n\\end{aligned}\n\\] Thus, we can use the distribution we learn from the complete cases as the distribution for the cases that are missing. This idea holds for more general missingness patterns with more measurements. We’ll see this later in the course."
  },
  {
    "objectID": "survival-material/lecture-3.html#type-i-censoring",
    "href": "survival-material/lecture-3.html#type-i-censoring",
    "title": "Lecture 3",
    "section": "2.1 Type I censoring",
    "text": "2.1 Type I censoring\nThe simplest censoring scenario is one in which all individuals have the same, nonrandom censoring time. Imagine a study is designed to follow \\(5\\) startups that are spun out of a tech incubator to study how long it takes a company to land its first contract. This information will be used for designing investments \\(2\\) years from the study date, so the study has a length of \\(1.75\\) years. We can say that all observations will have to have occurred, or not, by \\(1.75\\) years.\nFigure 1 shows a potential result of the study, where \\(2\\) out of the \\(5\\) companies have not landed a contract.\n\n\n\n\n\n\n\n\nFigure 1: Type I censoring\n\n\n\n\n\nIn this case, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3.html#generalized-type-i-censoring",
    "href": "survival-material/lecture-3.html#generalized-type-i-censoring",
    "title": "Lecture 3",
    "section": "2.2 Generalized type I censoring",
    "text": "2.2 Generalized type I censoring\nA more general scenario, which is closer to most examples in clinical trials, is when each individual has a different study entry time and the investigator has a preset study end time. This is called generalized Type I censoring. These study entry times are typically assumed to be independent of the survival time. This is shown in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Example of generalized Type I censoring, where each individual has a separate study entry time.\n\n\n\n\n\nWhen study entry is independent from survival time, the analysis proceeds as shown in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Example of generalized Type I censoring, viewed in patient time.\n\n\n\n\n\nFor generalized type I censoring, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\).\n\nThis is different from Type I censoring in that each individual has a different censoring time."
  },
  {
    "objectID": "survival-material/lecture-3.html#type-ii-censoring",
    "href": "survival-material/lecture-3.html#type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.3 Type II censoring",
    "text": "2.3 Type II censoring\nType II censoring occurs when all units have the same study entry time, but researchers design the study to end when \\(r &lt; n\\) units fail out of \\(n\\) total units under observation.\n\nFor the first \\(r\\), lucky or unlucky participants, \\(\\delta_i = 1 \\implies T_i = X_{(i)}\\) or the \\(i^\\mathrm{th}\\) order statistic.\nFor the remaining \\(n - r\\) individuals, \\(\\delta_i = 0 \\implies X_i &gt; X_{(r)}\\)."
  },
  {
    "objectID": "survival-material/lecture-3.html#generalized-type-ii-censoring",
    "href": "survival-material/lecture-3.html#generalized-type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.4 Generalized Type II censoring",
    "text": "2.4 Generalized Type II censoring\nYou may be wondering, what happens when units have differing start times but we want to end the trial after the \\(r\\)-th failure? It turns out that this was not a solved problem until Rühl, Beyersmann, and Friedrich (2023), which was quite surprising to me."
  },
  {
    "objectID": "survival-material/lecture-3.html#independent-censoring",
    "href": "survival-material/lecture-3.html#independent-censoring",
    "title": "Lecture 3",
    "section": "2.5 Independent censoring",
    "text": "2.5 Independent censoring\nA third type of censoring, helpfully called independent censoring, takes \\(X_i \\indy C_i\\), and thus conclusions similar to those of generalized type I censoring can be drawn:\n\nFor all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3.html#reasons-for-informative-censoring",
    "href": "survival-material/lecture-3.html#reasons-for-informative-censoring",
    "title": "Lecture 3",
    "section": "3.1 Reasons for informative censoring",
    "text": "3.1 Reasons for informative censoring\nA simple hypothetical situation with informative censoring would be one in which sick patients are lost to follow-up."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#bias-of-cca",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#bias-of-cca",
    "title": "Missing data lecture 2",
    "section": "Bias of CCA",
    "text": "Bias of CCA\nWe showed earlier that the bias of the complete case analysis was dependent on the covariance between the missingness indicator and the response; another way to decompose the bias is via the following representation of the sample mean. Recall that \\(\\mathcal{R} = \\{i \\mid \\sum_{j=1}^K \\tilde{m}_{ij} = 0, i = 1, \\dots, n\\}\\); we can define the complement of this set \\(\\mathcal{R}^\\comp\\). Then we can represent the sample mean as: \\[\n\\bar{y} = \\frac{r}{n}\\bar{y}^\\mathrm{CC} + (1 - \\frac{r}{n})\\bar{y}^{\\mathcal{R}^\\comp}\n\\] Note that the sample mean may be unobservable because some units may be missing \\(y_i\\). Then, conditional on the missingness pattern that was observed, the bias of the\n\\[\n\\Exp{\\bar{y}^\\mathrm{CC} - \\bar{y} \\mid M = \\tilde{m}} = \\lp 1 - \\frac{r}{n}\\rp(\\Exp{\\bar{y}^\\mathrm{CC} \\mid M = \\tilde{m}} - \\Exp{\\bar{y}^{\\mathcal{R}^\\comp} \\mid M = \\tilde{m}})\n\\]"
  },
  {
    "objectID": "survival-material/lecture-3.html#likelihood-construction",
    "href": "survival-material/lecture-3.html#likelihood-construction",
    "title": "Lecture 3",
    "section": "4.1 Likelihood construction",
    "text": "4.1 Likelihood construction\nWe now turn to how to construct likelihoods in each of the prior scenarios, under censored or truncated data. As a reminder:\n\nLet \\(X_i\\) be the time to failure, or time to event for individual \\(i\\).\nLet \\(C_i\\) be the time to censoring. It may be helpful to think about \\(C_i\\) as the time to investigator measurement.\nLet \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\).\nLet \\(T_i = \\min(X_i, C_i)\\).\n\nWhen \\(\\delta_i = 1\\), we observe \\(T_i = X_i\\); this is the event that \\(\\{X_i = T_i, C_i \\geq X_i\\}\\). When \\(\\delta_i = 0\\), we observe \\(T_i = C_i\\); this is the event that \\(\\{C_i = T_i, C_i &lt; X_i\\}\\). Let the joint distribution of \\(X_i, C_i\\) be written as \\(P_\\theta(X &gt; x, C &gt; c)\\), and further let \\(\\theta = (\\eta, \\phi)\\) such that \\(P_\\theta(X &gt; x, C &gt; c) = P_\\eta(X &gt; x) P_\\phi(C &gt; c \\mid X &gt; x)\\). We showed that the likelihood corresponding to the random variables \\(T_i, \\delta_i\\), \\(f_{T_i, \\delta_i}(t, \\delta;\\,\\theta)\\), can be written in terms of partial derivatives of the joint density function when \\(X_i\\) and \\(C_i\\) are absolutely continuous random variables. \\[\\begin{align*}\n  f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq u, C \\geq t) \\mid_{u = t}  \\right)^\\delta \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq t, C \\geq u) \\mid_{u = t}  \\right)^{1 - \\delta}\n\\end{align*}\\] Let’s rewrite the partial derivatives in terms of their limits: \\[\\begin{align*}\n   f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t, C \\geq t) \\right)^\\delta\n    \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(X \\geq t, t \\leq C &lt; t + \\Delta t)   \\right)^{1 - \\delta}\n\\end{align*}\\] We can factorize the distribution function: \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^{1 - \\delta}\n\\end{align*}\\] . Let’s make the following substitutions for brevity’s sake: \\[\n\\begin{aligned}\n\\lambda_1(t; \\theta) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\\\\n\\lambda_2(t; \\theta) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\\\\n\\end{aligned}\n\\] Furthermore, let’s rewrite \\(P_{\\theta}(X \\geq t, C \\geq t)\\) in terms of \\(\\lambda_1(t)\\) and \\(\\lambda_2(t)\\). This will require a bit of work. First, we can write the \\(P_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\) as\n\\[\n\\begin{aligned}\nP_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) & = \\lambda_1(t;\\theta)\\Delta t  + o(\\Delta t)\\\\\nP_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) & = \\lambda_2(t;\\theta)\\Delta t  + o(\\Delta t)\n\\end{aligned}\n\\] We’ll also assume the following: \\[\nP_\\theta(t \\leq X &lt; t + \\Delta t, t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) = o(\\Delta t)\n\\]\nWe’ll work with discretized time steps. Let \\(t_0 = 0\\) and \\(t_n = t\\). Then \\([0, t) = \\bigcup_{j=0}^{n-1} [\\frac{j}{n}, \\frac{j}{n} + \\frac{1}{n})\\) and \\(\\Delta t = \\frac{1}{n}\\).\nThis construction means that we can write the survival function recursively for each \\(t\\): \\[\n\\begin{aligned}\nP_\\theta(X \\geq t + \\Delta t, C \\geq t + \\Delta t) & = P_\\theta(X \\geq t_{n} + \\Delta t, C \\geq t_{n}  + \\Delta t \\mid X \\geq t_{n}, C \\geq t_{n}) P_\\theta(X \\geq t_{n}, C \\geq t_{n}) \\\\\n& =  P_\\theta(X \\geq t_{n} + \\Delta t, C \\geq t_{n}  + \\Delta t \\mid X \\geq t_{n}, C \\geq t_{n}) \\\\\n& \\times P_\\theta(X \\geq t_{n-1} + \\Delta t, C \\geq t_{n-1} + \\Delta t \\mid X \\geq t_{n-1}, C \\geq t_{n-1}) P_\\theta(X \\geq t_{n-1}, C \\geq t_{n-1}) \\\\\n& \\vdots \\\\\n& = \\prod_{i=0}^{n-1} P_\\theta(X \\geq t_{i} + \\Delta t, C \\geq t_{i} + \\Delta t \\mid X \\geq t_{i}, C \\geq t_{i}) P_\\theta(X \\geq 0, C \\geq 0) \\\\\n& = \\prod_{i=0}^{n-1} P_\\theta(X \\geq t_{i} + \\Delta t, C \\geq t_{i} + \\Delta t \\mid X \\geq t_{i}, C \\geq t_{i})\n\\end{aligned}\n\\]\nLet’s write the conditional survival function, \\(P_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i)\\), or the probability that neither \\(A_i \\equiv \\{t_i \\leq X &lt; t_i \\Delta t\\}\\) nor \\(B_i \\equiv \\{t \\leq C &lt; t_i \\Delta t\\}\\) occur. In set notation, this is: \\[\nA_i\\comp \\cap B_i\\comp = (A_i \\cup B_i)\\comp\n\\]\n\\[\n\\begin{aligned}\nP_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i) & =  P_\\theta((A_i \\cup B_i)\\comp \\mid X \\geq t_i, C \\geq t_i) \\\\\n& = 1 - P_\\theta(A_i \\cup B_i \\mid X \\geq t_i, C \\geq t_i) \\\\\n& = 1 - (P_\\theta(A_i \\mid X \\geq t_i, C \\geq t_i) + P_\\theta(B_i \\mid X \\geq t_i, C \\geq t_i) \\\\\n& \\quad - P_\\theta(A_i \\cap B_i \\mid X \\geq t_i, C \\geq t_i))\n\\end{aligned}\n\\] Recall that \\[\n\\begin{aligned}\nP_\\theta(A_i \\cap B_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq X &lt; t_i + \\Delta t, t_i \\leq C &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = o(\\Delta t)\n\\end{aligned}\n\\] and \\[\n\\begin{aligned}\nP_\\theta(A_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq X &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = \\lambda_1(t_i; \\theta) \\Delta t + o(\\Delta t) \\\\\nP_\\theta(B_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq C &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = \\lambda_2(t_i; \\theta) \\Delta t + o(\\Delta t)\n\\end{aligned}\n\\]\nPutting this into the expression above, we get: \\[\nP_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i) = 1 - \\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t)\n\\] Finally: \\[\nP_\\theta(X \\geq t + \\Delta t, C \\geq t + \\Delta t) = \\prod_{i=0}^{n-1} \\lp 1 - \\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t) \\rp\n\\] We take logs of both sides to get: \\[\n\\log P_\\theta(X \\geq t_j + \\Delta t, C \\geq t_j + \\Delta t) = \\sum_{i=0}^n \\log \\lp 1 - \\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t) \\rp\n\\] Suppose that \\(\\lambda_1(t), \\lambda_2(t)\\) are bounded. Then we can use the Taylor series for \\(\\log 1 - x\\) as \\(x \\searrow 0\\).: \\[\n\\log(1 - x) = -x + o(x)\n\\]\nThen\n\\[\n\\begin{aligned}\n& \\log(1 - \\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t)) \\\\\n& = -(\\lambda_1(t_i; \\theta) + \\lambda_2(t_i; \\theta))\\Delta t + o(\\Delta t) + o(\\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t)) \\\\\n& = -(\\lambda_1(t_i; \\theta) + \\lambda_2(t_i; \\theta))\\Delta t + o(\\Delta t)\n\\end{aligned}\n\\] Putting this back into our expression gives us:\n\\[\n\\begin{aligned}\n\\log P_\\theta(X \\geq t + \\Delta t, C \\geq t + \\Delta t) = -\\sum_{i=1}^n (\\lambda_1(t_i; \\theta) + \\lambda_2(t_i; \\theta)) \\Delta t  + n o(\\Delta t)\n\\end{aligned}\n\\] Taking \\(n \\to \\infty\\) gives the integral: \\[\n\\begin{aligned}\n\\log P_\\theta(X &gt; t, C &gt; t) = -\\int_0^t (\\lambda_1(u; \\theta) + \\lambda_2(u; \\theta)) du \\\\\nP_\\theta(X &gt; t, C &gt; t) = \\exp \\lp -\\int_0^t (\\lambda_1(u;\\theta) + \\lambda_2(u;\\theta)) du \\rp\n\\end{aligned}\n\\] By noting that \\(\\Delta t = \\frac{1}{n}\\), so for \\(o(\\Delta t)\\), \\(n o(\\Delta t) \\overset{n\\to\\infty}{\\to} 0\\)\nThis is a long way to show that \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^{1 - \\delta} \\\\\n    & = \\lambda_1(t;\\theta)^\\delta \\lambda_2(t;\\theta)^{1-\\delta} e^{-\\int_0^t (\\lambda_1(u;\\theta) + \\lambda_2(u;\\theta)) du}\n\\end{align*} \\]\nAssuming noninformative censoring equates to:\n\\[\n\\begin{align*}\n    \\lambda_1(t;\\theta) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t) \\\\\n    & = \\lambda_X(t;\\eta)\n\\end{align*}.\n\\] and that \\(\\lambda_2(t;\\phi) = \\lambda_2(t;\\phi)\\).\nThis yields: \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\lambda_1(t;\\theta)^\\delta \\lambda_2(t;\\theta)^{1-\\delta} e^{-\\int_0^t (\\lambda_1(u;\\theta) + \\lambda_2(u;\\theta)) du} \\\\\n    & = \\lambda_X(t;\\eta)^\\delta e^{-\\int_0^t \\lambda_X(u;\\eta)du}  \\lambda_2(t;\\phi)^{1-\\delta}e^{-\\int_0^t\\lambda_2(u;\\phi)du}\n\\end{align*}\n\\]\nThis means that we can factorize the joint density into a piece that depends only on the distribution of \\(X\\) and \\(\\eta\\), and a piece that depends on \\(\\phi\\) and the rate of censoring given survival up to time \\(t\\). Thus, noninformative censoring and parameter separability yield a separable joint density. This means that when we want to do maximum likelihood for survival data, we can ignore the model for the censoring times, \\(f_{C_i \\mid X_i \\geq t, \\delta_i}(t, \\delta;\\,\\phi)\\), and focus on only the model for the failure times: \\[f_{X_i, \\delta_i}(t, \\delta;\\,\\eta) = \\lambda_{X_i}(t;\\eta)^\\delta P_\\eta(X \\geq t).\\]\n\\[\n\\begin{align*}\n    f_{X_i, \\delta_i}(t, \\delta;\\,\\eta) & \\propto \\lambda_X(t;\\eta)^\\delta e^{-\\int_0^t \\lambda_X(u;\\eta)du}\n\\end{align*}\n\\]\n\nMLE for exponential survival time Let \\(X_i \\overset{\\text{iid}}{\\sim} \\text{Exp}(\\alpha)\\) and assume we have independent censoring (\\(X_i \\perp\\!\\!\\!\\perp C_i\\)), the parameters for the censoring process are separable from \\(\\alpha\\), and that \\(C_i\\) are iid such that \\(\\Exp{C_i} &lt; \\infty\\). Then our observed data are \\(T_i = \\min(X_i, C_i)\\) and \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\). According to ?@eq-separable-like we can write the likelihood as \\[\\begin{align*}\nf_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\prod_{i=1}^n \\alpha^{\\delta_i} \\exp(-\\textstyle{\\sum_{i=1}^n \\int_0^{t_i} }\\alpha du)\\\\\n& = \\alpha^{\\sum_{i=1}^n \\delta_i} \\exp(-\\alpha \\textstyle\\sum_{i=1}^n t_i)\n\\end{align*}\\] The log-likelihood is \\[\\log(f_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) ) = \\log(\\alpha)\\sum_{i=1}^n \\delta_i -\\alpha \\sum_{i=1}^n t_i\\] which has the maximizer \\[\\hat{\\alpha} = \\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}.\\] Let’s show that this converges a.s. to \\(\\alpha\\) as \\(n\\to\\infty\\). We can rewrite \\(\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}\\) as \\[\\begin{align*}\n    \\frac{\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right)}{\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}\n\\end{align*}\\] The top and bottom expressions converge a.s. by Kolmogorov’s Strong Law of Large Numbers to \\[\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right) & \\overset{\\text{a.s.}}{\\to} \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} \\\\\n\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right) & \\overset{\\text{a.s.}}{\\to}  \\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)}\n\\end{align*}\\] We can evaluate the top expression using the tower property of expectation: \\[\\begin{align*}\n    \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} & =  \\ExpA{\\ExpA{\\mathbbm{1}\\left(X_i \\leq c\\right)\\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n    & = \\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] where the second line follows from the independent censoring condition. The bottom expression becomes: \\[\\begin{align*}\n\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)} & = \\ExpA{\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& + \\ExpA{\\ExpA{c \\mathbbm{1}\\left(X_i &gt; c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& = \\ExpA{\\frac{1}{\\alpha}(1 - (1 + \\alpha C_i) e^{-\\alpha C_i})}{C_i} + \\ExpA{C_i e^{-\\alpha C_i}}{C_i} \\\\\n& = \\frac{1}{\\alpha}\\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] Thus \\[\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i} \\overset{\\text{a.s.}}{\\to} \\alpha\\] To show that \\(\\int_0^c x \\alpha e^{-\\alpha x} dx = \\frac{1}{\\alpha}(1 - (1 + \\alpha c) e^{-\\alpha c})\\), we can use the trick of differentiating under the integral sign. \\[\\begin{align*}\n  \\alpha \\int_0^c xe^{-\\alpha x} dx & = \\alpha \\int_0^c -\\frac{d}{d \\alpha} e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\int_0^c e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\frac{1}{\\alpha} (1 - e^{-\\alpha c}) \\\\\n  & = \\alpha \\left(\\frac{1 - (1 + \\alpha c) e^{-\\alpha c}}{\\alpha^2}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#simple-estimator",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#simple-estimator",
    "title": "Missing data lecture 2",
    "section": "Simple estimator",
    "text": "Simple estimator\nThis same idea can be used for missing data and is called the inverse probability weighted complete case estimator, or IPWCC for short.\nSuppose that we have a sample of pairs \\((y_i, m_i), i = 1, \\dots, n\\).\nFor the next section, we’ll make the more stringent assumption that there are completely observed covariates, \\(X_i\\) such that the following holds \\(M_i \\indy Y_i \\mid X_i\\) and that we have specified the correct model for \\(M_i\\): \\[\nP(M_i = 0 \\mid X_i = x, \\beta) = \\pi(x,\\beta)\n\\]\n\\[\n\\bar{y}^\\mathrm{IPWCC} = \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i (1 - M_i)}{\\pi(x_i, \\beta)} \\overset{p}{\\to} \\ExpA{\\frac{Y_i (1 - M_i)}{\\pi(X_i, \\beta)}}{(Y_i, M_i, X_i)}\n\\] where the convergence in probability follows from the weak law of large numbers.\n\\[\n\\begin{aligned}\n\\ExpA{\\frac{Y_i (1 - M_i)}{\\pi(X_i, \\beta)}}{(Y_i, M_i, X_i)} & = \\ExpA{\\ExpA{\\frac{Y_i (1 - M_i)}{\\pi(X_i, \\beta)}}{M_i \\mid Y_i, X_i}}{(Y_i, X_i)} \\\\\n& = \\ExpA{\\frac{Y_i}{\\pi(X_i, \\beta)}\\ExpA{ (1 - M_i)}{M_i \\mid Y_i, X_i}}{(Y_i, X_i)} \\\\\n& = \\ExpA{\\frac{Y_i}{\\pi(X_i, \\beta)}\\pi(X_i, \\beta)}{(Y_i, X_i)} \\\\\n& = \\ExpA{Y_i}{(Y_i, X_i)}\n\\end{aligned}\n\\]\nThere are two drawbacks with the IPWCC estimator:\n\nIt requires that \\(\\pi(X_i, \\beta) &gt; 0\\) for all \\(X_i\\) in the population; this may not be a good assumption.\nEven if \\(\\pi(X_i, \\beta) &gt; 0\\) is strictly true, small values of \\(\\pi(X_i, \\beta)\\) may occur (the interpretation is that the \\(i^\\mathrm{th}\\) observation will represent an inordinate number of missing cases because its probability of being missing is so high). If this happens, these estimators can have very large variance\nIn practice, we don’t know \\(\\pi(X_i, \\beta)\\) and typically have to estimate \\(\\beta\\) with \\(\\hat{\\beta}\\). If the sample is small, even if the form \\(\\pi(X_i, \\beta)\\) is right, there may be high variance. If the model for \\(M_i \\mid X_i\\) is wrong, then we have no guarantees that our IPWCC estimator will be unbiased."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#correctly-specified-pix_ibeta",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#correctly-specified-pix_ibeta",
    "title": "Missing data lecture 2",
    "section": "Correctly specified \\(\\pi(x_i,\\beta)\\)",
    "text": "Correctly specified \\(\\pi(x_i,\\beta)\\)\nSuppose that \\(\\pi(X_i, \\beta)\\) is correctly specified, but \\(\\mu(x_i, \\gamma)\\) is misspecified. Then \\(\\hat{\\beta} \\overset{p}{\\to} \\beta^\\dagger\\) but \\(\\hat{\\gamma} \\overset{p}{\\to} \\gamma^\\prime\\) such that \\(\\gamma^\\prime \\neq \\gamma^\\dagger\\). By the WLLN, \\(\\bar{y}^\\mathrm{AIPWCC} \\overset{p}{\\to}\\) the expression: \\[\n\\ExpA{Y_i}{(Y_i, M_i, X_i)} - \\ExpA{\\lp\\frac{(1 - M_i) + \\pi(X_i, \\beta^\\dagger)}{\\pi(X_i, \\beta^\\dagger)}\\rp(Y_i - \\mu(x_i, \\gamma^\\prime))}{(Y_i, M_i, X_i)}\n\\] The second term simplifies: \\[\n\\begin{aligned}\n& \\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\dagger)}{\\pi(X_i, \\beta^\\dagger)}\\rp(Y_i - \\mu(x_i, \\gamma^\\prime))}{(Y_i, M_i, X_i)} \\\\\n& = \\ExpA{\\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\dagger)}{\\pi(X_i, \\beta^\\dagger)}\\rp(Y_i - \\mu(x_i, \\gamma^\\prime))}{M_i \\mid Y_i, X_i}}{(Y_i, X_i)} \\\\\n& = \\ExpA{(Y_i - \\mu(x_i, \\gamma^\\prime))\\frac{1}{\\pi(X_i, \\beta^\\dagger)}\\ExpA{(1 - M_i) - \\pi(X_i, \\beta^\\dagger)}{M_i \\mid Y_i, X_i}}{(Y_i, X_i)} \\\\\n& = 0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#correctly-specified-mux_igamma",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#correctly-specified-mux_igamma",
    "title": "Missing data lecture 2",
    "section": "Correctly specified \\(\\mu(x_i,\\gamma)\\)",
    "text": "Correctly specified \\(\\mu(x_i,\\gamma)\\)\nSuppose that \\(\\mu(X_i, \\gamma)\\) is correctly specified, but \\(\\pi(x_i, \\beta)\\) is misspecified. Then \\(\\hat{\\gamma} \\overset{p}{\\to} \\gamma^\\dagger\\) but \\(\\hat{\\beta} \\overset{p}{\\to} \\beta^\\prime\\) such that \\(\\beta^\\prime \\neq \\beta^\\dagger\\). By the WLLN, \\(\\bar{y}^\\mathrm{AIPWCC} \\overset{p}{\\to}\\) the expression: \\[\n\\ExpA{Y_i}{(Y_i, M_i, X_i)} - \\ExpA{\\lp\\frac{(1 - M_i) + \\pi(X_i, \\beta^\\prime)}{\\pi(X_i, \\beta^\\prime)}\\rp(Y_i - \\mu(x_i, \\gamma^\\dagger))}{(Y_i, M_i, X_i)}\n\\] The second term simplifies: \\[\n\\begin{aligned}\n& \\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\prime)}{\\pi(X_i, \\beta^\\prime)}\\rp(Y_i - \\mu(x_i, \\gamma^\\dagger))}{(Y_i, M_i, X_i)} \\\\\n& = \\ExpA{\\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\prime)}{\\pi(X_i, \\beta^\\prime)}\\rp(Y_i - \\mu(x_i, \\gamma^\\dagger))}{Y_i \\mid M_i, X_i}}{(M_i, X_i)} \\\\\n& = \\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\prime)}{\\pi(X_i, \\beta^\\prime)}\\rp\\ExpA{(Y_i - \\mu(x_i, \\gamma^\\dagger))}{Y_i \\mid M_i, X_i}}{(M_i, X_i)} \\\\\n& = 0\n\\end{aligned}\n\\]\nThis is great if you get one of the forms right. However, if both are wrong, there are no guarantees about the bias. Kang and Schafer (2007) investigates, via simulation study, how poorly AIPWCC estimators perform when both models are wrong. They find that regression imputation estimators, which are what we’ll study in the next section, perform better, both in terms of coverage and MSE, even when the regression equation is misspecified.\nThe key is that the AIPWCC estimators have high variance because of the denominator \\(\\pi(X_i, \\beta)\\)."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html",
    "href": "survival-material/lecture-3-alt.html",
    "title": "Lecture 3",
    "section": "",
    "text": "Now let’s delve into more detail about censoring, and how the likelihood can be built up from the hazard function and the survival function. define censoring as imprecise knowledge about an event time. If we observe a failure or an event exactly, the observation is not censored, but if we know only that an observation occurred within a range of values, we say the observation is censored. Let \\(X_i\\), as usual, be our failure time, which is not completely observed. Instead if:\n\n\\(X_i \\in [C, \\infty)\\), the observation is right censored\n\\(X_i \\in [0, U)\\), the observation is left censored\n\\(X_i \\in [C, U)\\), the observation is interval censored"
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#type-i-censoring",
    "href": "survival-material/lecture-3-alt.html#type-i-censoring",
    "title": "Lecture 3",
    "section": "2.1 Type I censoring",
    "text": "2.1 Type I censoring\nThe simplest censoring scenario is one in which all individuals have the same, nonrandom censoring time. Imagine a study is designed to follow \\(5\\) startups that are spun out of a tech incubator to study how long it takes a company to land its first contract. This information will be used for designing investments \\(2\\) years from the study date, so the study has a length of \\(1.75\\) years. We can say that all observations will have to have occurred, or not, by \\(1.75\\) years.\nFigure 1 shows a potential result of the study, where \\(2\\) out of the \\(5\\) companies have not landed a contract.\n\n\n\n\n\n\n\n\nFigure 1: Type I censoring\n\n\n\n\n\nIn this case, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#generalized-type-i-censoring",
    "href": "survival-material/lecture-3-alt.html#generalized-type-i-censoring",
    "title": "Lecture 3",
    "section": "2.2 Generalized type I censoring",
    "text": "2.2 Generalized type I censoring\nA more general scenario, which is closer to most examples in clinical trials, is when each individual has a different study entry time and the investigator has a preset study end time. This is called generalized Type I censoring. These study entry times are typically assumed to be independent of the survival time. This is shown in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Example of generalized Type I censoring, where each individual has a separate study entry time.\n\n\n\n\n\nWhen study entry is independent from survival time, the analysis proceeds as shown in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Example of generalized Type I censoring, viewed in patient time.\n\n\n\n\n\nFor generalized type I censoring, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\).\n\nThis is different from Type I censoring in that each individual has a different censoring time."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#type-ii-censoring",
    "href": "survival-material/lecture-3-alt.html#type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.3 Type II censoring",
    "text": "2.3 Type II censoring\nType II censoring occurs when all units have the same study entry time, but researchers design the study to end when \\(r &lt; n\\) units fail out of \\(n\\) total units under observation.\n\nFor the first \\(r\\), lucky or unlucky participants, \\(\\delta_i = 1 \\implies T_i = X_{(i)}\\) or the \\(i^\\mathrm{th}\\) order statistic.\nFor the remaining \\(n - r\\) individuals, \\(\\delta_i = 0 \\implies X_i &gt; X_{(r)}\\)."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#generalized-type-ii-censoring",
    "href": "survival-material/lecture-3-alt.html#generalized-type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.4 Generalized Type II censoring",
    "text": "2.4 Generalized Type II censoring\nYou may be wondering, what happens when units have differing start times but we want to end the trial after the \\(r\\)-th failure? It turns out that this was not a solved problem until , which was quite surprising to me."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#independent-censoring",
    "href": "survival-material/lecture-3-alt.html#independent-censoring",
    "title": "Lecture 3",
    "section": "2.5 Independent censoring",
    "text": "2.5 Independent censoring\nA third type of censoring, helpfully called independent censoring, takes \\(X_i \\indy C_i\\), and thus conclusions similar to those of generalized type I censoring can be drawn:\n\nFor all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#reasons-for-informative-censoring",
    "href": "survival-material/lecture-3-alt.html#reasons-for-informative-censoring",
    "title": "Lecture 3",
    "section": "3.1 Reasons for informative censoring",
    "text": "3.1 Reasons for informative censoring\nA simple hypothetical situation with informative censoring would be one in which sick patients are lost to follow-up."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#likelihood-construction",
    "href": "survival-material/lecture-3-alt.html#likelihood-construction",
    "title": "Lecture 3",
    "section": "4.1 Likelihood construction",
    "text": "4.1 Likelihood construction\nWe now turn to how to construct likelihoods in each of the prior scenarios, under censored or truncated data. As a reminder:\n\nLet \\(X_i\\) be the time to failure, or time to event for individual \\(i\\).\nLet \\(C_i\\) be the time to censoring. It may be helpful to think about \\(C_i\\) as the time to investigator measurement.\nLet \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\).\nLet \\(T_i = \\min(X_i, C_i)\\).\n\nWhen \\(\\delta_i = 1\\), we observe \\(T_i = X_i\\); this is the event that \\(\\{X_i = T_i, C_i \\geq X_i\\}\\). When \\(\\delta_i = 0\\), we observe \\(T_i = C_i\\); this is the event that \\(\\{C_i = T_i, C_i &lt; X_i\\}\\). Let the joint distribution of \\(X_i, C_i\\) be written as \\(P_\\theta(X &gt; x, C &gt; c)\\), and further let \\(\\theta = (\\eta, \\phi)\\) such that \\(P_\\theta(X &gt; x, C &gt; c) = P_\\eta(X &gt; x) P_\\phi(C &gt; c \\mid X &gt; x)\\). We showed that the likelihood corresponding to the random variables \\(T_i, \\delta_i\\), \\(f_{T_i, \\delta_i}(t, \\delta;\\,\\theta)\\), can be written in terms of partial derivatives of the joint density function when \\(X_i\\) and \\(C_i\\) are absolutely continuous random variables. \\[\\begin{align*}\n  f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq u, C \\geq t) \\mid_{u = t}  \\right)^\\delta \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq t, C \\geq u) \\mid_{u = t}  \\right)^{1 - \\delta}\n\\end{align*}\\] Let’s rewrite the partial derivatives in terms of their limits: \\[\\begin{align*}\n   f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t, C \\geq t) \\right)^\\delta\n    \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(X \\geq t, t \\leq C &lt; t + \\Delta t)   \\right)^{1 - \\delta}\n\\end{align*}\\] We can factorize the distribution function: \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^{1 - \\delta}\n\\end{align*}\\] . Let’s make the following substitutions for brevity’s sake: \\[\n\\begin{aligned}\n\\lambda_1(t) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\\\\n\\lambda_2(t) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\\\\n\\end{aligned}\n\\] Furthermore, let’s rewrite \\(P_{\\theta}(X \\geq t, C \\geq t)\\) in terms of \\(\\lambda_1(t)\\) and \\(\\lambda_2(t)\\). This will require a bit of work. First, we can write the \\(P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\) as \\[\nP_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) = \\lambda(t)\\Delta t  + o(\\Delta t)\n\\] We’ll also assume the following: \\[\nP_\\theta(t \\leq X &lt; t + \\Delta t, t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) = o(\\Delta t)\n\\]\nWe’ll work with discretized time steps. Let \\(t_0 = 0\\) and \\(t_n = t\\). Then \\([0, t) = \\bigcup_{j=0}^{n-1} [\\frac{j}{n}, \\frac{j}{n} + \\frac{1}{n})\\) and \\(\\Delta t = \\frac{1}{n}\\).\nThis construction means that we can write the survival function recursively for each \\(t_j\\): \\[\n\\begin{aligned}\nP(X \\geq t + \\Delta t, C \\geq t + \\Delta t) & = P(X \\geq t_{n} + \\Delta t, C \\geq t_{n}  + \\Delta t \\mid X \\geq t_{n}, C \\geq t_{n}) P(X \\geq t_{n}, C \\geq t_{n}) \\\\\n& =  P(X \\geq t_{n} + \\Delta t, C \\geq t_{n}  + \\Delta t \\mid X \\geq t_{n}, C \\geq t_{n}) \\\\\n& \\times P(X \\geq t_{n-1} + \\Delta t, C \\geq t_{n-1} + \\Delta t \\mid X \\geq t_{n-1}, C \\geq t_{n-1}) P(X \\geq t_{n-1}, C \\geq t_{n-1}) \\\\\n& \\vdots \\\\\n& = \\prod_{i=0}^{n-1} P(X \\geq t_{i} + \\Delta t, C \\geq t_{i} + \\Delta t \\mid X \\geq t_{i}, C \\geq t_{i}) P(X \\geq 0, C \\geq 0) \\\\\n& = \\prod_{i=0}^{n-1} P(X \\geq t_{i} + \\Delta t, C \\geq t_{i} + \\Delta t \\mid X \\geq t_{i}, C \\geq t_{i})\n\\end{aligned}\n\\]\nLet’s write the conditional survival function, \\(P(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i)\\), or the probability that neither \\(A_i \\equiv \\{t_i \\leq X &lt; t_i \\Delta t\\}\\) nor \\(B_i \\equiv \\{t \\leq C &lt; t_i \\Delta t\\}\\) occur. In set notation, this is: \\[\nA_i\\comp \\cap B_i\\comp = (A_i \\cup B_i)\\comp\n\\]\n\\[\n\\begin{aligned}\nP_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i) & =  P_\\theta((A_i \\cup B_i)\\comp \\mid X \\geq t_i, C \\geq t_i) \\\\\n& = 1 - P_\\theta(A_i \\cup B_i \\mid X \\geq t_i, C \\geq t_i) \\\\\n& = 1 - (P_\\theta(A_i \\mid X \\geq t_i, C \\geq t_i) + P_\\theta(B_i \\mid X \\geq t_i, C \\geq t_i) \\\\\n& \\quad - P_\\theta(A_i \\cap B_i \\mid X \\geq t_i, C \\geq t_i))\n\\end{aligned}\n\\] Recall that \\[\n\\begin{aligned}\nP_\\theta(A_i \\cap B_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq X &lt; t_i + \\Delta t, t_i \\leq C &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = o(\\Delta t)\n\\end{aligned}\n\\] and \\[\n\\begin{aligned}\nP_\\theta(A_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq X &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = \\lambda_1(t_i) \\Delta t + o(\\Delta t) \\\\\nP_\\theta(B_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq C &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = \\lambda_2(t_i) \\Delta t + o(\\Delta t)\n\\end{aligned}\n\\]\nPutting this into the expression above, we get: \\[\nP_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i) = 1 - \\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t)\n\\] Finally: \\[\nP(X \\geq t + \\Delta t, C \\geq t + \\Delta t) = \\prod_{i=0}^{n-1} \\lp 1 - \\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t) \\rp\n\\] We take logs of both sides to get: \\[\n\\log P(X \\geq t_j + \\Delta t, C \\geq t_j + \\Delta t) = \\sum_{i \\mid t_i \\leq t_j} \\log \\lp 1 - \\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t) \\rp\n\\] Suppose that \\(\\lambda_1(t), \\lambda_2(t)\\) are bounded. Then we can use the Taylor series for \\(\\log 1 - x\\) as \\(x \\searrow 0\\).: \\[\n\\log(1 - x) = -x + o(x)\n\\]\nThen\n\\[\n\\begin{aligned}\n& \\log(1 - \\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t)) \\\\\n& = -(\\lambda_1(t_i) + \\lambda_2(t_i))\\Delta t + o(\\Delta t) + o(\\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t)) \\\\\n& = -(\\lambda_1(t_i) + \\lambda_2(t_i))\\Delta t + o(\\Delta t)\n\\end{aligned}\n\\] Putting this back into our expression gives us:\n\\[\n\\begin{aligned}\n\\log P(X \\geq t + \\Delta t, C \\geq t + \\Delta t) = -\\sum_{i=1}^n (\\lambda_1(t_i) + \\lambda_2(t_i)) \\Delta t  + n o(\\Delta t)\n\\end{aligned}\n\\] Taking \\(n \\to \\infty\\) gives the integral: \\[\n\\begin{aligned}\n\\log P(X &gt; t, C &gt; t) = -\\int_0^t (\\lambda_1(u) + \\lambda_2(u)) du \\\\\nP(X &gt; t, C &gt; t) = \\exp \\lp -\\int_0^t (\\lambda_1(u) + \\lambda_2(u)) du \\rp\n\\end{aligned}\n\\] By noting that \\(\\Delta t = \\frac{1}{n}\\), so for \\(o(\\Delta t)\\), \\(n o(\\Delta t) \\overset{n\\to\\infty}{\\to} 0\\)\nThis is a long way to show that \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^{1 - \\delta} \\\\\n    & = \\lambda_1(t)^\\delta \\lambda_2(t)^{1-\\delta} e^{-\\int_0^t (\\lambda_1(u) + \\lambda_2(u)) du}\n\\end{align*} \\]\nAssuming noninformative censoring equates to:\n\\[\n\\begin{align*}\n    \\lambda_1(t) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t) \\\\\n    & = \\lambda_X(t)\n\\end{align*}.\n\\] This yields: \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\lambda_1(t)^\\delta \\lambda_2(t)^{1-\\delta} e^{-\\int_0^t (\\lambda_1(u) + \\lambda_2(u)) du} \\\\\n    & = \\lambda_X(t)^\\delta e^{-\\int_0^t \\lambda_X(u)du}  \\lambda_2(t)^{1-\\delta}e^{-\\int_0^t\\lambda_2(u)du}\n\\end{align*}\n\\]\nThis means that we can factorize the joint density into a piece that Thus, noninformative censoring and parameter separability yield a separable joint density. This means that when we want to do maximum likelihood for survival data, we can ignore the model for the censoring times, \\(f_{C_i, \\delta_i}(t, \\delta;\\,\\phi)\\), and focus on only the model for the failure times: \\[f_{X_i, \\delta_i}(t, \\delta;\\,\\eta) = \\lambda_{X_i}(t)^\\delta P_\\eta(X \\geq t).\\] We can write this expression fully in terms of the hazard function by recalling [eq:exp-hazard]: \\[\\begin{align}\nf_{X_i, \\delta_i}(t, \\delta;\\,\\eta) = \\lambda_{X_i}(t)^\\delta \\exp\\left(-\\int_0^t \\lambda_{X_i}(u) du \\right).\n\\end{align}\n\\tag{4}\\]\n\nMLE for exponential survival time Let \\(X_i \\overset{\\text{iid}}{\\sim} \\text{Exp}(\\alpha)\\) and assume we have independent censoring (\\(X_i \\perp\\!\\!\\!\\perp C_i\\)), the parameters for the censoring process are separable from \\(\\alpha\\), and that \\(C_i\\) are iid such that \\(\\Exp{C_i} &lt; \\infty\\). Then our observed data are \\(T_i = \\min(X_i, C_i)\\) and \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\). According to Equation 4 we can write the likelihood as \\[\\begin{align*}\nf_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\prod_{i=1}^n \\alpha^{\\delta_i} \\exp(-\\textstyle{\\sum_{i=1}^n \\int_0^{t_i} }\\alpha du)\\\\\n& = \\alpha^{\\sum_{i=1}^n \\delta_i} \\exp(-\\alpha \\textstyle\\sum_{i=1}^n t_i)\n\\end{align*}\\] The log-likelihood is \\[\\log(f_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) ) = \\log(\\alpha)\\sum_{i=1}^n \\delta_i -\\alpha \\sum_{i=1}^n t_i\\] which has the maximizer \\[\\hat{\\alpha} = \\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}.\\] Let’s show that this converges a.s. to \\(\\alpha\\) as \\(n\\to\\infty\\). We can rewrite \\(\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}\\) as \\[\\begin{align*}\n    \\frac{\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right)}{\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}\n\\end{align*}\\] The top and bottom expressions converge a.s. by Kolmogorov’s Strong Law of Large Numbers to \\[\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right) & \\overset{\\text{a.s.}}{\\to} \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} \\\\\n\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right) & \\overset{\\text{a.s.}}{\\to}  \\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)}\n\\end{align*}\\] We can evaluate the top expression using the tower property of expectation: \\[\\begin{align*}\n    \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} & =  \\ExpA{\\ExpA{\\mathbbm{1}\\left(X_i \\leq c\\right)\\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n    & = \\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] where the second line follows from the independent censoring condition. The bottom expression becomes: \\[\\begin{align*}\n\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)} & = \\ExpA{\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& + \\ExpA{\\ExpA{c \\mathbbm{1}\\left(X_i &gt; c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& = \\ExpA{\\frac{1}{\\alpha}(1 - (1 + \\alpha C_i) e^{-\\alpha C_i})}{C_i} + \\ExpA{C_i e^{-\\alpha C_i}}{C_i} \\\\\n& = \\frac{1}{\\alpha}\\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] Thus \\[\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i} \\overset{\\text{a.s.}}{\\to} \\alpha\\] To show that \\(\\int_0^c x \\alpha e^{-\\alpha x} dx = \\frac{1}{\\alpha}(1 - (1 + \\alpha c) e^{-\\alpha c})\\), we can use the trick of differentiating under the integral sign. \\[\\begin{align*}\n  \\alpha \\int_0^c xe^{-\\alpha x} dx & = \\alpha \\int_0^c -\\frac{d}{d \\alpha} e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\int_0^c e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\frac{1}{\\alpha} (1 - e^{-\\alpha c}) \\\\\n  & = \\alpha \\left(\\frac{1 - (1 + \\alpha c) e^{-\\alpha c}}{\\alpha^2}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-4.html",
    "href": "survival-material/lecture-4.html",
    "title": "Lecture 4",
    "section": "",
    "text": "When we have \\((X_i, C_i) \\overset{\\text{iid}}{\\sim} F\\) such that noninformative censoring and parameter separability hold, we showed that we can write the likelihood for the survival process: \\[f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i} \\exp\\left(-\\int_0^{t_i} \\lambda_\\eta(u) du \\right).\\] This can again be simplified by collecting terms inside the exponential: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\sum_{i=1}^n \\int_0^{t_i} \\lambda_\\eta(u) du \\right).\n\\end{align}\\]\nLet’s make a slight change to how we write the survival function. Define the indicator function \\(Y(u)\\) to be \\[Y_i(u) = \\mathbbm{1}\\left(t_i \\geq u\\right).\\] This function is left-continuous, with right-hand limits, an example of which is shown in\n\n\n\n\n\n\n\n\nFigure 1: Example plot of an at-risk function\n\n\n\n\n\nThis allows us to rewrite our likelihood as follows: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\sum_{i=1}^n \\int_0^{\\infty} Y_i(u) \\lambda_\\eta(u) du \\right)\\\\\n& = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\int_0^{\\infty} \\lambda_\\eta(u) \\sum_{i=1}^n Y_i(u)  du \\right)\n\\end{align}\\] For notational convenience, we’ll define the function \\(\\widebar{Y}(u)\\) as: \\[\\widebar{Y}(u) = \\sum_{i=1}^n Y_i(u).\\] Then our likelihood is: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\int_0^{\\infty} \\lambda_\\eta(u) \\widebar{Y}(u)  du \\right)\n\\end{align}\\]\nWe can consider a nonparametric model for the hazard, estimating \\(\\lambda\\) at each \\(t_i\\) as a separate parameter. An example of this is shown in Figure 2, which corresponds to the discrete survival function in\n\n\n\n\n\n\n\n\nFigure 2: Example plot of a discrete hazard function\n\n\n\n\n\nIn order to evaluate the integral \\[\\int_0^{\\infty} \\lambda(u) \\widebar{Y}(u)  du,\\] note that we can rewrite \\(\\lambda(t_i)\\) as \\[\\lambda(t_i) = \\Lambda(t_i) - \\Lambda(t_i-),\\] where \\(\\Lambda(t)\\) is the cumulative hazard function. We’ll write as \\(\\lambda(u)\\) as \\(d\\Lambda(u)\\). Finally, recall that because \\(S(t)\\) is right-continuous, \\(\\Lambda(t)\\) is also right-continuous.\n\n\n\n\n\n\n\n\nFigure 3: Example plot of a discrete cumulative hazard function\n\n\n\n\n\nWe’ll also need a bit of integration theory from Lebesgue-Stieltjies integrals. Suppose that \\(G\\) is a right-continuous, monotone function on \\([0,\\infty)\\) with countably many discontinuities at \\(a_i\\), and let \\(dG(a_i) = G(a_i) - G(a_i-)\\). Then for a measurable function \\(F\\) on \\([0,\\infty)\\), the integral over a set \\(B\\) \\[\\int_B F(x) dG(x) = \\sum_{i \\mid a_i \\in B} F(a_i) dG(a_i).\\] Using these results, the integral can be evaluated to \\[\\int_0^{\\infty} \\left(\\widebar{Y}(u)\\right)d\\Lambda(u)  du = \\sum_{j = 1}^{n} \\lambda(t_j) \\widebar{Y}(t_j)\\] Let’s take the log of the expression to get a log-likelihood: \\[\\begin{align}\n\\log f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\sum_{i=1}^n \\delta_i \\log (\\lambda_\\eta(t_i)) -\\sum_{j=1}^{n} \\lambda_\\eta(t_j) \\widebar{Y}(t_j)\n\\end{align}\\] Taking the gradient with respect to \\(\\lambda_\\eta(t_i)\\) gives \\[\\begin{align}\n\\nabla \\log f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\frac{\\delta_i}{\\lambda_\\eta(t_i)} -\\widebar{Y}(t_i).\n\\end{align}\\] Note that the Hessian is also diagonal, which implies asymptotic independence of \\(\\lambda(t_i)\\). This is solved at \\[\\begin{align}\n  \\hat{\\lambda}_\\eta(t_i) & = \\frac{\\delta_i}{\\widebar{Y}(t_i)}\n\\end{align}\\] This gives an expression for \\(\\Lambda(t)\\): \\[\\begin{align}\n\\Lambda^{\\text{NA}}(t) =   \\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\widebar{Y}(t_i)}\n\\end{align} \\tag{1}\\] This also gives an expression for \\(S(t)\\): \\[\\begin{align}\n  S^{\\text{KM}}(t) & = \\prod_{i \\mid \\delta_i = 1, t_i \\leq t}(1 - \\frac{1}{\\widebar{Y}(t_i)}).\n\\end{align}\n\\tag{2}\\] This is also known as the Kaplan-Meier estimator. An alternative expression is: \\[\\begin{align}\n\\label{eq:surival-na}\n  S^{\\text{NA}}(t) & = \\exp\\left(-\\textstyle\\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\widebar{Y}(t_i)}\\right)\n\\end{align}\\] We can show that the cumulative hazard as implied by Equation 2 is asymptotically equivalent to Equation 1. Given \\[\nS_X(t) = \\exp\\lp-\\Lambda_X(t) \\rp\n\\] \\[\\begin{align}\n\\Lambda^{\\text{KM}} & = - \\log \\left(\\prod_{i \\mid \\delta_i = 1, t_i \\leq t}(1 - \\frac{1}{\\widebar{Y}(t_i)}) \\right)\\\\\n& = -\\sum_{i \\mid \\delta_i = 1, t_i \\leq t}\\log(1 - \\frac{1}{\\widebar{Y}(t_i)}) \\\\\n& \\approx \\sum_{i \\mid \\delta_i = 1, t_i \\leq t}\\frac{1}{\\widebar{Y}(t_i)}\n\\end{align}\\] where the last line follows from the Taylor approximation of \\(\\log(1 - x) \\approx -x\\) when \\(x \\approx 0\\).\n\n\nIn order to get the standard errors for the Kaplan-Meier estimator, we’ll use a Taylor expansion: \\[\\begin{align}\n  \\log(S^{\\text{KM}}(t)) \\approx \\log(S(t)) + \\frac{1}{S(t)}(S^{\\text{KM}}(t) - S(t))\n\\end{align}\\] which leads to \\[\\Var{\\log(S^{\\text{KM}}(t))} = \\frac{1}{S(t)^2}\\Var{S^{\\text{KM}}(t)}\\] or \\[\\Var{S^{\\text{KM}}(t)} = \\Var{\\log(S^{\\text{KM}}(t))}S(t)^2.\\] We use the plug-in estimator for \\(S(t)\\) here, so we get: \\[\\Var{S^{\\text{KM}}(t)} = \\Var{\\log(S^{\\text{KM}}(t))}(S^{\\text{KM}}(t))^2.\\] Now we need an expression for \\(\\Var{\\log(S^{\\text{KM}}(t))}\\). First we write the log of the KM estimator: \\[\\begin{align}\n  \\log \\hat{S}^{\\text{KM}}(t) = \\sum_{i \\mid t_i \\leq t} \\log(1 - \\hat{\\lambda}(t_i)).\n\\end{align}\\] First we find the Taylor expansion for each term, which is justified by the fact that \\((1 - \\hat{\\lambda}(t_i)) \\approx (1 - \\lambda(t_i))\\) for large samples: \\[\\begin{align}\n  \\log(1 - \\hat{\\lambda}(t_i)) \\approx  \\log(1 - \\lambda(t_i)) - \\frac{1}{1 - \\lambda(t_i)}(\\hat{\\lambda}(t_i) - \\lambda(t_i))\n\\end{align}\\] Then \\[\\Var{\\log(1 - \\hat{\\lambda}(t_i))} \\approx \\frac{1}{(1 - \\lambda(t_i))^2}\\Var{\\hat{\\lambda}(t_i)}\\] We can estimate the \\(\\Var{\\hat{\\lambda}(t_i)}\\) as: \\[\\Var{\\hat{\\lambda}(t_i)} = \\Var{\\delta_i} / \\bar{Y}^2(t_i)\\] Treating \\(\\delta_i\\) as a binomial random variable with \\(\\bar{Y}(t_i)\\) number of trials: \\[\\delta_i \\sim \\text{Binomial}(\\bar{Y}(t_i), p_i)\\] The variance of \\(\\delta_i\\) is \\(p_i (1 - p_i) \\bar{Y}(t_i)\\). Using \\(\\hat{\\lambda}(t_i)\\) as a plug-in estimator for \\(p_i\\) as \\(\\hat{\\lambda}(t_i)\\), this gives: \\[\\Var{\\delta_i} = \\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i)) \\bar{Y}(t_i)\\] Putting this together with the \\(\\bar{Y}^2(t_i)\\) in the denominator gives the following estimate for the variance of \\(\\hat{\\lambda}(t_i)\\): \\[\\frac{\\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i))}{\\bar{Y}(t_i)}.\\] Finally using the plug-in estimator for \\((1 - \\lambda(t_i))^2\\) in the denominator of the Taylor expansion formula gives: \\[\\begin{align}\n\\Var{\\log(1 - \\hat{\\lambda}(t_i))}  & \\approx \\frac{1}{(1 - \\hat{\\lambda}(t_i))^2}\\frac{\\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i))}{\\bar{Y}(t_i)}\\\\\n                                    & \\approx \\frac{\\hat{\\lambda}(t_i)}{\\bar{Y}(t_i)(1 - \\hat{\\lambda}(t_i))} \\\\\n              & \\approx \\frac{\\delta_i}{\\bar{Y}(t_i)(\\bar{Y}(t_i) - \\hat{\\lambda}(t_i))}\n\\end{align}\\] Putting this all together along with the fact that \\(\\lambda(t_i) \\overset{\\text{asymp}}{\\perp\\!\\!\\!\\perp} \\lambda(t_j)\\), yields what is known as Greenwood’s formula: \\[\\Var{S^{\\text{KM}}(t)} = (S^{\\text{KM}}(t))^2 \\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{\\delta_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - \\delta_i)}.\\]\n\n\n\n\nIf we wanted to construct asymptotic, point-wise confidence intervals for the KM estimator, we can go about it in several ways. The most straightforward way to compute confidence intervals is to directly use the estimated survival function at \\(t_0\\) and the standard error estimator from Greenwood’s formula. Let \\(\\hat{\\sigma}(t)\\) be \\[\\sqrt{\\sum_{i \\mid d_i = 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}}.\\] Then our confidence interval, \\(C^{\\text{KM}}\\), is \\[\\begin{align*}\n    C^{\\text{KM}} = \\left(\\hat{S}^{\\text{KM}}(t_0) - z_{1-\\alpha / 2} \\hat{\\sigma} \\hat{S}^{\\text{KM}}(t_0), \\hat{S}^{\\text{KM}}(t_0) + z_{1-\\alpha / 2} \\hat{\\sigma} \\hat{S}^{\\text{KM}}(t_0)\\right)\n\\end{align*}\\] The issue with this confidence interval is that it is not guaranteed to be greater than zero or less than 1, so we may have nonsensical results for upper and lower bounds. A solution is to build a confidence set for a suitably transformed Kaplan Meier estimator, and use the inverse transformation to enforce the natural \\([0,1]\\) bounds. One option is to use the logit transformation, another is to use the log-log transformation.\nWe’ll walk through the log-log transformation:\nNote that we have the following result: \\[\\begin{align}\n\\Var{\\log(\\hat{S}^{\\text{KM}}(t))} & = \\frac{1}{S(t)^2}\\Var{\\hat{S}^{\\text{KM}}(t)}\\\\\n& = \\sum_{i \\mid d_i = 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}.\n\\end{align}\\] Then \\[\\begin{align*}\n   \\log(-\\log(\\hat{S}^\\text{KM}(t))) \\approx  \\log(-\\log(S(t))) - \\frac{1}{\\log(S(t))}(\\log(S^{\\text{KM}}(t)) - \\log(S(t)))\n\\end{align*}\\] So \\[\\begin{align*}\n   \\Var{\\log(-\\log(\\hat{S}^\\text{KM}(t)))} \\approx  \\frac{1}{\\log(S(t))^2}\\Var{\\log(\\hat{S}^{\\text{KM}}(t)}\n\\end{align*}\\] or \\[\\begin{align*}\n   \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) \\approx  \\frac{1}{\\left| \\log(S(t)) \\right|}\\hat{\\sigma}(t)\n\\end{align*}\\] We don’t know \\(S(t)\\), so we’ll plug-in KM estimator for \\(S(t)\\): \\[\\begin{align*}\n   \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) \\approx  \\frac{1}{\\left| \\log(S^{\\text{KM}}(t)) \\right|}\\hat{\\sigma}(t)\n\\end{align*}\\]\nLet \\(u = \\log(-\\log S(t))\\), \\(\\hat{u} = \\log(-\\log(\\hat{S}^\\text{KM}(t)))\\), and \\(\\hat{\\sigma}_u = \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t))))\\). Then \\[\\hat{S}^\\text{KM}(t) = \\exp(-e^{\\hat{u}}).\\] Note that \\(\\exp(-e^u)\\) is a monotone decreasing function of its input, \\(u\\). This means that for a set \\([a,b]\\) \\[a \\leq u \\leq b \\implies \\exp(-e^a) \\geq \\exp(-e^u) \\geq \\exp(-e^b).\\] We’ll take it as a given that asymptotically, \\[\\frac{\\hat{u} - u}{\\hat{\\sigma}_u} \\overset{d}{\\to} \\mathcal{N}(0,1).\\] Then we can derive an alternative asymptotic confidence interval for the Kaplan-Meier estimator of survival at time \\(t_0\\) by transforming a confidence interval for \\(u\\). Let \\(z_{1 - \\alpha/2}\\) be the \\(1 - \\alpha/2\\) quantile of a standard normal distribution with CDF \\(\\Phi\\), or \\[z_{1 - \\alpha/2} = \\Phi^{-1}(1 - \\alpha/2).\\] \\[\\begin{align*}\nP(-z_{1 - \\alpha/2} \\leq \\frac{\\hat{u} - u}{\\hat{\\sigma}_u} \\leq z_{1 - \\alpha/2}) & =\nP(\\hat{u}-\\hat{\\sigma}_u z_{1 - \\alpha/2} \\leq u \\leq \\hat{u} + \\hat{\\sigma}_u z_{1 - \\alpha/2}) \\\\\n& = P(\\exp(-e^{\\hat{u}-\\hat{\\sigma}_u z_{1 - \\alpha/2}}) \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u} + \\hat{\\sigma}_u z_{1 - \\alpha/2}}))  \\\\\n& = P(\\exp(-e^{\\hat{u}}e^{-\\hat{\\sigma}_u z_{1 - \\alpha/2}}) \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u}}e^{\\hat{\\sigma}_u z_{1 - \\alpha/2}})) \\\\\n& = P(\\exp(-e^{\\hat{u}})^{e^{-\\hat{\\sigma}_u z_{1 - \\alpha/2}}} \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u}})^{e^{\\hat{\\sigma}_u z_{1 - \\alpha/2}}}) \\\\\n& = P((\\hat{S}^\\text{KM}(t))^{e^{-\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}} \\geq S(t) \\\\\n&\\qquad \\geq (\\hat{S}^\\text{KM}(t))^{e^{\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}}).\n\\end{align*}\\] So \\[\\begin{align}\n\\label{eq:log-log-CI}\nP\\left(S(t) \\in \\left((\\hat{S}^\\text{KM}(t))^{e^{\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}},(\\hat{S}^\\text{KM}(t))^{e^{-\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}}\\right)\\right)\\overset{\\text{asympt.}}{=} 1 - \\alpha\n\\end{align}\\]"
  },
  {
    "objectID": "survival-material/lecture-4.html#derivation-of-nelson-aalen-and-kaplan-meier-estimators",
    "href": "survival-material/lecture-4.html#derivation-of-nelson-aalen-and-kaplan-meier-estimators",
    "title": "Lecture 4",
    "section": "",
    "text": "When we have \\((X_i, C_i) \\overset{\\text{iid}}{\\sim} F\\) such that noninformative censoring and parameter separability hold, we showed that we can write the likelihood for the survival process: \\[f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i} \\exp\\left(-\\int_0^{t_i} \\lambda_\\eta(u) du \\right).\\] This can again be simplified by collecting terms inside the exponential: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\sum_{i=1}^n \\int_0^{t_i} \\lambda_\\eta(u) du \\right).\n\\end{align}\\]\nLet’s make a slight change to how we write the survival function. Define the indicator function \\(Y(u)\\) to be \\[Y_i(u) = \\mathbbm{1}\\left(t_i \\geq u\\right).\\] This function is left-continuous, with right-hand limits, an example of which is shown in\n\n\n\n\n\n\n\n\nFigure 1: Example plot of an at-risk function\n\n\n\n\n\nThis allows us to rewrite our likelihood as follows: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\sum_{i=1}^n \\int_0^{\\infty} Y_i(u) \\lambda_\\eta(u) du \\right)\\\\\n& = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\int_0^{\\infty} \\lambda_\\eta(u) \\sum_{i=1}^n Y_i(u)  du \\right)\n\\end{align}\\] For notational convenience, we’ll define the function \\(\\widebar{Y}(u)\\) as: \\[\\widebar{Y}(u) = \\sum_{i=1}^n Y_i(u).\\] Then our likelihood is: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\int_0^{\\infty} \\lambda_\\eta(u) \\widebar{Y}(u)  du \\right)\n\\end{align}\\]\nWe can consider a nonparametric model for the hazard, estimating \\(\\lambda\\) at each \\(t_i\\) as a separate parameter. An example of this is shown in Figure 2, which corresponds to the discrete survival function in\n\n\n\n\n\n\n\n\nFigure 2: Example plot of a discrete hazard function\n\n\n\n\n\nIn order to evaluate the integral \\[\\int_0^{\\infty} \\lambda(u) \\widebar{Y}(u)  du,\\] note that we can rewrite \\(\\lambda(t_i)\\) as \\[\\lambda(t_i) = \\Lambda(t_i) - \\Lambda(t_i-),\\] where \\(\\Lambda(t)\\) is the cumulative hazard function. We’ll write as \\(\\lambda(u)\\) as \\(d\\Lambda(u)\\). Finally, recall that because \\(S(t)\\) is right-continuous, \\(\\Lambda(t)\\) is also right-continuous.\n\n\n\n\n\n\n\n\nFigure 3: Example plot of a discrete cumulative hazard function\n\n\n\n\n\nWe’ll also need a bit of integration theory from Lebesgue-Stieltjies integrals. Suppose that \\(G\\) is a right-continuous, monotone function on \\([0,\\infty)\\) with countably many discontinuities at \\(a_i\\), and let \\(dG(a_i) = G(a_i) - G(a_i-)\\). Then for a measurable function \\(F\\) on \\([0,\\infty)\\), the integral over a set \\(B\\) \\[\\int_B F(x) dG(x) = \\sum_{i \\mid a_i \\in B} F(a_i) dG(a_i).\\] Using these results, the integral can be evaluated to \\[\\int_0^{\\infty} \\left(\\widebar{Y}(u)\\right)d\\Lambda(u)  du = \\sum_{j = 1}^{n} \\lambda(t_j) \\widebar{Y}(t_j)\\] Let’s take the log of the expression to get a log-likelihood: \\[\\begin{align}\n\\log f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\sum_{i=1}^n \\delta_i \\log (\\lambda_\\eta(t_i)) -\\sum_{j=1}^{n} \\lambda_\\eta(t_j) \\widebar{Y}(t_j)\n\\end{align}\\] Taking the gradient with respect to \\(\\lambda_\\eta(t_i)\\) gives \\[\\begin{align}\n\\nabla \\log f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\frac{\\delta_i}{\\lambda_\\eta(t_i)} -\\widebar{Y}(t_i).\n\\end{align}\\] Note that the Hessian is also diagonal, which implies asymptotic independence of \\(\\lambda(t_i)\\). This is solved at \\[\\begin{align}\n  \\hat{\\lambda}_\\eta(t_i) & = \\frac{\\delta_i}{\\widebar{Y}(t_i)}\n\\end{align}\\] This gives an expression for \\(\\Lambda(t)\\): \\[\\begin{align}\n\\Lambda^{\\text{NA}}(t) =   \\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\widebar{Y}(t_i)}\n\\end{align} \\tag{1}\\] This also gives an expression for \\(S(t)\\): \\[\\begin{align}\n  S^{\\text{KM}}(t) & = \\prod_{i \\mid \\delta_i = 1, t_i \\leq t}(1 - \\frac{1}{\\widebar{Y}(t_i)}).\n\\end{align}\n\\tag{2}\\] This is also known as the Kaplan-Meier estimator. An alternative expression is: \\[\\begin{align}\n\\label{eq:surival-na}\n  S^{\\text{NA}}(t) & = \\exp\\left(-\\textstyle\\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\widebar{Y}(t_i)}\\right)\n\\end{align}\\] We can show that the cumulative hazard as implied by Equation 2 is asymptotically equivalent to Equation 1. Given \\[\nS_X(t) = \\exp\\lp-\\Lambda_X(t) \\rp\n\\] \\[\\begin{align}\n\\Lambda^{\\text{KM}} & = - \\log \\left(\\prod_{i \\mid \\delta_i = 1, t_i \\leq t}(1 - \\frac{1}{\\widebar{Y}(t_i)}) \\right)\\\\\n& = -\\sum_{i \\mid \\delta_i = 1, t_i \\leq t}\\log(1 - \\frac{1}{\\widebar{Y}(t_i)}) \\\\\n& \\approx \\sum_{i \\mid \\delta_i = 1, t_i \\leq t}\\frac{1}{\\widebar{Y}(t_i)}\n\\end{align}\\] where the last line follows from the Taylor approximation of \\(\\log(1 - x) \\approx -x\\) when \\(x \\approx 0\\).\n\n\nIn order to get the standard errors for the Kaplan-Meier estimator, we’ll use a Taylor expansion: \\[\\begin{align}\n  \\log(S^{\\text{KM}}(t)) \\approx \\log(S(t)) + \\frac{1}{S(t)}(S^{\\text{KM}}(t) - S(t))\n\\end{align}\\] which leads to \\[\\Var{\\log(S^{\\text{KM}}(t))} = \\frac{1}{S(t)^2}\\Var{S^{\\text{KM}}(t)}\\] or \\[\\Var{S^{\\text{KM}}(t)} = \\Var{\\log(S^{\\text{KM}}(t))}S(t)^2.\\] We use the plug-in estimator for \\(S(t)\\) here, so we get: \\[\\Var{S^{\\text{KM}}(t)} = \\Var{\\log(S^{\\text{KM}}(t))}(S^{\\text{KM}}(t))^2.\\] Now we need an expression for \\(\\Var{\\log(S^{\\text{KM}}(t))}\\). First we write the log of the KM estimator: \\[\\begin{align}\n  \\log \\hat{S}^{\\text{KM}}(t) = \\sum_{i \\mid t_i \\leq t} \\log(1 - \\hat{\\lambda}(t_i)).\n\\end{align}\\] First we find the Taylor expansion for each term, which is justified by the fact that \\((1 - \\hat{\\lambda}(t_i)) \\approx (1 - \\lambda(t_i))\\) for large samples: \\[\\begin{align}\n  \\log(1 - \\hat{\\lambda}(t_i)) \\approx  \\log(1 - \\lambda(t_i)) - \\frac{1}{1 - \\lambda(t_i)}(\\hat{\\lambda}(t_i) - \\lambda(t_i))\n\\end{align}\\] Then \\[\\Var{\\log(1 - \\hat{\\lambda}(t_i))} \\approx \\frac{1}{(1 - \\lambda(t_i))^2}\\Var{\\hat{\\lambda}(t_i)}\\] We can estimate the \\(\\Var{\\hat{\\lambda}(t_i)}\\) as: \\[\\Var{\\hat{\\lambda}(t_i)} = \\Var{\\delta_i} / \\bar{Y}^2(t_i)\\] Treating \\(\\delta_i\\) as a binomial random variable with \\(\\bar{Y}(t_i)\\) number of trials: \\[\\delta_i \\sim \\text{Binomial}(\\bar{Y}(t_i), p_i)\\] The variance of \\(\\delta_i\\) is \\(p_i (1 - p_i) \\bar{Y}(t_i)\\). Using \\(\\hat{\\lambda}(t_i)\\) as a plug-in estimator for \\(p_i\\) as \\(\\hat{\\lambda}(t_i)\\), this gives: \\[\\Var{\\delta_i} = \\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i)) \\bar{Y}(t_i)\\] Putting this together with the \\(\\bar{Y}^2(t_i)\\) in the denominator gives the following estimate for the variance of \\(\\hat{\\lambda}(t_i)\\): \\[\\frac{\\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i))}{\\bar{Y}(t_i)}.\\] Finally using the plug-in estimator for \\((1 - \\lambda(t_i))^2\\) in the denominator of the Taylor expansion formula gives: \\[\\begin{align}\n\\Var{\\log(1 - \\hat{\\lambda}(t_i))}  & \\approx \\frac{1}{(1 - \\hat{\\lambda}(t_i))^2}\\frac{\\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i))}{\\bar{Y}(t_i)}\\\\\n                                    & \\approx \\frac{\\hat{\\lambda}(t_i)}{\\bar{Y}(t_i)(1 - \\hat{\\lambda}(t_i))} \\\\\n              & \\approx \\frac{\\delta_i}{\\bar{Y}(t_i)(\\bar{Y}(t_i) - \\hat{\\lambda}(t_i))}\n\\end{align}\\] Putting this all together along with the fact that \\(\\lambda(t_i) \\overset{\\text{asymp}}{\\perp\\!\\!\\!\\perp} \\lambda(t_j)\\), yields what is known as Greenwood’s formula: \\[\\Var{S^{\\text{KM}}(t)} = (S^{\\text{KM}}(t))^2 \\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{\\delta_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - \\delta_i)}.\\]"
  },
  {
    "objectID": "survival-material/lecture-4.html#confidence-intervals",
    "href": "survival-material/lecture-4.html#confidence-intervals",
    "title": "Lecture 4",
    "section": "",
    "text": "If we wanted to construct asymptotic, point-wise confidence intervals for the KM estimator, we can go about it in several ways. The most straightforward way to compute confidence intervals is to directly use the estimated survival function at \\(t_0\\) and the standard error estimator from Greenwood’s formula. Let \\(\\hat{\\sigma}(t)\\) be \\[\\sqrt{\\sum_{i \\mid d_i = 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}}.\\] Then our confidence interval, \\(C^{\\text{KM}}\\), is \\[\\begin{align*}\n    C^{\\text{KM}} = \\left(\\hat{S}^{\\text{KM}}(t_0) - z_{1-\\alpha / 2} \\hat{\\sigma} \\hat{S}^{\\text{KM}}(t_0), \\hat{S}^{\\text{KM}}(t_0) + z_{1-\\alpha / 2} \\hat{\\sigma} \\hat{S}^{\\text{KM}}(t_0)\\right)\n\\end{align*}\\] The issue with this confidence interval is that it is not guaranteed to be greater than zero or less than 1, so we may have nonsensical results for upper and lower bounds. A solution is to build a confidence set for a suitably transformed Kaplan Meier estimator, and use the inverse transformation to enforce the natural \\([0,1]\\) bounds. One option is to use the logit transformation, another is to use the log-log transformation.\nWe’ll walk through the log-log transformation:\nNote that we have the following result: \\[\\begin{align}\n\\Var{\\log(\\hat{S}^{\\text{KM}}(t))} & = \\frac{1}{S(t)^2}\\Var{\\hat{S}^{\\text{KM}}(t)}\\\\\n& = \\sum_{i \\mid d_i = 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}.\n\\end{align}\\] Then \\[\\begin{align*}\n   \\log(-\\log(\\hat{S}^\\text{KM}(t))) \\approx  \\log(-\\log(S(t))) - \\frac{1}{\\log(S(t))}(\\log(S^{\\text{KM}}(t)) - \\log(S(t)))\n\\end{align*}\\] So \\[\\begin{align*}\n   \\Var{\\log(-\\log(\\hat{S}^\\text{KM}(t)))} \\approx  \\frac{1}{\\log(S(t))^2}\\Var{\\log(\\hat{S}^{\\text{KM}}(t)}\n\\end{align*}\\] or \\[\\begin{align*}\n   \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) \\approx  \\frac{1}{\\left| \\log(S(t)) \\right|}\\hat{\\sigma}(t)\n\\end{align*}\\] We don’t know \\(S(t)\\), so we’ll plug-in KM estimator for \\(S(t)\\): \\[\\begin{align*}\n   \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) \\approx  \\frac{1}{\\left| \\log(S^{\\text{KM}}(t)) \\right|}\\hat{\\sigma}(t)\n\\end{align*}\\]\nLet \\(u = \\log(-\\log S(t))\\), \\(\\hat{u} = \\log(-\\log(\\hat{S}^\\text{KM}(t)))\\), and \\(\\hat{\\sigma}_u = \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t))))\\). Then \\[\\hat{S}^\\text{KM}(t) = \\exp(-e^{\\hat{u}}).\\] Note that \\(\\exp(-e^u)\\) is a monotone decreasing function of its input, \\(u\\). This means that for a set \\([a,b]\\) \\[a \\leq u \\leq b \\implies \\exp(-e^a) \\geq \\exp(-e^u) \\geq \\exp(-e^b).\\] We’ll take it as a given that asymptotically, \\[\\frac{\\hat{u} - u}{\\hat{\\sigma}_u} \\overset{d}{\\to} \\mathcal{N}(0,1).\\] Then we can derive an alternative asymptotic confidence interval for the Kaplan-Meier estimator of survival at time \\(t_0\\) by transforming a confidence interval for \\(u\\). Let \\(z_{1 - \\alpha/2}\\) be the \\(1 - \\alpha/2\\) quantile of a standard normal distribution with CDF \\(\\Phi\\), or \\[z_{1 - \\alpha/2} = \\Phi^{-1}(1 - \\alpha/2).\\] \\[\\begin{align*}\nP(-z_{1 - \\alpha/2} \\leq \\frac{\\hat{u} - u}{\\hat{\\sigma}_u} \\leq z_{1 - \\alpha/2}) & =\nP(\\hat{u}-\\hat{\\sigma}_u z_{1 - \\alpha/2} \\leq u \\leq \\hat{u} + \\hat{\\sigma}_u z_{1 - \\alpha/2}) \\\\\n& = P(\\exp(-e^{\\hat{u}-\\hat{\\sigma}_u z_{1 - \\alpha/2}}) \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u} + \\hat{\\sigma}_u z_{1 - \\alpha/2}}))  \\\\\n& = P(\\exp(-e^{\\hat{u}}e^{-\\hat{\\sigma}_u z_{1 - \\alpha/2}}) \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u}}e^{\\hat{\\sigma}_u z_{1 - \\alpha/2}})) \\\\\n& = P(\\exp(-e^{\\hat{u}})^{e^{-\\hat{\\sigma}_u z_{1 - \\alpha/2}}} \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u}})^{e^{\\hat{\\sigma}_u z_{1 - \\alpha/2}}}) \\\\\n& = P((\\hat{S}^\\text{KM}(t))^{e^{-\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}} \\geq S(t) \\\\\n&\\qquad \\geq (\\hat{S}^\\text{KM}(t))^{e^{\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}}).\n\\end{align*}\\] So \\[\\begin{align}\n\\label{eq:log-log-CI}\nP\\left(S(t) \\in \\left((\\hat{S}^\\text{KM}(t))^{e^{\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}},(\\hat{S}^\\text{KM}(t))^{e^{-\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}}\\right)\\right)\\overset{\\text{asympt.}}{=} 1 - \\alpha\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#ipw-z-estimators",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#ipw-z-estimators",
    "title": "Missing data lecture 2",
    "section": "IPW-Z-estimators",
    "text": "IPW-Z-estimators\nMost estimators of interest can be formulated as solutions in \\(\\theta\\) to the following system of equations: \\[\n\\frac{1}{n} \\sum_{i=1}^n \\psi(\\theta, y_i, x_i) = 0\n\\] The key property of \\(\\psi(\\theta, Y_i, X_i)\\) is that the following identity holds: \\[\n\\ExpA{\\psi(\\theta^\\dagger, Y_i, X_i)}{(Y_i, X_i \\mid \\theta^\\dagger)} = 0\n\\] We can use the same idea as above: fit the complete cases with weights: \\[\n\\frac{1}{n} \\sum_{i=1}^n \\frac{(1 - m_i)}{\\pi(x_i, \\beta)} \\psi(\\theta, y_i, x_i) = 0\n\\] Given the inverse weighting, we’ll end up with an unbiased estimator of \\[\n\\ExpA{\\psi(\\theta, Y_i, X_i)}{(Y_i, X_i \\mid \\theta^\\dagger)}\n\\] which we can then use to find \\(\\hat{\\theta}\\)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "It makes a good bit of sense to fill in missing values with some sort of estimate of the missing values. There are myriad ways to fill in unknown values with imputations.\nThe first way to fill in missing values we’ll cover are single imputation methods, whereby we fill in each missing value with one value. The values of the missing observations can be generated from an explicit model or an implicit model. We’ll demonstrate our methods on the following example dataset.\nIn the example shown in Figure 1, \\(n=500\\) values of \\(Y_1, Y_2\\) are a sample from a bivariate normal distribution; each random variable is marginally distributed as standard normal (i.e. mean \\(0\\) and variance equal to \\(1\\)), with covariance equal to \\(\\sqrt{0.5}\\). The green values in Figure 1 are observed, while the red values are missing. Visually, we can see that the values are not missing completely at random because a preponderance of values are missing from the bottom left quadrant of the plot.\n\n\n\n\n\n\n\n\nFigure 1: How do you fill in the red points?\n\n\n\n\n\nSingle imputation methods are akin to poststratified estimators:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{ps} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\mu(x, \\hat{\\gamma})(1 - p_x)) \\frac{N_x}{N}\n\\end{aligned}\n\\] The estimator \\(\\bar{y}^{\\mathrm{ps}}\\) replaces all responses within a certain value of \\(x\\) that are unobserved with \\(\\mu(x,\\hat{\\gamma})\\).\nThis seems like a good approach at first because we’re taking into account the model for \\(Y_i \\mid X_i = x\\) when doing our imputation. The downside is that we’re understating uncertainty in our resulting estimator because all values lie exactly on the regression function.\nFigure 2 shows conditional mean imputation applied to the dataset above. The bright red dots are imputed data.\n\n\n\n\n\n\n\n\nFigure 2: Conditional mean imputation\n\n\n\n\n\nThe imputations don’t fit with the rest of the points because they lie directly on the regression line.\nThere are other even simpler approaches to imputation.\n\n\nThere is mean imputation, whereby we impute each missing observation with the overall mean of the dataset:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\bar{y}^\\mathrm{res} (1 - p_x)) \\frac{n_x}{n}\n\\end{aligned}\n\\]\nThis technique seems sketchy because if \\(Y_i\\) depends on \\(X_i\\) then we’ll incur some bias by ignoring this relationship. Furthermore, we’ll ignore the added uncertainty by the fact that we’ve used \\(\\bar{y}\\) for every missing response in the dataset.\n\n\n\n\n\n\n\n\nFigure 3: Conditional mean imputation\n\n\n\n\n\nFigure 3 shows this technique applied to our example. This is worse than the conditional mean imputation because we are ignoring the relationship between \\(Y_1\\) and \\(Y_2\\).\n\n\n\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\tilde{y}^\\mathrm{res} (1 - p_x)) \\frac{n_x}{n} \\\\\n\\tilde{y}^\\mathrm{res} & \\sim \\text{Normal}(\\bar{y}^\\mathrm{res}, \\frac{\\hat{\\sigma}^2_{\\mathrm{res}}}{r_x})\n\\end{aligned}\n\\]\nThis is a better approach than mean imputation, but we’ll still be ignoring the relationship between the covariates and our response.\n\n\n\n\n\n\n\n\nFigure 4: Conditional mean imputation\n\n\n\n\n\n\n\n\nThe most faithful single imputation method might be something like the following:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\tilde{y}_x (1 - p_x)) \\frac{n_x}{n} \\\\\n\\tilde{y}_x \\mid x & \\overset{\\text{indy}}{\\sim} \\text{Normal}\\lp\\mu(x, \\hat{\\gamma}), \\frac{\\hat{\\sigma}^2_{\\mathrm{res}}(x)}{r_x}\\rp\n\\end{aligned}\n\\]\nThis approach is shown in Figure 5. The purple short-dashed line is the regression obtained from the completed dataset. We can see that despite the added uncertainty in the imputation method, the resulting complete-data estimator is still biased compared to the true relationship. We’ll still be ignoring the uncertainty that arises from the fact that we could have generated many different values for \\(\\tilde{y}_x\\) versus the values that we used for the estimator.\n\n\n\n\n\n\n\n\nFigure 5: Conditional mean imputation\n\n\n\n\n\n\n\n\nThere are still more single imputation models.\n\n\nSuppose we consider \\(Y_1\\) the value of the first measurement in a longitudinal study with \\(2\\) measurement occasions. A popular method of single imputation in longitudinal analyses is last-observation-carried-forward, or LOCF for short. This is shown below:\n\n\n\n\n\n\n\n\nFigure 6: LOCF imputation\n\n\n\n\n\nWe have the same problem with conditional mean imputation, namely the lack of randomness in our imputations.\n\n\n\nHot deck imputation is something that is employed by the Census Bureau to handle item nonresponse on surveys. The simplest implementation is described as follows. Let there be \\(n\\) observations on \\(y_i\\), a univariate measurement, of which \\(n-r\\) are missing. The hot deck estimator is defined as follows: \\[\n\\begin{aligned}\n\\bar{y}^{\\mathrm{HD}} & = \\frac{1}{n} \\lp r \\bar{y}_{(0)} + (n-r) \\bar{y}_{(1)} \\rp \\\\\n\\bar{y}_{(1)} \\mid y_{(0)} & = \\frac{1}{n-r}\\sum_{i=1}^r h_i y_{(0)i} \\\\\n(H_1, \\dots, H_r) & \\sim \\text{Multinomial}((n-r) \\mid (1/r, \\dots, 1/r))\n\\end{aligned}\n\\] The units included in the fully observed data are called “donor” units because the set provides the values that can replace the missing values.\nWe can get the mean and variance of these estimators by using the law of total expectation and total variance:\n\\[\n\\begin{aligned}\n\\Exp{\\bar{y}^{\\mathrm{HD}}} & = \\ExpA{\\ExpA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}} \\\\\n\\Var{\\bar{y}^{\\mathrm{HD}}} & = \\VarA{\\ExpA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}} + \\ExpA{\\VarA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}}\n\\end{aligned}\n\\] Example 4.8 in Little and Rubin (2019) walks through the derivation of these expressions.\n\n\n\nGiven that the prior example is only useful when data are MCAR (as our book mentions), one can modify the donor units for each observation such that we only choose donors from a pool that is “close” to the unit with a missing value in terms of other completely observed values. For a unit for which \\(y_i\\) is missing but for which we have fully observed covariates \\(x_{i} = x_{i1}, \\dots x_{iK}\\), we would choose a donor unit with observed \\(y_j\\) and \\(x_j = x_{j1}, \\dots x_{jK}\\) such that some distance metric, \\(d(x_i, x_j)\\) is below a threshold value, \\(d_0\\).\n\n\n\nIn predictive mean matching, we fit a regression model on the completely observed units. Then we compute the predicted value for \\(y_i\\), or \\(\\hat{y}(x_i)\\) using \\(x_i\\) and pick donors for which \\((\\hat{y}(x_i) - \\hat{y}(x_j))^2\\) are below \\(d_0\\)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#mean-imputation",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#mean-imputation",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "There is mean imputation, whereby we impute each missing observation with the overall mean of the dataset:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\bar{y}^\\mathrm{res} (1 - p_x)) \\frac{n_x}{n}\n\\end{aligned}\n\\]\nThis technique seems sketchy because if \\(Y_i\\) depends on \\(X_i\\) then we’ll incur some bias by ignoring this relationship. Furthermore, we’ll ignore the added uncertainty by the fact that we’ve used \\(\\bar{y}\\) for every missing response in the dataset.\n\n\n\n\n\n\n\n\nFigure 3: Conditional mean imputation\n\n\n\n\n\nFigure 3 shows this technique applied to our example. This is worse than the conditional mean imputation because we are ignoring the relationship between \\(Y_1\\) and \\(Y_2\\)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#random-imputation",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#random-imputation",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\tilde{y}^\\mathrm{res} (1 - p_x)) \\frac{n_x}{n} \\\\\n\\tilde{y}^\\mathrm{res} & \\sim \\text{Normal}(\\bar{y}^\\mathrm{res}, \\frac{\\hat{\\sigma}^2_{\\mathrm{res}}}{r_x})\n\\end{aligned}\n\\]\nThis is a better approach than mean imputation, but we’ll still be ignoring the relationship between the covariates and our response.\n\n\n\n\n\n\n\n\nFigure 4: Conditional mean imputation"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#random-regression-imputation",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#random-regression-imputation",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "The most faithful single imputation method might be something like the following:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\tilde{y}_x (1 - p_x)) \\frac{n_x}{n} \\\\\n\\tilde{y}_x \\mid x & \\overset{\\text{indy}}{\\sim} \\text{Normal}\\lp\\mu(x, \\hat{\\gamma}), \\frac{\\hat{\\sigma}^2_{\\mathrm{res}}(x)}{r_x}\\rp\n\\end{aligned}\n\\]\nThis approach is shown in Figure 5. The purple short-dashed line is the regression obtained from the completed dataset. We can see that despite the added uncertainty in the imputation method, the resulting complete-data estimator is still biased compared to the true relationship. We’ll still be ignoring the uncertainty that arises from the fact that we could have generated many different values for \\(\\tilde{y}_x\\) versus the values that we used for the estimator.\n\n\n\n\n\n\n\n\nFigure 5: Conditional mean imputation"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#aipwcc-z-estimators",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#aipwcc-z-estimators",
    "title": "Missing data lecture 2",
    "section": "AIPWCC-Z-estimators",
    "text": "AIPWCC-Z-estimators\nThe same idea can be used for Z-estimators; \\(\\hat{\\theta}^\\mathrm{AIPWCC}\\) solves the following systems of equations: \\[\n\\frac{1}{n} \\sum_{i=1}^n \\frac{1 - m_i}{\\pi(x_i, \\hat{\\beta})}\\psi(\\theta, y_i, x_i) -\n\\lp\\frac{(1 - m_i) - \\pi(x_i, \\hat{\\beta})}{\\pi(x_i, \\hat{\\beta})}\\rp\\ExpA{\\psi(y_i,x_i, \\theta) \\mid X_i = x_i}{Y_i \\mid X_i}\n= 0\n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#last-observation-carried-forward-imputation-locf",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#last-observation-carried-forward-imputation-locf",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "Figure 6: Conditional mean imputation"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#implicit-imputation-models",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#implicit-imputation-models",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "There are still more single imputation models.\n\n\nSuppose we consider \\(Y_1\\) the value of the first measurement in a longitudinal study with \\(2\\) measurement occasions. A popular method of single imputation in longitudinal analyses is last-observation-carried-forward, or LOCF for short. This is shown below:\n\n\n\n\n\n\n\n\nFigure 6: LOCF imputation\n\n\n\n\n\nWe have the same problem with conditional mean imputation, namely the lack of randomness in our imputations.\n\n\n\nHot deck imputation is something that is employed by the Census Bureau to handle item nonresponse on surveys. The simplest implementation is described as follows. Let there be \\(n\\) observations on \\(y_i\\), a univariate measurement, of which \\(n-r\\) are missing. The hot deck estimator is defined as follows: \\[\n\\begin{aligned}\n\\bar{y}^{\\mathrm{HD}} & = \\frac{1}{n} \\lp r \\bar{y}_{(0)} + (n-r) \\bar{y}_{(1)} \\rp \\\\\n\\bar{y}_{(1)} \\mid y_{(0)} & = \\frac{1}{n-r}\\sum_{i=1}^r h_i y_{(0)i} \\\\\n(H_1, \\dots, H_r) & \\sim \\text{Multinomial}((n-r) \\mid (1/r, \\dots, 1/r))\n\\end{aligned}\n\\] The units included in the fully observed data are called “donor” units because the set provides the values that can replace the missing values.\nWe can get the mean and variance of these estimators by using the law of total expectation and total variance:\n\\[\n\\begin{aligned}\n\\Exp{\\bar{y}^{\\mathrm{HD}}} & = \\ExpA{\\ExpA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}} \\\\\n\\Var{\\bar{y}^{\\mathrm{HD}}} & = \\VarA{\\ExpA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}} + \\ExpA{\\VarA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}}\n\\end{aligned}\n\\] Example 4.8 in Little and Rubin (2019) walks through the derivation of these expressions.\n\n\n\nGiven that the prior example is only useful when data are MCAR (as our book mentions), one can modify the donor units for each observation such that we only choose donors from a pool that is “close” to the unit with a missing value in terms of other completely observed values. For a unit for which \\(y_i\\) is missing but for which we have fully observed covariates \\(x_{i} = x_{i1}, \\dots x_{iK}\\), we would choose a donor unit with observed \\(y_j\\) and \\(x_j = x_{j1}, \\dots x_{jK}\\) such that some distance metric, \\(d(x_i, x_j)\\) is below a threshold value, \\(d_0\\).\n\n\n\nIn predictive mean matching, we fit a regression model on the completely observed units. Then we compute the predicted value for \\(y_i\\), or \\(\\hat{y}(x_i)\\) using \\(x_i\\) and pick donors for which \\((\\hat{y}(x_i) - \\hat{y}(x_j))^2\\) are below \\(d_0\\)."
  },
  {
    "objectID": "survival-material/lecture-5.html",
    "href": "survival-material/lecture-5.html",
    "title": "Lecture 5",
    "section": "",
    "text": "We had assumed that no two events could occur at the same time, but for most real datasets this isn’t realistic. A distinction must be made between a) assuming that ties are present in the data because, despite the true events happening in continuous time and thus no two events exactly coincide, the data have been rounded such that this exact ordering of events is lost, or b) that the true events happen in discrete time, and so there are truly events that co-occur.\nIn the continuous time scenario, (Aalen, Borgan, and Gjessing 2008) suggests using a modified estimator for hazard at time \\(t_i\\) when there are multiple \\(\\delta_i = 1\\). Let \\(d_i\\) be the number of events observed at time \\(t_i\\). Then the proposed estimator for \\(\\hat{\\lambda}(t_i)\\) is: \\[\\begin{align}\n    \\hat{\\lambda}(t_i) = \\sum_{j=0}^{d_i - 1} \\frac{1}{\\widebar{Y}(t_i) - j}\n\\end{align} \\tag{1}\\]\nIn discrete time the proposal is to use: \\[\\begin{align}\n    \\hat{\\lambda}(t_i) = \\frac{d_i}{\\widebar{Y}(t_i)}\n\\end{align} \\tag{2}\\]\n\n\nIt turns out, after some algebra, that using either Equation 1 or Equation 2 results in the following tie-corrected estimator for the KM estimator: \\[\\begin{align}\n    \\hat{S}^\\text{KM}(t) = \\prod_{i \\mid d_i \\geq 1, t_i \\leq t} (1 - \\frac{d_i}{\\widebar{Y}(t_i)})\n\\end{align}\\]\nGreenwood’s formula is then \\[\\Var{S^{\\text{KM}}(t)} = (S^{\\text{KM}}(t))^2 \\sum_{i \\mid d_i \\geq 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}.\\] This is the more commonly known form."
  },
  {
    "objectID": "survival-material/lecture-5.html#handling-ties-in-the-nelson-aalen-estimator",
    "href": "survival-material/lecture-5.html#handling-ties-in-the-nelson-aalen-estimator",
    "title": "Lecture 5",
    "section": "",
    "text": "We had assumed that no two events could occur at the same time, but for most real datasets this isn’t realistic. A distinction must be made between a) assuming that ties are present in the data because, despite the true events happening in continuous time and thus no two events exactly coincide, the data have been rounded such that this exact ordering of events is lost, or b) that the true events happen in discrete time, and so there are truly events that co-occur.\nIn the continuous time scenario, (Aalen, Borgan, and Gjessing 2008) suggests using a modified estimator for hazard at time \\(t_i\\) when there are multiple \\(\\delta_i = 1\\). Let \\(d_i\\) be the number of events observed at time \\(t_i\\). Then the proposed estimator for \\(\\hat{\\lambda}(t_i)\\) is: \\[\\begin{align}\n    \\hat{\\lambda}(t_i) = \\sum_{j=0}^{d_i - 1} \\frac{1}{\\widebar{Y}(t_i) - j}\n\\end{align} \\tag{1}\\]\nIn discrete time the proposal is to use: \\[\\begin{align}\n    \\hat{\\lambda}(t_i) = \\frac{d_i}{\\widebar{Y}(t_i)}\n\\end{align} \\tag{2}\\]"
  },
  {
    "objectID": "survival-material/lecture-5.html#handling-ties-in-the-kaplan-meier-estimator",
    "href": "survival-material/lecture-5.html#handling-ties-in-the-kaplan-meier-estimator",
    "title": "Lecture 5",
    "section": "",
    "text": "It turns out, after some algebra, that using either Equation 1 or Equation 2 results in the following tie-corrected estimator for the KM estimator: \\[\\begin{align}\n    \\hat{S}^\\text{KM}(t) = \\prod_{i \\mid d_i \\geq 1, t_i \\leq t} (1 - \\frac{d_i}{\\widebar{Y}(t_i)})\n\\end{align}\\]\nGreenwood’s formula is then \\[\\Var{S^{\\text{KM}}(t)} = (S^{\\text{KM}}(t))^2 \\sum_{i \\mid d_i \\geq 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}.\\] This is the more commonly known form."
  },
  {
    "objectID": "survival-material/weibull-mappings.html",
    "href": "survival-material/weibull-mappings.html",
    "title": "Weibull parameterizations",
    "section": "",
    "text": "Our course notes (and (Klein, Moeschberger, et al. 2003)) define the Weibull hazard as: \\[\\lambda(t) = \\gamma \\alpha t^{\\alpha - 1}\\] Base R defines the Weibull parameterization for rweibull(n, shape=\\(\\alpha\\), scale=\\(\\sigma\\)) as \\[\\lambda(t) = \\frac{\\alpha}{\\sigma} \\left(\\frac{t}{\\sigma}\\right)^{\\alpha - 1}\\] The survival package parameterizes the Weibull, with intercept=\\(\\mu\\), scale =\\(\\tau\\), as \\[\\lambda(t) = \\frac{1}{\\tau e^{\\mu/\\tau}} t^{1/\\tau - 1}\\] Thus, we can see that the following identities hold: \\[\\begin{align*}\n    \\gamma & = \\frac{1}{\\sigma^\\alpha} \\implies \\sigma = \\frac{1}{\\gamma^{1/\\alpha}} \\\\\n    \\gamma & = e^{-\\mu/\\tau} \\implies \\mu = -\\tau \\log(\\gamma)\n\\end{align*}\\] This also implies that regression coefficients from survreg are interpreted differently from the typical interpretation from a proportional hazards model. The proportional hazards Weibull model is typically written \\[\\gamma e^{\\boldsymbol{\\beta}^T\\mathbf{z}_i} \\alpha t^{\\alpha - 1}\\] But survreg parameterizes the model as \\[\\frac{1}{\\tau e^{(\\mu + \\boldsymbol{\\theta}^T \\mathbf{z}_i)/\\tau}} t^{1/\\tau - 1}\\] This means that: \\[\\begin{align*}\n\\boldsymbol{\\beta} & = -\\boldsymbol{\\theta} / \\tau \\\\\n\\gamma & = e^{-\\mu/\\tau}\n\\end{align*}\\] Thus, a positive coefficient in the parametric hazard which indicates that the variable increases hazard, all else being equal, will be negative in survreg’s coefficient results and vice versa.\n\n\n\n\nReferences\n\nKlein, John P, Melvin L Moeschberger, et al. 2003. Survival Analysis: Techniques for Censored and Truncated Data. Vol. 1230. Springer."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html",
    "href": "missing-data-material-W-26/homeworks/HW-1.html",
    "title": "HW 1",
    "section": "",
    "text": "In this question, you’ll be replicating some of the results of the Kang and Schafer (2007) paper and expanding on them. In the paper, the authors test the robustness of IPW, AIPW, and regression estimators via. a simulation study. The authors simulate a survey where an outcome \\(y_i\\) is measured for \\(n\\) individuals, but a substantial number of observations are missing (\\(\\sim 50\\%\\)). The target estimand is the population mean of \\(y_i\\), which is simulated to be \\(210\\). The \\(y_i\\) are simulated via a linear model conditional on covariates \\(Z_{i1}, Z_{i2}, Z_{i3}, Z_{i4}\\); the missingness indicators are simulated via a logistic regression model, again using a linear model related to the \\(Z\\) predictors.\nThe catch is that it is assumed that the analyst has access only to complex transformations of the predictors, in the form of \\(X_{i1}, X_{i2}, X_{i3}, X_{i4}\\). These transformed predictors can be used in a linear model to produce fits that are “good enough” meaning that they predict the outcomes pretty well and don’t indicate poor fits to the data.\nThe operating characteristics, namely bias, variance and MSE are measured for several different estimators that involve linear models and logistic regressions for \\(y_i \\mid x_i\\) and \\(m_i \\mid x_i\\). Let \\(e(x_i, \\hat{\\gamma})\\) be the fitted probabilities of not being missing, or \\(P(M_i = 0 \\mid X_i = x_i)\\) from the missingness model and let \\(\\mu(x_i, \\hat{\\beta})\\) be the fitted values from a linear regression of \\(y_i\\) on \\(x_i\\).\n\nAn IPW estimator: \\[\n\\bar{y}^{\\text{IPW}} = \\frac{\\sum_{i \\mid m_i = 0} y_i e(x_i, \\hat{\\gamma})^{-1}}{\\sum_{i \\mid m_i = 0} e(x_i, \\hat{\\gamma})^{-1}}\n\\]\nA regression estimator: \\(\\bar{y}^{\\text{reg}} = \\frac{1}{n} \\sum_{i=1}^n \\mu(x_i, \\hat{\\beta})\\)\nAn AIPW estimator: \\(\\bar{y}^{\\text{AIPW}} =  \\bar{y}^{\\text{reg}} +  \\frac{\\sum_{i \\mid m_i = 0}\\hat{\\epsilon}_i e(x_i, \\hat{\\gamma})^{-1}}{\\sum_{i \\mid m_i = 0} e(x_i, \\hat{\\gamma})^{-1}}\\)\n\nThe \\(\\hat{\\epsilon}_i\\) is the fitted residual from the regression on \\(i\\) such that \\(m_i = 0\\).\nI want you to roughly recreate the results of part of the paper by running the smallest simulation study scenario. I’ve written the simulation function for you, you have to write the code to run the regressions and the code to compute the estimators.\n\ngen_data &lt;- function(n=200) {\n  Z &lt;- matrix(rnorm(n * 4), n, 4)\n  y_hat &lt;- 210 + 27.4 * Z[,1] + 13.7 * rowSums(Z[,2:4])\n  y &lt;- y_hat + rnorm(n)\n  pi_y &lt;- plogis(-Z[,1] + 0.5 * Z[,2] - 0.25 * Z[,3] - 0.1 * Z[,4])\n  m &lt;- rbinom(n, 1, 1 - pi_y)\n  X &lt;- Z\n  X[,1] &lt;- exp(Z[,1] / 2)\n  X[,2] &lt;- Z[,2] / (1 + exp(Z[,1])) + 10\n  X[,3] &lt;- (Z[,1] * Z[,3] / 25 + 0.6)^3\n  X[,4] &lt;- (Z[,2] + Z[,4] + 20)^2\n  y_o &lt;- y\n  y_o[m == 1] &lt;- NA\n  df &lt;- data.frame(\n    y = y_o,\n    m = m,\n    x1 = X[,1],\n    x2 = X[,2],\n    x3 = X[,3],\n    x4 = X[,4]\n  )\n  return(df)\n}\n\n\n\nWrite functions that can take the data frame returned by gen_data and return a linear model for \\(\\Exp{Y_i \\mid X_i}\\) and a logistic regression for \\(P(M_i = 0 \\mid X_i)\\).\n\n\n\nWrite functions that compute \\(\\bar{y}^{\\text{IPW}},\\bar{y}^{\\text{AIPW}},\\bar{y}^{\\text{reg}}\\). These functions should take a model fit and a datatset as arguments to return the estimators.\n\n\n\nGenerate 1000 simulated datasets and use the functions you wrote above to compute the three estimators for each dataset\n\n\n\nCompute the bias, root mean square error, and % bias as a function of standard deviation of the estimator.\n\n\n\nPick your favorite machine learning method (random forests, support vector machines, gradient boosting machines, xgboost, etc.) and use your method to compute nonparametric versions of \\(\\mu(x_i, \\beta)\\) and \\(e(x_i, \\gamma)\\). Recompute the estimators using your machine learning methods and compute the same summary statistics as you did in question 1d\n\n\n\nWrite several sentences summarizing your results"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1a",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1a",
    "title": "HW 1",
    "section": "",
    "text": "Write functions that can take the data frame returned by gen_data and return a linear model for \\(\\Exp{Y_i \\mid X_i}\\) and a logistic regression for \\(P(M_i = 0 \\mid X_i)\\)."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1b",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1b",
    "title": "HW 1",
    "section": "",
    "text": "Write functions that compute \\(\\bar{y}^{\\text{IPW}},\\bar{y}^{\\text{AIPW}},\\bar{y}^{\\text{reg}}\\). These functions should take a model fit and a datatset as arguments to return the estimators."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1c",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1c",
    "title": "HW 1",
    "section": "",
    "text": "Generate 1000 simulated datasets and use the functions you wrote above to compute the three estimators for each dataset"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1d",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1d",
    "title": "HW 1",
    "section": "",
    "text": "Compute the bias, root mean square error, and % bias as a function of standard deviation of the estimator."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1e",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1e",
    "title": "HW 1",
    "section": "",
    "text": "Pick your favorite machine learning method (random forests, support vector machines, gradient boosting machines, xgboost, etc.) and use your method to compute nonparametric versions of \\(\\mu(x_i, \\beta)\\) and \\(e(x_i, \\gamma)\\). Recompute the estimators using your machine learning methods and compute the same summary statistics as you did in question 1d"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1f",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1f",
    "title": "HW 1",
    "section": "",
    "text": "Write several sentences summarizing your results"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-3a",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-3a",
    "title": "HW 1",
    "section": "2.1 Question 3a",
    "text": "2.1 Question 3a\nSummarize the missingness patterns in the hw_data dataset. How many missingness patterns are there? What are they? What proportion of patients are associated with each missingness pattern?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-3b",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-3b",
    "title": "HW 1",
    "section": "2.2 Question 3b",
    "text": "2.2 Question 3b\nHow would you assess whether there is evidence that treatment affects missingness? Is there evidence that treatment affects the missingness pattern?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-3c",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-3c",
    "title": "HW 1",
    "section": "2.3 Question 3c",
    "text": "2.3 Question 3c\nHow many people dropped out of the study after Week 1, or Week 4 vs. had intermittent missingness? For patients who dropped out, is there evidence that PANSS at the prior measurement predicted dropout? What about for the patients with intermittent missingness?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-3d",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-3d",
    "title": "HW 1",
    "section": "2.4 Question 3d",
    "text": "2.4 Question 3d\nIs it reasonable to assume that missingness is MCAR for this dataset? Why or why not? What about MAR?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-2a",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-2a",
    "title": "HW 1",
    "section": "2.1 Question 2a",
    "text": "2.1 Question 2a\nSummarize the missingness patterns in the hw_data dataset. How many missingness patterns are there? What are they? What proportion of patients are associated with each missingness pattern?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-2b",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-2b",
    "title": "HW 1",
    "section": "2.2 Question 2b",
    "text": "2.2 Question 2b\nHow would you assess whether there is evidence that treatment affects missingness? Is there evidence that treatment affects the missingness pattern?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-2c",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-2c",
    "title": "HW 1",
    "section": "2.3 Question 2c",
    "text": "2.3 Question 2c\nHow many people dropped out of the study after Week 1, or Week 4 vs. had intermittent missingness? For patients who dropped out, is there evidence that PANSS at the prior measurement predicted dropout? What about for the patients with intermittent missingness?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-2d",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-2d",
    "title": "HW 1",
    "section": "2.4 Question 2d",
    "text": "2.4 Question 2d\nIs it reasonable to assume that missingness is MCAR for this dataset? Why or why not? What about MAR?"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-4-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-4-notes.html",
    "title": "Missing data lecture 4",
    "section": "",
    "text": "This bit of the notes follows 6.1 pretty closely.\nFor the moment, let’s forget about missing data, and move to the simpler case where we have no missing observations and we’re just focused on learning the unknown parameters \\(\\theta\\) in the density \\(f_Y(y_i \\mid \\theta)\\). This unknown parameter is going to live in the space \\(\\Omega_\\theta\\). For example, if \\(\\theta = (\\mu, \\sigma^2)\\) and \\(f_Y(y_i \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2} (y_i - \\mu)^2}\\), then \\(\\Omega_\\theta\\) would be \\((\\R, \\R^+)\\).\nWe know densities are functions of \\(y_i\\) for fixed values of \\(\\theta\\). If we instead fix \\(y_i\\) and let \\(\\theta\\) vary, we get a likelihood function.\nLet the likelihood be defined for a fixed \\(y_i\\) as \\[\nL_Y(\\theta \\mid y_i) \\propto \\begin{cases}\nf(y_i \\mid \\theta) & \\theta \\in \\Omega_\\theta \\\\\n0 & \\theta \\not\\in \\Omega_\\theta\n\\end{cases}\n\\] This is a bit odd, because the expression means that anything proportional to the density of \\(Y\\) such that the factor by which \\(L_Y(\\theta \\mid y_i)\\) differs from \\(f(y_i \\mid \\theta)\\) is constant in \\(\\theta\\).\nTypically, we’ll have more than one observation, and under independence of \\(y_i\\) we get the likelihood for the full sample: \\[\nL_Y(\\theta \\mid y_1, \\dots, y_n) \\propto \\begin{cases}\n\\prod_{i=1}^n f(y_i \\mid \\theta) & \\theta \\in \\Omega_\\theta \\\\\n0 & \\theta \\not\\in \\Omega_\\theta\n\\end{cases}\n\\] We’ll often work with the log-likelihood, which is denoted in our book as: \\[\n\\ell_Y(\\theta \\mid y_1, \\dots, y_n) = \\log(L_Y(\\theta \\mid y_1, \\dots, y_n))\n\\] When we have independence, we get\n\\[\n\\ell_Y(\\theta \\mid y_1, \\dots, y_n) = \\sum_{i=1}^n \\log f(y_i \\mid \\theta) + C\n\\] where \\(C\\) is any term that doesn’t depend on \\(\\theta\\).\nMoving forward with the normal example from above, the likelihood of \\(n\\) observations from the normal distribution is: \\[\n\\begin{aligned}\nL_Y(\\mu, \\sigma^2 \\mid y_1, \\dots, y_n) & = \\prod_{i=1}^n  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2} (y_i - \\mu)^2} \\\\\n& = (2\\pi \\sigma^2)^{-\\frac{n}{2}} \\exp \\lp -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\\rp\n\\end{aligned}\n\\] Our definition of likelihood means that we can drop the factor of \\((2\\pi)^{-\\frac{n}{2}}\\) from the front of the expression. Taking logs and dropping that constant leads to the log-likelihood:\n\\[\n\\ell_Y(\\mu, \\sigma^2) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\n\\] which is a bivariate function of \\(\\mu\\) and \\(\\sigma^2\\).\nThe way that we’ll use the likelihood is to use the intuition that for two parameter values \\(\\theta^\\prime\\) and \\(\\theta^{\\prime\\prime}\\) if \\(L(\\theta^\\prime \\mid y) = 2L(\\theta^{\\prime\\prime} \\mid y)\\), then there is evidence against \\(\\theta^{\\prime\\prime}\\) being the parameter that generated the dataset.\nIf we were to find a \\(\\hat{\\theta}\\) such that \\(L_Y(\\hat{\\theta} \\mid y) \\geq L_Y(\\theta^* \\mid y)\\) for all \\(\\theta* \\neq \\hat{\\theta}\\), then this would be evidence against \\(\\theta\\) being anything other than \\(\\hat{theta}\\). This is the intuition behind maximum likelihood, or finding the value of the unknown parameters \\(\\theta\\) that maximizes the likelihood function (or, equivalently, the log-likelihood).\nWe’ll say that the maximum likelihood estimator (MLE) of \\(\\theta\\) is the value \\(\\hat{\\theta} \\in \\Omega_\\theta\\) that maximizes the log-likelihood.\nIn order to maximize the likelihood, we need to find the point at which the gradient of the log-likelihood, also known as the score function, is zero, or \\(\\frac{\\partial\\ell_Y(\\theta \\mid y)}{\\partial \\theta}\\mid_{\\theta = \\hat{\\theta}} = 0\\). If \\(\\theta\\) is \\(d\\)-dimensional, then there are \\(d\\) equations that need to be solved. In addition, the Hessian of the log-likelihood needs to be checked to see if it is negative semidefinite at the MLE: \\[\nz^T \\frac{\\partial^2 \\ell_Y(\\theta \\mid y)}{\\partial \\theta \\,\\partial \\theta^T}\\mid_{\\theta = \\hat{\\theta}} z \\leq 0 \\,\\forall\\, z \\in \\R^d\n\\] This ensures that we’ve found a maximum. Note that we can have several maxima, all of which lead to the same log-likelihood value.\n\n\nLet’s say that \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Exp}(\\lambda)\\). Then the likelihood will be \\[\nL(\\lambda \\mid y_1, \\dots, y_n) = \\lambda^{-n} e^{-\\frac{1}{\\lambda}\\sum_i y_i}\n\\] Then the score equation is:\n\\[\n-\\frac{n}{\\lambda} + \\sum_i \\frac{y_i}{\\lambda^2} = 0\n\\] This leads to \\(\\hat{\\lambda} = \\bar{y}\\) or the sample mean.\n\n\n\nSuppose \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\sigma^2)\\) for \\(i = 1, \\dots, n\\).\nWe wrote the log-likelihood above as: \\[\n\\ell_Y(\\mu, \\sigma^2) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\n\\]\nThis leads to two likelihood equations:\n\\[\n\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial \\mu } & = \\frac{1}{\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2 \\\\\n\\frac{\\partial \\ell}{\\partial \\sigma^2 } & =-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\textstyle\\sum_i(y_i - \\mu)^2\n\\end{aligned}\n\\] Setting these to zero and solving for \\((\\mu, \\sigma^2)\\) gives the MLEs \\(\\hat{\\mu} = \\bar{y}\\) and \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_i (y_i - \\bar{y})^2\\).\nIt’s straightforward to show that \\(\\Exp{\\hat{\\mu}}\\) under the data generating process above is equal to \\(\\mu\\). It is less straightforward and somewhat distressing that \\(\\Exp{\\hat{\\sigma^2}}\\) not equal to \\(\\sigma^2\\).\n\\[\n\\begin{aligned}\n\\Exp{\\hat{\\sigma^2}} & = \\frac{1}{n}\\Exp{\\sum_i (y_i^2 - 2 y_i \\bar{y} + \\bar{y}^2)} \\\\\n& = \\frac{1}{n}\\lp \\Exp{\\textstyle\\sum_i y_i^2 - 2 n \\bar{y}^2 + n\\bar{y}^2 \\rp}\\\\\n& = \\frac{1}{n}\\textstyle\\sum_i \\Exp{y_i^2} - \\Exp{\\bar{y}^2} \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2}\\Exp{\\textstyle \\sum_i y_i^2 + 2 \\sum_{i&lt;j} y_i y_j} \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2}\\lp \\textstyle \\sum_i \\Exp{y_i^2} + 2 \\sum_{i&lt;j} \\Exp{y_i y_j}\\rp \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2} \\lp n (\\mu^2 + \\sigma^2) + n(n-1) \\mu^2\\rp \\\\\n& = \\mu^2 + \\sigma^2 - \\mu^2 - \\frac{1}{n}\\sigma^2 \\\\\n& = \\frac{n-1}{n} \\sigma^2\n\\end{aligned}\n\\] Sorta odd that MLEs can give us biased estimates, but, you might say, as \\(n\\to\\infty\\) all is fine and we recover \\(\\sigma^2\\). Indeed, you can show that the MLE for \\(\\sigma^2\\) is , which means that as \\(n\\to\\infty\\) \\(\\hat{\\sigma}^2 \\to \\sigma\\) in probability.\n\n\n\nIs this always the case, that the MLE is consistent? The answer is no, and the MLE can fail pretty spectacularly. Consider the following problem:\n\\[\ny_{i1}, y_{i2} \\sim \\text{Normal}(\\mu_i, \\sigma^2)\n\\] Let’s say that we don’t care about inferring \\(\\mu_i\\) but we care only about \\(\\sigma^2\\). One way we could come up with an estimator is to difference the two observations for each group, so \\(z_i = y_{i1} - y_{i2}\\) and : \\[\nz_i \\sim \\text{Normal}(0, 2\\sigma^2)\n\\] Then we could reuse our work from above to solve for \\(\\sigma^2\\):\n\\[\n\\begin{aligned}\n2 \\sigma^2 & = \\frac{1}{n} \\textstyle \\sum_i z_i^2 \\\\\n\\end{aligned}\n\\] which leads to an estimator for \\(\\sigma^2\\) of: \\[\n\\hat{\\sigma}^2 = \\frac{1}{2n} \\textstyle \\sum_i z_i^2\n\\]\nBut this isn’t quite maximum likelihood because we’ve transformed the data, and then used the transformed data to derive an MLE for \\(\\sigma^2\\) using \\(z_i\\). What if we just use \\(y_{i1},y_{i2}\\)?\nThe likelihood is straightforward:\n\\[\nL_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y) = \\prod_{i=1}^n \\frac{1}{2 \\pi \\sigma^2} \\exp \\lp -\\frac{1}{2\\sigma^2} \\lp (y_{i1} - \\mu_i)^2 + (y_{i2} - \\mu_i)^2 \\rp \\rp\n\\]\nThe likelihood equations for \\(\\mu_i\\) are \\[\n\\frac{\\partial \\ell_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y)}{\\partial \\mu_i} = \\frac{1}{\\sigma^2}\\lp (y_{i1} - \\mu_i) + (y_{i2} - \\mu_i) \\rp\n\\] Which just leads to the estimator \\(\\hat{\\mu}_i = \\frac{y_{i1} + y_{i2}}{2} = \\bar{y}_i\\).\nLet’s write out the likelihood equations for \\(\\sigma^2\\) after plugging in our \\(\\hat{\\mu}_i = \\bar{y}_i\\): \\[\n\\frac{\\partial \\ell_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y)}{\\partial \\sigma^2} = -\\frac{n}{\\sigma^2} + \\frac{1}{2 \\sigma^2}\\textstyle\\sum_i \\sum_{j=1}^2 (y_{ij} - \\bar{y}_i)^2\n\\] This leads to an MLE of\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{2n}\\textstyle\\sum_i \\sum_{j=1}^2 (y_{ij} - \\bar{y}_i)^2\n\\] Which seems reasonable enough. But let’s write the inner sum in terms of \\(z_i\\): \\[\n\\begin{aligned}\n(y_{i1} - \\bar{y}_i)^2 + (y_{i2} - \\bar{y}_i)^2 & = y_{i1}^2 + y_{i2}^2 + 2\\bar{y}_i^2 - 2 \\bar{y_i}(y_{i1} + y_{i2}) \\\\\n& = y_{i1}^2 + y_{i2}^2 + \\frac{(y_{i1} + y_{i2})^2}{2} - (y_{i1} + y_{i2})^2 \\\\\n& = y_{i1}^2 + y_{i2}^2 - \\frac{(y_{i1} + y_{i2})^2}{2} \\\\\n& = \\frac{1}{2}(y_{i1}^2 + y_{i2}^2 - 2 y_{i1}y_{i2}) \\\\\n& = \\frac{1}{2} z_i^2\n\\end{aligned}\n\\]\nThis means that the MLE is equal to \\[\n\\hat{\\sigma}^2 = \\frac{1}{4n}\\textstyle\\sum_i z_i^2\n\\] This estimator will never approach \\(\\sigma^2\\), as \\(n\\to\\infty\\).\nThis is due to the fact that the dimension of the parameter space grows linearly with the sample size as \\(n\\to\\infty\\).\nThe point is that MLEs (and any other sort of likelihood-based inference) can lead you astray if you’re not careful, and they are not a panacea."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-4-notes.html#likelihood-based-inference",
    "href": "missing-data-material-W-26/notes/lecture-4-notes.html#likelihood-based-inference",
    "title": "Missing data lecture 4",
    "section": "",
    "text": "This bit of the notes follows 6.1 pretty closely.\nFor the moment, let’s forget about missing data, and move to the simpler case where we have no missing observations and we’re just focused on learning the unknown parameters \\(\\theta\\) in the density \\(f_Y(y_i \\mid \\theta)\\). This unknown parameter is going to live in the space \\(\\Omega_\\theta\\). For example, if \\(\\theta = (\\mu, \\sigma^2)\\) and \\(f_Y(y_i \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2} (y_i - \\mu)^2}\\), then \\(\\Omega_\\theta\\) would be \\((\\R, \\R^+)\\).\nWe know densities are functions of \\(y_i\\) for fixed values of \\(\\theta\\). If we instead fix \\(y_i\\) and let \\(\\theta\\) vary, we get a likelihood function.\nLet the likelihood be defined for a fixed \\(y_i\\) as \\[\nL_Y(\\theta \\mid y_i) \\propto \\begin{cases}\nf(y_i \\mid \\theta) & \\theta \\in \\Omega_\\theta \\\\\n0 & \\theta \\not\\in \\Omega_\\theta\n\\end{cases}\n\\] This is a bit odd, because the expression means that anything proportional to the density of \\(Y\\) such that the factor by which \\(L_Y(\\theta \\mid y_i)\\) differs from \\(f(y_i \\mid \\theta)\\) is constant in \\(\\theta\\).\nTypically, we’ll have more than one observation, and under independence of \\(y_i\\) we get the likelihood for the full sample: \\[\nL_Y(\\theta \\mid y_1, \\dots, y_n) \\propto \\begin{cases}\n\\prod_{i=1}^n f(y_i \\mid \\theta) & \\theta \\in \\Omega_\\theta \\\\\n0 & \\theta \\not\\in \\Omega_\\theta\n\\end{cases}\n\\] We’ll often work with the log-likelihood, which is denoted in our book as: \\[\n\\ell_Y(\\theta \\mid y_1, \\dots, y_n) = \\log(L_Y(\\theta \\mid y_1, \\dots, y_n))\n\\] When we have independence, we get\n\\[\n\\ell_Y(\\theta \\mid y_1, \\dots, y_n) = \\sum_{i=1}^n \\log f(y_i \\mid \\theta) + C\n\\] where \\(C\\) is any term that doesn’t depend on \\(\\theta\\).\nMoving forward with the normal example from above, the likelihood of \\(n\\) observations from the normal distribution is: \\[\n\\begin{aligned}\nL_Y(\\mu, \\sigma^2 \\mid y_1, \\dots, y_n) & = \\prod_{i=1}^n  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2} (y_i - \\mu)^2} \\\\\n& = (2\\pi \\sigma^2)^{-\\frac{n}{2}} \\exp \\lp -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\\rp\n\\end{aligned}\n\\] Our definition of likelihood means that we can drop the factor of \\((2\\pi)^{-\\frac{n}{2}}\\) from the front of the expression. Taking logs and dropping that constant leads to the log-likelihood:\n\\[\n\\ell_Y(\\mu, \\sigma^2) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\n\\] which is a bivariate function of \\(\\mu\\) and \\(\\sigma^2\\).\nThe way that we’ll use the likelihood is to use the intuition that for two parameter values \\(\\theta^\\prime\\) and \\(\\theta^{\\prime\\prime}\\) if \\(L(\\theta^\\prime \\mid y) = 2L(\\theta^{\\prime\\prime} \\mid y)\\), then there is evidence against \\(\\theta^{\\prime\\prime}\\) being the parameter that generated the dataset.\nIf we were to find a \\(\\hat{\\theta}\\) such that \\(L_Y(\\hat{\\theta} \\mid y) \\geq L_Y(\\theta^* \\mid y)\\) for all \\(\\theta* \\neq \\hat{\\theta}\\), then this would be evidence against \\(\\theta\\) being anything other than \\(\\hat{theta}\\). This is the intuition behind maximum likelihood, or finding the value of the unknown parameters \\(\\theta\\) that maximizes the likelihood function (or, equivalently, the log-likelihood).\nWe’ll say that the maximum likelihood estimator (MLE) of \\(\\theta\\) is the value \\(\\hat{\\theta} \\in \\Omega_\\theta\\) that maximizes the log-likelihood.\nIn order to maximize the likelihood, we need to find the point at which the gradient of the log-likelihood, also known as the score function, is zero, or \\(\\frac{\\partial\\ell_Y(\\theta \\mid y)}{\\partial \\theta}\\mid_{\\theta = \\hat{\\theta}} = 0\\). If \\(\\theta\\) is \\(d\\)-dimensional, then there are \\(d\\) equations that need to be solved. In addition, the Hessian of the log-likelihood needs to be checked to see if it is negative semidefinite at the MLE: \\[\nz^T \\frac{\\partial^2 \\ell_Y(\\theta \\mid y)}{\\partial \\theta \\,\\partial \\theta^T}\\mid_{\\theta = \\hat{\\theta}} z \\leq 0 \\,\\forall\\, z \\in \\R^d\n\\] This ensures that we’ve found a maximum. Note that we can have several maxima, all of which lead to the same log-likelihood value.\n\n\nLet’s say that \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Exp}(\\lambda)\\). Then the likelihood will be \\[\nL(\\lambda \\mid y_1, \\dots, y_n) = \\lambda^{-n} e^{-\\frac{1}{\\lambda}\\sum_i y_i}\n\\] Then the score equation is:\n\\[\n-\\frac{n}{\\lambda} + \\sum_i \\frac{y_i}{\\lambda^2} = 0\n\\] This leads to \\(\\hat{\\lambda} = \\bar{y}\\) or the sample mean.\n\n\n\nSuppose \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\sigma^2)\\) for \\(i = 1, \\dots, n\\).\nWe wrote the log-likelihood above as: \\[\n\\ell_Y(\\mu, \\sigma^2) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\n\\]\nThis leads to two likelihood equations:\n\\[\n\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial \\mu } & = \\frac{1}{\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2 \\\\\n\\frac{\\partial \\ell}{\\partial \\sigma^2 } & =-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\textstyle\\sum_i(y_i - \\mu)^2\n\\end{aligned}\n\\] Setting these to zero and solving for \\((\\mu, \\sigma^2)\\) gives the MLEs \\(\\hat{\\mu} = \\bar{y}\\) and \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_i (y_i - \\bar{y})^2\\).\nIt’s straightforward to show that \\(\\Exp{\\hat{\\mu}}\\) under the data generating process above is equal to \\(\\mu\\). It is less straightforward and somewhat distressing that \\(\\Exp{\\hat{\\sigma^2}}\\) not equal to \\(\\sigma^2\\).\n\\[\n\\begin{aligned}\n\\Exp{\\hat{\\sigma^2}} & = \\frac{1}{n}\\Exp{\\sum_i (y_i^2 - 2 y_i \\bar{y} + \\bar{y}^2)} \\\\\n& = \\frac{1}{n}\\lp \\Exp{\\textstyle\\sum_i y_i^2 - 2 n \\bar{y}^2 + n\\bar{y}^2 \\rp}\\\\\n& = \\frac{1}{n}\\textstyle\\sum_i \\Exp{y_i^2} - \\Exp{\\bar{y}^2} \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2}\\Exp{\\textstyle \\sum_i y_i^2 + 2 \\sum_{i&lt;j} y_i y_j} \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2}\\lp \\textstyle \\sum_i \\Exp{y_i^2} + 2 \\sum_{i&lt;j} \\Exp{y_i y_j}\\rp \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2} \\lp n (\\mu^2 + \\sigma^2) + n(n-1) \\mu^2\\rp \\\\\n& = \\mu^2 + \\sigma^2 - \\mu^2 - \\frac{1}{n}\\sigma^2 \\\\\n& = \\frac{n-1}{n} \\sigma^2\n\\end{aligned}\n\\] Sorta odd that MLEs can give us biased estimates, but, you might say, as \\(n\\to\\infty\\) all is fine and we recover \\(\\sigma^2\\). Indeed, you can show that the MLE for \\(\\sigma^2\\) is , which means that as \\(n\\to\\infty\\) \\(\\hat{\\sigma}^2 \\to \\sigma\\) in probability.\n\n\n\nIs this always the case, that the MLE is consistent? The answer is no, and the MLE can fail pretty spectacularly. Consider the following problem:\n\\[\ny_{i1}, y_{i2} \\sim \\text{Normal}(\\mu_i, \\sigma^2)\n\\] Let’s say that we don’t care about inferring \\(\\mu_i\\) but we care only about \\(\\sigma^2\\). One way we could come up with an estimator is to difference the two observations for each group, so \\(z_i = y_{i1} - y_{i2}\\) and : \\[\nz_i \\sim \\text{Normal}(0, 2\\sigma^2)\n\\] Then we could reuse our work from above to solve for \\(\\sigma^2\\):\n\\[\n\\begin{aligned}\n2 \\sigma^2 & = \\frac{1}{n} \\textstyle \\sum_i z_i^2 \\\\\n\\end{aligned}\n\\] which leads to an estimator for \\(\\sigma^2\\) of: \\[\n\\hat{\\sigma}^2 = \\frac{1}{2n} \\textstyle \\sum_i z_i^2\n\\]\nBut this isn’t quite maximum likelihood because we’ve transformed the data, and then used the transformed data to derive an MLE for \\(\\sigma^2\\) using \\(z_i\\). What if we just use \\(y_{i1},y_{i2}\\)?\nThe likelihood is straightforward:\n\\[\nL_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y) = \\prod_{i=1}^n \\frac{1}{2 \\pi \\sigma^2} \\exp \\lp -\\frac{1}{2\\sigma^2} \\lp (y_{i1} - \\mu_i)^2 + (y_{i2} - \\mu_i)^2 \\rp \\rp\n\\]\nThe likelihood equations for \\(\\mu_i\\) are \\[\n\\frac{\\partial \\ell_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y)}{\\partial \\mu_i} = \\frac{1}{\\sigma^2}\\lp (y_{i1} - \\mu_i) + (y_{i2} - \\mu_i) \\rp\n\\] Which just leads to the estimator \\(\\hat{\\mu}_i = \\frac{y_{i1} + y_{i2}}{2} = \\bar{y}_i\\).\nLet’s write out the likelihood equations for \\(\\sigma^2\\) after plugging in our \\(\\hat{\\mu}_i = \\bar{y}_i\\): \\[\n\\frac{\\partial \\ell_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y)}{\\partial \\sigma^2} = -\\frac{n}{\\sigma^2} + \\frac{1}{2 \\sigma^2}\\textstyle\\sum_i \\sum_{j=1}^2 (y_{ij} - \\bar{y}_i)^2\n\\] This leads to an MLE of\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{2n}\\textstyle\\sum_i \\sum_{j=1}^2 (y_{ij} - \\bar{y}_i)^2\n\\] Which seems reasonable enough. But let’s write the inner sum in terms of \\(z_i\\): \\[\n\\begin{aligned}\n(y_{i1} - \\bar{y}_i)^2 + (y_{i2} - \\bar{y}_i)^2 & = y_{i1}^2 + y_{i2}^2 + 2\\bar{y}_i^2 - 2 \\bar{y_i}(y_{i1} + y_{i2}) \\\\\n& = y_{i1}^2 + y_{i2}^2 + \\frac{(y_{i1} + y_{i2})^2}{2} - (y_{i1} + y_{i2})^2 \\\\\n& = y_{i1}^2 + y_{i2}^2 - \\frac{(y_{i1} + y_{i2})^2}{2} \\\\\n& = \\frac{1}{2}(y_{i1}^2 + y_{i2}^2 - 2 y_{i1}y_{i2}) \\\\\n& = \\frac{1}{2} z_i^2\n\\end{aligned}\n\\]\nThis means that the MLE is equal to \\[\n\\hat{\\sigma}^2 = \\frac{1}{4n}\\textstyle\\sum_i z_i^2\n\\] This estimator will never approach \\(\\sigma^2\\), as \\(n\\to\\infty\\).\nThis is due to the fact that the dimension of the parameter space grows linearly with the sample size as \\(n\\to\\infty\\).\nThe point is that MLEs (and any other sort of likelihood-based inference) can lead you astray if you’re not careful, and they are not a panacea."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-5-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-5-notes.html",
    "title": "Missing data lecture 5",
    "section": "",
    "text": "Let \\(y_i \\in \\R^K\\), \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\Sigma)\\) for \\(n\\) samples so that the density for \\(y_i\\) is \\[\nf_{Y}(y_i \\mid \\mu, \\Sigma) = (2\\pi)^{-\\frac{K}{2}} (\\det\\Sigma)^{-\\frac{1}{2}} \\exp\\lp-\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu) \\rp\n\\]\nThe log-likelihood is: \\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = \\frac{1}{2} \\log \\det\\Sigma -\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu)\n\\]\nThe book gives the expressions for the MLEs of the mean and covariance matrix of the multivariate normal distribution without details. Going through the algebra can be useful for other more complicated problems. But in order to do so, we’ll need a slight change to how we’re used to thinking about partial differentiation. The following blurb on differentials is based on Magnus and Neudecker (2019)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-6-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-6-notes.html",
    "title": "Missing data lecture 6",
    "section": "",
    "text": "Last class we talked about MLEs for a simple repeated measure model:\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j\n\\end{aligned}\n\\] Let \\(y = (y_1^T, y_2^T, \\dots, y_n^T)^T\\) and let \\(X = (X_1^T, X_2^T, \\dots, X_n^T)^T\\), and let \\(\\epsilon = (\\epsilon_1^T, \\epsilon_2^T, \\dots, \\epsilon_n^T)^T\\). Then the model can be written: \\[\n\\begin{aligned}\ny \\mid X \\, & = X \\beta + \\epsilon \\\\\n\\epsilon & \\sim \\text{Normal}(0, I_n \\otimes \\Sigma)\n\\end{aligned}\n\\]\nWe showed that if \\(\\Sigma\\) were known, we could write the MLE for \\(\\beta\\) as: \\[\n\\hat{\\beta} = \\textstyle(\\sum_i X_i^T  \\Sigma^{-1} X)^{-1} (\\sum_i X_i \\Sigma^{-1} y_i)  \n\\] We can also show that the MLE for \\(\\Sigma\\) if \\(\\beta\\) were known is:\n\\[\n\\Sigma = \\frac{1}{n} \\sum_i (y_i - X_i \\beta) (y_i - X_i)^T\n\\] Combining these two fact together, we can iteratively maximize the MLE by doing:\n\\[\n\\begin{aligned}\n\\Sigma^{(t+1)} & = \\frac{1}{n} \\sum_i (y_i - X_i \\beta^{(t)}) (y_i - X_i \\beta)^T \\\\\n\\beta^{(t+1)} & = \\textstyle(\\sum_i X_i^T  (\\Sigma^{(t)})^{-1} X_i)^{-1} (\\sum_i X_i (\\Sigma^{(t)})^{-1} y_i)  \n\\end{aligned}\n\\] Which is similar to what the book has."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-6-notes.html#mles-in-repeated-measure-models",
    "href": "missing-data-material-W-26/notes/lecture-6-notes.html#mles-in-repeated-measure-models",
    "title": "Missing data lecture 6",
    "section": "",
    "text": "Last class we talked about MLEs for a simple repeated measure model:\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j\n\\end{aligned}\n\\] Let \\(y = (y_1^T, y_2^T, \\dots, y_n^T)^T\\) and let \\(X = (X_1^T, X_2^T, \\dots, X_n^T)^T\\), and let \\(\\epsilon = (\\epsilon_1^T, \\epsilon_2^T, \\dots, \\epsilon_n^T)^T\\). Then the model can be written: \\[\n\\begin{aligned}\ny \\mid X \\, & = X \\beta + \\epsilon \\\\\n\\epsilon & \\sim \\text{Normal}(0, I_n \\otimes \\Sigma)\n\\end{aligned}\n\\]\nWe showed that if \\(\\Sigma\\) were known, we could write the MLE for \\(\\beta\\) as: \\[\n\\hat{\\beta} = \\textstyle(\\sum_i X_i^T  \\Sigma^{-1} X)^{-1} (\\sum_i X_i \\Sigma^{-1} y_i)  \n\\] We can also show that the MLE for \\(\\Sigma\\) if \\(\\beta\\) were known is:\n\\[\n\\Sigma = \\frac{1}{n} \\sum_i (y_i - X_i \\beta) (y_i - X_i)^T\n\\] Combining these two fact together, we can iteratively maximize the MLE by doing:\n\\[\n\\begin{aligned}\n\\Sigma^{(t+1)} & = \\frac{1}{n} \\sum_i (y_i - X_i \\beta^{(t)}) (y_i - X_i \\beta)^T \\\\\n\\beta^{(t+1)} & = \\textstyle(\\sum_i X_i^T  (\\Sigma^{(t)})^{-1} X_i)^{-1} (\\sum_i X_i (\\Sigma^{(t)})^{-1} y_i)  \n\\end{aligned}\n\\] Which is similar to what the book has."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-6-notes.html#inference-for-mles",
    "href": "missing-data-material-W-26/notes/lecture-6-notes.html#inference-for-mles",
    "title": "Missing data lecture 6",
    "section": "Inference for MLEs",
    "text": "Inference for MLEs\nWe’ve shown that we can find a point in parameter space \\(\\hat{\\theta}\\) such that there is evidence against any other point \\(\\theta \\neq \\hat{\\theta}\\) being the parameter that generated the data under an assumed statistical model \\(f_{Y}(y_i \\mid \\theta)\\).\nHow do assess the uncertainty in this point estimate? One way to think about this is to consider the distribution of MLEs under different hypothetical datasets.\nIf we can characterize the distribution, we can build a confidence interval for \\(\\theta\\) that will contain the true value of \\(\\theta\\) for some prescribed proportion of our hypothetical datasets.\nConsider the model \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known one-dimensional normal model. Then we know that \\(\\hat{\\mu} = \\bar{y}\\), and that its distribution is \\(\\bar{y} \\sim \\text{Normal}(\\mu, \\frac{\\sigma^2}{n})\\). Using this distribution, we can construct a statistic whose distribution doesn’t depend on any unknown parameters. This is also known as a pivotal quantity. \\[\n\\sqrt{n}(\\bar{y} - \\mu)/\\sigma \\sim \\text{Normal}(0, 1)\n\\] We know the cumulative distribution function for \\(N(0,1)\\), \\(\\Phi(x)\\) and we can use this distribution to build a confidence interval for \\(\\mu\\): \\[\nP(z_{\\alpha/2} &lt; \\sqrt{n}(\\bar{y} - \\mu)/\\sigma &lt; z_{1-\\alpha/2})\n\\] where \\(z_{p} = \\Phi^{-1}(p)\\) and \\(\\alpha\\) is usually \\(0.05\\). Now we solve the system of inequalities for \\(\\mu\\) to get our \\(1 - \\alpha\\) confidence interval:\n\\[\n\\begin{aligned}\nP(z_{\\alpha/2} &lt; \\sqrt{n}(\\bar{y} - \\mu)/\\sigma &lt; z_{1-\\alpha/2}) & = P(\\frac{\\sigma}{\\sqrt{n}} z_{\\alpha/2} &lt; \\bar{y} - \\mu  &lt; \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2}) \\\\\n& = P(\\frac{\\sigma}{\\sqrt{n}} z_{\\alpha/2} - \\bar{y} &lt; -\\mu  &lt; -\\bar{y} + \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2}) \\\\\n& = P(\\bar{y} - \\frac{\\sigma}{\\sqrt{n}} z_{\\alpha/2} &gt; \\mu  &gt; \\bar{y} - \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2}) \\\\\n\\end{aligned}\n\\] Because the distribution is symmetric, \\(z_{\\alpha/2} = -z_{1-\\alpha/2}\\) which gives us: \\[\nP(\\bar{y} - \\frac{\\sigma}{\\sqrt{n}} z_{1-\\alpha/2} &lt; \\mu  &lt; \\bar{y} + \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2})\n\\] Then the interval \\((\\bar{y} - \\frac{\\sigma}{\\sqrt{n}} z_{1-\\alpha/2},  \\bar{y} + \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2})\\) will contain \\(\\mu\\) in \\(0.95\\) of datasets generated under the assumed \\(N(\\mu, \\sigma^2)\\).\nFor all but the simplest models, the sampling distribution is intractable, but we can approximate the distribution using asymptotics.\nWe’ll need the multivariate central limit theorem, which we’ll just take as a given:\nWe can use this idea to get a pivotal quantity that involves the MLE for \\(\\theta\\) and the asymptotic variance covariance matrix of the MLE.\nLet the log-likelihood be denoted \\(\\ell(\\theta)\\), in which we suppress the dependence of the likelihood on the data. If we need to indicate the dependence on data, we’ll write it as \\(\\ell(\\theta; y)\\). Finally, denote the gradient of the log-likelihood with respect to \\(\\theta\\) evaluated at \\(\\theta^\\prime\\) as: \\[\n\\ell_{\\theta}(\\theta^\\prime; y) \\equiv \\nabla_\\theta \\log f_\\theta(y)\\mid_{\\theta = \\theta^\\prime}\n\\] and the Hessian of the log-likelihood (i.e. the matrix of second derivatives of the log-likelihood) be: \\[\n\\ell_{\\theta\\theta}(\\theta^\\prime; y) \\equiv \\nabla^2_\\theta \\log f_\\theta(y)\\mid_{\\theta = \\theta^\\prime}\n\\] Given that \\(\\theta \\in \\R^p\\), we’ll denote an element of the vector \\(\\ell_\\theta(\\theta^\\prime)\\) as \\((\\ell_\\theta(\\theta^\\prime))_j\\) and a row of \\(\\ell_{\\theta\\theta}(\\theta^\\prime; y)\\) as \\((\\ell_{\\theta\\theta}(\\theta^\\prime; y))_j\\).\nWe’ll start with some key assumptions:\n\nThe MLE is consistent for \\(\\theta\\), which means that as we collect more samples the MLE with converge in probability to \\(\\theta\\).\n\nThis will rule out the Neyman-Scott problem.\n\n\\(y_i\\) are iid with density \\(f_Y(y_i \\mid \\theta)\\), \\(\\theta \\subseteq \\R^d\\)\nThe support of the random variable \\(y_i\\) doesn’t depend on \\(\\theta\\).\n\nIn our earlier presentation of how things can go wrong with MLE, having support that depends on the value of the parameter can make things go awry, so we’ll assume that we’re not in that scenario (like the \\(y_i \\sim \\text{Uniform}(0, \\theta)\\)).\n\nThe true parameter \\(\\theta\\) is in the interior of the parameter space. This ensures that the gradient of the likelihood value at the maximizer \\(\\hat{\\theta}\\) will be zero.\nLocal uniform continuity in \\(\\theta\\) of second derivatives for all \\(j\\): \\[\n\\sup_{\\norm{\\theta^\\prime - \\theta^\\dagger} \\leq r} \\norm{(\\ell_{\\theta\\theta}(\\theta^\\prime; x))_j - (\\ell_{\\theta\\theta}(\\theta^\\dagger; x))_j} \\leq H_r(x, \\theta^\\dagger),\\, \\lim_{r\\to 0}\\Exp{H_r(X, \\theta^\\dagger)} = 0\n\\]\n\nThe following proof is cobbled together from Schervish (2012) and Lehmann and Casella (1998).\nThe gradient of the log-likelihood evaluated at the MLE \\(\\hat{\\theta}\\) can be expanded around the true parameter value \\(\\theta^\\dagger\\): \\[\n(\\ell_\\theta(\\hat{\\theta}))_j = (\\ell_\\theta(\\theta^\\dagger))_j + (\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j(\\hat{\\theta} + \\theta)\n\\] where \\(\\tilde{\\theta}_j \\in \\R^p\\) and lies on the cord between \\(\\hat{\\theta}\\) and \\(\\theta^\\dagger\\) and may differ by the row \\(j\\) of \\(\\ell_\\theta(\\hat{\\theta})\\) of which we’re expanding.\nWe multiply each side by \\(\\frac{\\sqrt{n}}{n}\\):\n\\[\n\\frac{\\sqrt{n}}{n}(\\ell_\\theta(\\hat{\\theta}))_j = \\frac{1}{n}(\\ell_\\theta(\\theta^\\dagger))_j + (\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j\\sqrt{n}(\\hat{\\theta} + \\theta)\n\\]\nWe can show the following given assumption 5: \\[\n\\frac{1}{n}(\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j \\overset{p}{\\to}\\Exp{(\\ell_{\\theta\\theta}(\\theta^\\dagger; Y))_j} \\forall j\n\\] \\[\n\\begin{aligned}\n\\frac{1}{n}(\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j & = \\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j + \\frac{1}{n}\\lp(\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j - (\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\rp\n\\end{aligned}\n\\] In order to show that \\(\\Delta_{n,j} = \\frac{1}{n}(\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j - (\\ell_{\\theta\\theta}(\\theta^\\dagger))_j \\overset{p}{\\to} 0\\), we need to show that \\(P(\\Delta_{n,j} &gt; \\epsilon) \\overset{n\\to\\infty}{\\to} 0\\).\n\\(\\Delta_{n,j} &gt; \\epsilon\\) when \\(\\hat{\\theta}_{n}\\) is not in \\(B_r(\\theta^\\dagger)\\), or the ball centered at \\(\\theta^\\dagger\\) of size \\(r\\) or when \\(\\frac{1}{n} \\sum_{i=1}^n H_r(x_i,\\theta^\\dagger) &gt; \\epsilon\\). If we pick \\(r\\) small enough, we can ensure \\(\\Exp{H_r(X, \\theta^\\dagger)} &lt; \\epsilon / 2\\).\nThen \\[\n\\begin{aligned}\n\\Prob{\\Delta_{n,j} &gt; \\epsilon}{\\theta^\\dagger} & \\leq \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r \\cup \\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) &gt; \\epsilon}{\\theta^\\dagger} \\\\\n& \\leq \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r}{\\theta^\\dagger} \\\\\n& + \\Prob{\\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) &gt; \\epsilon}{\\theta^\\dagger} \\\\\n& = \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r}{\\theta^\\dagger} \\\\\n& + \\Prob{\\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) - \\Exp{H_r(X_i, \\theta^\\dagger)} &gt; \\epsilon - \\Exp{H_r(X_i, \\theta^\\dagger)}}{\\theta^\\dagger} \\\\\n& \\leq \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r}{\\theta^\\dagger} \\\\\n& + \\Prob{\\abs{\\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) - \\Exp{H_r(X_i, \\theta^\\dagger)}} &gt; \\epsilon - \\Exp{H_r(X_i, \\theta^\\dagger)}}{\\theta^\\dagger} \\\\\n& \\leq \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r}{\\theta^\\dagger} \\\\\n& + \\Prob{\\abs{\\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) - \\Exp{H_r(X_i, \\theta^\\dagger)}} &gt; \\epsilon / 2}{\\theta^\\dagger} \\\\\n\\end{aligned}\n\\] By the weak law of large numbers, the second term goes to zero, and the first term goes to zero by consistency. This leaves \\(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\) which \\(\\overset{p}{\\to} \\Exp{(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j}\\) by the WLLN.\nCollecting our \\(p\\) equations into one set of equations yields: \\[\n\\sqrt{n}\\lp\\frac{1}{n} \\ell_\\theta(\\theta^\\dagger)\\rp = (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))} + o_p(1)) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\tag{1}\\]\nWriting out the expressions as explicit sums: \\[\\begin{align*}\n\\frac{\\sqrt{n}}{n}  \\sum_{i=1}^n \\ell_\\theta(\\theta^\\dagger; x_i) = (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))} + o_p(1)) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*} \\tag{2}\\]\nThe left-hand side of Equation 2 will be amenable to a multivariate version of the CLT. We’ll take the following multivariate CLT as given:\n\nTheorem 1. Multivariate CLT, (Keener 2010) Let \\(X_1, X_2, \\dots\\) be i.i.d random vectors in \\(\\R^k\\) with a common mean \\(\\Exp{X_i} = \\mu\\) and common covariance matrix \\(\\Sigma = \\Exp{(X_i - \\mu)(X_i - \\mu)^T}\\). If \\(\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\\), then \\[\\sqrt{n}(\\bar{X} - \\mu) \\overset{d}{\\to} \\text{Normal}(0, \\Sigma)\\]\n\nRecall that \\[\n\\Exp{\\ell_\\theta(\\theta; X_i)} = 0\n\\] By the multivariate central limit (MCLT) theorem, Equation 2 converges in distribution to a multivariate normal distribution with mean zero and covariance matrix \\(\\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}\\).\nWe’ll also need a lemma about the solutions to random linear equations:\n\nLemma 1 (Lemma 5.2 in (Lehmann and Casella 1998)) Suppose there are a set of \\(p\\) equations, \\(j = 1, \\dots, p\\): \\[\\sum_{k=1}^p A_{jkn} Y_{kn} = T_{jn}.\\] Let \\(T_{1n}, \\dots, T_{pn}\\) converge in distribution to \\(T_1, \\dots, T_p\\). Furthermore, suppose that for each \\(j,k\\), \\(A_{jkn} \\overset{p}{\\to} a_{jk}\\) such that the matrix \\(A\\) with \\((j,k)^{\\mathrm{th}}\\) element \\(a_{jk}\\) is nonsingular. Then if the distribution of \\(T_1, \\dots, T_p\\) has a disitribution with repsect to the Lebesgue measure over \\(\\R^p\\), \\(Y_{1n}, \\dots, Y_{pn}\\) tend in probability to \\(A^{-1} T\\).\nWritten in matrix form and using the fact that convergence in probability implies convergence in distribution: \\[\nT_n = A_n Y_n  \\implies Y_n \\overset{d}{\\to} A^{-1}T  \n\\]\n\nWe have that the left-hand side of Equation 1 converges in distribtuion to a multivariate normal distribution, and we have that the matrix on the RHS of Equation 1 convegens in probability to \\(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))}\\), which by assumption is invertble. Thus by Lemma 1 \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\) converges in probability to \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\overset{p}{\\to} (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}^{1/2}\\mathcal{Z}\n\\end{align*}\\] where \\(\\mathcal{Z} \\sim \\text{Normal}(0, I_p)\\), or \\[\\begin{align*}\n&\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)  \\overset{d}{\\to} \\mathcal{N}\\left(0, (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\right)\n\\end{align*}\\]\nAssuming further that \\[\n\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)} + \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}=0 \\implies (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T} = I_p\n\\] Putting this all together shows that \\[\\begin{align*}\n     \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger)^{-1})\n\\end{align*}\\] where \\(\\mathcal{I}(\\theta^\\dagger) = -\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)}\\)\nThen we can build an asymptotic confidence interval using the pivotal quantity strategy we had above:\n\\[\nP(\\bar{y} - \\frac{\\sigma}{\\sqrt{n}} z_{1-\\alpha/2} &lt; \\mu  &lt; \\bar{y} + \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2})\n\\] But with \\(\\bar{y}\\) equaling \\(\\hat{\\theta}_1\\) and \\(\\sigma = \\sqrt{\\mathcal{I}(\\hat{\\theta})^{-1}_{1,1}}\\). It is sometimes hard to calculate the Fisher information because it involves taking expectations of the negative Hessian of the log-likelihood function. If we’d prefer, we can instead use an estimator for \\(\\mathcal{I}(\\hat{\\theta})\\) as\n\\[\n\\mathcal{I}(\\hat{\\theta}) = -\\frac{1}{n} \\sum_i \\ell_{\\theta\\theta}(\\hat{\\theta}; y_i)\n\\] The \\(\\mathcal{I}\\) should have a hat over it, but I can’t get the math to compile correctly when I add the \\hat over it.\nThere are ways to build multivariate confidence intervals, but I won’t go over those right now, though they are covered in the book in Chapter 6."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-6-notes.html#bayes",
    "href": "missing-data-material-W-26/notes/lecture-6-notes.html#bayes",
    "title": "Missing data lecture 6",
    "section": "Bayes",
    "text": "Bayes\nThe machinery for Frequentist inference often relies on asymptotic arguments for complex models. Bayesian inference, on the other hand, does not, and gives exact finite sample inference. There are caveats though, which we’ll cover.\nMLEs are concerned with finding a single point that, in some sense agrees with the dataset at hand, though our inference depends on hypothetical replications of the experiment that would generate alternative datasets. Bayesian inference requires that we characterize the distribution of parameters that agree with the dataset at hand.\nThe idea of Bayesian inference starts with Bayes rule. Given a prior distribution \\(p(\\theta)\\) we can combine that with the observational density of data \\(f_Y(y \\mid \\theta)\\) to give us an updated distribution \\(p(\\theta \\mid y)\\) given the dataset at hand: \\[\np(\\theta \\mid y) = \\frac{f_Y(y \\mid \\theta) p(\\theta)}{\\int_{\\Omega_\\theta}f_Y(y \\mid \\theta) p(\\theta) d \\theta}\n\\] The nice thing about Bayesian inference is that we get a distribution over \\(\\theta\\) given the dataset that we can now use to make probability statements about \\(\\theta\\). The statement With 0.95 probability \\(\\theta\\) is in the interval \\(C\\) is just a manipulation of the posterior density: \\(P(\\theta \\in C \\mid y) = \\int_C p(\\theta \\mid y) d\\theta\\).\nLet’s look at a specific example: the standard \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Bernuolli}(\\theta)\\) with \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\).\nThe likelihood is:\n\\[\n\\prod_i \\theta^{y_i}(1 - \\theta)^{1 - y_i} = \\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i}\n\\] The prior for \\(\\theta\\) will be:\n\\[\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}\n\\] The numerator the posterior is the product of these two expressions, where we let \\(s=\\sum_i y_i\\) for convenience:\n\\[\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{s + \\alpha - 1}(1 - \\theta)^{n - s + \\beta - 1}\n\\] Integrating over \\(\\theta\\) will give us the denominator of our expression:\n\\[\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + s)\\Gamma(\\beta + n - s)}{\\Gamma(\\alpha + \\beta + n)}\n\\] The ratio of the numerator and the denominator gives us: \\[\n\\frac{\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{s + \\alpha - 1}(1 - \\theta)^{n - s + \\beta - 1}}{\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + s)\\Gamma(\\beta + n - s)}{\\Gamma(\\alpha + \\beta + n)}\n}\n\\] which simplifies to: \\[\n\\frac{\\Gamma(\\alpha + \\beta + n)}{\\Gamma(\\alpha + s)\\Gamma(\\beta + n - s)}\\theta^{s + \\alpha - 1}(1 - \\theta)^{n - s + \\beta - 1}\n\\] This is just the Beta distribution with updated coefficients.\nThe prior mean is \\(\\frac{\\alpha}{\\alpha + \\beta}\\), while the posterior mean is \\(\\frac{s + \\alpha}{n + \\alpha + \\beta}\\). We can rewrite this to get a better understanding of the posterior mean represents in this circumstance: \\[\n\\begin{aligned}\n\\frac{s + \\alpha}{n + \\alpha + \\beta} = \\frac{\\alpha}{\\alpha + \\beta}\\frac{\\alpha + \\beta}{n + \\alpha + \\beta} + \\lp 1 - \\frac{\\alpha + \\beta}{n + \\alpha + \\beta}\\rp \\frac{s}{n}\n\\end{aligned}\n\\] This shows that the posterior mean is a weighted average of the prior mean and the data mean. This sort of exemplifies what we would hope for from Bayesian inference, some adjudication between the prior and the data. The posterior distribution is a Beta distribution, so we can get probability statements easily by using qbeta in R.\nWe didn’t have to go through all of the marginalization above. We could have noticed that the kernel of the posterior, namely the expression that depends on the unknown parmaeters, had a familiar form: \\[\np(\\theta \\mid s) \\propto \\theta^{s + \\alpha - 1}(1 - \\theta)^{n - s + \\beta - 1}\n\\] This is the kernel of the beta distribution, so we could have just stopped here and said that \\[\np(\\theta \\mid s) \\equiv \\text{Beta}(\\alpha + s, \\beta + n - s)\n\\] This procedure is aided by conjugate priors, which match the likelihood in a way; the functional form of the prior slots into the way the parameters are expressed in the likelihood to yield a family of posteriors that are in the same family as the prior.\nThese probabilities are strictly “right” under our prior assumption because of the math of Bayes’ theorem. Whether or not these are useful is another question.\nThere are some downsides to using a prior. MLEs have a nice property called invariance, namely that if \\(\\hat{\\theta}\\) is the MLE then the MLE for a function of \\(\\theta\\), say \\(g(\\theta)\\), is just \\(g(\\hat{\\theta})\\). This isn’t true in Bayesian inference generally, because we’re now dealing with distributions rather than points. So usually the posterior for \\(\\theta\\) won’t be the same as the posterior for \\(g(\\theta)\\): Let \\(\\eta = g(\\theta)\\), and assume for simplicity’s sake that \\(g\\) is one-to-one. Then \\(\\theta = g^{-1}(\\eta)\\). If \\(\\theta\\) has posterior \\(p(\\theta \\mid y)\\), the posterior for \\(g(\\theta)\\) is:\n\\[\np(g^{-1}(\\eta) \\mid y) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\] In fact, this is true of priors too! Given \\(p(\\theta)\\) and a transformation \\(\\eta = g(\\theta)\\) the prior for \\(\\eta\\) is: \\[\np(\\eta) = p(g^{-1}(\\eta)) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\] This is somewhat problematic if you think about how to represent ignorance. Let’s say you want to learn about a parameter \\(\\theta \\in [0,1]\\). The simplest prior for this is the flat prior \\(p(\\theta) \\propto 1\\). That implies that \\(\\theta\\) is equally likely to be anywhere in the interval of \\([0,1]\\). But what does that imply about \\(\\eta = \\theta^2\\)? Well on \\([0,1]\\), \\(g(x) =x^2\\) is one-to-one, and the inverse is \\(\\sqrt{g(\\eta)} = \\theta\\). The derivative of \\(\\sqrt{\\eta}\\) is proportional to \\(\\eta^{-1/2}\\), which implies a downward sloping distribution on \\([0,1]\\). But why would you have knowledge of \\(\\theta^2\\) without knowledge of \\(\\theta\\)? It seems counterintuitive.\nThe dyed-in-the-wool Bayesian would argue that there is no such thing as true ignorance, and that the problem that you face will have consequences for where you expect your parameter to lie. Suppose you’re modeling the proportion of Corvallis residents with synovial sarcoma, which is a very rare cancer. You’re probably going to use a prior that favors small values of \\(\\theta\\).\nWhat if instead you’re looking at the proportion of rainy days in Corvallis from November to April? You’d probably use a prior that at the very least avoided \\(\\theta\\) near 0."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-7-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-7-notes.html",
    "title": "Missing data lecture 7",
    "section": "",
    "text": "If \\(\\hat{\\theta}\\) is the MLE then the MLE for a function of \\(\\theta\\), say \\(g(\\theta)\\), is just \\(g(\\hat{\\theta})\\).\nBayesian (in)variance:\nLet \\(\\eta = g(\\theta)\\), and assume for simplicity’s sake that \\(g\\) is one-to-one. Then \\(\\theta = g^{-1}(\\eta)\\). If \\(\\theta\\) has posterior \\(p(\\theta \\mid y)\\), the posterior for \\(g(\\theta)\\) is:\n\\[\np(g^{-1}(\\eta) \\mid y) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\]\nThis can lead to contradictions under ``ignorance”.\nThis presentation follows Gelman et al. (2013) somewhat.\nThere are priors called Jeffreys’ priors (for Harold Jeffreys) that are invariant to reparameterizations. Remember that the Fisher information, or: \\[\n\\mathcal{I}(\\theta) = \\Exp{\\ell_\\theta(\\theta; Y)\\ell_\\theta(\\theta; Y)^T}\n\\] under a reparameterization \\(\\eta = g(\\theta)\\) with Jacobian \\((J_{\\eta,\\theta})_{ij} = \\frac{\\partial \\eta_i}{\\partial \\theta_j}\\) is: \\[\n\\mathcal{I}(\\theta(\\eta)) = J_{\\eta,\\theta}^T\\Exp{\\ell_\\theta(g^{-1}(\\eta); Y)\\ell_\\theta(g^{-1}(\\eta); Y)^T}J_{\\eta,\\theta}\n\\] For \\(\\eta = g(\\theta)\\), assume for simplicity that \\(g\\) is one-to-one, then a prior for \\(\\theta\\) that is proportional to the square root of the determinant of the Fisher information will be invariant to reparameterization:\n\\[\np(\\theta) \\propto \\det(\\mathcal{I}(\\theta))^{1/2}\n\\] Why is this the case? Because under the change of measure formula above the prior for \\(\\eta\\) is: \\[\np(\\eta) \\propto p(g^{-1}(\\eta)) \\det J_{\\eta,\\theta}\n\\] which is \\[\n\\begin{aligned}\np(\\eta) & \\propto \\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det J_{\\eta,\\theta} \\\\\n& \\propto \\det (J_{\\eta,\\theta})^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T)^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T\\mathcal{I}(g^{-1}(\\eta))J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det(\\mathcal{I}(\\eta))^{1/2}\n\\end{aligned}\n\\] Thus giving some sense of invariance under a coordinate change. As stated in Gelman et al. (2013), more or less:\n\nAny rule for determining the prior density \\(p(\\theta)\\) should yield an equivalent result if applied to the transformed parameter; that is, \\(p(\\eta)\\) generated using \\(p(\\theta)\\) using the change of measure formula should yield the same prior as would have been obtained directly from the model \\(p(\\eta) p(y \\mid \\eta)\\)\n\nOne issue with Jeffreys’ prior is that it is dependent on a likelihood, which can be controversial.\nFor the Bernoulli trial example from last class, the Jeffreys prior is \\(\\text{Beta}(1/2, 1/2)\\).\n\n\n\nPosterior probabilities are strictly “right” under our prior assumption because of the math of Bayes’ theorem. However, if we take a Frequentist view of probability, namely that probabilities are defined as limiting proportions of events, we’ll need to think about alternative draws of our prior and of our data.\nThe coverage of our posterior credible intervals will only match the nominal probabilities if the prior we use for our analysis matches that which generated the data. We can show this as computing the marginal posterior \\(p(\\theta \\mid y)\\) under repeated draws from the prior and data distribution \\(p(y \\mid \\theta)\\), which is the distribution associated with the density \\(f_Y(y \\mid \\theta)\\) we’ll use in our posterior:\n\\[\n\\begin{aligned}\n\\theta^\\prime & \\sim p(\\theta) \\\\\ny & \\sim p(y \\mid \\theta^\\prime) \\\\\n\\theta & \\sim p(\\theta \\mid y)\n\\end{aligned}\n\\] Another way to represent this sampling diagram is through integrals:\n\\[\n\\begin{aligned}\n\\int_{\\Omega_\\theta}\\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime) dy \\, d\\theta^\\prime & = \\int_{\\mathcal{Y}} \\int_{\\Omega_\\theta}\\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} \\int_{\\Omega_\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} p(\\theta) f_Y(y \\mid \\theta) \\\\\n& = p(\\theta)\n\\end{aligned}\n\\]\nSee Talts et al. (2018) for more info about how we can use this identity to test whether our algorithms are working correctly.\n\n\n\nLike Frequentist confidence intervals, we can only compute \\(p(\\theta \\mid y)\\) exactly under special circumstances, like conjugate priors. The reason for this is that the integral in the denominator is usually intractable.\nWe will usually have to do approximate inference on Bayesian models by using Markov Chain Monte Carlo samplers, which iteratively generate samples that converge in distribution to the true posterior distribution. Bayesian approximate methods instead operate on an expression that is proportional to the posterior:\n\\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta) p(\\theta)\n\\]\nOne way to think about the MLE is that it is the posterior mode under a prior of \\(p(\\theta) \\propto 1\\): \\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta)\n\\] The difference between the likelihood \\(L_Y(\\theta \\mid y)\\) and the posterior \\(p(\\theta \\mid y)\\) lies in how we treat the expression. In MLE we’re going to maximize the likelihood. In Bayesian inference we care about the full distribution of \\(\\theta\\).\nThis gives some intuition about Bayesian inference. We can think of doing MLE and penalizing certain values of \\(\\theta\\):\n\\[\n\\ell_Y(\\theta \\mid y) + \\text{penalty}(\\theta)\n\\] that will allow the maximizer to favor certain values of \\(\\theta\\) over others.\nIf we look at the implied log-posterior ignoring the constant that doesn’t depend on \\(\\theta\\):\n\\[\n\\log p(\\theta \\mid y) = \\log f_Y(y \\mid \\theta) + \\log p(\\theta)\n\\]\nIf we maximize this expression we can rewrite this as \\[\n\\log p(\\theta \\mid y) = \\ell_Y(\\theta \\mid y) + \\log p(\\theta)\n\\] and we get the penalized likelihood expression where the penalty is a probability density.\nOne question might be: ok, we have a full distribution for \\(\\theta\\). What do we do with it? While the MLE is a single choice, we now have myriad choices for point estimates derived from Bayesian models. We could use the posterior mean: \\[\n\\Exp{\\theta \\mid y}\n\\] We could use the posterior median, \\(\\theta_m\\): \\[\nP(\\theta &gt; \\theta_m \\mid y) = P(\\theta \\leq \\theta_m \\mid y) = 1/2.\n\\]\nWe could use another posterior quantile. We could use the mode of the posterior as well.\nAsymptotically, one might hope that the Bayesian estimates converge to the Frequentist estimates, and this is true, though one needs to be careful in scenarios where the dimensionality of the parameter space increases with sample size and about how one uses priors.\nIn Frequentist inference, the only limits on the parameter space come from the likelihood; the normal density requires that \\(\\mu \\in \\R\\) and \\(\\sigma^2 \\in (0, \\infty)\\). In Bayesian inference, the prior can also restrict the parameter space. For example, in the normal example, one could use a prior for \\(\\mu\\) that enforced \\(\\mu &gt; 0\\). The posterior would then only be able to represent \\(\\mu &gt; 0\\). If the true \\(\\mu\\) were negative, a Bayesian point-estimator wouldn’t converge to the true \\(\\mu\\).\nWhile the prior adds an extra degree of freedom which seems dangerous, it can yield better estimates when there are small datasets, because there isn’t as much information in the data. An example of this would be a simple regression model: \\[\ny_i \\sim \\text{Normal}(X_i^T \\beta, \\sigma^2)\n\\] We might have some good information that we don’t expect \\(\\beta\\) to be nearly infinite, and in fact we expect it to be pretty well concentrated to \\([-10, 10]\\). Then we could use independent \\(\\text{Normal}(0,5^2)\\) priors for the regression coefficients."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-7-notes.html#bayes-recap",
    "href": "missing-data-material-W-26/notes/lecture-7-notes.html#bayes-recap",
    "title": "Missing data lecture 7",
    "section": "",
    "text": "If \\(\\hat{\\theta}\\) is the MLE then the MLE for a function of \\(\\theta\\), say \\(g(\\theta)\\), is just \\(g(\\hat{\\theta})\\).\nBayesian (in)variance:\nLet \\(\\eta = g(\\theta)\\), and assume for simplicity’s sake that \\(g\\) is one-to-one. Then \\(\\theta = g^{-1}(\\eta)\\). If \\(\\theta\\) has posterior \\(p(\\theta \\mid y)\\), the posterior for \\(g(\\theta)\\) is:\n\\[\np(g^{-1}(\\eta) \\mid y) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\]\nThis can lead to contradictions under ``ignorance”.\nThis presentation follows Gelman et al. (2013) somewhat.\nThere are priors called Jeffreys’ priors (for Harold Jeffreys) that are invariant to reparameterizations. Remember that the Fisher information, or: \\[\n\\mathcal{I}(\\theta) = \\Exp{\\ell_\\theta(\\theta; Y)\\ell_\\theta(\\theta; Y)^T}\n\\] under a reparameterization \\(\\eta = g(\\theta)\\) with Jacobian \\((J_{\\eta,\\theta})_{ij} = \\frac{\\partial \\eta_i}{\\partial \\theta_j}\\) is: \\[\n\\mathcal{I}(\\theta(\\eta)) = J_{\\eta,\\theta}^T\\Exp{\\ell_\\theta(g^{-1}(\\eta); Y)\\ell_\\theta(g^{-1}(\\eta); Y)^T}J_{\\eta,\\theta}\n\\] For \\(\\eta = g(\\theta)\\), assume for simplicity that \\(g\\) is one-to-one, then a prior for \\(\\theta\\) that is proportional to the square root of the determinant of the Fisher information will be invariant to reparameterization:\n\\[\np(\\theta) \\propto \\det(\\mathcal{I}(\\theta))^{1/2}\n\\] Why is this the case? Because under the change of measure formula above the prior for \\(\\eta\\) is: \\[\np(\\eta) \\propto p(g^{-1}(\\eta)) \\det J_{\\eta,\\theta}\n\\] which is \\[\n\\begin{aligned}\np(\\eta) & \\propto \\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det J_{\\eta,\\theta} \\\\\n& \\propto \\det (J_{\\eta,\\theta})^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T)^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T\\mathcal{I}(g^{-1}(\\eta))J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det(\\mathcal{I}(\\eta))^{1/2}\n\\end{aligned}\n\\] Thus giving some sense of invariance under a coordinate change. As stated in Gelman et al. (2013), more or less:\n\nAny rule for determining the prior density \\(p(\\theta)\\) should yield an equivalent result if applied to the transformed parameter; that is, \\(p(\\eta)\\) generated using \\(p(\\theta)\\) using the change of measure formula should yield the same prior as would have been obtained directly from the model \\(p(\\eta) p(y \\mid \\eta)\\)\n\nOne issue with Jeffreys’ prior is that it is dependent on a likelihood, which can be controversial.\nFor the Bernoulli trial example from last class, the Jeffreys prior is \\(\\text{Beta}(1/2, 1/2)\\).\n\n\n\nPosterior probabilities are strictly “right” under our prior assumption because of the math of Bayes’ theorem. However, if we take a Frequentist view of probability, namely that probabilities are defined as limiting proportions of events, we’ll need to think about alternative draws of our prior and of our data.\nThe coverage of our posterior credible intervals will only match the nominal probabilities if the prior we use for our analysis matches that which generated the data. We can show this as computing the marginal posterior \\(p(\\theta \\mid y)\\) under repeated draws from the prior and data distribution \\(p(y \\mid \\theta)\\), which is the distribution associated with the density \\(f_Y(y \\mid \\theta)\\) we’ll use in our posterior:\n\\[\n\\begin{aligned}\n\\theta^\\prime & \\sim p(\\theta) \\\\\ny & \\sim p(y \\mid \\theta^\\prime) \\\\\n\\theta & \\sim p(\\theta \\mid y)\n\\end{aligned}\n\\] Another way to represent this sampling diagram is through integrals:\n\\[\n\\begin{aligned}\n\\int_{\\Omega_\\theta}\\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime) dy \\, d\\theta^\\prime & = \\int_{\\mathcal{Y}} \\int_{\\Omega_\\theta}\\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} \\int_{\\Omega_\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} p(\\theta) f_Y(y \\mid \\theta) \\\\\n& = p(\\theta)\n\\end{aligned}\n\\]\nSee Talts et al. (2018) for more info about how we can use this identity to test whether our algorithms are working correctly.\n\n\n\nLike Frequentist confidence intervals, we can only compute \\(p(\\theta \\mid y)\\) exactly under special circumstances, like conjugate priors. The reason for this is that the integral in the denominator is usually intractable.\nWe will usually have to do approximate inference on Bayesian models by using Markov Chain Monte Carlo samplers, which iteratively generate samples that converge in distribution to the true posterior distribution. Bayesian approximate methods instead operate on an expression that is proportional to the posterior:\n\\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta) p(\\theta)\n\\]\nOne way to think about the MLE is that it is the posterior mode under a prior of \\(p(\\theta) \\propto 1\\): \\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta)\n\\] The difference between the likelihood \\(L_Y(\\theta \\mid y)\\) and the posterior \\(p(\\theta \\mid y)\\) lies in how we treat the expression. In MLE we’re going to maximize the likelihood. In Bayesian inference we care about the full distribution of \\(\\theta\\).\nThis gives some intuition about Bayesian inference. We can think of doing MLE and penalizing certain values of \\(\\theta\\):\n\\[\n\\ell_Y(\\theta \\mid y) + \\text{penalty}(\\theta)\n\\] that will allow the maximizer to favor certain values of \\(\\theta\\) over others.\nIf we look at the implied log-posterior ignoring the constant that doesn’t depend on \\(\\theta\\):\n\\[\n\\log p(\\theta \\mid y) = \\log f_Y(y \\mid \\theta) + \\log p(\\theta)\n\\]\nIf we maximize this expression we can rewrite this as \\[\n\\log p(\\theta \\mid y) = \\ell_Y(\\theta \\mid y) + \\log p(\\theta)\n\\] and we get the penalized likelihood expression where the penalty is a probability density.\nOne question might be: ok, we have a full distribution for \\(\\theta\\). What do we do with it? While the MLE is a single choice, we now have myriad choices for point estimates derived from Bayesian models. We could use the posterior mean: \\[\n\\Exp{\\theta \\mid y}\n\\] We could use the posterior median, \\(\\theta_m\\): \\[\nP(\\theta &gt; \\theta_m \\mid y) = P(\\theta \\leq \\theta_m \\mid y) = 1/2.\n\\]\nWe could use another posterior quantile. We could use the mode of the posterior as well.\nAsymptotically, one might hope that the Bayesian estimates converge to the Frequentist estimates, and this is true, though one needs to be careful in scenarios where the dimensionality of the parameter space increases with sample size and about how one uses priors.\nIn Frequentist inference, the only limits on the parameter space come from the likelihood; the normal density requires that \\(\\mu \\in \\R\\) and \\(\\sigma^2 \\in (0, \\infty)\\). In Bayesian inference, the prior can also restrict the parameter space. For example, in the normal example, one could use a prior for \\(\\mu\\) that enforced \\(\\mu &gt; 0\\). The posterior would then only be able to represent \\(\\mu &gt; 0\\). If the true \\(\\mu\\) were negative, a Bayesian point-estimator wouldn’t converge to the true \\(\\mu\\).\nWhile the prior adds an extra degree of freedom which seems dangerous, it can yield better estimates when there are small datasets, because there isn’t as much information in the data. An example of this would be a simple regression model: \\[\ny_i \\sim \\text{Normal}(X_i^T \\beta, \\sigma^2)\n\\] We might have some good information that we don’t expect \\(\\beta\\) to be nearly infinite, and in fact we expect it to be pretty well concentrated to \\([-10, 10]\\). Then we could use independent \\(\\text{Normal}(0,5^2)\\) priors for the regression coefficients."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-7-notes.html#linear-regression-with-conjugate-priors",
    "href": "missing-data-material-W-26/notes/lecture-7-notes.html#linear-regression-with-conjugate-priors",
    "title": "Missing data lecture 7",
    "section": "Linear regression with conjugate priors",
    "text": "Linear regression with conjugate priors\nThis and the following section follow Chapter 2 in Rossi, Allenby, and Misra (2024) quite closely.\nLet’s look at the linear regression model with conjugate priors. \\[\ny_i = x_i^T \\beta + \\epsilon_i, \\quad \\epsilon_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(0, \\sigma^2)\n\\] where \\(x_i \\in \\R^p\\). A full model would imply a model for \\(x_i\\) as well: \\[\nf_{X,Y}((x_1, y_1), \\dots, (x_n, y_n) \\mid \\beta, \\psi) = \\prod_i f_{X}(x_i \\mid \\psi) f_Y(y_i \\mid x_i,\\beta, \\sigma^2)\n\\] If we have a prior for \\(\\psi,\\beta, \\sigma^2\\) that is independent, \\(p(\\psi, \\beta, \\sigma^2) = p(\\psi) p(\\beta, \\sigma^2)\\), then the posterior will factorize into independent distributions as well: \\[\n\\begin{aligned}\np_(\\beta, \\psi,\\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) & \\propto \\prod_i f_{X}(x_i \\mid \\psi) p(\\psi) f_Y(y_i \\mid x_i,\\beta, \\sigma^2) p(\\beta, \\sigma^2) \\\\\n& \\propto (\\prod_i f_{X}(x_i \\mid \\psi) p(\\psi)) \\prod_i f_Y(y_i \\mid x_i,\\beta, \\sigma^2) p(\\beta)  \\\\\n& \\propto p(\\psi \\mid x_1, \\dots, x_n) p(\\beta, \\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) \\\\\n\\end{aligned}\n\\] This means we can do inference on \\((\\beta,\\sigma)\\) without worrying about a model for \\(x_i\\).\nRemember from last class how we could intuit the form of the joint prior if we examined the likelihood for \\(\\theta\\) and chose a prior with the same functional form as that of the likelihood.\nIn the Bernoulli example, we had a likelihood of the form: \\(L_Y(\\theta \\mid y) = \\theta^{k}(1 - \\theta)^{n - k}\\), where \\(k = \\sum_i = y\\), which suggested a prior of the form \\(\\theta^{a}(1-\\theta)^b\\), which we could recognize as a Beta distribution.\nWe’ll do the same for the regression example. The likelihood for the linear model is: \\[\n(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2}\\sum_i (y_i - x_i^T \\beta)^2 \\rp\n\\] which can be simplified somewhat by writing the sum as a dot product between the vector of errors, \\(e = y - X \\beta\\) where \\(y = (y_1, \\dots, y_n)\\) and \\(X^T = (x_1, \\dots, x_n)\\). \\[\n(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2} (y - X \\beta)^T(y - X \\beta) \\rp\n\\]\nWe can rewrite the term \\((y - X \\beta)^T (y - X \\beta)\\) in terms of the least-squares estimator for \\(\\beta\\), \\(\\hat{\\beta} = (X^T X)^{-1}X^T y\\) by decomposing \\(y\\) as \\(y = X \\hat{\\beta} + y - X \\hat{\\beta}\\):\n\\[\n\\begin{aligned}\n(y - X \\beta)^T (y - X \\beta) & = (X \\hat{\\beta} + y - X \\hat{\\beta} - X \\beta)^T(X \\hat{\\beta} + y - X \\hat{\\beta} - X \\beta) \\\\\n& = (y - X \\hat{\\beta})^T(y - X \\hat{\\beta}) + (X \\beta - X\\hat{\\beta})^T(X \\beta - X\\hat{\\beta}) - 2(X \\beta - X\\hat{\\beta})^T (y - X \\hat{\\beta}) \\\\\n& = (y - X \\hat{\\beta})^T(y - X \\hat{\\beta}) + (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\n\\end{aligned}\n\\]\nLet \\(s^2 = \\frac{1}{n-p}(y-X\\hat{\\beta})^T(y-X\\hat{\\beta})\\), and \\(\\nu = n - p\\), so we can rewrite the sum more compactly as: \\[\n(y - X \\beta)^T (y - X \\beta) = \\nu s^2 + (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\n\\] This leads to a likelihood:\n\\[\nL_Y(\\beta, \\sigma^2 \\mid y, X) \\propto (\\sigma^2)^{-\\nu/2} \\exp\\lp\\frac{\\nu s^2}{2 \\sigma^2}\\rp (\\sigma^2)^{-(n - \\nu) / 2} \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\\rp\n\\] Before we derive conjugate priors from this likelihood, we can see that the posterior under flat priors for \\(\\beta\\) and a prior for \\(\\sigma^2\\), \\(\\sigma^{-2}\\), leads to a posterior: \\[\np(\\beta, \\sigma^2 \\mid y, X) \\propto (\\sigma^2)^{-(\\nu/2+1)} \\exp\\lp\\frac{\\nu s^2}{2 \\sigma^2}\\rp (\\sigma^2)^{-(n - \\nu) / 2} \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\\rp\n\\] which is a conditional normal posterior for \\(\\beta\\) with a scaled inverse chi-squared posterior for \\(\\sigma^2\\).\nThis suggests a conjugate prior of the form \\(p(\\beta,\\sigma^2) = p(\\sigma^2)p(\\beta \\mid \\sigma^2)\\): \\[\np(\\sigma^2) \\propto (\\sigma^2)^{-(\\nu_0/2 + 1)} \\exp\\lp \\frac{\\nu_0 s_0}{2 \\sigma^2} \\rp\n\\]\nand a conditional normal prior for \\(\\beta\\):\n\\[\np(\\beta \\mid \\sigma^2) \\propto (\\sigma^2)^{-p / 2} \\exp\\lp-\\frac{1}{2\\sigma^2}(\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta - \\mu_0) \\rp\n\\]\nThis can be seen as the posterior from a regression run with a prior of \\(p(\\sigma^2) \\propto \\sigma^{-2}\\) and a flat prior on \\(\\beta\\).\nThen the posterior for \\(\\sigma^2, \\beta\\) is simply the product of the priors and the likelihood, which we write as above:\n\\[\n\\begin{aligned}\np(\\beta, \\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) \\propto & (\\sigma^2)^{-(\\nu_0/2 + 1)} \\exp\\lp \\frac{\\nu_0 s_0}{2 \\sigma^2} \\rp(\\sigma^2)^{-p / 2} \\exp\\lp-\\frac{1}{2\\sigma^2}(\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta - \\mu_0) \\rp \\\\\n&(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2} (y - X \\beta)^T(y - X \\beta) \\rp\n\\end{aligned}\n\\]\nThis is definitley formidable, but we can simplify things a bit by collecting the terms with \\(\\beta\\):\n\\[\n(y - X\\beta)^T (y - X\\beta) + (\\mu_0 - \\beta)^T \\Sigma_0^{-1}(\\mu_0 - \\beta)\n\\] and decomposing \\(\\Sigma_0^{-1} = L^T L\\), and noting that we can write the sum as the following inner product: \\[\n\\begin{aligned}\n\\begin{bmatrix}\n(y - X\\beta)^T & (L\\mu_0 - L\\beta)^T\n\\end{bmatrix}\n\\begin{bmatrix}\n(y - X\\beta) \\\\\nL \\mu_0 - L\\beta)\n\\end{bmatrix}\n\\end{aligned}\n\\] This can be further simplified by constructing a vector \\[\nu = \\begin{bmatrix} y \\\\ L\\mu_0 \\end{bmatrix}\n\\] and a matrix \\(W\\) \\[\nW = \\begin{bmatrix} X \\\\ L \\end{bmatrix}\n\\] and writing the expresssion as \\((u - W\\beta)^T(u - W\\beta)\\). We can then use the same trick as above, by representing \\(u\\) as the projection into the column space of \\(W\\) and the residual:\n\\[\n(W \\bar{\\beta} + u - W \\bar{\\beta} - W \\beta)^T(W \\bar{\\beta} + u - W \\bar{\\beta} - W \\beta)\n\\] The expression for \\(\\bar{\\beta}\\) is:\n\\[\n\\begin{aligned}\n\\bar{\\beta} & = (X^T X + L^T L)^{-1}(X^T y + L^T L \\mu_0) \\\\\n& = (X^T X + \\Sigma_0^{-1})^{-1}(X^T y + \\Sigma_0^{-1} \\mu_0)\n\\end{aligned}\n\\]\nWhich simplifies as\n\\[\n(u - W \\bar{\\beta})^T(u - W \\bar{\\beta}) +(\\beta - \\bar{\\beta})^T W^T W (\\beta - \\bar{\\beta})\n\\] and after some algebra comes to \\[\n(y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) + (\\mu_0 - \\bar{\\beta})^T\\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta}) + (\\beta - \\bar{\\beta})^T (X^T X + \\Sigma_0^{-1}) (\\beta - \\bar{\\beta})\n\\] In the following, let \\(n s^2 = (y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) + (\\mu_0 - \\bar{\\beta})^T\\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta})\\). The posterior is:\n\\[\n\\begin{aligned}\np(\\beta, \\sigma^2 \\mid y, X) \\propto & (\\sigma^2)^{-(n + \\nu_0)/2 + 1} \\exp\\lp\\frac{(n + \\nu_0)(n s^2 + \\nu_0 s_0^2)/(n + \\nu_0)}{2 \\sigma^2}\\rp \\times (\\sigma^2)^{-p / 2} \\\\\n& \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\bar{\\beta})^T (X^T X + \\Sigma_\\beta^{-1})(\\beta - \\bar{\\beta})\\rp\n\\end{aligned}\n\\] \\[\n\\bar{\\beta} = (X^T X + \\Sigma_\\beta^{-1})^{-1}(\\Sigma_\\beta^{-1} \\mu_\\beta + X^T X \\hat{\\beta})\n\\] Like the Bernoulli problem, the posterior mean for \\(\\beta\\) is a weighted average between the prior mean and the information from the likelihood, which in this case is the least-squared estimator for \\(\\beta\\). This is often a consequence of using conjugate priors, that the posterior is a compromise between the prior and the likelihood.\nThis implies the following distributions for \\(\\sigma^2\\) and \\(\\beta \\mid \\sigma^2\\): \\[\n\\begin{aligned}\n\\sigma^2 & \\sim \\text{Inv-}\\chi^2\\lp n + \\nu_0, \\frac{n s^2 + \\nu_0 s^2_0}{n + \\nu_0}\\rp \\\\\n\\beta \\mid \\sigma^2 & \\sim \\text{Normal}(\\bar{\\beta}, \\sigma^2 \\lp X^T X + \\Sigma_0^{-1} \\rp^{-1})\n\\end{aligned}\n\\]\nThe posterior mean for \\(\\sigma^2\\) is: \\[\n\\Exp{\\sigma^2 \\mid y, X} = \\frac{n + \\nu_0}{n + \\nu_0 - 2}\\frac{n s^2 + \\nu_0 s^2_0}{n + \\nu_0}\n\\] The expression for \\(n s^2\\) is interesting because it involves the squared error of the posterior linear predictor for \\(y\\): \\[\n\\begin{aligned}\n(y - \\Exp{X \\beta \\mid y, X})^T(y - \\Exp{X \\beta \\mid y, X})^T & = (y - X \\Exp{\\beta \\mid y, X})^T(y - X \\Exp{\\beta \\mid y, X}) \\\\\n& = (y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) \\\\\n\\end{aligned}\n\\]\nbut it also involves the error in the prior mean with respect to the prior covariance matrix: \\[\n(\\mu_0 - \\bar{\\beta})^T \\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta})\n\\] The effect of this term will decrease as the number of observations increases, but it elucidates how the posterior mean of the error variance is decomposed into several pieces depending on different aspects of the prior and the data.\n\nBayesian inference in repeated measure models\nTwo lectures ago we went through how to compute the MLE from this regression model:\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j.\n\\end{aligned}\n\\]\nThis required sequentially computing the MLE for \\(\\beta\\) given an estimate for \\(\\Sigma\\) and computing \\(\\hat{\\Sigma}\\) given the last estimate for \\(\\hat{\\beta}\\).\nLet’s write down the likelihood for this model to see if we can come up with a conjugate prior for the problem. \\[\nL_{Y}(\\beta, \\Sigma \\mid y, X) \\propto \\det(I_n \\otimes \\Sigma)^{-1/2} \\exp\\lp-\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\\rp\n\\] If we start with the prior for \\(\\beta \\mid \\Sigma\\) we can ignore the determinant and focus on the term in the exponential:\n\\[\n-\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\n\\]\nLet’s try a multivariate normal prior:\n\\[\n\\beta \\sim \\text{Normal}(\\mu_0, \\Sigma_0)\n\\]\nso we can multiply the likelihood by the prior to get\n\\[\n-\\frac{1}{2}\\lp (y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta) + (\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta-\\mu_0) \\rp\n\\] which we’ll rewrite for convenience as \\[\n-\\frac{1}{2}\\lp (A(y - X \\beta))^T A(y - X \\beta) + (L(\\beta - \\mu_0))^T L(\\beta-\\mu_0) \\rp\n\\] where \\(A^T A = (I_n \\otimes \\Sigma)^{-1}\\) and \\(L^T L = \\Sigma_0^{-1}\\).\nThis looks familiar! We can use the same trick as we did above: create a new vector \\(u\\) and matrix \\(W\\): \\[\nu =\n\\begin{bmatrix}\nA y \\\\\nL \\mu_0\n\\end{bmatrix},\\quad\nW =\n\\begin{bmatrix}\nA X \\\\\nL\n\\end{bmatrix}\n\\] and can write \\[\n(u - W\\beta)^T(u - W \\beta) = \\lp (A(y - X \\beta))^T A(y - X \\beta) + (L(\\beta - \\mu_0))^T L(\\beta-\\mu_0) \\rp.\n\\] Furthermore, write \\(u = W\\bar{\\beta} + u - W\\bar{\\beta}\\), where \\(\\bar{\\beta}\\) is the least-squares coeffients of the regression of \\(u\\) on \\(W\\): \\[\n\\bar{\\beta} = (W^T W + L^T L)^{-1}W^T u = (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma_0^{-1})^{-1}(X^T (I_n \\otimes \\Sigma)^{-1}y + \\Sigma_0^{-1} \\mu_0)\n\\] This leads to \\((u - W\\bar{\\beta})^T W = 0\\), which allows us to cleanly partition \\((u-W\\beta)^T (u-W\\beta)\\) into two pieces: \\(u - W\\bar{\\beta}\\) and \\(W\\beta\\):\n\\[\n\\begin{aligned}\n(W\\bar{\\beta} + u - W\\bar{\\beta} - &W \\beta)^T(W\\bar{\\beta} + u - W\\bar{\\beta} - W \\beta) \\\\\n& = (u - W\\bar{\\beta} + W\\bar{\\beta}  - W \\beta)^T(u - W\\bar{\\beta}+ W\\bar{\\beta}  - W \\beta) \\\\\n& = (u - W\\bar{\\beta})^T(u - W\\bar{\\beta}) + (W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta) + 2(u - W\\bar{\\beta})^T(W\\bar{\\beta}  - W \\beta) \\\\\n& = (u - W\\bar{\\beta})^T(u - W\\bar{\\beta}) + (W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta)\n\\end{aligned}\n\\] where the last line follows because \\((u - W\\bar{\\beta})^T W = 0\\). Because we’re focusing only on the posterior, which is a function of \\(\\beta\\) and not data, we can ignore the \\((u - W\\bar{\\beta})^T(u - W\\bar{\\beta})\\) term because it does not involve \\(\\beta\\) and involves only functions of \\(X,y,A,L\\), which are fixed with respect to \\(\\beta\\).\nWe rewrite \\[\n(W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta)\n\\] as \\[\n(\\beta - \\bar{\\beta})^T W^T W (\\beta - \\bar{\\beta}) = (\\beta - \\bar{\\beta})^T(X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1}) (\\beta - \\bar{\\beta})\n\\] This shows that \\(\\beta \\mid \\Sigma\\) is multivariate normal with: \\[\n\\beta \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1})^{-1})\n\\] Now let’s focus on the conditional distribution of \\(\\Sigma \\mid \\beta\\). We’ll start with the likelihood written in simpler terms: \\[\nL_Y(\\beta, \\Sigma \\mid y, X) \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i (y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta)\\rp\n\\]\nWe can use the trace trick to rearrange things:\n\\[\n\\begin{aligned}\nL_Y(\\beta, \\Sigma \\mid y, X) & \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i (y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta)\\rp \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i \\text{tr}((y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta))\\rp \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i \\text{tr}((y_i - X_i \\beta) (y_i - X_i \\beta)^T\\Sigma^{-1})\\rp  \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\text{tr}((\\sum_i (y_i - X_i \\beta) (y_i - X_i \\beta)^T)\\Sigma^{-1})\\rp  \\\\\n\\end{aligned}\n\\]\nThis suggests that a conjugate prior for \\(\\Sigma\\) has the form:\n\\[\np(\\Sigma) \\propto \\det(\\Sigma)^{-a/2} \\exp\\lp-\\frac{1}{2}\\text{tr}(V_0 \\Sigma^{-1})\\rp\n\\]\nFortunately, we’re in luck! The Inverse Wishart distribution has the density:\n\\[\np(\\Sigma) \\propto \\det(\\Sigma)^{-(\\nu_0 + p + 1)/2} \\exp\\lp-\\frac{1}{2}\\text{tr}(V_0 \\Sigma^{-1})\\rp\n\\]\nCombining the likelihood with the prior we get something proportional to the conditional posterior for \\(\\Sigma\\):\n\\[\np(\\Sigma \\mid y, X, \\beta) \\propto \\det(\\Sigma)^{-(n + \\nu_0 + p + 1)/2} \\exp\\lp-\\frac{1}{2}\\text{tr}((V_0 + \\textstyle\\sum_i (y_i - X_i \\beta)(y_i - X_i \\beta)^T) \\Sigma^{-1})\\rp\n\\]\nPutting this together we get the following two conditional posteriors:\n\\[\n\\begin{aligned}\n\\beta \\mid \\Sigma, y, X & \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1})^{-1}) \\\\\n\\Sigma \\mid \\beta, y, X & \\sim \\text{Inverse-Wishart}(n + \\nu_0, V_0 + \\textstyle\\sum_i (y_i - X_i \\beta)(y_i - X_i \\beta)^T)\n\\end{aligned}\n\\]\nWe can use the theory of integral operators to show that given intial conditions \\(\\Sigma^0\\) and \\(\\beta^0\\) the following algorithm for \\(t = 1, \\dots, S\\):\n\\[\n\\begin{aligned}\n\\beta^{t+1} \\mid \\Sigma^{t}, y, X & \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma^t)^{-1} X + (\\Sigma^t)^{-1})^{-1}) \\\\\n\\Sigma^{t+1} \\mid \\beta^{t}, y, X & \\sim \\text{Inverse-Wishart}(n + \\nu_0, V_0 + \\textstyle\\sum_i (y_i - X_i \\beta^t)(y_i - X_i \\beta^t)^T)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-5-notes.html#maximum-likelihood-for-multivariate-normal-distribution",
    "href": "missing-data-material-W-26/notes/lecture-5-notes.html#maximum-likelihood-for-multivariate-normal-distribution",
    "title": "Missing data lecture 5",
    "section": "",
    "text": "Let \\(y_i \\in \\R^K\\), \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\Sigma)\\) for \\(n\\) samples so that the density for \\(y_i\\) is \\[\nf_{Y}(y_i \\mid \\mu, \\Sigma) = (2\\pi)^{-\\frac{K}{2}} (\\det\\Sigma)^{-\\frac{1}{2}} \\exp\\lp-\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu) \\rp\n\\]\nThe log-likelihood is: \\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = \\frac{1}{2} \\log \\det\\Sigma -\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu)\n\\]\nThe book gives the expressions for the MLEs of the mean and covariance matrix of the multivariate normal distribution without details. Going through the algebra can be useful for other more complicated problems. But in order to do so, we’ll need a slight change to how we’re used to thinking about partial differentiation. The following blurb on differentials is based on Magnus and Neudecker (2019)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-5-notes.html#differentials-and-matrix-differentiation",
    "href": "missing-data-material-W-26/notes/lecture-5-notes.html#differentials-and-matrix-differentiation",
    "title": "Missing data lecture 5",
    "section": "Differentials and matrix differentiation",
    "text": "Differentials and matrix differentiation\nIt all starts with rearranging the derivative:\n\\[\nf^\\prime(c)  = \\lim_{u \\to 0} \\frac{f(c + u) - f(c)}{u},\n\\] to get a linear approximation to \\(f\\) at the point \\(c\\):\n\\[\nf(c + u)  = f(c) + f^\\prime(c) u + r_c(u)\n\\] where \\(r_c(u) = o(u)\\) or \\(\\lim_{u\\to0} \\frac{r_c(u)}{u} = 0\\). This is the one term Taylor expansion of the function \\(f\\) at \\(c + u\\) about \\(c\\).\nBringing \\(f(u)\\) to the left-hand side gives: \\(f(c + u) - f(u) = f^\\prime(c) u + r_c(u)\\). We can define the change in the linear approximation of \\(f\\) from \\(c\\) to \\(c+u\\) as \\(\\mathrm{d}f(c;\\,u)\\), or the first differential of \\(f\\) at \\(c\\) with increment \\(u\\): \\[\n\\mathrm{d} f(c ; u) = u f^\\prime(c)\n\\]\nSubbing this back into the linear approximation for \\(f\\) gives: \\[\nf(c + u)  = f(c) + \\mathrm{d} f(c ; u) + r_c(u)\n\\tag{1}\\]\nWe can identify the differential by finding the linear approximation to a function at \\(c\\): \\[\nf(c + u)  = f(c) + \\alpha u + r_c(u).\n\\] If we can find an \\(\\alpha\\) that depends on \\(c\\) but not on \\(u\\) such that \\(r_c(u) = o(u)\\) we say that \\(\\alpha = f^\\prime(c)\\).\nLet \\(f\\) now be a function \\(\\R^m \\to \\R\\) and let the differential be constructed via the same argument as above, but now let \\(c, u \\in \\R^m\\), and define \\(r_c(u)\\) such that \\(\\lim_{u\\to 0}\\frac{r_c(u)}{\\lVert u \\rVert} = 0\\): \\[\nf(c + u)  = f(c) + A(c) u + r_c(u).\n\\] If we equate the row vector \\(A(c)\\) with the partial derivative of \\(f\\) with respect to \\(u\\), we can recognize this as the multivariate Taylor expansion of \\(f(c + u)\\) around \\(f(c)\\).\nIn fact, this is exactly what \\(A(c)\\) is, and \\(A(c) \\equiv \\nabla_x f(x) \\mid_{x = c}\\)\n\nExample 1 (Product example) Let’s determine the differential of \\(f(x, y) = x^T y\\) for \\(x,y \\in \\R^m\\), denoted as: \\[\n\\mathrm{d}(x^T y).\n\\] We’re looking to use the left-hand side of in Equation 1 to yield something that looks like \\(x^T y + \\diff{x^Ty} u + r_c(u)\\).\nLet \\(c_x\\) and \\(c_y\\) be the values of \\(x\\) and \\(y\\) about which we’ll define our linear approximation. Specifically, we’ll define the linear approximation at the coordinates \\(c_x + u_x, c_y + u_y\\) In other words, we’d like to approximate the function: \\((c_x + u_x)^T (c_y + u_y)\\) with the value at \\(c_x^T c_y\\) plus the differential and a small remainder term. We’ll accomplish this by expanding the product \\((c_x + u_x)^T (c_y + u_y)\\) into four parts: \\[\n\\begin{align}\n(u_x + c_x)^T (u_y + c_y) & = c_x^T c_y + c_x^T u_y + u_x^T c_y + u_x^T u_y \\\\\n& = c_x^T c_y + c_x^T u_y + c_y^T u_x  + u_x^T u_y \\\\\n& = c_x^T c_y + \\begin{bmatrix}c_x^T & c_y^T \\end{bmatrix}  \\begin{bmatrix}u_y \\\\ u_x \\end{bmatrix} + u_x^T u_y \\tag{a}\\label{eq:1}\n\\end{align}\n\\] In line \\(\\eqref{eq:1}\\) we can see that \\(f(c) \\equiv c_x^T c_y\\), and \\(\\lim_{u_x, u_y \\to 0} \\frac{u_x^T u_y}{\\sqrt{\\lVert u_x \\rVert^2 + \\lVert u_y \\rVert^2}} = 0\\), so \\(u_x^T u_y\\) is our \\(r_c(u)\\). That means that the vector \\(A(c)\\) here is \\((c_x^T, c_y^T)\\) as we’ve ordered our variables as \\((u_y,u_x)\\).\n\n\nRules for the differential operatior\n\nThe differential operator is denote \\(\\diff{\\cdot}\\). It is linear: \\[\n\\diff{a x + b y} = a\\diff{x} + b\\diff{y}\n\\]\nThe differential of a constant is zero: \\[\n\\diff{a} = 0\n\\]\nThe differential of a transposed variable is the transpose of the differential \\[\n\\diff{x^T} = \\diff{x}^T\n\\]\nThe differential of a product is the sum of the differential applied to each variable (as shown in Example 1) \\[\n\\diff{XY} = \\diff{X}Y + X\\diff{Y}\n\\]\n\nExample 1 demonstrates another useful property of differentials. We recognize the fact that our variables partition naturally into two vectors, \\(x\\) and \\(y\\). When we have a natural partition of variables \\(u\\) into \\(u_1\\) and \\(u_2\\) we can write the differential for \\(f(u)\\) more easily in terms of two differentials: \\[\n\\begin{aligned}\n\\mathrm{d}f(c;\\,u) & = A(c) u \\\\\n& = A(c_1) u_1 + A(c_2)u_2\n\\end{aligned}\n\\] which just differentiates between the two sets of variables, so that \\(A(c_1)\\) is the partial derivative of \\(f\\) with respect to \\(u_1\\) and \\(A(c_2)\\) is the partial derivative with respect to \\(u_2\\). In Example 1, \\(u_1\\) is \\(u_y\\) and \\(u_2\\) is \\(u_x\\), so we can write the differnetial above in the equivalent form:\n\\[\n\\begin{aligned}\n\\mathrm{d}(x^T y) & = x^T u_y + u_x^T y \\\\\n& = x^T u_y + y^T u_x\n\\end{aligned}\n\\] In order to simplify the notation, we’ll write \\(\\diff{x}\\) instead of \\(u_x\\), so the above would be: \\[\n\\begin{aligned}\n\\mathrm{d}(x^T y) & = x^T \\diff{y} + y^T \\diff{x}\n\\end{aligned}\n\\]\nThis expression shows that we can read off \\(\\nabla_y x^T y\\) as \\(x^T\\) and \\(\\nabla_x x^T y\\) as \\(y^T\\). In fact, if we cared only about \\(\\diff{y}\\) then we could ignore the differential \\(\\diff{x}\\), essentially treating \\(x\\) as a constant so \\(\\diff{x} = 0\\).\nThis is important for thinking about differentials of log-likelihoods like the multivariate normal where we’ll have two sets of parameters that we’d like to find the partial derivatives with respect to, \\(\\Sigma\\) and \\(\\mu\\): \\[\n\\ell_Y(\\mu, \\Sigma \\mid y) = \\frac{1}{2} \\log \\det\\Sigma -\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu)\n\\] \\[\n\\mathrm{d}\\ell_Y(\\mu, \\Sigma \\mid y) = \\frac{1}{2} \\mathrm{d}(\\log \\det\\Sigma) -\\frac{1}{2}\\mathrm{d}((y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu))\n\\tag{2}\\]\nTo the extent we’d prefer to focus only on \\(\\diff{\\mu}\\) for example, we would ignore the first term on the RHS of Equation 2, and focus only on the second term.\n\n\nGeneralizing to vector valued functions\nWe can generalize to vector functions: Let \\(f(x): \\R^m \\to \\R^n\\): \\[\nf(c + u)  = f(c) + A(c) u + r_c(u).\n\\] for \\(\\lim_{u\\to 0} r_c(u) / \\norm{u} = 0\\). Then \\(\\mathrm{d}f(c;u) = A(c)u\\) is the differential of \\(f\\) evaluated at \\(c\\) of increment \\(u\\).\nIn fact, this is the multivariate Taylor expansion. Each row of \\(A(c)\\) is the term \\(\\nabla_x f_i(x) \\mid_{x = c}\\) where \\(f_i\\) is the \\(i^\\mathrm{th}\\) entry of the length-\\(n\\) vector \\(f(x)\\).\n\n\nGeneralizing to matrix valued functions\nThe same idea applies to matrices, when combined with the \\(\\text{vec}\\) function, which concatenates an \\(n \\times p\\) matrix column by column into an \\(n \\times p\\)-length vector. Let \\(F\\) be a matrix function \\(\\R^{n \\times q} \\to \\R^{m \\times p}\\). Let \\(C\\) and \\(U\\) be in \\(\\R^{m\\times q}\\). If \\(A(C) \\in \\R^{mp \\times nq}\\) such that: \\[\n\\text{vec}(F(C + U))  = \\text{vec}(F(C)) + A(C) \\text{vec}(U) + \\text{vec}(R_c(U)).\n\\] Then the \\(m\\times p\\) matrix \\(\\mathrm{d} F(C;\\,U)\\) is defined by \\(\\text{vec}(\\mathrm{d} F(C;\\,U)) = A(C) \\text{vec}(U)\\).\nThe reason to do this is because the differential generalizes to matrices a bit easier than do partial derivatives. This is because it isn’t clear along which dimensions the partial derivatives should lie: Should the partials of a matrix function become a third dimension, like a 3-d array?\nUnder this framework, the rows of the matrices \\(A(c)\\) and \\(A(C)\\) correspond to a dimension of the range of the function \\(f(c)\\) or \\(F(C)\\), while the columns correspond to a dimension of the domain.\n\n\nThe chain rule\nThe power of the differentials is laid bare when working through the chain rule, which is called Cauchy’s rule of invariance in differential-land. Let \\(f: \\R^m \\to \\R^p\\) and \\(g: \\R^p \\to \\R^n\\), and let \\(h = g \\circ f\\). Then \\(h: \\R^m \\to \\R^n\\). If \\(b = f(c)\\) and \\(h = g(b)\\) the differential of \\(h\\) is: \\[\n\\begin{aligned}\n\\mathrm{d}(h;\\,u) & = \\mathrm{d}(h;\\,\\mathrm{d}(f;\\,c)) \\\\\n& = A_{g}(b) A_{f}(c) u\n\\end{aligned}\n\\] where \\(A_g(b) \\in R^{n \\times p}\\) and \\(A_f(c) \\in R^{p \\times m}\\) and \\(u \\in \\R^m\\).\nWe can show this rigorously with our previous definitions. \\[\n\\begin{align}\nh(c + u) & = g(f(c + u)) \\\\\n& = g(f(c) + A_f(c)u + r_c(u))  \\\\\n& = g(b + v) \\tag{$v = A_f(c)u + r_c(u)$}\\\\\n& = g(b) + A_g(b) v + r_b(v) \\\\\n& = g(b) + A_g(b) \\lp A_f(c) u  + r_c(u) \\rp + r_b(A_f(c) u + r_c(u)) \\\\\n& = h(c) + A_g(b) A_f(c) u  + A_g(b) r_c(u) + r_c(u) \\\\\n& = h(c) + A_g(b) A_f(c) u  + r_c(u)\n\\end{align}\n\\]\n\n\nDifferential with respect to \\(\\mu\\)\nFirst we’ll ignore the differential with respect to \\(\\Sigma\\). We’ll expand out that quadratic form into the parts that depend only on \\(\\mu\\): \\[\n\\mathrm{d} \\ell_{Y}(\\mu, \\Sigma \\mid y_i) = y_i^T\\Sigma^{-1}\\mathrm{d}\\mu - \\frac{1}{2}\\mathrm{d}(\\mu^T\\Sigma^{-1}\\mu)\n\\]\nTaking the gradient with respect to \\(\\mu\\) we get: \\[\n\\begin{aligned}\n\\mathrm{d} \\ell_{Y}(\\mu, \\Sigma \\mid y_i) & = y_i^T\\Sigma^{-1}\\mathrm{d} \\mu - \\frac{1}{2}\\mathrm{d}(\\mu^T)\\Sigma^{-1}\\mu - \\frac{1}{2}\\mu^T\\mathrm{d}(\\Sigma^{-1}\\mu) \\\\\n& = y_i^T\\Sigma^{-1}\\mathrm{d} \\mu - \\frac{1}{2}\\mathrm{d}(\\mu)^T\\Sigma^{-1}\\mu - \\frac{1}{2}\\mu^T\\Sigma^{-1}\\mathrm{d}\\mu \\\\\n& = y_i^T\\Sigma^{-1}\\mathrm{d} \\mu - \\frac{1}{2}\\mu^T\\Sigma^{-1}\\mathrm{d}\\mu - \\frac{1}{2}\\mu^T\\Sigma^{-1}\\mathrm{d}\\mu \\\\\n& = y_i^T\\Sigma^{-1}\\mathrm{d} \\mu - \\mu^T\\Sigma^{-1}\\mathrm{d}\\mu \\\\\n& = (y_i - \\mu)^T\\Sigma^{-1}\\mathrm{d} \\mu  \\\\\n\\end{aligned}\n\\] If we sum over the \\(n\\) terms of the log-likelihood we get: \\[\n\\begin{aligned}\n\\frac{\\partial \\ell_{Y}(\\mu, \\Sigma \\mid y_i)}{\\partial \\mu} & = (\\sum_i y_i - n \\mu)^T\\Sigma^{-1}\n\\end{aligned}\n\\] leading to the MLE for \\(\\mu\\): \\[\n\\hat{\\mu} = \\frac{1}{n}\\sum_i y_i\n\\]\nIt’ll be useful to write the log-likelihood a bit differently to find the MLE for \\(\\Sigma\\). Remember that \\(\\det A^{-1} = (\\det A)^{-1}\\). This will enable us to write everything in terms of \\(\\Sigma^{-1}\\) instead of \\(\\Sigma\\): \\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = \\frac{1}{2} \\log (\\det\\Sigma^{-1}) - \\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu)\n\\] Also remember that \\(\\text{tr}(A) = \\sum_{i} A_{ii}\\), \\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\), and that \\(f(x) = \\text{tr}(f(x))\\) for a univariate function \\(f(x)\\). Finally, recall that \\(\\text{tr}(ABC) = \\text{tr}(CAB) = \\text{tr}(BCA)\\). This will let us rewrite the\nPutting all this together allows us to write the log-likelihood for the multivariate normal as such: \\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = \\frac{1}{2} \\log (\\det\\Sigma^{-1}) -\\frac{1}{2}\\text{tr}\\lp(y_i - \\mu) (y_i - \\mu)^T \\Sigma^{-1}\\rp\n\\]\nFor the partial derivative of \\(\\det \\Sigma^{-1}\\) with respect to \\(\\Sigma^{-1}\\), we get \\[\n\\frac{\\partial \\det \\Sigma^{-1}}{\\partial \\Sigma^{-1}} = \\det \\Sigma^{-1} ((\\Sigma^{-1})^{-1})^{T}\n\\] and for the partial derivative of \\(\\text{tr}(AB)\\) with respect to \\(B\\) we get \\(A^T\\), so the partial derivative with respect to \\(\\Sigma^{-1}\\) of the log-likelihood gives us: \\[\n\\frac{\\partial \\ell_{Y}(\\mu, \\Sigma \\mid y_i)}{\\partial \\Sigma^{-1}} = \\frac{1}{2} \\Sigma -\\frac{1}{2}(y_i - \\mu) (y_i - \\mu)^T\n\\] Summing over the \\(n\\) terms gives: \\[\n\\frac{\\partial \\ell_{Y}(\\mu, \\Sigma \\mid Y)}{\\partial \\Sigma^{-1}} = \\frac{n}{2} \\Sigma -\\frac{1}{2}\\sum_i (y_i - \\mu) (y_i - \\mu)^T\n\\] \\[\n\\hat{\\Sigma} = \\frac{1}{n}\\textstyle\\sum_i (y_i - \\hat{\\mu}) (y_i - \\hat{\\mu})^T\n\\]\n\n\nNormal repeated measures models\nIn many longitudinal studies where some outcome of interest is measured for participants \\(K\\) times, the following model may describe the data generating process well, where \\(y_i \\in \\R^K\\) and \\(X_i\\) is a \\(K \\times m\\) design matrix: \\[\n\\begin{aligned}\ny_i \\mid X_i \\sim \\text{MultiNormal}(X_i \\beta, \\Sigma(\\psi))\n\\end{aligned}\n\\] The textbook lists several models that could describe different scenarios.\n\nIndependent-but-not-identically-distributed observations within groups: \\[\n\\begin{aligned}\ny_{ik} \\mid (X_{i})_k \\, & = (X_{i})_k \\beta + \\epsilon_{ik} \\\\\n\\epsilon_{ik} & \\sim \\text{Normal}(0, \\sigma^2_k) \\\\\n\\epsilon_{ik} & \\indy \\epsilon_{jl} \\forall i\\neq j \\cup k \\neq l\n\\end{aligned}\n\\] This implies the following simple structure for \\(\\Sigma(\\psi)\\) above:\n\n\\(\\Sigma(\\psi) = \\text{diag}(\\sigma^2_1, \\dots, \\sigma^2_K)\\).\n\nCompound symmetry (I’ll call this a random intercept model): \\[\n\\begin{aligned}\ny_{ik} \\mid (X_{i})_k \\, & = (X_{i})_k \\beta + \\gamma_i + \\epsilon_{ik} \\\\\n\\epsilon_{ik} & \\sim \\text{Normal}(0, \\sigma^2) \\\\\n\\epsilon_{ik} & \\indy \\epsilon_{jl} \\forall i\\neq j \\cup k \\neq l \\\\\n\\gamma_i & \\sim \\text{Normal}(0, \\tau^2) \\\\\n\\gamma_i & \\indy \\gamma_j \\forall i \\neq j \\\\\n\\gamma_i & \\indy \\epsilon_{ij} \\forall j\n\\end{aligned}\n\\] Conditional on \\(X_i\\), the covariance between \\(y_{ik}\\) and \\(y_{ij}\\) is: \\[\n\\begin{aligned}\n\\text{Cov}(y_{ik}, y_{ij} \\mid X_i) & = \\text{Cov}((X_{i})_k \\beta + \\gamma_i + \\epsilon_{ik}, (X_{i})_j \\beta + \\gamma_i + \\epsilon_{ij} \\mid X_i) \\\\\n& = \\tau^2\n\\end{aligned}\n\\] While the variance is \\(\\tau^2 + \\sigma^2\\). This implies that we can write the variance-covariance matrix as \\[\n\\Sigma(\\psi) = \\tau^2 1_{K} 1_K^T + \\sigma^2 I_K.\n\\]\nAutoregressive (I’ll call this a random intercept model):\n\n\\[\n\\begin{aligned}\ny_{ik} \\mid (X_{i})_k \\, & = (X_{i})_k \\beta + \\epsilon_{ik} \\\\\n\\epsilon_{ik} \\mid \\epsilon_{i,k-1} & \\sim \\text{Normal}(\\rho \\epsilon_{i,k-1}, \\sigma^2) \\\\\n\\epsilon_{i1} & \\sim \\text{Normal}(0, \\frac{\\sigma^2}{1 - \\rho^2}) \\\\\n\\epsilon_{ik} & \\indy \\epsilon_{jl} \\forall i\\neq j \\cap \\forall k, l\n\\end{aligned}\n\\]\nThis implies the following simple structure for \\(\\Sigma(\\psi)\\):\n\\[\\Sigma(\\psi)_{ij} = \\frac{\\sigma^2}{1-\\rho^2} \\rho^{\\abs{i-j}}\\]\n\nRandom effects model\n\nThis is a more general version of the random intercept model. Let \\(z_k \\in \\R^q\\).\n\\[\n\\begin{aligned}\ny_{ik} \\mid (X_{i})_k \\, & = (X_{i})_k \\beta + z_k^T \\gamma_i + \\epsilon_{ik} \\\\\n\\epsilon_{ik} & \\sim \\text{Normal}(0, \\sigma^2) \\\\\n\\epsilon_{ik} & \\indy \\epsilon_{jl} \\forall i\\neq j \\cup k \\neq l \\\\\n\\gamma_i & \\sim \\text{Normal}(0, \\Omega) \\\\\n\\gamma_i & \\indy \\gamma_j \\forall i \\neq j \\\\\n\\gamma_i & \\indy \\epsilon_{ij} \\forall j\n\\end{aligned}\n\\] We can write this in matrix form as:\n\\[\ny_i \\mid X_i = X_i \\beta + Z \\gamma + \\epsilon_i\n\\] The conditional covariance is \\[\n\\begin{aligned}\n\\text{Cov}(y_i \\mid X_i) & = \\text{Cov}(X_i \\beta + Z \\gamma + \\epsilon_i \\mid X_i) \\\\\n& = Z \\Omega Z^T + \\sigma^2 I_K\n\\end{aligned}\n\\]\n\nHierarchical Gaussian process model\n\nSuppose we have time points \\(t_{i1}, \\dots, t_{iK}\\) associated with each measurement \\(y_{i1}, \\dots, y_{iK}\\).\nLet’s define the function \\(\\Omega\\), which is from \\(\\R^K \\to \\Sigma\\) where \\(\\Sigma\\) is the space of positive definite \\(m\\times m\\) matrices. Let \\(t_i \\in \\R^K\\) and let \\(\\Omega(\\mathbf{t} \\mid \\ell, \\sigma^2)\\) be defined \\[\n\\Omega(t_i \\mid \\ell, \\sigma^2)_{jk} = \\sigma^2 \\exp(-\\lp t_{ij} - t_{ik} \\rp^2/(2\\ell^2))\n\\] Then the following model is a hierarchical Gaussian process model\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & \\sim \\text{MultiNormal} \\lp X_{i} \\beta, \\Omega(t_i \\mid \\ell, \\sigma^2)\\rp\n\\end{aligned}\n\\]\n\n\nMLEs in repeated measure models\nThe book suggests the following strategy to find the MLEs in the unstructured case, which is \\(1\\) above:\nTake \\(\\beta^{(0)}\\) and \\(\\Sigma^{(0)}\\) as initial guesses. Then for \\(t = 1\\) until some termination criterion iterate:\n\\[\n\\beta^{(t+1)} = \\left(\\sum_i X_i^T (\\Sigma^{(t)})^{-1}X_i\\right)^{-1} \\sum_i X_i^T (\\Sigma^{(t)})^{-1}y_i\n\\] and \\[\n\\Sigma^{(t+1)} = \\frac{1}{n}\\sum_i (y_i - X_i \\beta^{(t+1)}) (y_i - X_i \\beta^{(t+1)})^T\n\\] We can derive these update rules from the log-likelihood, but we’ll need to rewrite the model so that it looks a little more familiar.\nThe model as written in matrix form by unit \\(i\\) is: \\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j\n\\end{aligned}\n\\] Let \\(y = (y_1^T, y_2^T, \\dots, y_n^T)^T\\) and let \\(X = (X_1^T, X_2^T, \\dots, X_n^T)^T\\), and let \\(\\epsilon = (\\epsilon_1^T, \\epsilon_2^T, \\dots, \\epsilon_n^T)^T\\). Then the model can be written: \\[\n\\begin{aligned}\ny \\mid X \\, & = X \\beta + \\epsilon \\\\\n\\epsilon & \\sim \\text{Normal}(0, I_n \\otimes \\Sigma)\n\\end{aligned}\n\\] so \\(\\text{Cov}(\\epsilon)\\) is block-diagonal: \\[\n\\begin{bmatrix}\n\\Sigma & 0 & \\dots & 0 \\\\\n0 & \\Sigma & \\dots & 0 \\\\\n0 & 0 & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\Sigma\n\\end{bmatrix}\n\\] The log-likelihood is:\n\\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = -\\frac{1}{2} \\log \\det(I_n \\otimes \\Sigma) -\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\n\\] The determinant of \\(I_n \\otimes \\Sigma\\) is \\(\\det(\\Sigma)^n\\) because it’s just block-diagonal, and the inverse of \\(I_n \\otimes \\Sigma\\) is similarly \\(I_n \\otimes \\Sigma^{-1}\\).\nLet’s focus on the \\(\\beta\\) terms. Expanding the quadratic form gives: \\[\n-\\frac{1}{2}(y^T(I_n \\otimes \\Sigma)^{-1}y + y^T (I_n \\otimes \\Sigma)^{-1}X \\beta\n-\\frac{1}{2} \\beta^T X^T (I_n \\otimes \\Sigma)^{-1}X \\beta\n\\]\nTaking the derivative with respect to \\(\\beta\\) gives: \\[\ny^T (I_n \\otimes \\Sigma)^{-1}X \\mathrm{d}\\beta\n-\\frac{1}{2} \\mathrm{d} \\beta^T X^T (I_n \\otimes \\Sigma)^{-1}X \\beta -\\frac{1}{2}  \\beta^T X^T (I_n \\otimes \\Sigma)^{-1}X \\mathrm{d}\\beta\n\\] Collecting terms gives: \\[\n(y^T (I_n \\otimes \\Sigma^{-1})X\n- \\beta^T X^T (I_n \\otimes \\Sigma)^{-1}X) \\mathrm{d}\\beta\n\\] This looks more daunting than it is, we can use block matrix multiplication to get: \\[\n(\\sum_i y_i^T \\Sigma^{-1} X_i - \\beta^T \\sum_i X_i^T  \\Sigma^{-1} X ) \\mathrm{d}\\beta\n\\] If \\(\\Sigma\\) were known, we could solve this equation simply:\n\\[\n\\hat{\\beta} = \\textstyle(\\sum_i X_i^T  \\Sigma^{-1} X)^{-1} (\\sum_i X_i \\Sigma^{-1} y_i)  \n\\]\nLike we did above, we can rewrite the likelihood in terms of \\(\\Sigma^{-1}\\) to give:\n\\[\n\\begin{aligned}\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) & = \\frac{n}{2} \\log \\det(\\Sigma^{-1}) - \\frac{1}{2}\\sum_i(y_i - X_i \\beta)^T\\Sigma^{-1}(y_i - X_i \\beta)  \\\\\n& = \\frac{n}{2} \\log \\det(\\Sigma^{-1}) -\\frac{1}{2}\\sum_i \\text{tr}(y_i - X_i \\beta)^T\\Sigma^{-1}(y_i - X_i \\beta)  \\\\\n& = \\frac{n}{2} \\log \\det(\\Sigma^{-1}) -\\frac{1}{2}\\sum_i \\text{tr}(y_i - X_i \\beta)(y_i - X_i \\beta)^T\\Sigma^{-1}  \\\\\n& = \\frac{n}{2} \\log \\det(\\Sigma^{-1}) -\\frac{1}{2}\\lp \\sum_i \\text{tr}(y_i - X_i \\beta)(y_i - X_i \\beta)^T\\rp\\Sigma^{-1}  \\\\\n\\end{aligned}\n\\] Taking derivatives with respect to \\(\\Sigma^{-1}\\) gives:\n\\[\n\\begin{aligned}\n\\mathrm{d} \\ell_{Y}(\\mu, \\Sigma \\mid y_i) & = \\frac{n}{2} \\Sigma\\,\\mathrm{d} \\Sigma^{-1} -\\frac{1}{2}\\sum_i (y_i - X_i \\beta) (y_i - X_i \\beta)^T \\mathrm{d} \\Sigma^{-1} \\\\\n& = \\lp \\frac{n}{2} \\Sigma -\\frac{1}{2}\\sum_i (y_i - X_i \\beta) (y_i - X_i \\beta)^T\\rp \\mathrm{d} \\Sigma^{-1}\n\\end{aligned}\n\\] Both of these derivatives have to be zero at the maximum likeihood estimate (assuming we’re not on a boundary of the parameter space), so we’ll get two sets of equations:\n\\[\n\\Sigma^{(t+1)} = \\frac{1}{n} \\sum_i (y_i - X_i \\beta^{(t)}) (y_i - X_i \\beta)^T\n\\]\n\\[\n\\beta^{(t+1)} = \\textstyle(\\sum_i X_i^T  (\\Sigma^{(t)})^{-1} X_i)^{-1} (\\sum_i X_i (\\Sigma^{(t)})^{-1} y_i)  \n\\]"
  },
  {
    "objectID": "survival-material/lecture-6.html",
    "href": "survival-material/lecture-6.html",
    "title": "Lecture 6",
    "section": "",
    "text": "1 More on log-rank tests\nI motivated the log-rank test by stating that we wanted to compare estimates of the hazard function. Let’s do a quick derivation to show why this is the case: We start with the weighted log-rank test as we have derived it: \\[\\begin{align}\n    Z_j(\\tau) & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(d_{ij} - d_i\\frac{\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\n\\end{align}\\] We can express this in terms of hazard estimators \\(\\hat{\\lambda}_j(t_i) = \\frac{d_{ij}}{\\widebar{Y}_j(t_i)}\\): Let’s let \\(j \\in \\{1,2\\}\\). Then \\[\\begin{align*}\n    \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(d_{ij} - d_i\\frac{\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)& = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(\\frac{d_{ij}\\widebar{Y}(t_i)-d_i\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\\\\\n    & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(\\frac{d_{ij}\\widebar{Y}(t_i)-(d_{ij} + d_{ij^\\prime})\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\\\\\n    & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(\\frac{d_{ij}\\widebar{Y}_{j^\\prime}(t_i)-d_{ij^\\prime}\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\\\\\n    & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\frac{\\widebar{Y}_{j^\\prime}(t_i)\\widebar{Y}_{j}(t_i)}{\\widebar{Y}(t_i)}\\left(\\frac{d_{ij}}{\\widebar{Y}_j(t_i)}-\\frac{d_{ij^\\prime}}{\\widebar{Y}_{j^\\prime}(t_i)}\\right)\n\\end{align*}\\] Thus we can see that \\(Z_1(\\tau) = -Z_2(\\tau)\\). Let’s rewrite this in terms of integrals over the positive reals \\[\\begin{align*}\n    \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\frac{\\widebar{Y}_{j^\\prime}(t_i)\\widebar{Y}_{j}(t_i)}{\\widebar{Y}(t_i)}\\left(\\frac{d_{ij}}{\\widebar{Y}_j(t_i)}-\\frac{d_{ij^\\prime}}{\\widebar{Y}_{j^\\prime}(t_i)}\\right)& = \\int_0^\\infty W(u) \\frac{\\widebar{Y}_{j^\\prime}(u)\\widebar{Y}_{j}(u)}{\\widebar{Y}(u)} \\left(d\\hat{\\Lambda}_1(u) - d\\hat{\\Lambda}_2(u)\\right)\\\\\n    & = \\int_0^\\infty W(u) \\frac{\\widebar{Y}_{j^\\prime}(u)\\widebar{Y}_{j}(u)}{\\widebar{Y}(u)}  d\\left(\\hat{\\Lambda}_1(u) - \\hat{\\Lambda}_2(u) \\right)\n\\end{align*}\\] A more general Lebesgue-Stieltjies theory will show that the integral above is well-defined. More on this later…\nLet’s say we’re going to test multiple groups for equality of hazard rates. Then we will write the log-rank statistic like so, with \\(n = \\sum_{j=1}^J n_j\\): \\[\\begin{align}\n    Z_j(\\tau) & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n} W(t_i) \\left(d_{ij} - d_i\\frac{\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\n\\end{align}\\] The variance of \\(Z_j(\\tau)\\) is as was derived. We can show that \\(d_{i1}, \\dots, d_{iJ} \\mid d_i, \\widebar{Y}_1(t_i), \\dots, \\widebar{Y}_J(t_i)\\) is multivariate hypergeometric distributed. That means we can derive the variance and the covariance for these random variables. I’ll spare the details here. Given the result that in the two-group test, \\(Z_1(\\tau) = -Z_2(\\tau)\\), we might expect the \\(Z_j(\\tau)\\) to be linearly dependent. This is indeed the case, which we can see from the fact that the sum of all \\(Z_j(\\tau)\\) is zero. Then we might ask how do we construct a test statistic from a degenerate random variable. The answer is that we choose \\(J-1\\) of the statistics, and it doesn’t matter which statistics we choose. Given the covariance matrix \\(\\Sigma\\), we can construct a quadratic form: \\[\\begin{align}\n    \\chi^2 & = (Z_1(\\tau), Z_2(\\tau), \\dots, Z_{J-1}(\\tau)) \\Sigma^{-1} (Z_1(\\tau), Z_2(\\tau), \\dots, Z_{J-1}(\\tau))^T\n\\end{align}\\] which, under \\(H_0\\), is asymptotically distributed \\(\\chi^2\\) with \\(J-1\\) degrees of freedom.\nLet \\(\\mathbf{Z}(\\tau) = (Z_1(\\tau), Z_2(\\tau), \\dots, Z_{J}(\\tau))^T\\) and let \\(\\Sigma = \\text{Cov}(\\mathbf{Z}(\\tau))\\). To show why it doesn’t matter which groups we choose, imagine we have two matrices \\(A\\in\\R^{J-1 \\times J}\\) and \\(B\\in\\R^{J-1 \\times J}\\) which, when left multiplying the vector\\(\\mathbf{Z}(\\tau)\\) select subsets of the \\(J-1\\) groups. An example of \\(A\\) for \\(J = 3\\) might be: \\[\\begin{align}\n    \\begin{bmatrix}\n        1 & 0 & 0 \\\\\n        0 & 1 & 0\n    \\end{bmatrix}\n\\end{align}\\] Let both \\(A\\) and \\(B\\) be rank \\(J - 1\\). We define \\(\\chi^2_A\\) to be \\[\\begin{align}\n    \\chi^2_A & = (A \\mathbf{Z}(\\tau))^T (A \\Sigma A^T)^{-1} A\\mathbf{Z}(\\tau) \\\\\n    \\chi^2_B & = (B \\mathbf{Z}(\\tau))^T (B \\Sigma B^T)^{-1} B\\mathbf{Z}(\\tau)\n\\end{align}\\] As \\(A\\) and \\(B\\) are full-row-rank there exists an invertible matrix \\(C\\) such that \\(B = C A\\). Then \\[\\begin{align}\n    \\chi^2_B & = (C A \\mathbf{Z}(\\tau))^T (C A \\Sigma A^T C^T)^{-1} C A \\mathbf{Z}(\\tau) \\\\\n    & = \\mathbf{Z}(\\tau))^T A^T C^T (C^T)^{-1}(A \\Sigma A^T)^{-1} C^{-1} C A \\mathbf{Z}(\\tau)  \\\\\n    & = \\mathbf{Z}(\\tau))^T A^T (A \\Sigma A^T)^{-1} A \\mathbf{Z}(\\tau)   \\\\\n    & = (A \\mathbf{Z}(\\tau))^T (A \\Sigma A^T)^{-1} A \\mathbf{Z}(\\tau)    \\\\\n    & = \\chi^2_A\n\\end{align}\\]"
  },
  {
    "objectID": "survival-material/lecture-7.html",
    "href": "survival-material/lecture-7.html",
    "title": "Lecture 7",
    "section": "",
    "text": "1 Parametric and nonparametric regression models\nThis chapter combines content from (Aalen, Borgan, and Gjessing 2008), (Klein, Moeschberger, et al. 2003), (Harrell et al. 2001), (Collett 1994), and (Keener 2010).\nThus far we have dealt exclusively with simple univariate estimation. More often than not, we will also have covariates associated with our failure time observations. Let the observed failure data, be, as usual \\(X_i\\) is time to failure, \\(C_i\\) is time to censoring, \\(T_i = \\min(X_i, C_i)\\), is the observed event time, and \\(\\delta = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\) is the censoring indicator. Suppose we also have covariates for each individual \\(i\\) \\(\\mathbf{z}_i \\in \\R^k\\). These could be age, sex at birth, comorbidities. Over a short enough timespan, these covariates can be considered fixed over time. Other covariates, like blood pressure, or time since last colonoscopy, would be time varying covariates, which we’ll denote as \\(\\mathbf{z}(x)_i\\).\nMuch of our study has been on the hazard function \\(\\lambda(t)\\). We’ll consider this parameterized by a vector of parameters \\(\\boldsymbol{\\theta}\\), so we’ll write \\(\\lambda(t \\mid \\boldsymbol{\\theta})\\) for the hazard function. In order to incorporate covariates into the hazard rate, we’ll work with relative risk regression, or \\[\\lambda_i(t) = \\lambda_0(t \\mid \\boldsymbol{\\theta}) r(\\boldsymbol{\\beta}, \\mathbf{z}_i)\\] where \\(r\\) is a function \\(\\R \\to \\R^+\\). Note that this assumes that all individuals share a common baseline hazard, \\(\\lambda_0(t \\mid \\boldsymbol{\\theta})\\), and have time-invariant, individual relative risk contributions \\(r(\\boldsymbol{\\beta},\\boldsymbol{z}_i)\\). A common choice is that \\(r(\\boldsymbol{\\beta},\\boldsymbol{z}_i) \\equiv \\exp(\\boldsymbol{z}_i^T \\boldsymbol{\\beta})\\).\nThe function is called the relative risk function because when we compare the hazard rates for two individuals \\(i\\) and \\(j\\), the common baseline hazard drops out of the comparison: \\[\\frac{\\lambda_i(t)}{\\lambda_j(t)} = \\exp(\\boldsymbol{z}_i)^T \\boldsymbol{\\beta}) / \\exp(\\boldsymbol{z}_j^T \\boldsymbol{\\beta}).\\] Of course, the above holds with general \\(r(\\boldsymbol{\\beta},\\boldsymbol{z}_i)\\). Let’s see what this implies for the survival function for \\(i\\) vs. \\(j\\): \\[\\begin{align*}\n    S_i(t) & = \\exp\\left(-\\int_0^t  e^{\\mathbf{z}_i^T \\boldsymbol{\\beta}} \\lambda_0(u \\mid \\boldsymbol{\\theta}) du\\right)\\\\\n    & = \\exp\\left(-\\int_0^t \\lambda_0(u \\mid \\boldsymbol{\\theta}) du \\right)^{e^{\\mathbf{z}_i^T \\boldsymbol{\\beta}}} \\\\\n    & = \\left(\\exp\\left(-\\int_0^t \\lambda_0(u \\mid \\boldsymbol{\\theta}) du\\right)^{e^{\\mathbf{z}_j^T \\boldsymbol{\\beta}}}\\right)^{\\frac{e^{\\mathbf{z}_i^T \\boldsymbol{\\beta}}}{e^{\\mathbf{z}_j^T \\boldsymbol{\\beta}}}}  \\\\\n    & = \\left(\\exp\\left(-\\int_0^t \\lambda_0(u \\mid \\boldsymbol{\\theta}) du\\right)^{e^{\\mathbf{z}_j^T \\boldsymbol{\\beta}}}\\right)^{e^{(\\mathbf{z}_i^T - \\mathbf{z}_j^T) \\boldsymbol{\\beta}}}  \\\\\n    & = S_j(t)^{e^{(\\mathbf{z}_i^T - \\mathbf{z}_j^T) \\boldsymbol{\\beta}}}\n\\end{align*}\\] What this means is that the survival curves never cross. To see why, note that \\(S_i(0) = S_j(0) = 1\\), and WLOG, suppose \\((\\mathbf{z}_i^T - \\mathbf{z}_j^T) \\boldsymbol{\\beta} \\leq 0\\). Then \\(S_i(t) \\geq S_j(t)\\) for all \\(t\\). See (Figure 1) for a demonstration of proportional hazards.\n\n\n\n\n\n\n\n\nFigure 1: Example of survival functions with proportional\n\n\n\n\n\nSee (Figure 1) for a demonstration of proportional hazards and (Figure 2) for a demonstration of survival functions which do not exhibit proportional hazards.\n\n\n\n\n\n\n\n\nFigure 2: Example of survival functions that do not adhere to proportional hazards\n\n\n\n\n\nProportional hazards (or relative risk) models assume that the survival functions never cross, which is a strong assumption.\nLet’s do a simple example.\n\nExample 1.1. Simple exponential regressionThe following example is adapted from (Collett 1994). Suppose we have individuals grouped into two groups, groups 1 and 2, and let \\(\\mathbf{z}_i\\) equal \\(1\\) for those in group 2 and \\(0\\) for those in group 1. Suppose further we have noninformative censoring, parameter separability, and exponentially distributed survival times with common baseline hazard of \\(\\lambda\\), so we have observed the following dataset: \\[\\{(t_i, \\delta_i, z_i), i = 1, \\dots, n\\}\\] Then the hazard rate for group \\(1\\) is \\(\\lambda\\), while the hazard in group \\(2\\) is \\(\\lambda e^\\beta\\). Let \\(n_1 = \\sum_i (1 - z_i)\\) and \\(n_2 = \\sum_i z_i\\). Then the likelihood contribution for the individuals for whom \\(z_i = 0\\) is \\[\\prod_{i \\mid z_i = 0} \\lambda^{\\delta_i} e^{-\\lambda t_i}\\] and the likelihood contribution for individuals in group 2 is \\[\\prod_{i \\mid z_i = 1} (\\lambda e^\\beta)^{\\delta_i} e^{-\\lambda e^{\\beta} t_i}\\] We can simplify this. Let \\(r_1 = \\sum_i (1 - z_i) \\delta_i\\), and let \\(r_2 = \\sum_i z_i \\delta_i\\). Let \\(T_1 = \\sum_i (1 - z_i) t_i\\), and \\(T_2 = \\sum_i z_i t_i\\). Then the joint likelihood may be written: \\[\\lambda^{r_1} e^{-\\lambda T_1} (\\lambda e^{\\beta})^{r_2} e^{-\\lambda e^\\beta T_2} = \\lambda^{r_1 + r_2} e^{-\\lambda T_1} e^{r_2 \\beta} e^{-\\lambda e^\\beta T_2}.\\] Let \\(\\ell(\\lambda, \\beta)\\) be the log-likelihood function. Then the score equations are \\[\\begin{align*}\n\\frac{\\partial}{\\partial \\lambda} \\ell(\\lambda, \\beta) &: \\frac{r_1 + r_2}{\\lambda} - T_1 - e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta} \\ell(\\lambda, \\beta) &: r_2 - \\lambda e^\\beta T_2\n\\end{align*}\\] solving these for the unknowns is \\[\\begin{align*}\n\\frac{r_1 + r_2}{T_1 + e^\\beta T_2} = \\lambda \\\\\n\\frac{r_2}{\\lambda T_2} = e^\\beta\n\\end{align*}\\] which simplifies to \\[\\begin{align*}\n\\hat{\\lambda} & = \\frac{r_1}{T_1} \\\\\n\\hat{e^\\beta} & = \\frac{T_1/r_1}{T_2/r_2} \\\\\n& = \\frac{r_2}{T_2}\\frac{T_1}{r_1}\n\\end{align*}\\] These estimates make sense: The first is the reciprocal of the average survival time for those in Group 1, and the second is the ratio of the average survival times in each group.\nWe can show using [exmp:mle-exp] that both of these estimators converge a.s. to the true values. \\(\\frac{r_2}{T_2} \\overset{\\text{a.s.}}{\\to} \\lambda e^{\\beta}\\), \\(\\frac{T_1}{r_1} \\overset{\\text{a.s.}}{\\to} \\frac{1}{\\lambda}\\)\nLet’s find the asymptotic variance of the estimand \\(\\beta\\)\n\\[\\begin{align}\n\\frac{\\partial}{\\partial \\lambda}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -\\frac{r_1 + r_2}{\\lambda^2} \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\beta}\\ell(\\lambda, \\psi)\\right)& = -\\lambda e^\\beta T_2\n\\end{align}\\] Then the observed information matrix is \\[\\begin{align}\n    \\begin{bmatrix}\n        \\frac{r_1 + r_2}{\\lambda^2} & e^\\beta T_2 \\\\\n        e^\\beta T_2 & \\lambda e^\\beta T_2\n    \\end{bmatrix}\n\\end{align}\\] which has the inverse: \\[\\begin{align}\n    \\frac{1}{\\frac{(r_1 + r_2)e^\\beta T_2}{\\lambda} - e^{2 \\beta} T_2^2}\\begin{bmatrix}\n      \\lambda e^\\beta T_2  & -e^\\beta T_2 \\\\\n        -e^\\beta T_2 & \\frac{r_1 + r_2}{\\lambda^2}\n    \\end{bmatrix}\n\\end{align}\\] So the plug-in standard error for \\(\\beta\\) is \\[\\sqrt{\\frac{\\frac{r_1 + r_2}{\\lambda^2}}{\\frac{(r_1 + r_2)e^\\beta T_2}{\\lambda} - e^{2 \\beta} T_2^2}}\\] Plugging in the MLEs gives \\[\\sqrt{\\frac{\\frac{r_1 + r_2}{(r_1 / T_1)^2}}{\\frac{(r_1 + r_2)\\frac{T_1r_2}{r_1}}{r_1 / T_1} - (\\frac{T_1r_2}{r_1})^2 }} = \\sqrt{\\frac{r_1 + r_2}{r_1 r_2}}\\] We can use this expression to generate an asymptotic confidence interval for \\(\\beta\\): \\[\\begin{align*}\n    P(\\beta \\in C^\\beta) = P\\left(\\beta \\in \\left(e^{\\hat{\\beta}} - z_{1-\\alpha/2} \\sqrt{\\frac{r_1 + r_2}{r_1 r_2}}, e^{\\hat{\\beta}} + z_{1-\\alpha/2} \\sqrt{\\frac{r_1 + r_2}{r_1 r_2}}\\right)\\right)\n\\end{align*}\\]\n\n\n\n\n\n\nReferences\n\nAalen, Odd, Ornulf Borgan, and Hakon Gjessing. 2008. Survival and Event History Analysis: A Process Point of View. Springer Science & Business Media.\n\n\nCollett, David. 1994. Modelling Survival Data in Medical Research. Chapman & Hall.\n\n\nHarrell, Frank E et al. 2001. Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis. Vol. 608. Springer.\n\n\nKeener, Robert W. 2010. Theoretical Statistics. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-93839-4.\n\n\nKlein, John P, Melvin L Moeschberger, et al. 2003. Survival Analysis: Techniques for Censored and Truncated Data. Vol. 1230. Springer."
  },
  {
    "objectID": "survival-material/lecture-8.html",
    "href": "survival-material/lecture-8.html",
    "title": "Lecture 8",
    "section": "",
    "text": "In the preceding example, we shied away from using the Fisher information because \\(T_2\\) was not easily accessible. But we can use the results from [[exmp:mle-exp]] to derive an exact expression for the asymptotic sampling variance for the MLE.\n\n\n\nContinued example This is an expansion of the example in (Collett 1994). \\[\\begin{align}\n\\frac{\\partial}{\\partial \\lambda}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -\\frac{r_1 + r_2}{\\lambda^2} \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\beta}\\ell(\\lambda, \\psi)\\right)& = -\\lambda e^\\beta T_2\n\\end{align}\\]\nWe know that \\[\\Exp{r_1} = n_1\\Exp{1 - e^{-\\lambda C_i}}{C_i}, \\, \\Exp{r_2} = n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}, \\textrm{and} \\, \\Exp{T_2} = n_2 \\frac{1}{\\lambda e^\\beta} \\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}\\] Then the Fisher information is \\[\\begin{align}\n    \\begin{bmatrix}\n        \\frac{n_1\\Exp{1 - e^{-\\lambda C_i}}{C_i} + n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}}{\\lambda^2} & \\frac{1}{\\lambda} n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i} \\\\\n        \\frac{1}{\\lambda} n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i} & n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}\n    \\end{bmatrix}\n\\end{align}\\] Let \\(\\Exp{r_{i1}} = \\Exp{1 - e^{-\\lambda C_i}}{C_i}\\) and \\(\\Exp{r_{i2}} = \\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}\\). We know the asymptotic variance of the MLE is the inverse of the Fisher information matrix. The inverse is: \\[\\begin{align}\n    \\frac{\\lambda^2}{n_{1} n_{2} \\Exp{r_{i1}}\\Exp{r_{i2}}}\n    \\begin{bmatrix}\n      n_2 \\Exp{r_{i2}}  & -n_2\\Exp{r_{i2}}/\\lambda \\\\\n        -n_2\\Exp{r_{i2}}/\\lambda & \\frac{n_1\\Exp{r_{i1}} + n_2\\Exp{r_{i2}}}{\\lambda^2}\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n      \\frac{\\lambda^2}{n_1\\Exp{r_{i1}}}  & -\\frac{\\lambda}{n_1 \\Exp{r_{i1}}} \\\\\n       -\\frac{\\lambda}{n_1 \\Exp{r_{i1}}} &\n\\frac{n_1\\Exp{r_{i1}} + n_2\\Exp{r_{i2}}}{n_1 n_2 \\Exp{r_{i1}}\\Exp{r_{i2}}}\n    \\end{bmatrix}\n\\end{align}\\] So the asymptotic standard error for \\(\\beta\\) is \\[\\sqrt{\\frac{n_1\\Exp{1 - e^{-\\lambda C_i}}{C_i} + n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}}{n_1 n_2\\Exp{1 - e^{-\\lambda C_i}}{C_i}\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}}}\\]"
  },
  {
    "objectID": "survival-material/lecture-8.html#asymptotic-confidence-intervals",
    "href": "survival-material/lecture-8.html#asymptotic-confidence-intervals",
    "title": "Lecture 8",
    "section": "2.3 Asymptotic confidence intervals",
    "text": "2.3 Asymptotic confidence intervals\nFor the most part, we’ll be concerned with univariate confidence intervals, but in multivariate models like the Weibull distribution we’ll need to compute the full inverse of the Fisher information. WLOG, let the index of the parameter of interest be \\(1\\), so the asymptotic variance of our MLE for the parameter of interest is \\(\\sigma_1^2(\\theta^\\dagger) = \\mathcal{I}(\\theta^\\dagger)^{-1}_{1,1}\\). We can also define \\[\\sigma_1^2(\\hat{\\theta}) = \\mathcal{I}(\\hat{\\theta})^{-1}_{1,1}.\\] I’ll also ditch the \\(n\\) subscript and just let \\(\\hat{\\theta}\\) be our MLE based on \\(n\\) observations. By [eq:cont-map], \\[\\frac{\\sigma_1^2(\\hat{\\theta})}{\\sigma_1^2(\\theta^\\dagger)} \\overset{p}{\\to} 1.\\] This allows us to use a plug-in estimator for \\(\\mathcal{I}(\\theta^\\dagger)^{-1}\\), \\(\\mathcal{I}(\\hat{\\theta})^{-1}\\). \\[\\begin{align*}\n     \\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta_1^\\dagger)}{\\sigma_1(\\hat{\\theta})} & = \\frac{\\sigma_1(\\theta^\\dagger)}{\\sigma_1(\\hat{\\theta})}\\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta_1^\\dagger)}{\\sigma_1(\\theta^\\dagger)} \\\\\n     & \\overset{d}{\\to} \\mathcal{N}(0, 1)\n\\end{align*}\\] Using [eq:prod-slutsky], we can create an asymptotic confidence interval by noting that: \\[P\\left(\\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta^\\dagger_1)}{\\sigma_1(\\hat{\\theta})} \\leq x\\right) = \\Phi(x),\\] where \\(\\Phi(x)\\) is the CDF a normal distribution with zero mean and unit variance.\nThen \\[\\begin{align*}\nP\\left(\\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta^\\dagger_1)}{\\sigma_1(\\hat{\\theta}_1)} \\in (-z_{1-\\alpha/2},z_{1-\\alpha/2})\\right) & = P\\left(\\theta_1^\\dagger \\in \\left(\\hat{\\theta}_1 - z_{1-\\alpha/2} \\frac{\\sigma_1(\\hat{\\theta}_1)}{\\sqrt{n}}, \\hat{\\theta}_1 + z_{1-\\alpha/2} \\frac{\\sigma_1(\\hat{\\theta}_1)}{\\sqrt{n}}\\right)\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-8.html#asymptotic-tests",
    "href": "survival-material/lecture-8.html#asymptotic-tests",
    "title": "Lecture 8",
    "section": "2.4 Asymptotic tests",
    "text": "2.4 Asymptotic tests\n\n2.4.1 Wald test\nThe Wald test is derived directly from the asymptotic distribution of the MLE. Under the null hypothesis \\(\\theta^\\dagger = \\theta_0\\), the test statistic: \\[\\begin{align*}\n     \\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta_0)^{-1})\n\\end{align*}\\] so \\[n (\\hat{\\theta}_n - \\theta_0)^T \\mathcal{I}(\\theta_0) (\\hat{\\theta}_n - \\theta_0) \\sim \\chi^2(p)\\] This follows from the simple fact that if a random vector in \\(\\R^n\\), \\(Z\\), is distributed multivariate normal, or \\(Z \\sim \\mathcal{N}(0, \\Sigma)\\), then \\(\\Sigma^{-1/2} Z \\sim \\mathcal{N}(0, I)\\), so \\(Z^T\\Sigma^{-1/2}\\Sigma^{-1/2}Z = \\sum_{i=1}^n X_i^2\\) where \\(X_i \\sim \\mathcal{N}(0,1)\\).\n\n\n2.4.2 Rao’s score test\nIn our proof of the asymptotic distribution of the MLE, we used the fact that \\[\\sqrt{n}\\frac{1}{n} \\sum_{i=1}^n \\ell_\\theta(\\theta^\\dagger;X_i) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger)).\\] This idea can be used to derive the Rao’s Score test, which uses the fact that under \\(H_0: \\theta \\in \\Theta_0\\), the gradient evaluated at the restricted MLE (i.e. the MLE restricted to the parameter space \\(\\Theta_0\\)) is nearly zero, and we can recover a similar limiting distribution.\nAssuming that under the null distribution the restricted MLE \\(\\hat{\\theta}_0\\) is consistent for \\(\\theta^\\dagger \\in \\Theta_0\\), then \\[\\sqrt{n} \\ell_\\theta(\\hat{\\theta}_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger))\\] The Score test statistic is: \\[T_S = \\left(\\sqrt{n}\\ell_\\theta(\\hat{\\theta}_0) \\right)^T \\mathcal{I}(\\hat{\\theta}_0)^{-1}\\lp \\sqrt{n}\\ell_\\theta(\\hat{\\theta}_0)\\rp\\] This test statistic is distribution \\(\\chi^2(p)\\) under \\(H_0\\).\n\n\n2.4.3 Likelihood ratio test\nThe LRT comes from a two-term asymptotic expansion of the log-likelihood, as opposed to the one term expansion: \\[\\begin{align*}\n    -\\ell(\\theta_0) & = -\\ell(\\hat{\\theta}_n) - \\ell_\\theta(\\hat{\\theta}_n)(\\hat{\\theta}-\\theta_0) - \\frac{1}{2}(\\hat{\\theta}-\\theta_0)^T\\ell_{\\theta\\theta}(\\tilde{\\theta}_n)(\\hat{\\theta}-\\theta_0) \\\\\n    \\ell(\\hat{\\theta}) - \\ell(\\theta_0)  & = -\\frac{1}{2}(\\hat{\\theta}-\\theta_0)^T\\ell_{\\theta\\theta}(\\tilde{\\theta}_n)(\\hat{\\theta}-\\theta_0) \\\\\n    & = \\frac{1}{2}(\\sqrt{n}(\\hat{\\theta}-\\theta_0))^T\\frac{-\\ell_{\\theta\\theta}(\\tilde{\\theta}_n)}{n}(\\sqrt{n}(\\hat{\\theta}-\\theta_0)) \\\\\n\\end{align*}\\] As before, \\[\\sqrt{n}(\\hat{\\theta}-\\theta_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta_0)^{-1})\\] and \\[-\\frac{\\ell_{\\theta\\theta}(\\tilde{\\theta}_n)}{n} \\overset{p}{\\to} \\mathcal{I}(\\theta_0)\\] so \\[2 (\\ell(\\hat{\\theta}) - \\ell(\\theta_0)) \\overset{d}{\\to} \\chi^2(p)\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-8-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-8-notes.html",
    "title": "Missing data lecture 8: Likelihood-based inference with incomplete data",
    "section": "",
    "text": "Likelihood inference with incomplete data\nWe said that the likelihood function is really a set of functions that are proportional the probability density such that the constant of proportionality doesn’t depend the parameters.\nMissing data methods distinguish themselves from other methods by modeling the joint distribution of \\(Y\\) and \\(M\\). Let \\(y\\) represent a possible set of values for \\(Y\\) and let \\(m\\) represent an element in the space \\(\\{0,1\\}^{n \\times K}\\), the missingness indicators for all \\(i\\) units. Let \\(\\theta \\in \\Omega_\\theta\\) be parameters that govern the marginal distribution of the data: \\(f_Y(y \\mid \\theta)\\) and \\(\\phi \\in \\Omega_\\phi\\) be parameters that govern the missingness mechanism: \\(f_{M \\mid Y}(M = m \\mid Y = y, \\phi)\\).\nThen the joint distribution for these RVs, \\(f_{Y,M}(y,m \\mid \\theta, \\phi)\\) can be written as the product of the marginal distribution for the data and the conditional distribution :\n\\[\nf_{Y,M}(y, M = m \\mid \\theta, \\phi) = f_{Y}(y \\mid \\theta) f_{M \\mid Y}(M = m \\mid Y = y, \\phi)\n\\] When we have missing data, we can partition the matrix \\(y\\) into \\(y_{(1)}\\) and \\(y_{(0)}\\), representing the components of \\(y\\) that are missing and observed, respectively.\nLet \\(\\mathcal{Y}\\) be the sample space for \\(y_i\\) and let \\(\\mathcal{Y}_{(1)}\\) and \\(\\mathcal{Y}_{(0)}\\) be the sample space for the missing and observed components of \\(y\\).\nThen the distribution of the observed data is: \\[\n\\int_{\\mathcal{Y}_{(1)}} f_{Y,M}(y, M = m \\mid \\theta, \\phi) dy_{(1)} = \\int_{\\mathcal{Y}_{(1)}} f_{Y}(y_{(0)},y_{(1)}  \\mid \\theta) f_{M \\mid Y}(M = m \\mid Y_{(0)} = y_{(0)},Y_{(1)} = y_{(1)}, \\phi)dy_{(1)}\n\\] This joint density is proportional to what we’ll call the full-data likelihood:\n\\[\nL_{\\text{full}}(\\theta, \\phi \\mid \\tilde{y}_{(0)}, \\tilde{m}) = \\int_{\\mathcal{Y}_{(1)}} f_{Y}(\\tilde{y}_{(0)},y_{(1)}  \\mid \\theta) f_{M \\mid Y}(M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)},Y_{(1)} = y_{(1)}, \\phi)dy_{(1)}\n\\] We can also compute the likelihood the missingness process:\n\\[\nL_{\\text{ign}}(\\theta \\mid \\tilde{y}_{(0)}) = \\int_{\\mathcal{Y}_{(1)}} f_{Y}(\\tilde{y}_{(0)},y_{(1)}  \\mid \\theta) dy_{(1)}\n\\] We’ll say that the missingness mechanism is if inferences based on \\(L_{\\text{ign}}(\\theta \\mid \\tilde{y}_{(0)})\\) and \\(L_{\\text{full}}(\\theta, \\phi \\mid \\tilde{y}_{(0)}, \\tilde{m})\\) are the same given \\(\\tilde{m}, \\tilde{y}_{(0)}\\).\nFormally, the missingness mechanism is ignorable for direct likelihood inference if the likelihood ratios for any two \\(\\theta, \\theta^*\\) given \\(\\tilde{m}, \\tilde{y}_{(0)}\\) are equal: \\[\n\\frac{L_{\\text{full}}(\\theta, \\phi \\mid \\tilde{y}_{(0)}, \\tilde{m})}{L_{\\text{full}}(\\theta^*, \\phi \\mid \\tilde{y}_{(0)}, \\tilde{m})} = \\frac{L_{\\text{ign}}(\\theta \\mid \\tilde{y}_{(0)})}{L_{\\text{ign}}(\\theta^* \\mid \\tilde{y}_{(0)})} \\forall \\theta, \\theta^*, \\phi\n\\] There are two sufficient conditions that ensure ignorability:\n\nParameters \\(\\theta\\) and \\(\\phi\\) are variationally independent, i.e. the joint parameter space \\(\\Omega_{\\theta,\\phi} = \\Omega_\\theta \\times \\Omega_\\phi\\)\nThe full likelihood factorizes as \\[\nL_{\\text{full}}(\\theta, \\phi \\mid \\tilde{y}_{(0)}, \\tilde{m}) = L_{\\text{ign}}(\\theta \\mid \\tilde{y}_{(0)}) L_\\text{rest}(\\phi \\mid \\tilde{y}_{(0)}, \\tilde{m})\n\\] The first condition is sufficient to ensure that the value of \\(\\phi\\) doesn’t lead to a different likelihood value for \\(\\theta\\) vs. \\(\\theta^*\\).\n\nIf the data are MAR, then we will satisfy the second condition:\n\\[\nf_{M \\mid Y} (M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}, \\phi) = f_{M \\mid Y} (M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}^*, \\phi)\n\\] for all \\(y_{(1)}, y_{(1)}^*, \\phi\\). Then we can write the full-likelihood as: \\[\nf_{M \\mid Y}(M = \\tilde{m} \\mid Y_{(0)}=\\tilde{y}_{(0)}, \\phi) \\int_{\\mathcal{Y}_{(1)}} f_{Y}(\\tilde{y}_{(0)},y_{(1)}  \\mid \\theta) dy_{(1)} = f_{M \\mid Y}(M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\phi)  f_{Y}(\\tilde{y}_{(0)}  \\mid \\theta)\n\\] Then by the above theorem, parameter distinctness and MAR are sufficient for ignorability.\nWhen we do Bayesian inference we need to ensure that the posterior for \\(\\theta\\) when using the ignorable likelihood is equal to the posterior for \\(\\theta\\) when using the full likelihood. Under the full likleihood, the posterior for \\((\\theta, \\phi)\\) is:\n\\[\np(\\theta, \\phi \\mid y_{(0)},m) \\propto p(\\theta, \\phi) L_{\\text{full}}(\\theta, \\phi \\mid y_{(0)}, m)\n\\] and under the ignorable likelihood we have:\n\\[\np(\\theta \\mid \\tilde{y}_{(0)},\\tilde{m}) \\propto p(\\theta) L_{\\text{ign}}(\\theta \\mid \\tilde{y}_{(0)})\n\\] Thus, sufficient conditions for the posteriors to be equal is that\n\n\\(p(\\theta, \\phi) = p(\\theta)p(\\phi)\\), or the prior independence of \\(\\theta\\) and \\(\\phi\\)\nThe full likelihood factorizes as \\[\nL_{\\text{full}}(\\theta, \\phi \\mid \\tilde{y}_{(0)}, \\tilde{m}) = L_{\\text{ign}}(\\theta \\mid \\tilde{y}_{(0)}) L_\\text{rest}(\\phi \\mid \\tilde{y}_{(0)}, \\tilde{m})\n\\]\n\n\n\nExample: Incomplete exponential sample\nLet \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Exponential}(\\theta)\\) for \\(i = 1, \\dots, n\\). Let \\(m_i\\) be the missingness indicators, and suppose \\(r = \\sum_{i=1}^n (1 - m_i)\\). The full likelihood is \\[\nf_Y(y \\mid \\theta) = \\theta^{-n}\\exp\\lp-\\sum_{i=1}^n y_i / \\theta \\rp\n\\] Let \\(y_{(0)} = (y_1, \\dots, y_r)\\) and \\(y_{(1)} = (y_{r+1}, \\dots, y_n)\\). The likelihood that ignores the likelihood is \\[\nL_{\\text{ign}}(\\theta \\mid y_{(0)}) = \\theta^{-r} \\exp\\lp-\\sum_{i=1}^r y_i / \\theta \\rp\n\\] Let \\(m_i \\overset{\\text{iid}}{\\sim} \\text{Bernoulli}(\\phi)\\), so \\[\nf_{M \\mid Y}(m \\mid y, \\phi) = \\phi^{r}(1 - \\phi)^{n-r}\n\\] Then \\(f(y_{(0)}, m \\mid \\theta, \\phi) = \\phi^{r}(1 - \\phi)^{n-r}\\theta^{-r} \\exp\\lp-\\sum_{i=1}^r y_i / \\theta \\rp\\), which factorizes into a factor related to \\(\\theta\\) and a factor related to \\(\\phi\\). This means we can base inferences on \\(\\theta\\) on \\(L_{\\text{ign}}(\\theta \\mid y_{(0)}\\) instead of the full likelihood. The MLE is \\(\\hat{\\theta} = \\sum_{i=1}^n y_i / r\\).\nNow suppose we observe only observations for which \\(y_i \\leq c\\), so \\[\nf(m_i \\mid y_i, \\phi) = \\ind{y_i \\geq c}^{m_i}\\ind{y_i &lt; c}^{1 - m_i}\n\\] Putting this together, the full likelihood is: \\[\n\\prod_{i=1}^r f_{Y}(y_i \\mid \\theta) \\ind{y_i &lt; c} \\prod_{i={r+1}}^n \\int_{\\R^+} \\ind{y_i \\geq c} f(y \\mid \\theta) dy\n\\]\nWhich of course simplifies to \\[\n\\theta^{-r} \\exp\\lp-\\sum_{i=1}^r y_i / \\theta \\rp\\exp(-(n - r) c / \\theta)\n\\] This shows that the missingness is nonignorable, because the full likelihood isn’t equal to the ignorable likelihood we used in the first part of the problem.\nThe log-likelihood is: \\[\n\\ell(\\theta \\mid y_{(0)}, m) = -r \\log \\theta - \\sum_{i=1}^r y_i / \\theta - (n - r) c / \\theta\n\\]\n\\[\n\\frac{\\partial \\ell(\\theta \\mid y_{(0)}, m)}{\\partial \\theta} = -r/\\theta + \\sum_{i=1}^r y_i / \\theta^2 + (n - r) c / \\theta^2\n\\] Setting this equal to zero and solving for \\(\\theta\\) gives the MLE: \\[\n\\hat{\\theta} = \\frac{\\sum_{i=1}^r y_i + (n - r) c}{r}\n\\] This of course doesn’t equal the ignorable MLE, \\(\\bar{y}\\) for the observed values.\n\n\nMissing data example: Parameter distinctness\nSuppose \\(y_{ij}, \\{i = 1, \\dots, n\\}, \\{j = 1, \\dots, n_i\\}\\) are univariate normal with mean \\(\\alpha_i\\) and standard deviation \\(\\sigma\\). Let \\(\\theta = (\\sigma^2, \\tau^2, \\mu)\\): \\[\n\\begin{aligned}\ny_{ij} \\mid \\alpha_i, \\theta & \\sim \\text{Normal}(\\alpha_i, \\sigma^2) \\\\\n\\alpha_i \\mid \\theta & \\sim \\text{Normal}(\\mu, \\tau^2)\n\\end{aligned}\n\\] Let the missingness mechanism be: \\[\nf_{M \\mid Y}(m_{ij} \\mid y, \\alpha_i, \\phi) = \\pi(\\alpha_i, \\phi) = (1 + e^{-(\\phi_0 + \\phi_1 \\alpha_i)})^{-1}\n\\] The joint density of the observations and parameters, also known as the complete data likelihood, is: \\[\n\\begin{aligned}\n\\prod_{i=1}^I \\lp\\prod_{j=1}^{n_i} \\frac{1}{\\sqrt{2 \\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(y_{ij} - \\alpha_i)^2} \\pi(\\alpha_i,\\phi)^{m_{ij}}(1 - \\pi(\\alpha_i,\\phi))^{1-m_{ij}}\\rp \\frac{1}{\\sqrt{2 \\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(\\alpha_i - \\mu)^2}\n\\end{aligned}\n\\] Because the \\(\\alpha_i\\) aren’t observed, but do have a density, we need to integrate over them to compute the full likelihood: \\[\n\\begin{aligned}\n\\prod_{i=1}^I \\int_{\\R} \\prod_{j=1}^{n_i} \\lp \\frac{1}{\\sqrt{2 \\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(y_{ij} - \\alpha_i)^2}\\rp^{1 - m_{ij}} \\pi(\\alpha_i,\\phi)^{m_{ij}}(1 - \\pi(\\alpha_i,\\phi))^{1-m_{ij}} \\frac{1}{\\sqrt{2 \\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(\\alpha_i - \\mu)^2}d\\alpha_i\n\\end{aligned}\n\\] This shows that the missingness process isn’t ignorable here, even though we don’t technically have the distribution of missingness depending on missing obesrvable data, per se. This shows that in some sense, \\(\\alpha_i\\) is missing data, and, indeed, this is what our textbook considers missing data; namely anything that has a distribution that is unobserved. This makes the problem MNAR.\nCompare this to the ANOVA model with the same missingness mechanism: \\[\n\\begin{aligned}\ny_{ij} \\mid \\alpha_i, \\theta & \\sim \\text{Normal}(\\alpha_i, \\sigma^2)\n\\end{aligned}\n\\] Then the joint likelihood is \\[\n\\begin{aligned}\n\\prod_{i=1}^I \\prod_{j=1}^{n_i} \\frac{1}{\\sqrt{2 \\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(y_{ij} - \\alpha_i)^2} \\pi(\\alpha_i,\\phi)^{m_{ij}}(1 - \\pi(\\alpha_i,\\phi))^{1-m_{ij}}\n\\end{aligned}\n\\] This shows that the data are MAR, because the missingness mechanism doesn’t depend on missing data. However, the parameters for the missingness mechanism and the observations don’t satisfy the distinctness condition, so the missingness is nonignorable.\n\n\nPartial MAR\nSuppose we can partition \\(\\theta\\) into two pieces, \\(\\theta_1\\) and \\(\\theta_2\\) so that the parameter of interest is \\(\\theta_1\\). The data are partially MAR for \\(\\theta_1\\) if we can factorize the full likelihood: \\[\nL_{\\text{full}}(\\theta_1, \\theta_2, \\phi \\mid y_{(0)}, m) = L_1(\\theta \\mid y_{(0)}) L_{\\text{rest}}(\\theta_2, \\phi \\mid y_{(0)}, m)\n\\]\n\n\nExample: Regression with missing data\nAn example of this is when we have covariates paired with each observation so that the complete data is \\((y_i, x_i), i = 1, \\dots, n\\) where \\(y_i \\in \\R^d\\) and \\(x_i \\in \\R^p\\). let \\(y_{(0)},y_{(1)}\\) be the observed and missing elements of \\(y_i\\) and \\(x_{(0)},x_{(1)}\\) are the observed and missing elements of \\(x_i\\). Let \\(m^X_{i}\\) be the missingness indicators for the covariates, \\(X\\), and let \\(m^Y_i\\) be the missingness indicators for the observations \\(y_i\\). Let \\(m_i = (m^Y_{i},m^X_{i})\\) be the combined missingness indicators for unit \\(i\\). Suppose that for \\(i=1, \\dots, r\\) \\(x_i\\) is fully observed, while at least one component of \\(y_i\\) is observed, and for the remaining \\(i = r+1, \\dots, n\\) \\(y_i\\) is completely missing and each \\(x_i\\) has at least one missing component. Let \\(y_i, x_i, m_i\\) be unit iid, so: \\[\nf_{Y,X,M} (y_i, x_i, m_i \\mid \\theta_1, \\theta_2, \\phi) = f_{Y \\mid X}(y_i \\mid x_i, \\theta_1) f_{X}(x_i \\mid \\theta_2) f_{M \\mid Y, X}(m_i \\mid y_i, x_i, \\phi)\n\\] We’ll assume the missingness mechanism takes the following form: \\[\nf_{M\\mid Y, X}(m_i \\mid x_{i(1)}, x_{i(0)}, y_i, \\phi) = f_{M\\mid Y, X}(m_i \\mid x_{i(1)}, x_{i(0)}, y_i^{\\star}, \\phi)\n\\] for all \\(y_i, y_i^\\star, x_{i(0)}, i = 1, \\dots, n\\).\nThis missingness mechanism is MNAR because it depends on unobserved components of \\(x_{i(1)}\\). Luckily we’ll be able to factorize our likelihood so inference \\(\\theta_1\\) is partially ignorable: \\[\nL_{\\text{full}}(\\theta_1, \\theta_2, \\phi \\mid y_{(0)}, x_{(0)}, m) = L_{\\text{p-ign}}(\\theta_1 \\mid y_{(0)}, x_{(0)}) L_{\\text{rest}}(\\theta_2, \\phi \\mid m, x_{(0)})\n\\] Let \\(\\mathcal{Y}_i\\) be the sample space corresponding to the missing \\(y_{i(1)}\\). Then we can write the ignorable part of the likelihood: \\[\nL_{\\text{p-ign}}(\\theta_1 \\mid y_{(0)}, x_{(0)}) = \\prod_{i=1}^r \\int_{\\mathcal{Y}_i} f_{Y\\mid X}(y_{i(0)}, y_{i(1)} \\mid x_i, \\theta_1) dy_{i(1)}\n\\] Let \\(\\mathcal{X}_i\\) be the sample space of the missing covariates for the \\(i^\\mathrm{th}\\) unit. Then the rest of the likelihood can be written as \\[\nL_{\\text{rest}}(\\theta_2, \\phi \\mid x_{i(0)}, m) =\\prod_{i=1}^r f_X(x_i \\mid \\theta_2) f_{M \\mid X}(m_i \\mid x_i, \\phi) \\prod_{i=r+1}^n \\int_{\\mathcal{X}_i} f_{X}(x_{i(0)}, x_{i(1)} \\mid \\theta_2) f_{M \\mid X} (m_i \\mid x_{i(1)}, x_{i(0)}, \\phi) dx_{i(1)}\n\\] Note the book has a typo here."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-8-notes.html#bayes-recap",
    "href": "missing-data-material-W-26/notes/lecture-8-notes.html#bayes-recap",
    "title": "Missing data lecture 5: Bayes",
    "section": "",
    "text": "If \\(\\hat{\\theta}\\) is the MLE then the MLE for a function of \\(\\theta\\), say \\(g(\\theta)\\), is just \\(g(\\hat{\\theta})\\).\nBayesian (in)variance:\nLet \\(\\eta = g(\\theta)\\), and assume for simplicity’s sake that \\(g\\) is one-to-one. Then \\(\\theta = g^{-1}(\\eta)\\). If \\(\\theta\\) has posterior \\(p(\\theta \\mid y)\\), the posterior for \\(g(\\theta)\\) is:\n\\[\np(g^{-1}(\\eta) \\mid y) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\]\nThis can lead to contradictions under ``ignorance”.\nThis presentation follows Gelman et al. (2013) somewhat.\nThere are priors called Jeffreys’ priors (for Harold Jeffreys) that are invariant to reparameterizations. Remember that the Fisher information, or: \\[\n\\mathcal{I}(\\theta) = \\Exp{\\nabla_\\theta \\ell_Y(\\theta \\mid y)\\nabla_\\theta \\ell_Y(\\theta \\mid y)^T}\n\\] under a reparameterization \\(\\eta = g(\\theta)\\) with Jacobian \\((J_{\\eta,\\theta})_{ij} = \\frac{\\partial \\eta_i}{\\partial \\theta_j}\\) is: \\[\n\\mathcal{I}(\\theta(\\eta)) = J_{\\eta,\\theta}^T\\Exp{(\\nabla_\\theta \\ell_Y(\\theta \\mid y))\\mid_{\\theta = g^{-1}(\\eta)} \\nabla_\\theta (\\ell_Y(\\theta \\mid y)\\mid_{\\theta = g^{-1}(\\eta))}^T}J_{\\eta,\\theta}\n\\] For \\(\\eta = g(\\theta)\\), assume for simplicity that \\(g\\) is one-to-one, then a prior for \\(\\theta\\) that is proportional to the square root of the determinant of the Fisher information will be invariant to reparameterization:\n\\[\np(\\theta) \\propto \\det(\\mathcal{I}(\\theta))^{1/2}\n\\] Why is this the case? Because under the change of measure formula above the prior for \\(\\eta\\) is: \\[\np(\\eta) \\propto p(g^{-1}(\\eta)) \\det J_{\\eta,\\theta}\n\\] which is \\[\n\\begin{aligned}\np(\\eta) & \\propto \\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det J_{\\eta,\\theta} \\\\\n& \\propto \\det (J_{\\eta,\\theta})^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T)^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T\\mathcal{I}(g^{-1}(\\eta))^{1/2}J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det(\\mathcal{I}(\\eta))^{1/2}\n\\end{aligned}\n\\] Thus giving some sense of invariance under a coordinate change. As stated in Gelman et al. (2013), more or less:\n\nAny rule for determining the prior density \\(p(\\theta)\\) should yield an equivalent result if aplied to the transformed parameter; that is, \\(p(\\eta)\\) generated using \\(p(\\theta)\\) using the change of measure formula should yield the same prior as would have been obtained directly from the model \\(p(\\eta) p(y \\mid \\eta)\\)\n\nOne issue with Jeffreys’ prior is that it is dependent on a likelihood, which can be controversial.\nFor the Bernoulli trial example from last class, the Jeffreys prior is \\(\\text{Beta}(1/2, 1/2)\\).\n\n\n\nPosterior probabilities are strictly ``right” under our prior assumption because of the math of Bayes’ theorem. However, if we take a Frequentist view of probability, namely that probabilities are defined as limiting proportions of events, we’ll need to think about alternative draws of our prior and of our data.\nThe coverage of our posterior credible intervals will only match the nominal probabilities if the prior we use for our analysis matches that which generated the data. We can show this as computing the marginal posterior \\(p(\\theta \\mid y)\\) under repeated draws from the prior and data distribution \\(p(y \\mid \\theta)\\), which is the distribution associated with the density \\(f_Y(y \\mid \\theta)\\) we’ll use in our posterior:\n\\[\n\\begin{aligned}\n\\theta^\\prime & \\sim p(\\theta) \\\\\ny & \\sim p(y \\mid \\theta^\\prime) \\\\\n\\theta & \\sim p(\\theta \\mid y)\n\\end{aligned}\n\\] Another way to represent this sampling diagram is through integrals:\n\\[\n\\begin{aligned}\n\\int_{\\Omega_\\theta}\\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime) dy \\, d\\theta^\\prime & = \\int_{\\mathcal{Y}} \\int_{\\Omega_\\theta}\\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} \\int_{\\Omega_\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} p(\\theta) f_Y(y \\mid \\theta) \\\\\n& = p(\\theta)\n\\end{aligned}\n\\]\nSee Talts et al. (2018) for more info about how we can use this identity to test whether our algorithms are working correctly.\n\n\n\nLike Frequentist confidence intervals, we can only compute \\(p(\\theta \\mid y)\\) exactly under special circumstances, like conjugate priors. The reason for this is that the integral in the denominator is usually intractable.\nWe will usually have to do approximate inference on Bayesian models by using Markov Chain Monte Carlo samplers, which iteratively generate samples that converge in distribution to the true posterior distribution. Bayesian approximate methods instead operate on an expression that is proportional to the posterior:\n\\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta) p(\\theta)\n\\]\nOne way to think about the MLE is that it is the posterior mode under a prior of \\(p(\\theta) \\propto 1\\): \\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta)\n\\] The difference between the likelihood \\(L_Y(\\theta \\mid y)\\) and the posterior \\(p(\\theta \\mid y)\\) lies in how we treat the expression. In MLE we’re going to maximize the likelihood. In Bayesian inference we care about the full distribution of \\(\\theta\\).\nThis gives some intuition about Bayesian inference. We can think of doing MLE and penalizing certain values of \\(\\theta\\):\n\\[\n\\ell_Y(\\theta \\mid y) + \\text{penalty}(\\theta)\n\\] that will allow the maximizer to favor certain values of \\(\\theta\\) over others.\nIf we look at the implied log-posterior ignoring the constant that doesn’t depend on \\(\\theta\\):\n\\[\n\\log p(\\theta \\mid y) = \\log f_Y(y \\mid \\theta) + \\log p(\\theta)\n\\]\nIf we maximize this expression we can rewrite this as \\[\n\\log p(\\theta \\mid y) = \\ell_Y(\\theta \\mid y) + \\log p(\\theta)\n\\] and we get the penalized likelihood expression where the penalty is a probability density.\nOne question might be: ok, we have a full distribution for \\(\\theta\\). What do we do with it? While the MLE is a single choice, we now have myriad choices for point estimates derived from Bayesian models. We could use the posterior mean: \\[\n\\Exp{\\theta \\mid y}\n\\] We could use the posterior median, \\(\\theta_m\\): \\[\nP(\\theta &gt; \\theta_m \\mid y) = P(\\theta \\leq \\theta_m \\mid y) = 1/2.\n\\]\nWe could use another posterior quantile. We could use the mode of the posterior as well.\nAsymptotically, one might hope that the Bayesian estimates converge to the Frequentist estimates, and this is true, though one needs to be careful in scenarios where the dimensionality of the parameter space increases with sample size and about how one uses priors.\nIn Frequentist inference, the only limits on the parameter space come from the likelihood; the normal density requires that \\(\\mu \\in \\R\\) and \\(\\sigma^2 \\in (0, \\infty)\\). In Bayesian inference, the prior can also restrict the parameter space. For example, in the normal example, one could use a prior for \\(\\mu\\) that enforced \\(\\mu &gt; 0\\). The posterior would then only be able to represent \\(\\mu &gt; 0\\). If the true \\(\\mu\\) were negative, a Bayesian point-estimator wouldn’t converge to the true \\(\\mu\\).\nWhile the prior adds an extra degree of freedom which seems dangerous, it can yield better estimates when there are small datasets, because there isn’t as much information in the data. An example of this would be a simple regression model: \\[\ny_i \\sim \\text{Normal}(X_i^T \\beta, \\sigma^2)\n\\] We might have some good information that we don’t expect \\(\\beta\\) to be nearly infinite, and in fact we expect it to be pretty well concentrated to \\([-10, 10]\\). Then we could use independent \\(\\text{Normal}(0,5^2)\\) priors for the regression coefficients."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-8-notes.html#linear-regression-with-conjugate-priors",
    "href": "missing-data-material-W-26/notes/lecture-8-notes.html#linear-regression-with-conjugate-priors",
    "title": "Missing data lecture 5: Bayes",
    "section": "Linear regression with conjugate priors",
    "text": "Linear regression with conjugate priors\nThis and the following section follow Chapter 2 in Rossi, Allenby, and Misra (2024) quite closely.\nLet’s look at the linear regression model with conjugate priors. \\[\ny_i = x_i^T \\beta + \\epsilon_i, \\quad \\epsilon_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(0, \\sigma^2)\n\\] where \\(x_i \\in \\R^p\\). A full model would imply a model for \\(x_i\\) as well: \\[\nf_{X,Y}((x_1, y_1), \\dots, (x_n, y_n) \\mid \\beta, \\psi) = \\prod_i f_{X}(x_i \\mid \\psi) f_Y(y_i \\mid x_i,\\beta, \\sigma^2)\n\\] If we have a prior for \\(\\psi,\\beta, \\sigma^2\\) that is independent, \\(p(\\psi, \\beta, \\sigma^2) = p(\\psi) p(\\beta, \\sigma^2)\\), then the posterior will factorize into independent distributions as well: \\[\n\\begin{aligned}\np_(\\beta, \\psi,\\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) & \\propto \\prod_i f_{X}(x_i \\mid \\psi) p(\\psi) f_Y(y_i \\mid x_i,\\beta, \\sigma^2) p(\\beta, \\sigma^2) \\\\\n& \\propto (\\prod_i f_{X}(x_i \\mid \\psi) p(\\psi)) \\prod_i f_Y(y_i \\mid x_i,\\beta, \\sigma^2) p(\\beta)  \\\\\n& \\propto p(\\psi \\mid x_1, \\dots, x_n) p(\\beta, \\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) \\\\\n\\end{aligned}\n\\] Remember from last class how we could intuit the form of the joint prior if we examined the likelihood for \\(\\theta\\) and chose a prior with the same functional form as that of the likelihood.\nIn the Bernoulli example, we had a likelihood of the form: \\(L_Y(\\theta \\mid y) = \\theta^{k}(1 - \\theta)^{n - k}\\), where \\(k = \\sum_i = y\\), which suggested a prior of the form \\(\\theta^{a}(1-\\theta)^b\\), which we could regonize as a Beta distribution.\nWe’ll do the same for the regression example. The likelihood for the linear model is: \\[\n(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2}\\sum_i (y_i - x_i^T \\beta)^2 \\rp\n\\] which can be simplified somewhat by writing the sum as a dot product between the vector of errors, \\(e = y - X \\beta\\) where \\(y = (y_1, \\dots, y_n)\\) and \\(X^T = (x_1, \\dots, x_n)\\). \\[\n(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2} (y - X \\beta)^T(y - X \\beta) \\rp\n\\]\nWe can rewrite the term \\((y - X \\beta)^T (y - X \\beta)\\) in terms of the least-squares estimator for \\(\\beta\\), \\(\\hat{\\beta} = (X^T X)^{-1}X^T y\\) by decomposing \\(y\\) as \\(y = X \\hat{\\beta} + y - X \\hat{\\beta}\\):\n\\[\n\\begin{aligned}\n(y - X \\beta)^T (y - X \\beta) & = (X \\hat{\\beta} + y - X \\hat{\\beta} - X \\beta)^T(X \\hat{\\beta} + y - X \\hat{\\beta} - X \\beta) \\\\\n& = (y - X \\hat{\\beta})^T(y - X \\hat{\\beta}) + (X \\beta - X\\hat{\\beta})^T(X \\beta - X\\hat{\\beta}) - 2(X \\beta - X\\hat{\\beta})^T (y - X \\hat{\\beta}) \\\\\n& = (y - X \\hat{\\beta})^T(y - X \\hat{\\beta}) + (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\n\\end{aligned}\n\\]\nLet \\(s^2 = \\frac{1}{n-p}(y-X\\hat{\\beta})^T(y-X\\hat{\\beta})\\), and \\(\\nu = n - p\\), so we can rewrite the sum more compactly as: \\[\n(y - X \\beta)^T (y - X \\beta) = \\nu s^2 + (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\n\\] This leads to a likelihood:\n\\[\nL_Y(\\beta, \\sigma^2 \\mid y, X) \\propto (\\sigma^2)^{-\\nu/2} \\exp\\lp\\frac{\\nu s^2}{2 \\sigma^2}\\rp (\\sigma^2)^{-(n - \\nu) / 2} \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\\rp\n\\] Before we derive conjugate priors from this likelihood, we can see that the posterior under flat priors for \\(\\beta\\) and a prior for \\(\\sigma^2\\), \\(\\sigma^{-2}\\), leads to a posterior: \\[\np(\\beta, \\sigma^2 \\mid y, X) \\propto (\\sigma^2)^{-(\\nu/2+1)} \\exp\\lp\\frac{\\nu s^2}{2 \\sigma^2}\\rp (\\sigma^2)^{-(n - \\nu) / 2} \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\\rp\n\\] which is a conditional normal posterior for \\(\\beta\\) with a scaled inverse chi-squared posterior for \\(\\sigma^2\\).\nThis suggests a conjugate prior of the form \\(p(\\beta,\\sigma^2) = p(\\sigma^2)p(\\beta \\mid \\sigma^2)\\): \\[\np(\\sigma^2) \\propto (\\sigma^2)^{-(\\nu_0/2 + 1)} \\exp\\lp \\frac{\\nu_0 s_0}{2 \\sigma^2} \\rp\n\\]\nand a conditional normal prior for \\(\\beta\\):\n\\[\np(\\beta \\mid \\sigma^2) \\propto (\\sigma^2)^{-p / 2} \\exp\\lp-\\frac{1}{2\\sigma^2}(\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta - \\mu_0) \\rp\n\\]\nThis can be seen as the posterior from a regression run with a prior of \\(p(\\sigma^2) \\propto \\sigma^{-2}\\) and a flat prior on \\(\\beta\\).\nThen the posterior for \\(\\sigma^2, \\beta\\) is simply the product of the priors and the likelihood, which we write as above:\n\\[\n\\begin{aligned}\np(\\beta, \\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) \\propto & (\\sigma^2)^{-(\\nu_0/2 + 1)} \\exp\\lp \\frac{\\nu_0 s_0}{2 \\sigma^2} \\rp(\\sigma^2)^{-p / 2} \\exp\\lp-\\frac{1}{2\\sigma^2}(\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta - \\mu_0) \\rp \\\\\n&(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2} (y - X \\beta)^T(y - X \\beta) \\rp\n\\end{aligned}\n\\]\nThis is definitley formidable, but we can simplify things a bit by collecting the terms with \\(\\beta\\):\n\\[\n(y - X\\beta)^T (y - X\\beta) + (\\mu_0 - \\beta)^T \\Sigma_0^{-1}(\\mu_0 - \\beta)\n\\] and decomposing \\(\\Sigma_0^{-1} = L^T L\\), and noting that we can write the sum as the following inner product: \\[\n\\begin{aligned}\n\\begin{bmatrix}\n(y - X\\beta)^T & (L\\mu_0 - L\\beta)^T\n\\end{bmatrix}\n\\begin{bmatrix}\n(y - X\\beta) \\\\\nL \\mu_0 - L\\beta)\n\\end{bmatrix}\n\\end{aligned}\n\\] This can be further simplified by constructing a vector \\[\nu = \\begin{bmatrix} y \\\\ L\\mu_0 \\end{bmatrix}\n\\] and a matrix \\(W\\) \\[\nW = \\begin{bmatrix} X \\\\ L \\end{bmatrix}\n\\] and writing the expresssion as \\((u - W\\beta)^T(u - W\\beta)\\). We can then use the same trick as above, by representing \\(u\\) as the projection into the column space of \\(W\\) and the residual:\n\\[\n(W \\bar{\\beta} + u - W \\bar{\\beta} - W \\beta)^T(W \\bar{\\beta} + u - W \\bar{\\beta} - W \\beta)\n\\] The expression for \\(\\bar{\\beta}\\) is:\n\\[\n\\begin{aligned}\n\\bar{\\beta} & = (X^T X + L^T L)^{-1}(X^T y + L^T L \\mu_0) \\\\\n& = (X^T X + \\Sigma_0^{-1})^{-1}(X^T y + \\Sigma_0^{-1} \\mu_0)\n\\end{aligned}\n\\]\nWhich simplifies as\n\\[\n(u - W \\bar{\\beta})^T(u - W \\bar{\\beta}) +(\\beta - \\bar{\\beta})^T W^T W (\\beta - \\bar{\\beta})\n\\] and after some algebra comes to \\[\n(y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) + (\\mu_0 - \\bar{\\beta})^T\\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta}) + (\\beta - \\bar{\\beta})^T (X^T X + \\Sigma_0^{-1}) (\\beta - \\bar{\\beta})\n\\] In the following, let \\(n s^2 = (y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) + (\\mu_0 - \\bar{\\beta})^T\\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta})\\). The posterior is:\n\\[\n\\begin{aligned}\np(\\beta, \\sigma^2 \\mid y, X) \\propto & (\\sigma^2)^{-(n + \\nu_0)/2 + 1} \\exp\\lp\\frac{(n + \\nu_0)(n s^2 + \\nu_0 s_0^2)/(n + \\nu_0)}{2 \\sigma^2}\\rp \\times (\\sigma^2)^{-p / 2} \\\\\n& \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\bar{\\beta})^T (X^T X + \\Sigma_\\beta^{-1})(\\beta - \\bar{\\beta})\\rp\n\\end{aligned}\n\\] \\[\n\\bar{\\beta} = (X^T X + \\Sigma_\\beta^{-1})^{-1}(\\Sigma_\\beta^{-1} \\mu_\\beta + X^T X \\hat{\\beta})\n\\] Like the Bernoulli problem, the posterior mean for \\(\\beta\\) is a weighted average between the prior mean and the information from the likelihood, which in this case is the least-squared estimator for \\(\\beta\\). This is often a consequence of using conjugate priors, that the posterior is a compromise between the prior and the likelihood.\nThis implies the following distributions for \\(\\sigma^2\\) and \\(\\beta \\mid \\sigma^2\\): \\[\n\\begin{aligned}\n\\sigma^2 & \\sim \\text{Inv-}\\chi^2\\lp n + \\nu_0, \\frac{n s^2 + \\nu_0 s^2_0}{n + \\nu_0}\\rp \\\\\n\\beta \\mid \\sigma^2 & \\sim \\text{Normal}(\\bar{\\beta}, \\sigma^2 \\lp X^T X + \\Sigma_0^{-1} \\rp^{-1})\n\\end{aligned}\n\\]\nThe posterior mean for \\(\\sigma^2\\) is: \\[\n\\Exp{\\sigma^2 \\mid y, X} = \\frac{n + \\nu_0}{n + \\nu_0 - 2}\\frac{n s^2 + \\nu_0 s^2_0}{n + \\nu_0}\n\\] The expression for \\(n s^2\\) is interesting because it involves the squared error of the posterior linear predictor for \\(y\\): \\[\n\\begin{aligned}\n(y - \\Exp{X \\beta \\mid y, X})^T(y - \\Exp{X \\beta \\mid y, X})^T & = (y - X \\Exp{\\beta \\mid y, X})^T(y - X \\Exp{\\beta \\mid y, X}) \\\\\n& = (y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) \\\\\n\\end{aligned}\n\\]\nbut it also involves the error in the prior mean with respect to the prior covariance matrix: \\[\n(\\mu_0 - \\bar{\\beta})^T \\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta})\n\\] The effect of this term will decrease as the number of observations increases, but it elucidates how the posterior mean of the error variance is decomposed into several pieces depending on different aspects of the prior and the data.\n\nBayesian inference in repeated measure models\nTwo lectures ago we went through how to compute the MLE from this regression model:\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j.\n\\end{aligned}\n\\]\nThis required sequentially computing the MLE for \\(\\beta\\) given an estimate for \\(\\Sigma\\) and computing \\(\\hat{\\Sigma}\\) given the last estimate for \\(\\hat{\\beta}\\).\nLet’s write down the likelihood for this model to see if we can come up with a conjugate prior for the problem. \\[\nL_{Y}(\\beta, \\Sigma \\mid y, X) \\propto \\det(I_n \\otimes \\Sigma)^{-1/2} \\exp\\lp-\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\\rp\n\\] If we start with the prior for \\(\\beta \\mid \\Sigma\\) we can ignore the determinant and focus on the term in the exponential:\n\\[\n-\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\n\\]\nLet’s try a multivariate normal prior:\n\\[\n\\beta \\sim \\text{Normal}(\\mu_0, \\Sigma_0)\n\\]\nso we can multiply the likelihood by the prior to get\n\\[\n-\\frac{1}{2}\\lp (y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta) + (\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta-\\mu_0) \\rp\n\\] which we’ll rewrite for convenience as \\[\n-\\frac{1}{2}\\lp (A(y - X \\beta))^T A(y - X \\beta) + (L(\\beta - \\mu_0))^T L(\\beta-\\mu_0) \\rp\n\\] where \\(A^T A = (I_n \\otimes \\Sigma)^{-1}\\) and \\(L^T L = \\Sigma_0^{-1}\\).\nThis looks familiar! We can use the same trick as we did above: create a new vector \\(u\\) and matrix \\(W\\): \\[\nu =\n\\begin{bmatrix}\nA y \\\\\nL \\mu_0\n\\end{bmatrix},\\quad\nW =\n\\begin{bmatrix}\nA X \\\\\nL\n\\end{bmatrix}\n\\] and can write \\[\n(u - W\\beta)^T(u - W \\beta) = \\lp (A(y - X \\beta))^T A(y - X \\beta) + (L(\\beta - \\mu_0))^T L(\\beta-\\mu_0) \\rp.\n\\] Furthermore, write \\(u = W\\bar{\\beta} + u - W\\bar{\\beta}\\), where \\(\\bar{\\beta}\\) is the least-squares coeffients of the regression of \\(u\\) on \\(W\\): \\[\n\\bar{\\beta} = (W^T W + L^T L)^{-1}W^T u = (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma_0^{-1})^{-1}(X^T (I_n \\otimes \\Sigma)^{-1}y + \\Sigma_0^{-1} \\mu_0)\n\\] This leads to \\((u - W\\bar{\\beta})^T W = 0\\), which allows us to cleanly partition \\((u-W\\beta)^T (u-W\\beta)\\) into two pieces: \\(u - W\\bar{\\beta}\\) and \\(W\\beta\\):\n\\[\n\\begin{aligned}\n(W\\bar{\\beta} + u - W\\bar{\\beta} - &W \\beta)^T(W\\bar{\\beta} + u - W\\bar{\\beta} - W \\beta) \\\\\n& = (u - W\\bar{\\beta} + W\\bar{\\beta}  - W \\beta)^T(u - W\\bar{\\beta}+ W\\bar{\\beta}  - W \\beta) \\\\\n& = (u - W\\bar{\\beta})^T(u - W\\bar{\\beta}) + (W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta) + 2(u - W\\bar{\\beta})^T(W\\bar{\\beta}  - W \\beta) \\\\\n& = (u - W\\bar{\\beta})^T(u - W\\bar{\\beta}) + (W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta)\n\\end{aligned}\n\\] where the last line follows because \\((u - W\\bar{\\beta})^T W = 0\\). Because we’re focusing only on the posterior, which is a function of \\(\\beta\\) and not data, we can ignore the \\((u - W\\bar{\\beta})^T(u - W\\bar{\\beta})\\) term because it does not involve \\(\\beta\\) and involves only functions of \\(X,y,A,L\\), which are fixed with respect to \\(\\beta\\).\nWe rewrite \\[\n(W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta)\n\\] as \\[\n(\\beta - \\bar{\\beta})^T W^T W (\\beta - \\bar{\\beta}) = (\\beta - \\bar{\\beta})^T(X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1}) (\\beta - \\bar{\\beta})\n\\] This shows that \\(\\beta \\mid \\Sigma\\) is multivariate normal with: \\[\n\\beta \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1})^{-1})\n\\] Now let’s focus on the conditional distribution of \\(\\Sigma \\mid \\beta\\). We’ll start with the likelihood written in simpler terms: \\[\nL_Y(\\beta, \\Sigma \\mid y, X) \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i (y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta)\\rp\n\\]\nWe can use the trace trick to rearrange things:\n\\[\n\\begin{aligned}\nL_Y(\\beta, \\Sigma \\mid y, X) & \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i (y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta)\\rp \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i \\text{tr}((y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta))\\rp \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i \\text{tr}((y_i - X_i \\beta) (y_i - X_i \\beta)^T\\Sigma^{-1})\\rp  \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\text{tr}((\\sum_i (y_i - X_i \\beta) (y_i - X_i \\beta)^T)\\Sigma^{-1})\\rp  \\\\\n\\end{aligned}\n\\]\nThis suggests that a conjugate prior for \\(\\Sigma\\) has the form:\n\\[\np(\\Sigma) \\propto \\det(\\Sigma)^{-a/2} \\exp\\lp-\\frac{1}{2}\\text{tr}(V_0 \\Sigma^{-1})\\rp\n\\]\nFortunately, we’re in luck! The Inverse Wishart distribution has the density:\n\\[\np(\\Sigma) \\propto \\det(\\Sigma)^{-(\\nu_0 + p + 1)/2} \\exp\\lp-\\frac{1}{2}\\text{tr}(V_0 \\Sigma^{-1})\\rp\n\\]\nCombining the likelihood with the prior we get something proportional to the conditional posterior for \\(\\Sigma\\):\n\\[\np(\\Sigma \\mid y, X, \\beta) \\propto \\det(\\Sigma)^{-(n + \\nu_0 + p + 1)/2} \\exp\\lp-\\frac{1}{2}\\text{tr}((V_0 + \\textstyle\\sum_i (y_i - X_i \\beta)(y_i - X_i \\beta)^T) \\Sigma^{-1})\\rp\n\\]\nPutting this together we get the following two conditional posteriors:\n\\[\n\\begin{aligned}\n\\beta \\mid \\Sigma, y, X & \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1})^{-1}) \\\\\n\\Sigma \\mid \\beta, y, X & \\sim \\text{Inverse-Wishart}(n + \\nu_0, V_0 + \\textstyle\\sum_i (y_i - X_i \\beta)(y_i - X_i \\beta)^T)\n\\end{aligned}\n\\]\nWe can use the theory of integral operators to show that given intial conditions \\(\\Sigma^0\\) and \\(\\beta^0\\) the following algorithm for \\(t = 1, \\dots, S\\):\n\\[\n\\begin{aligned}\n\\beta^{t+1} \\mid \\Sigma^{t}, y, X & \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma^t)^{-1} X + (\\Sigma^t)^{-1})^{-1}) \\\\\n\\Sigma^{t+1} \\mid \\beta^{t}, y, X & \\sim \\text{Inverse-Wishart}(n + \\nu_0, V_0 + \\textstyle\\sum_i (y_i - X_i \\beta^t)(y_i - X_i \\beta^t)^T)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "survival-material/lecture-8.html#fisher-information",
    "href": "survival-material/lecture-8.html#fisher-information",
    "title": "Lecture 8",
    "section": "",
    "text": "Continued example This is an expansion of the example in (Collett 1994). \\[\\begin{align}\n\\frac{\\partial}{\\partial \\lambda}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -\\frac{r_1 + r_2}{\\lambda^2} \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\beta}\\ell(\\lambda, \\psi)\\right)& = -\\lambda e^\\beta T_2\n\\end{align}\\]\nWe know that \\[\\Exp{r_1} = n_1\\Exp{1 - e^{-\\lambda C_i}}{C_i}, \\, \\Exp{r_2} = n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}, \\textrm{and} \\, \\Exp{T_2} = n_2 \\frac{1}{\\lambda e^\\beta} \\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}\\] Then the Fisher information is \\[\\begin{align}\n    \\begin{bmatrix}\n        \\frac{n_1\\Exp{1 - e^{-\\lambda C_i}}{C_i} + n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}}{\\lambda^2} & \\frac{1}{\\lambda} n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i} \\\\\n        \\frac{1}{\\lambda} n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i} & n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}\n    \\end{bmatrix}\n\\end{align}\\] Let \\(\\Exp{r_{i1}} = \\Exp{1 - e^{-\\lambda C_i}}{C_i}\\) and \\(\\Exp{r_{i2}} = \\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}\\). We know the asymptotic variance of the MLE is the inverse of the Fisher information matrix. The inverse is: \\[\\begin{align}\n    \\frac{\\lambda^2}{n_{1} n_{2} \\Exp{r_{i1}}\\Exp{r_{i2}}}\n    \\begin{bmatrix}\n      n_2 \\Exp{r_{i2}}  & -n_2\\Exp{r_{i2}}/\\lambda \\\\\n        -n_2\\Exp{r_{i2}}/\\lambda & \\frac{n_1\\Exp{r_{i1}} + n_2\\Exp{r_{i2}}}{\\lambda^2}\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n      \\frac{\\lambda^2}{n_1\\Exp{r_{i1}}}  & -\\frac{\\lambda}{n_1 \\Exp{r_{i1}}} \\\\\n       -\\frac{\\lambda}{n_1 \\Exp{r_{i1}}} &\n\\frac{n_1\\Exp{r_{i1}} + n_2\\Exp{r_{i2}}}{n_1 n_2 \\Exp{r_{i1}}\\Exp{r_{i2}}}\n    \\end{bmatrix}\n\\end{align}\\] So the asymptotic standard error for \\(\\beta\\) is \\[\\sqrt{\\frac{n_1\\Exp{1 - e^{-\\lambda C_i}}{C_i} + n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}}{n_1 n_2\\Exp{1 - e^{-\\lambda C_i}}{C_i}\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}}}\\]"
  },
  {
    "objectID": "survival-material/lecture-8.html#sketch-of-asymptotic-normality-of-mle",
    "href": "survival-material/lecture-8.html#sketch-of-asymptotic-normality-of-mle",
    "title": "Lecture 8",
    "section": "2.1 Sketch of asymptotic normality of MLE",
    "text": "2.1 Sketch of asymptotic normality of MLE\nLet \\(X_i, i = 1, 2, \\dots\\) be distributed \\(i.i.d.\\) with density \\(f_\\theta\\) where \\(\\theta \\in \\R^p\\). We suppose that the support of \\(X_i\\) does not depend on \\(\\theta\\), and that our MLE’s are consistent for \\(\\theta\\). This is pretty mild, and only requires that likelihood ratios are integrable and our model is identifiable.\nLet the log-likelihood be denoted \\(\\ell(\\theta) = \\sum_{i=1}^n \\log f_{X}(x; \\theta)\\), in which we suppress the dependence of the likelihood on the data. Through an abuse of notation we let \\(\\ell(\\theta; x_i) \\equiv \\log f_{X}(x_i; \\theta)\\). We denote the gradient of the log-likelihood with respect to \\(\\theta\\) evaluated at \\(\\theta^\\prime\\) as: \\[\n\\ell_{\\theta}(\\theta^\\prime) \\equiv \\sum_{i=1}^n \\nabla_\\theta \\log f_X(x_i; \\theta)\\mid_{\\theta = \\theta^\\prime},\\,\\ell_{\\theta}(\\theta^\\prime; x_i) \\equiv \\nabla_\\theta \\log f_X(x_i; \\theta)\\mid_{\\theta = \\theta^\\prime}\n\\] The Hessian of the log-likelihood (i.e. the matrix of second derivatives of the log-likelihood) is: \\[\n\\ell_{\\theta\\theta}(\\theta^\\prime) \\equiv \\sum_{i=1}^n \\nabla^2_\\theta \\log f_X(x_i; \\theta)\\mid_{\\theta = \\theta^\\prime},\\,\\ell_{\\theta\\theta}(\\theta^\\prime;x_i) \\equiv \\nabla^2_\\theta \\log f_X(x_i; \\theta)\\mid_{\\theta = \\theta^\\prime}\n\\] Given that \\(\\theta \\in \\R^p\\), we’ll denote an element of the vector \\(\\ell_\\theta(\\theta^\\prime)\\) as \\((\\ell_\\theta(\\theta^\\prime))_j\\) and a row of \\(\\ell_{\\theta\\theta}(\\theta^\\prime)\\) as \\((\\ell_{\\theta\\theta}(\\theta^\\prime))_j\\). Finally, let \\((\\ell_{\\theta\\theta\\theta}(\\theta^\\prime))_j = \\nabla^2_\\theta (\\ell_{\\theta}(\\theta^\\prime))_j\\) which is a \\(p \\times p\\) matrix.\nFurther assumptions will be needed:\n\nTwice continuously differentiable \\(\\ell_\\theta(\\cdot;x)\\)\nFor \\(\\theta \\in N(\\theta^\\dagger)\\), \\(\\sup_{\\theta}\\ell_{\\theta\\theta\\theta}(\\theta) \\leq g(x)\\) where \\(g(x)\\) is integrable with respect to \\(f_\\theta(x) dx\\).\n\\(\\Exp{-\\ell_{\\theta\\theta}(\\theta; X_i)}\\) is invertible\n\nWe can expand each dimension of the gradient of the log-likelihood evaluated at the MLE \\(\\ell(\\hat{\\theta})\\) around the true parameter value \\(\\theta^\\dagger\\) in a two-term Taylor expansion: \\[\n\\begin{align*}\n    (\\ell_\\theta(\\hat{\\theta}_n))_j = (\\ell_\\theta(\\theta^\\dagger))_j + (\\ell_{\\theta\\theta}(\\theta^\\dagger))_j (\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T (\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*}\n\\] where \\(\\tilde{\\theta}_{n,j}\\) is a point on the chord between \\(\\hat{\\theta}_n\\) and \\(\\theta^\\dagger\\) and may depend on the coordinate \\(j\\).\nNoting that \\(\\ell_\\theta(\\hat{\\theta}_n)_j = 0\\) for all \\(j\\), we get the set of \\(p\\) linear equations: \\[\\begin{align*}\n(\\ell_\\theta(\\theta^\\dagger))_j =  -(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j (\\hat{\\theta}_n - \\theta^\\dagger) - \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T (\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*}\\] Multiplying both sides by \\(\\frac{\\sqrt{n}}{n}\\) gives: \\[\n\\begin{align*}\n\\sqrt{n}\\lp\\frac{1}{n} (\\ell_\\theta(\\theta^\\dagger))_j\\rp & =  -\\left(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\right) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) - \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\\\\n& = \\lp -\\left(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\right) - \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp\\rp\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*}\n\\] The term \\(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j \\overset{p}{\\to} \\Exp{(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j}\\) by the WLLN, while \\[\n\\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp \\overset{p}{\\to} 0\n\\] by the WLLN, the boundedness condition of the third-order derivatives and the consistency of our estimator.\nTo be precise, we want to show that \\[\n\\lim_{n\\to\\infty}\\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp} &gt; \\epsilon}{} = 0\n\\] \\[\n\\begin{aligned}\n\\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp} &gt; \\epsilon}{} & \\leq \\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{ \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j} &gt; \\epsilon}{} \\\\\n& \\leq \\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\frac{1}{n}\\sum_{i=1}^n g(Y_i)} &gt; \\epsilon}{} \\\\\n& \\leq \\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\frac{1}{n}\\sum_{i=1}^n g(Y_i) - \\Exp{g(Y_1)}} &gt; \\frac{\\epsilon}{2}}{} \\\\\n& \\quad + \\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\Exp{g(Y_1)}} &gt; \\frac{\\epsilon}{2}}{}\n\\end{aligned}\n\\] The last line follows from a trick used in a proof in § 6.3 in Resnick (2019). If the following holds\n\\[\n\\begin{aligned}\n& \\left\\{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\frac{1}{n}\\sum_{i=1}^n g(Y_i) - \\Exp{g(Y_1)}} \\leq \\frac{\\epsilon}{2}\\right\\}  \\bigcap\\\\\n& \\left\\{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\Exp{g(Y_1)}} \\leq \\frac{\\epsilon}{2}\\right\\}\n\\end{aligned}\n\\]\nThen the triangle inequality implies that \\[\n\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\frac{1}{n}\\sum_{i=1}^n g(Y_i)} \\leq \\epsilon.\n\\] Taking complements and using Boole’s inequality (\\(P(A \\cup B) \\leq P(A) + P(B)\\)) yields the final inequality. Both terms converge to zero due to the convergence in probability of the MLE, and the WLLN applied to the empirical average of \\(g(Y_i)\\).\nBy Slutsky’s theorem, \\[\n-\\left(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\right) - \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp \\overset{p}{\\to}-\\Exp{(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j}\n\\] Collecting our \\(p\\) equations into one set of equations yields: \\[\n\\sqrt{n}\\lp\\frac{1}{n} \\ell_\\theta(\\theta^\\dagger)\\rp = (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))} + o_p(1)) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\tag{1}\\]\nWriting out the expressions as explicit sums: \\[\\begin{align*}\n\\frac{\\sqrt{n}}{n}  \\sum_{i=1}^n \\ell_\\theta(\\theta^\\dagger; x_i) = (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))} + o_p(1)) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*} \\tag{2}\\]\nThe left-hand side of Equation 2 will be amenable to a multivariate version of the CLT. We’ll take the following multivariate CLT as given:\n\nTheorem 1. Multivariate CLT, (Keener 2010) Let \\(X_1, X_2, \\dots\\) be i.i.d random vectors in \\(\\R^k\\) with a common mean \\(\\Exp{X_i} = \\mu\\) and common covariance matrix \\(\\Sigma = \\Exp{(X_i - \\mu)(X_i - \\mu)^T}\\). If \\(\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\\), then \\[\\sqrt{n}(\\bar{X} - \\mu) \\overset{d}{\\to} \\text{Normal}(0, \\Sigma)\\]\n\nRecall that \\[\n\\Exp{\\ell_\\theta(\\theta; X_i)} = 0\n\\] By the multivariate central limit (MCLT) theorem, Equation 2 converges in distribution to a multivariate normal distribution with mean zero and covariance matrix \\(\\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}\\).\nWe’ll also need a lemma about the solutions to random linear equations:\n\nLemma 1 (Lemma 5.2 in (Lehmann and Casella 1998)) Suppose there are a set of \\(p\\) equations, \\(j = 1, \\dots, p\\): \\[\\sum_{k=1}^p A_{jkn} Y_{kn} = T_{jn}.\\] Let \\(T_{1n}, \\dots, T_{pn}\\) converge in distribution to \\(T_1, \\dots, T_p\\). Furthermore, suppose that for each \\(j,k\\), \\(A_{jkn} \\overset{p}{\\to} a_{jk}\\) such that the matrix \\(A\\) with \\((j,k)^{\\mathrm{th}}\\) element \\(a_{jk}\\) is nonsingular. Then if the distribution of \\(T_1, \\dots, T_p\\) has a disitribution with repsect to the Lebesgue measure over \\(\\R^p\\), \\(Y_{1n}, \\dots, Y_{pn}\\) tend in probability to \\(A^{-1} T\\).\nWritten in matrix form and using the fact that convergence in probability implies convergence in distribution: \\[\nT_n = A_n Y_n  \\implies Y_n \\overset{d}{\\to} A^{-1}T  \n\\]\n\nWe have that the left-hand side of Equation 1 converges in distribtuion to a multivariate normal distribution, and we have that the matrix on the RHS of Equation 1 convegens in probability to \\(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))}\\), which by assumption is invertble. Thus by Lemma 1 \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\) converges in probability to \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\overset{p}{\\to} (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}^{1/2}\\mathcal{Z}\n\\end{align*}\\] where \\(\\mathcal{Z} \\sim \\text{Normal}(0, I_p)\\), or \\[\\begin{align*}\n&\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)  \\overset{d}{\\to} \\mathcal{N}\\left(0, (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\right)\n\\end{align*}\\]\nAssuming further that \\[\n\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)} + \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}=0 \\implies (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T} = I_p\n\\] Putting this all together shows that \\[\\begin{align*}\n     \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger)^{-1})\n\\end{align*}\\] where \\(\\mathcal{I}(\\theta^\\dagger) = -\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)}\\)"
  },
  {
    "objectID": "survival-material/lecture-8.html#estimators-of-variance-covariance-matrix",
    "href": "survival-material/lecture-8.html#estimators-of-variance-covariance-matrix",
    "title": "Lecture 8",
    "section": "2.2 Estimators of variance-covariance matrix",
    "text": "2.2 Estimators of variance-covariance matrix\nIn the previous section, we encountered several consistent estimators of the variance covariance matrix: \\[\\begin{align*}\n  \\lp-\\frac{1}{n} \\ell_{\\theta\\theta}(\\hat{\\theta}_n)\\rp^{-1} & \\overset{p}{\\to} \\mathcal{I}(\\theta^\\dagger)^{-1} \\\\\n  \\lp\\frac{1}{n} \\ell_{\\theta}(\\hat{\\theta}_n)\\ell_{\\theta}(\\hat{\\theta}_n)^T\\rp^{-1}  & \\overset{p}{\\to} \\mathcal{I}(\\theta^\\dagger)\n\\end{align*}\\] These expressions assume that our inferential model matches the data generating model. In the event our inferential model is different than the true data generating model, it can be shown that the scaled MLE converges asymptotically to \\[\\begin{align*}\n&\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)  \\overset{d}{\\to} \\mathcal{N}\\left(0, (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\right)\n\\end{align*}\\] where the key difference is that \\(\\theta^\\dagger\\) is no longer the parameter for the true data generating process, but is instead the parameter the minimizes the KL divergence between the assumed inferential model and the true distribution generating the data.\nThus, the following sandwich estimator for the variance covariance matrix is often preferred over either of the above expressions: \\[\\begin{align}\n\\hat{\\Sigma}_{R} & = (-\\Prob{\\ell_{\\theta\\theta}(\\theta^\\dagger; X)}{n})^{-1} \\Prob{\\ell_\\theta(\\theta;X)\\ell_\\theta(\\theta;X)^T}{n}(-\\Prob{\\ell_{\\theta\\theta}(\\theta^\\dagger; X)}{n})^{-1} \\overset{p}{\\to} \\text{Var}\\left(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\right)\n\\end{align}\\] where \\(\\hat{\\theta}\\) is the MLE."
  },
  {
    "objectID": "survival-material/lecture-9.html",
    "href": "survival-material/lecture-9.html",
    "title": "Lecture 9",
    "section": "",
    "text": "For many of the prior examples, a convenient estimator for the Fisher information is the average of the observed information. The observed information is just the negative of the matrix of second derivatives of the log-likelihood: \\[\\begin{align}\n   -\\ell_{\\theta\\theta}(\\theta) =\n   -\\begin{bmatrix}\n       \\ddtA{\\theta_1}\\ell(\\theta) & \\ddtB{\\theta_1}{\\theta_2}\\ell(\\theta) & \\dots & \\ddtB{\\theta_1}{\\theta_p}\\ell(\\theta) \\\\\n       \\ddtB{\\theta_2}{\\theta_1} \\ell(\\theta) & \\ddtA{\\theta_2} \\ell(\\theta) & \\dots & \\ddtB{\\theta_2}{\\theta_p}\\ell(\\theta) \\\\\n       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n       \\ddtB{\\theta_p}{\\theta_1} \\ell(\\theta) & \\ddtB{\\theta_p}{\\theta_2} \\ell(\\theta) & \\dots & \\ddtA{\\theta_p}\\ell(\\theta) \\\\\n   \\end{bmatrix}\n\\end{align}\\] This is often denoted as \\[j(\\theta) \\equiv -\\ell_{\\theta\\theta}(\\theta).\\] Replacing \\(\\ell(\\theta) = \\sum_i \\log f_\\theta(X_i)\\) and using the fact that derivatives are linear operators: \\[\\begin{align}\nj(\\theta) =\n   -\\sum_i \\begin{bmatrix}\n       \\ddtA{\\theta_1}\\log f_\\theta(X_i) & \\ddtB{\\theta_1}{\\theta_2}\\log f_\\theta(X_i) & \\dots & \\ddtB{\\theta_1}{\\theta_p}\\log f_\\theta(X_i) \\\\\n       \\ddtB{\\theta_2}{\\theta_1} \\log f_\\theta(X_i) & \\ddtA{\\theta_2} \\log f_\\theta(X_i) & \\dots & \\ddtB{\\theta_2}{\\theta_p}\\log f_\\theta(X_i) \\\\\n       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n       \\ddtB{\\theta_p}{\\theta_1} \\log f_\\theta(X_i) & \\ddtB{\\theta_p}{\\theta_2} \\log f_\\theta(X_i) & \\dots & \\ddtA{\\theta_p}\\log f_\\theta(X_i) \\\\\n   \\end{bmatrix}\n\\end{align}\\] we can see that the natural estimator of \\(\\mathcal{I}(\\theta)\\) is the average observed information, which does indeed converge in probability to the Fisher information \\[\\frac{1}{n} j(\\theta) \\overset{p}{\\to} \\mathcal{I}(\\theta).\\] Of course, typically we won’t know \\(\\theta\\) (unless we’re evaluating \\(i(\\theta)\\) at \\(\\theta_0\\)), so we use the plug-in estimator, or \\(j(\\hat{\\theta}_n)\\) which still converges in probability to the Fisher information: \\[\\frac{1}{n} j(\\hat{\\theta}_n) \\overset{p}{\\to} \\mathcal{I}(\\theta).\\]"
  },
  {
    "objectID": "survival-material/lecture-9.html#fisher-information",
    "href": "survival-material/lecture-9.html#fisher-information",
    "title": "Lecture 9",
    "section": "",
    "text": "Continued example This is an expansion of the example in (Collett 1994). \\[\\begin{align}\n\\frac{\\partial}{\\partial \\lambda}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -\\frac{r_1 + r_2}{\\lambda^2} \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\beta}\\ell(\\lambda, \\psi)\\right)& = -\\lambda e^\\beta T_2\n\\end{align}\\]\nWe know that \\[\\Exp{r_1} = n_1\\Exp{1 - e^{-\\lambda C_i}}{C_i}, \\, \\Exp{r_2} = n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}, \\textrm{and} \\, \\Exp{T_2} = n_2 \\frac{1}{\\lambda e^\\beta} \\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}\\] Then the Fisher information is \\[\\begin{align}\n    \\begin{bmatrix}\n        \\frac{n_1\\Exp{1 - e^{-\\lambda C_i}}{C_i} + n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}}{\\lambda^2} & \\frac{1}{\\lambda} n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i} \\\\\n        \\frac{1}{\\lambda} n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i} & n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}\n    \\end{bmatrix}\n\\end{align}\\] Let \\(\\Exp{r_{i1}} = \\Exp{1 - e^{-\\lambda C_i}}{C_i}\\) and \\(\\Exp{r_{i2}} = \\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}\\). We know the asymptotic variance of the MLE is the inverse of the Fisher information matrix. The inverse is: \\[\\begin{align}\n    \\frac{\\lambda^2}{n_{1} n_{2} \\Exp{r_{i1}}\\Exp{r_{i2}}}\n    \\begin{bmatrix}\n      n_2 \\Exp{r_{i2}}  & -n_2\\Exp{r_{i2}}/\\lambda \\\\\n        -n_2\\Exp{r_{i2}}/\\lambda & \\frac{n_1\\Exp{r_{i1}} + n_2\\Exp{r_{i2}}}{\\lambda^2}\n    \\end{bmatrix} =\n    \\begin{bmatrix}\n      \\frac{\\lambda^2}{n_1\\Exp{r_{i1}}}  & -\\frac{\\lambda}{n_1 \\Exp{r_{i1}}} \\\\\n       -\\frac{\\lambda}{n_1 \\Exp{r_{i1}}} &\n\\frac{n_1\\Exp{r_{i1}} + n_2\\Exp{r_{i2}}}{n_1 n_2 \\Exp{r_{i1}}\\Exp{r_{i2}}}\n    \\end{bmatrix}\n\\end{align}\\] So the asymptotic standard error for \\(\\beta\\) is \\[\\sqrt{\\frac{n_1\\Exp{1 - e^{-\\lambda C_i}}{C_i} + n_2\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}}{n_1 n_2\\Exp{1 - e^{-\\lambda C_i}}{C_i}\\Exp{1 - e^{-\\lambda e^\\beta C_i}}{C_i}}}\\]"
  },
  {
    "objectID": "survival-material/lecture-9.html#sketch-of-asymptotic-normality-of-mle",
    "href": "survival-material/lecture-9.html#sketch-of-asymptotic-normality-of-mle",
    "title": "Lecture 9",
    "section": "2.1 Sketch of asymptotic normality of MLE",
    "text": "2.1 Sketch of asymptotic normality of MLE\nLet \\(X_i, i = 1, 2, \\dots\\) be distributed \\(i.i.d.\\) with density \\(f_\\theta\\) where \\(\\theta \\in \\R^p\\). We suppose that the support of \\(X_i\\) does not depend on \\(\\theta\\), and that our MLE’s are consistent for \\(\\theta\\). This is pretty mild, and only requires that likelihood ratios are integrable and our model is identifiable.\nLet the log-likelihood be denoted \\(\\ell(\\theta) = \\sum_{i=1}^n \\log f_{X}(x; \\theta)\\), in which we suppress the dependence of the likelihood on the data. Through an abuse of notation we let \\(\\ell(\\theta; x_i) \\equiv \\log f_{X}(x_i; \\theta)\\). We denote the gradient of the log-likelihood with respect to \\(\\theta\\) evaluated at \\(\\theta^\\prime\\) as: \\[\n\\ell_{\\theta}(\\theta^\\prime) \\equiv \\sum_{i=1}^n \\nabla_\\theta \\log f_X(x_i; \\theta)\\mid_{\\theta = \\theta^\\prime},\\,\\ell_{\\theta}(\\theta^\\prime; x_i) \\equiv \\nabla_\\theta \\log f_X(x_i; \\theta)\\mid_{\\theta = \\theta^\\prime}\n\\] The Hessian of the log-likelihood (i.e. the matrix of second derivatives of the log-likelihood) is: \\[\n\\ell_{\\theta\\theta}(\\theta^\\prime) \\equiv \\sum_{i=1}^n \\nabla^2_\\theta \\log f_X(x_i; \\theta)\\mid_{\\theta = \\theta^\\prime},\\,\\ell_{\\theta\\theta}(\\theta^\\prime;x_i) \\equiv \\nabla^2_\\theta \\log f_X(x_i; \\theta)\\mid_{\\theta = \\theta^\\prime}\n\\] Given that \\(\\theta \\in \\R^p\\), we’ll denote an element of the vector \\(\\ell_\\theta(\\theta^\\prime)\\) as \\((\\ell_\\theta(\\theta^\\prime))_j\\) and a row of \\(\\ell_{\\theta\\theta}(\\theta^\\prime)\\) as \\((\\ell_{\\theta\\theta}(\\theta^\\prime))_j\\). Finally, let \\((\\ell_{\\theta\\theta\\theta}(\\theta^\\prime))_j = \\nabla^2_\\theta (\\ell_{\\theta}(\\theta^\\prime))_j\\) which is a \\(p \\times p\\) matrix.\nFurther assumptions will be needed:\n\nTwice continuously differentiable \\(\\ell_\\theta(\\cdot;x)\\)\nFor \\(\\theta \\in N(\\theta^\\dagger)\\), \\(\\sup_{\\theta}\\ell_{\\theta\\theta\\theta}(\\theta) \\leq g(x)\\) where \\(g(x)\\) is integrable with respect to \\(f_\\theta(x) dx\\).\n\\(\\Exp{-\\ell_{\\theta\\theta}(\\theta; X_i)}\\) is invertible\n\nWe can expand each dimension of the gradient of the log-likelihood evaluated at the MLE \\(\\ell(\\hat{\\theta})\\) around the true parameter value \\(\\theta^\\dagger\\) in a two-term Taylor expansion: \\[\n\\begin{align*}\n    (\\ell_\\theta(\\hat{\\theta}_n))_j = (\\ell_\\theta(\\theta^\\dagger))_j + (\\ell_{\\theta\\theta}(\\theta^\\dagger))_j (\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T (\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*}\n\\] where \\(\\tilde{\\theta}_{n,j}\\) is a point on the chord between \\(\\hat{\\theta}_n\\) and \\(\\theta^\\dagger\\) and may depend on the coordinate \\(j\\).\nNoting that \\(\\ell_\\theta(\\hat{\\theta}_n)_j = 0\\) for all \\(j\\), we get the set of \\(p\\) linear equations: \\[\\begin{align*}\n(\\ell_\\theta(\\theta^\\dagger))_j =  -(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j (\\hat{\\theta}_n - \\theta^\\dagger) - \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T (\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*}\\] Multiplying both sides by \\(\\frac{\\sqrt{n}}{n}\\) gives: \\[\n\\begin{align*}\n\\sqrt{n}\\lp\\frac{1}{n} (\\ell_\\theta(\\theta^\\dagger))_j\\rp & =  -\\left(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\right) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) - \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\\\\n& = \\lp -\\left(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\right) - \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp\\rp\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*}\n\\] The term \\(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j \\overset{p}{\\to} \\Exp{(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j}\\) by the WLLN, while \\[\n\\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp \\overset{p}{\\to} 0\n\\] by the WLLN, the boundedness condition of the third-order derivatives and the consistency of our estimator.\nTo be precise, we want to show that \\[\n\\lim_{n\\to\\infty}\\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp} &gt; \\epsilon}{} = 0\n\\] \\[\n\\begin{aligned}\n\\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp} &gt; \\epsilon}{} & \\leq \\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{ \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j} &gt; \\epsilon}{} \\\\\n& \\leq \\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\frac{1}{n}\\sum_{i=1}^n g(Y_i)} &gt; \\epsilon}{} \\\\\n& \\leq \\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\frac{1}{n}\\sum_{i=1}^n g(Y_i) - \\Exp{g(Y_1)}} &gt; \\frac{\\epsilon}{2}}{} \\\\\n& \\quad + \\Prob{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\Exp{g(Y_1)}} &gt; \\frac{\\epsilon}{2}}{}\n\\end{aligned}\n\\] The last line follows from a trick used in a proof in § 6.3 in Resnick (2019). If the following holds\n\\[\n\\begin{aligned}\n& \\left\\{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\frac{1}{n}\\sum_{i=1}^n g(Y_i) - \\Exp{g(Y_1)}} \\leq \\frac{\\epsilon}{2}\\right\\}  \\bigcap\\\\\n& \\left\\{\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\Exp{g(Y_1)}} \\leq \\frac{\\epsilon}{2}\\right\\}\n\\end{aligned}\n\\]\nThen the triangle inequality implies that \\[\n\\norm{(\\hat{\\theta}_n - \\theta^\\dagger)^T} \\norm{\\frac{1}{n}\\sum_{i=1}^n g(Y_i)} \\leq \\epsilon.\n\\] Taking complements and using Boole’s inequality (\\(P(A \\cup B) \\leq P(A) + P(B)\\)) yields the final inequality. Both terms converge to zero due to the convergence in probability of the MLE, and the WLLN applied to the empirical average of \\(g(Y_i)\\).\nBy Slutsky’s theorem, \\[\n-\\left(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\right) - \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T \\lp \\frac{1}{n}(\\ell_{\\theta\\theta\\theta}(\\tilde{\\theta}_{n,j}))_j\\rp \\overset{p}{\\to}-\\Exp{(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j}\n\\] Collecting our \\(p\\) equations into one set of equations yields: \\[\n\\sqrt{n}\\lp\\frac{1}{n} \\ell_\\theta(\\theta^\\dagger)\\rp = (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))} + o_p(1)) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\tag{1}\\]\nWriting out the expressions as explicit sums: \\[\\begin{align*}\n\\frac{\\sqrt{n}}{n}  \\sum_{i=1}^n \\ell_\\theta(\\theta^\\dagger; x_i) = (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))} + o_p(1)) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*} \\tag{2}\\]\nThe left-hand side of Equation 2 will be amenable to a multivariate version of the CLT. We’ll take the following multivariate CLT as given:\n\nTheorem 1. Multivariate CLT, (Keener 2010) Let \\(X_1, X_2, \\dots\\) be i.i.d random vectors in \\(\\R^k\\) with a common mean \\(\\Exp{X_i} = \\mu\\) and common covariance matrix \\(\\Sigma = \\Exp{(X_i - \\mu)(X_i - \\mu)^T}\\). If \\(\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\\), then \\[\\sqrt{n}(\\bar{X} - \\mu) \\overset{d}{\\to} \\text{Normal}(0, \\Sigma)\\]\n\nRecall that \\[\n\\Exp{\\ell_\\theta(\\theta; X_i)} = 0\n\\] By the multivariate central limit (MCLT) theorem, Equation 2 converges in distribution to a multivariate normal distribution with mean zero and covariance matrix \\(\\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}\\).\nWe’ll also need a lemma about the solutions to random linear equations:\n\nLemma 1 (Lemma 5.2 in (Lehmann and Casella 1998)) Suppose there are a set of \\(p\\) equations, \\(j = 1, \\dots, p\\): \\[\\sum_{k=1}^p A_{jkn} Y_{kn} = T_{jn}.\\] Let \\(T_{1n}, \\dots, T_{pn}\\) converge in distribution to \\(T_1, \\dots, T_p\\). Furthermore, suppose that for each \\(j,k\\), \\(A_{jkn} \\overset{p}{\\to} a_{jk}\\) such that the matrix \\(A\\) with \\((j,k)^{\\mathrm{th}}\\) element \\(a_{jk}\\) is nonsingular. Then if the distribution of \\(T_1, \\dots, T_p\\) has a disitribution with repsect to the Lebesgue measure over \\(\\R^p\\), \\(Y_{1n}, \\dots, Y_{pn}\\) tend in probability to \\(A^{-1} T\\).\nWritten in matrix form and using the fact that convergence in probability implies convergence in distribution: \\[\nT_n = A_n Y_n  \\implies Y_n \\overset{d}{\\to} A^{-1}T  \n\\]\n\nWe have that the left-hand side of Equation 1 converges in distribtuion to a multivariate normal distribution, and we have that the matrix on the RHS of Equation 1 convegens in probability to \\(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))}\\), which by assumption is invertble. Thus by Lemma 1 \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\) converges in probability to \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\overset{p}{\\to} (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}^{1/2}\\mathcal{Z}\n\\end{align*}\\] where \\(\\mathcal{Z} \\sim \\text{Normal}(0, I_p)\\), or \\[\\begin{align*}\n&\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)  \\overset{d}{\\to} \\mathcal{N}\\left(0, (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\right)\n\\end{align*}\\]\nAssuming further that \\[\n\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)} + \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}=0 \\implies (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T} = I_p\n\\] Putting this all together shows that \\[\\begin{align*}\n     \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger)^{-1})\n\\end{align*}\\] where \\(\\mathcal{I}(\\theta^\\dagger) = -\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)}\\)"
  },
  {
    "objectID": "survival-material/lecture-9.html#estimators-of-variance-covariance-matrix",
    "href": "survival-material/lecture-9.html#estimators-of-variance-covariance-matrix",
    "title": "Lecture 9",
    "section": "2.2 Estimators of variance-covariance matrix",
    "text": "2.2 Estimators of variance-covariance matrix\nIn the previous section, we encountered several consistent estimators of the variance covariance matrix: \\[\\begin{align*}\n  \\lp-\\frac{1}{n} \\ell_{\\theta\\theta}(\\hat{\\theta}_n)\\rp^{-1} & \\overset{p}{\\to} \\mathcal{I}(\\theta^\\dagger)^{-1} \\\\\n  \\lp\\frac{1}{n} \\ell_{\\theta}(\\hat{\\theta}_n)\\ell_{\\theta}(\\hat{\\theta}_n)^T\\rp^{-1}  & \\overset{p}{\\to} \\mathcal{I}(\\theta^\\dagger)\n\\end{align*}\\] These expressions assume that our inferential model matches the data generating model. In the event our inferential model is different than the true data generating model, it can be shown that the scaled MLE converges asymptotically to \\[\\begin{align*}\n&\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)  \\overset{d}{\\to} \\mathcal{N}\\left(0, (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\right)\n\\end{align*}\\] where the key difference is that \\(\\theta^\\dagger\\) is no longer the parameter for the true data generating process, but is instead the parameter the minimizes the KL divergence between the assumed inferential model and the true distribution generating the data.\nThus, the following sandwich estimator for the variance covariance matrix is often preferred over either of the above expressions: \\[\\begin{align}\n\\hat{\\Sigma}_{R} & = (-\\Prob{\\ell_{\\theta\\theta}(\\theta^\\dagger; X)}{n})^{-1} \\Prob{\\ell_\\theta(\\theta;X)\\ell_\\theta(\\theta;X)^T}{n}(-\\Prob{\\ell_{\\theta\\theta}(\\theta^\\dagger; X)}{n})^{-1} \\overset{p}{\\to} \\text{Var}\\left(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\right)\n\\end{align}\\] where \\(\\hat{\\theta}\\) is the MLE."
  },
  {
    "objectID": "survival-material/lecture-9.html#asymptotic-confidence-intervals",
    "href": "survival-material/lecture-9.html#asymptotic-confidence-intervals",
    "title": "Lecture 9",
    "section": "2.3 Asymptotic confidence intervals",
    "text": "2.3 Asymptotic confidence intervals\nFor the most part, we’ll be concerned with univariate confidence intervals, but in multivariate models like the Weibull distribution we’ll need to compute the full inverse of the Fisher information. WLOG, let the index of the parameter of interest be \\(1\\), so the asymptotic variance of our MLE for the parameter of interest is \\(\\sigma_1^2(\\theta^\\dagger) = \\mathcal{I}(\\theta^\\dagger)^{-1}_{1,1}\\). We can also define \\[\\sigma_1^2(\\hat{\\theta}) = \\mathcal{I}(\\hat{\\theta})^{-1}_{1,1}.\\] I’ll also ditch the \\(n\\) subscript and just let \\(\\hat{\\theta}\\) be our MLE based on \\(n\\) observations. By [eq:cont-map], \\[\\frac{\\sigma_1^2(\\hat{\\theta})}{\\sigma_1^2(\\theta^\\dagger)} \\overset{p}{\\to} 1.\\] This allows us to use a plug-in estimator for \\(\\mathcal{I}(\\theta^\\dagger)^{-1}\\), \\(\\mathcal{I}(\\hat{\\theta})^{-1}\\). \\[\\begin{align*}\n     \\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta_1^\\dagger)}{\\sigma_1(\\hat{\\theta})} & = \\frac{\\sigma_1(\\theta^\\dagger)}{\\sigma_1(\\hat{\\theta})}\\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta_1^\\dagger)}{\\sigma_1(\\theta^\\dagger)} \\\\\n     & \\overset{d}{\\to} \\mathcal{N}(0, 1)\n\\end{align*}\\] Using [eq:prod-slutsky], we can create an asymptotic confidence interval by noting that: \\[P\\left(\\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta^\\dagger_1)}{\\sigma_1(\\hat{\\theta})} \\leq x\\right) = \\Phi(x),\\] where \\(\\Phi(x)\\) is the CDF a normal distribution with zero mean and unit variance.\nThen \\[\\begin{align*}\nP\\left(\\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta^\\dagger_1)}{\\sigma_1(\\hat{\\theta}_1)} \\in (-z_{1-\\alpha/2},z_{1-\\alpha/2})\\right) & = P\\left(\\theta_1^\\dagger \\in \\left(\\hat{\\theta}_1 - z_{1-\\alpha/2} \\frac{\\sigma_1(\\hat{\\theta}_1)}{\\sqrt{n}}, \\hat{\\theta}_1 + z_{1-\\alpha/2} \\frac{\\sigma_1(\\hat{\\theta}_1)}{\\sqrt{n}}\\right)\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-9.html#asymptotic-tests",
    "href": "survival-material/lecture-9.html#asymptotic-tests",
    "title": "Lecture 9",
    "section": "2.4 Asymptotic tests",
    "text": "2.4 Asymptotic tests\n\n2.4.1 Wald test\nThe Wald test is derived directly from the asymptotic distribution of the MLE. Under the null hypothesis \\(\\theta^\\dagger = \\theta_0\\), the test statistic: \\[\\begin{align*}\n     \\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta_0)^{-1})\n\\end{align*}\\] so \\[n (\\hat{\\theta}_n - \\theta_0)^T \\mathcal{I}(\\theta_0) (\\hat{\\theta}_n - \\theta_0) \\sim \\chi^2(p)\\] This follows from the simple fact that if a random vector in \\(\\R^n\\), \\(Z\\), is distributed multivariate normal, or \\(Z \\sim \\mathcal{N}(0, \\Sigma)\\), then \\(\\Sigma^{-1/2} Z \\sim \\mathcal{N}(0, I)\\), so \\(Z^T\\Sigma^{-1/2}\\Sigma^{-1/2}Z = \\sum_{i=1}^n X_i^2\\) where \\(X_i \\sim \\mathcal{N}(0,1)\\).\n\n\n2.4.2 Rao’s score test\nIn our proof of the asymptotic distribution of the MLE, we used the fact that \\[\\sqrt{n}\\frac{1}{n} \\sum_{i=1}^n \\ell_\\theta(\\theta^\\dagger;X_i) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger)).\\] This idea can be used to derive the Rao’s Score test, which uses the fact that under \\(H_0: \\theta \\in \\Theta_0\\), the gradient evaluated at the restricted MLE (i.e. the MLE restricted to the parameter space \\(\\Theta_0\\)) is nearly zero, and we can recover a similar limiting distribution.\nAssuming that under the null distribution the restricted MLE \\(\\hat{\\theta}_0\\) is consistent for \\(\\theta^\\dagger \\in \\Theta_0\\), then \\[\\sqrt{n} \\ell_\\theta(\\hat{\\theta}_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger))\\] The Score test statistic is: \\[T_S = \\left(\\sqrt{n}\\ell_\\theta(\\hat{\\theta}_0) \\right)^T \\mathcal{I}(\\hat{\\theta}_0)^{-1}\\lp \\sqrt{n}\\ell_\\theta(\\hat{\\theta}_0)\\rp\\] This test statistic is distribution \\(\\chi^2(p)\\) under \\(H_0\\).\n\n\n2.4.3 Likelihood ratio test\nThe LRT comes from a two-term asymptotic expansion of the log-likelihood, as opposed to the one term expansion: \\[\\begin{align*}\n    -\\ell(\\theta_0) & = -\\ell(\\hat{\\theta}_n) - \\ell_\\theta(\\hat{\\theta}_n)(\\hat{\\theta}-\\theta_0) - \\frac{1}{2}(\\hat{\\theta}-\\theta_0)^T\\ell_{\\theta\\theta}(\\tilde{\\theta}_n)(\\hat{\\theta}-\\theta_0) \\\\\n    \\ell(\\hat{\\theta}) - \\ell(\\theta_0)  & = -\\frac{1}{2}(\\hat{\\theta}-\\theta_0)^T\\ell_{\\theta\\theta}(\\tilde{\\theta}_n)(\\hat{\\theta}-\\theta_0) \\\\\n    & = \\frac{1}{2}(\\sqrt{n}(\\hat{\\theta}-\\theta_0))^T\\frac{-\\ell_{\\theta\\theta}(\\tilde{\\theta}_n)}{n}(\\sqrt{n}(\\hat{\\theta}-\\theta_0)) \\\\\n\\end{align*}\\] As before, \\[\\sqrt{n}(\\hat{\\theta}-\\theta_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta_0)^{-1})\\] and \\[-\\frac{\\ell_{\\theta\\theta}(\\tilde{\\theta}_n)}{n} \\overset{p}{\\to} \\mathcal{I}(\\theta_0)\\] so \\[2 (\\ell(\\hat{\\theta}) - \\ell(\\theta_0)) \\overset{d}{\\to} \\chi^2(p)\\]"
  },
  {
    "objectID": "survival-material/lecture-9.html#tests-in-terms-of-observed-information",
    "href": "survival-material/lecture-9.html#tests-in-terms-of-observed-information",
    "title": "Lecture 9",
    "section": "1 Tests in terms of observed information",
    "text": "1 Tests in terms of observed information\nWhen we use observed information in place of the Fisher information, the Wald and Score tests look a bit different:\n\n1.1 Wald test with the observed information\n\\[n (\\hat{\\theta}_n - \\theta_0)^T \\frac{1}{n}j(\\hat{\\theta}_n) (\\hat{\\theta}_n - \\theta_0) = (\\hat{\\theta}_n - \\theta_0)^T j(\\hat{\\theta}_n) (\\hat{\\theta}_n - \\theta_0) \\overset{\\text{asympt.}}{\\sim} \\chi^2(p)\\]\n\n\n1.2 Score test with the observed information\n\\[\\begin{align*}\nT_S & = \\left(\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T (\\frac{1}{n}j(\\hat{\\theta}_0))^{-1}\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}  \\\\\n& = \\left(\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T n(j(\\hat{\\theta}_0))^{-1}\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}  \\\\\n& = \\left(\\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T j(\\hat{\\theta}_0)^{-1}\\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-9.html#composite-tests",
    "href": "survival-material/lecture-9.html#composite-tests",
    "title": "Lecture 9",
    "section": "2 Composite tests",
    "text": "2 Composite tests\nThis section is an expansion of Appendix B in (Klein, Moeschberger, et al. 2003).\nWe can modify all of our tests to accommodate testing a subset of the parameters. Typically we’ll have a subset of our parameter vector, let’s call it \\(\\psi\\), that we’re interested in, and we have another subset, \\(\\phi\\), that are nuisance parameters. In the our prior exponential regression example, we’ll likely be interested in testing if \\(\\beta\\neq 0\\), and thus we won’t care about testing \\(\\lambda\\).\nLet’s let \\(\\theta=(\\psi, \\phi)\\), and let \\(\\theta \\in \\R^p\\) so \\(\\psi\\in\\R^k\\), \\(k &lt; p\\), \\(\\phi\\in\\R^{p-k}\\). Our null hypothesis will be: \\[H_0: \\psi = \\psi_0.\\] Let \\(\\hat{\\phi}(\\psi_0)\\) be the MLE for the nuisance parameter with \\(\\psi\\) fixed under the null hypothesis. We’ll also partition the information matrix into a 2 by 2 block matrix: \\[\\mathcal{I}(\\psi, \\phi) =\n\\begin{bmatrix}\n\\Exp{-\\nabla^2_\\psi \\log f_\\theta(X_1)} & \\Exp{-\\nabla^2_{\\psi, \\phi} \\log f_\\theta(X_1)} \\\\\n\\Exp{-\\nabla^2_{\\psi, \\phi} \\log f_\\theta(X_1)} & \\Exp{-\\nabla^2_\\phi \\log f_\\theta(X_1)}\n\\end{bmatrix} = \\begin{bmatrix}\n\\mathcal{I}_{\\psi,\\psi} & \\mathcal{I}_{\\psi,\\phi} \\\\\n\\mathcal{I}_{\\psi,\\phi}^T & \\mathcal{I}_{\\phi,\\phi}\n\\end{bmatrix}\\] The observed information matrix can also be partioned the same way: \\[j(\\psi^\\prime, \\phi^\\prime) =\n\\begin{bmatrix}\n-\\ell_{\\psi\\psi}(\\psi^\\prime, \\phi^\\prime) & -\\ell_{\\psi\\phi}(\\psi^\\prime, \\phi^\\prime) \\\\\n-\\ell_{\\psi\\phi}(\\psi^\\prime, \\phi^\\prime)^T & -\\ell_{\\phi\\phi}(\\psi^\\prime, \\phi^\\prime)\n\\end{bmatrix} = \\begin{bmatrix}\nj_{\\psi,\\psi}(\\psi^\\prime,\\phi^\\prime) & j_{\\psi,\\phi}(\\psi^\\prime,\\phi^\\prime) \\\\\nj_{\\psi,\\phi}(\\psi^\\prime,\\phi^\\prime)^T & j_{\\phi,\\phi}(\\psi^\\prime,\\phi^\\prime)\n\\end{bmatrix}\\]\nThe inverse can also be partitioned into a 2 by 2 block matrix: \\[\\mathcal{I}(\\psi, \\phi)^{-1} =\n\\begin{bmatrix}\n\\mathcal{I}^{\\psi,\\psi} & \\mathcal{I}^{\\psi,\\phi} \\\\\n\\left(\\mathcal{I}^{\\psi,\\phi}\\right)^T & \\mathcal{I}^{\\phi,\\phi}\n\\end{bmatrix}\\] The expression for \\(\\mathcal{I}^{\\psi,\\psi}\\) can be found from the block matrix inversion formula: \\[\\begin{align}\n\\mathcal{I}^{\\psi,\\psi} & = \\mathcal{I}_{\\psi,\\psi}^{-1} + \\mathcal{I}_{\\psi,\\psi}^{-1}\\mathcal{I}_{\\psi,\\phi}\\left(\\mathcal{I}_{\\phi,\\phi} -\\mathcal{I}_{\\psi,\\phi}^T\\mathcal{I}_{\\psi,\\psi}^{-1} \\mathcal{I}_{\\psi,\\phi}\\right)^{-1}\\mathcal{I}_{\\psi,\\phi}^T\\mathcal{I}_{\\psi,\\psi}^{-1} \\\\\n& = \\left(\\mathcal{I}_{\\psi,\\psi} - \\mathcal{I}_{\\psi,\\phi} \\mathcal{I}_{\\phi,\\phi}^{-1}\\mathcal{I}_{\\psi,\\phi}^T\\right)^{-1}\n\\end{align} \\tag{1}\\]\nAll of the same notation will be used for the observed information, \\(j(\\psi, \\phi)\\):\n\\[j(\\psi^\\prime, \\phi^\\prime)^{-1} =\n\\begin{bmatrix}\nj^{\\psi,\\psi}(\\psi^\\prime,\\phi^\\prime) & j^{\\psi,\\phi}(\\psi^\\prime,\\phi^\\prime) \\\\\nj^{\\psi,\\phi}(\\psi^\\prime,\\phi^\\prime)^T & j^{\\phi,\\phi}(\\psi^\\prime,\\phi^\\prime)\n\\end{bmatrix}\\]\n\n2.1 Composite Wald test\nAgain using normal distribution theory, we can derive the Wald test with the observed information: \\[\\sqrt{n}(\\hat{\\psi}_n - \\psi_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}^{\\psi,\\psi}).\\] The Wald test statistic is then: \\[\\begin{align*}\nT_W = \\sqrt{n}(\\hat{\\psi}_n - \\psi_0) ^T   \\left(\\mathcal{I}^{\\psi,\\psi}\\mid_{\\psi = \\psi_0, \\phi = \\phi_0}\\right)^{-1}(\\hat{\\psi}_n - \\psi_0)\\sqrt{n}\n\\end{align*}\\] Using the appropriate transformation for the observed information in place of the Fisher information, we get \\[\\begin{align}\nT_W = (\\hat{\\psi}_n - \\psi_0) ^T \\left(j^{\\psi,\\psi}(\\hat{\\psi}_n, \\hat{\\phi}_n)\\right)^{-1}(\\hat{\\psi}_n - \\psi_0) \\overset{d}{\\to} \\chi^2_k\n\\end{align}\\] This simplifies by plugging in the Schur complement of \\(j_{\\phi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)\\), or \\(j_{\\psi,\\psi}(\\hat{\\psi}_n, \\hat{\\phi}_n) - j_{\\psi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n) j_{\\phi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)^{-1}j_{\\psi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)^T\\), which was the inverse of \\(j^{\\psi,\\psi}(\\hat{\\psi}_n, \\hat{\\phi}_n)\\), or \\[\nT_W = (\\hat{\\psi}_n - \\psi_0) ^T \\left(j_{\\psi,\\psi}(\\hat{\\psi}_n, \\hat{\\phi}_n) - j_{\\psi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n) j_{\\phi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)^{-1}j_{\\psi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)^T\\right)(\\hat{\\psi}_n - \\psi_0)\n\\]\n\n\n2.2 Composite Score test\nThe composite score test is a bit more complicated. The joint asymptotic distribution of the score is: \\[\\sqrt{n} \\frac{1}{n} \\ell_{\\theta}(\\psi_0, \\hat{\\phi}(\\psi_0)) \\overset{d}{\\to} \\mathcal{N}\\left(0,\n\\begin{bmatrix}\n\\mathcal{I}_{\\psi,\\psi} & \\mathcal{I}_{\\psi,\\phi} \\\\\n\\mathcal{I}_{\\psi,\\phi}^T & \\mathcal{I}_{\\phi,\\phi}\n\\end{bmatrix}\\right)\\] But when we have a nuisance parameter, under the null distribution we solve the score equations \\[\\ell_{\\phi}(\\psi_0, \\phi) = 0,\\] leading to an MLE for \\(\\phi\\), \\(\\hat{\\phi}(\\psi_0)\\), that is dependent on \\(\\psi_0\\). This means the distribution for \\(\\sqrt{n} \\frac{1}{n} \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))\\) needs to condition on the score equations for \\(\\psi\\) being zero. If the score equations are asymptotically normally distributed, then the score equations for \\(\\psi\\) are conditionally normal. Recall that if vectors \\(X, Y\\) are multivariate normal with marginal variance covariance matrices \\(\\Sigma_X, \\Sigma_Y\\) and \\(\\Sigma_{X,Y}\\) is the covariance matrix of \\(X\\) with \\(Y\\), then \\(X \\mid Y\\) is multivariate normal with parameters \\[\\Exp{X} + \\Sigma_{X,Y} \\Sigma_Y^{-1}(Y - \\Exp{Y}),\\quad \\Sigma_X - \\Sigma_{X,Y} \\Sigma_Y^{-1}\\Sigma_{X,Y}^T.\\] In our case, the marginal mean of the score equations are zero, and \\(Y \\equiv \\ell_{\\phi}(\\psi_0, \\hat{\\phi}(\\psi_0))\\) is zero, so the conditional distribution of the score of \\(\\psi\\) is \\[\\sqrt{n}\\frac{1}{n} \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0)) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}_{\\psi,\\psi} - \\mathcal{I}_{\\psi,\\phi} \\mathcal{I}_{\\phi,\\phi}^{-1}\\mathcal{I}_{\\phi,\\psi}^T).\\] The test statistic is then \\[\\begin{align*}\n   n^{-1/2} \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))^T\\left(\\mathcal{I}_{\\psi,\\psi} - \\mathcal{I}_{\\psi,\\phi} \\mathcal{I}_{\\phi,\\phi}^{-1}\\mathcal{I}_{\\phi,\\psi}^T\\right)^{-1} n^{-1/2} \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))\n\\end{align*}\\] as we showed in Equation 1, the inverse matrix is the same as \\(\\mathcal{I}^{\\psi,\\psi}\\), so, subbing in our observed information matrix again, we get the final \\[\\begin{align*}\nT_S =  \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))^T j^{\\psi, \\psi}(\\psi_0, \\hat{\\phi}(\\psi_0)) \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))\n\\end{align*}\\] which is asymptotically distributed as \\(\\chi^2_k\\).\n\n\n2.3 Composite likelihood ratio test\nThe composite likelihood ratio test is similar to the likelihood ratio test: \\[T_{LR} = 2(\\ell(\\hat{\\psi},\\hat{\\phi}) - \\ell(\\psi_0,\\hat{\\phi}(\\psi_0)))\\] and this is again asymptotically distributed as \\(\\chi^2_k\\)\n\nExample 1. Continued relative risk example Suppose we are interested in testing the hypothesis \\(H_0: \\beta = 0\\) vs \\(H_a: \\beta \\neq 0\\).\nRecall the definitions of \\(r_1, r_2, T_1, T_2\\): \\[\\begin{alignat*}\n{2}\n    r_1 & = \\sum_{i=1}^n (1 - z_i) \\delta_i\\quad &&\n    T_1 = \\sum_{i=1}^n (1 - z_i) t_i \\\\\n    r_2 & = \\sum_{i=1}^n z_i \\delta_i\\quad &&\n    T_2 = \\sum_{i=1}^n z_i t_i\n\\end{alignat*}\\] We showed in our prior example that the log-likelihood was: \\[\\begin{align}\n   \\ell(\\beta, \\lambda) =  (r_1 + r_2)\\log\\lambda -\\lambda T_1 + r_2 \\beta -\\lambda e^\\beta T_2\n\\end{align}\\] The score equations are \\[\\begin{align*}\n\\frac{\\partial}{\\partial \\lambda} \\ell(\\beta,\\lambda) &: \\frac{r_1 + r_2}{\\lambda} - T_1 - e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta} \\ell(\\beta,\\lambda) &: r_2 - \\lambda e^\\beta T_2\n\\end{align*}\\] and the matrix of second derivatives of the log-likelihood with respect to \\(\\lambda, \\beta\\), also known as the observed information, is \\[\\begin{align}\n  \\nabla^2_{\\lambda, \\beta} \\ell(\\beta,\\lambda) = \\begin{bmatrix}\n        \\frac{r_1 + r_2}{\\lambda^2} & e^\\beta T_2 \\\\\n        e^\\beta T_2 & \\lambda e^\\beta T_2\n    \\end{bmatrix}\n\\end{align}\\] The unrestricted MLE, (i.e. the MLE under the alternative hypothesis), is: \\[\\begin{align*}\n\\hat{\\lambda} & = \\frac{r_1}{T_1} \\\\\n\\hat{e^\\beta} & = \\frac{r_2}{T_2}\\frac{T_1}{r_1}\n\\end{align*}\\] Under the null hypothesis that \\(\\beta = 0\\), we have the restricted likelihood: \\[\\begin{align}\n   \\ell(\\beta=0, \\lambda) =  (r_1 + r_2)\\log\\lambda -\\lambda T_1 -\\lambda T_2\n\\end{align}\\] which can be differentiated with respect to \\(\\lambda\\), set to zero, and solved for \\(\\lambda\\): \\[\\begin{align}\n\\hat{\\lambda}(\\beta = 0) & = \\frac{r_1 + r_2}{T_1 + T_2}\n\\end{align}\\] The inverse of the observed information evaluated at the unrestricted MLE was shown to be \\[\\begin{align}\n\\frac{r_1 + r_2}{r_1 r_2}\n\\end{align}\\] The inverse of the observed information is: \\[\\begin{align}\n\\hat{\\mathcal{I}}^{-1}(\\beta, \\lambda) =  \\frac{1}{\\frac{(r_1 + r_2)e^\\beta T_2}{\\lambda} - e^{2 \\beta} T_2^2}\\begin{bmatrix}\n      \\lambda e^\\beta T_2  & -e^\\beta T_2 \\\\\n        -e^\\beta T_2 & \\frac{r_1 + r_2}{\\lambda^2}\n    \\end{bmatrix}\n\\end{align}\\] which when the \\(2,2\\) element is evaluated at the \\(\\hat{\\lambda}(\\beta=0)\\), or \\[\\hat{\\mathcal{I}}^{-1}(0,\\hat{\\lambda}(\\beta=0))_{2,2} = \\frac{(T_1 + T_2)^2}{(r_1 + r_2) T_1 T_2}\\] Now for the test statistics:\n\nLikelihood ratio test: After some algebra, we get \\[T_{LR} = 2 r_1 \\left(\\log\\left(\\frac{r_1}{T_1}\\right)- \\log\\left(\\frac{r_1 + r_2}{T_1 + T_2}\\right)\\right)+ 2 r_2 \\left(\\log\\left(\\frac{r_2}{T_2}\\right)- \\log\\left(\\frac{r_1 + r_2}{T_1 + T_2}\\right)\\right)\\]\nWald test: The test statistic is: \\[T_W = \\left(\\log\\frac{r_2 / T_2}{r_1/T_1}\\right)^2 \\frac{r_1 r_2}{r_1 + r_2}.\\]\nScore test The starting test statistic is: \\[T_S = \\left(r_2 - (r_1+r_2)\\frac{T_2 }{T_1 + T_2} \\right)^2 \\frac{(T_1 + T_2)^2}{(r_1 + r_2) T_1 T_2}.\\] This is sort of interesting because it looks a bit like the log-rank statistic! \\(\\frac{T_2}{T_1 + T_2}\\) is a bit like the proportion of time at risk the second group experienced, and the expected total failures in the second group is this proportion multiplied by the total failures in both groups. It’s not too hard to see why you might want to reject the null that \\(\\beta=0\\) if this statistic were large. This simplifies to \\[T_S = \\frac{(T_1 r_2  - T_2 r_1)^2}{(r_1 + r_2)T_1 T_2 }.\\]\n\nFor an observed dataset of \\(r_1 = 10, r_2 = 12, T_1 = 25, T_2 = 27\\), they all yield values around \\(0.06\\), which is far below the critical value of \\(3.84\\), which is the \\(95^\\mathrm{th}\\) quantile from a \\(\\chi^2_1\\)."
  },
  {
    "objectID": "survival-material/higher-order-asymptotics.html",
    "href": "survival-material/higher-order-asymptotics.html",
    "title": "Higher-order expansion of MLE for multivariate parameters",
    "section": "",
    "text": "1 Higher-order Taylor series of score function\nLet \\(\\theta \\in \\R^p\\).\n\\[\n\\begin{aligned}\n\\ell_\\theta(\\hat{\\theta}_n)_j & = \\ell_\\theta(\\theta^\\dagger)_j + \\ell_{\\theta\\theta}(\\theta^\\dagger)_j(\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j(\\hat{\\theta}_n - \\theta^\\dagger) \\\\\n& \\quad + \\frac{1}{6}\\sum_{ikm} (\\ell_{\\theta\\theta\\theta\\theta}(\\tilde{\\theta}_{nj})_j)_{ikm}(\\hat{\\theta}_{ni} - \\theta^\\dagger_i)(\\hat{\\theta}_{nk} - \\theta^\\dagger_k)(\\hat{\\theta}_{nm} - \\theta^\\dagger_m)  \n\\end{aligned}\n\\] where \\[\n\\tilde{\\theta}_{nj} = \\hat{\\theta}_{n} \\pi_j + (1 - \\pi_j) \\theta^\\dagger, \\pi_j \\in (0, 1)\n\\]\nWe assume standard regularity conditions set out in lecture 8. Under these conditions it can be shown that the last term is \\(O_p(n^{-1/2})\\), so we can rewrite this as \\[\n\\begin{aligned}\n\\ell_\\theta(\\hat{\\theta}_n)_j & = \\ell_\\theta(\\theta^\\dagger)_j + \\ell_{\\theta\\theta}(\\theta^\\dagger)_j(\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j(\\hat{\\theta}_n - \\theta^\\dagger) + O_p(n^{-1/2})\n\\end{aligned}\n\\] Multiply each side by \\(\\frac{\\sqrt{n}}{n}\\): \\[\n\\begin{aligned}\n0  & = \\sqrt{n}\\frac{\\ell_\\theta(\\theta^\\dagger)_j}{n} + \\frac{\\ell_{\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) / \\sqrt{n} + O_p(n^{-1})\n\\end{aligned}\n\\] To make the above more amenable to matrix algebra, let’s use the trace trick and some properties of the vec operator to rewrite the quadratic term. Note that the matrix \\(\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j\\) is symmetric for each \\(j\\). \\[\n\\begin{aligned}\n\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) & = \\tr(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)) \\\\\n& = \\tr(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T) \\\\\n& = \\tr(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)^T_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T) \\\\\n& = \\asvec(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n})^T\\asvec(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T) \\\\\n\\end{aligned}\n\\] This will allow us to write our system of equations in matrix form with the matrix \\(D\\) as: \\[\nD =\n\\begin{bmatrix}\n\\asvec(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_1}{n})^T \\\\\n\\asvec(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_2}{n})^T \\\\\n\\vdots \\\\\n\\asvec(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_p}{n})^T\n\\end{bmatrix}\n\\] Then we write the system of equations: \\[\n0  = \\sqrt{n}\\frac{\\ell_\\theta(\\theta^\\dagger)}{n} + \\frac{\\ell_{\\theta\\theta}(\\theta^\\dagger)}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}D\\asvec(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T) / \\sqrt{n} + O_p(n^{-1})\n\\]\nWe’ll assume the following asymptotic representations of the sample means of the matrices of derivatives, where \\(Z_j = O_p(1)\\), \\(Z_{1j} \\in \\R, Z_{2j} \\in \\R^p, Z_{3j} \\in \\R^{p\\times p}\\) .\n\\[\n\\begin{aligned}\n\\frac{\\ell_\\theta(\\theta^\\dagger)_j}{n} & = \\frac{Z_{1j}}{\\sqrt{n}} \\\\\n\\frac{\\ell_{\\theta\\theta}(\\theta^\\dagger)_j}{n} & = -\\mathcal{I}_j(\\theta^\\dagger) + \\frac{Z_{2j}}{\\sqrt{n}} \\\\\n\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n} & = V_j + \\frac{Z_{3j}}{\\sqrt{n}} \\\\\n\\end{aligned}\n\\]\nFinally, we would like to find an asymptotic expansion of the form: \\[\n\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) = B + C / \\sqrt{n} + O_p(n^{-1})\n\\]"
  },
  {
    "objectID": "survival-material/higher-order-asymptotics.html#tests-in-terms-of-observed-information",
    "href": "survival-material/higher-order-asymptotics.html#tests-in-terms-of-observed-information",
    "title": "Lecture 9",
    "section": "1 Tests in terms of observed information",
    "text": "1 Tests in terms of observed information\nWhen we use observed information in place of the Fisher information, the Wald and Score tests look a bit different:\n\n1.1 Wald test with the observed information\n\\[n (\\hat{\\theta}_n - \\theta_0)^T \\frac{1}{n}i(\\hat{\\theta}_n) (\\hat{\\theta}_n - \\theta_0) = (\\hat{\\theta}_n - \\theta_0)^T i(\\hat{\\theta}_n) (\\hat{\\theta}_n - \\theta_0) \\overset{\\text{asympt.}}{\\sim} \\chi^2(p)\\]\n\n\n1.2 Score test with the observed information\n\\[\\begin{align*}\nT_S & = \\left(\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T (\\frac{1}{n}i(\\hat{\\theta}_0))^{-1}\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}  \\\\\n& = \\left(\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T n(i(\\hat{\\theta}_0))^{-1}\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}  \\\\\n& = \\left(\\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T i(\\hat{\\theta}_0)^{-1}\\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/higher-order-asymptotics.html#composite-tests",
    "href": "survival-material/higher-order-asymptotics.html#composite-tests",
    "title": "Lecture 9",
    "section": "2 Composite tests",
    "text": "2 Composite tests\nThis section is an expansion of Appendix B in (Klein, Moeschberger, et al. 2003).\nWe can modify all of our tests to accommodate testing a subset of the parameters. Typically we’ll have a subset of our parameter vector, let’s call it \\(\\psi\\), that we’re interested in, and we have another subset, \\(\\phi\\), that are nuisance parameters. In the [exmp:simple-exp-reg], we’ll likely be interested in testing if \\(\\beta\\neq 0\\), and thus we won’t care about testing \\(\\lambda\\).\nLet’s let \\(\\theta=(\\psi, \\phi)\\), and let \\(\\theta \\in \\R^p\\) so \\(\\psi\\in\\R^k\\), \\(k &lt; p\\), \\(\\phi\\in\\R^{p-k}\\). Our null hypothesis will be: \\[H_0: \\psi = \\psi_0.\\] Let \\(\\hat{\\phi}(\\psi_0)\\) be the MLE for the nuisance parameter with \\(\\psi\\) fixed under the null hypothesis. We’ll also partition the information matrix into a 2 by 2 block matrix: \\[\\mathcal{I}(\\psi, \\phi) =\n\\begin{bmatrix}\n\\Exp{-\\nabla^2_\\psi \\log f_\\theta(X_1)} & \\Exp{-\\nabla^2_{\\psi, \\phi} \\log f_\\theta(X_1)} \\\\\n\\Exp{-\\nabla^2_{\\psi, \\phi} \\log f_\\theta(X_1)} & \\Exp{-\\nabla^2_\\phi \\log f_\\theta(X_1)}\n\\end{bmatrix} = \\begin{bmatrix}\n\\mathcal{I}_{\\psi,\\psi} & \\mathcal{I}_{\\psi,\\phi} \\\\\n\\mathcal{I}_{\\psi,\\phi}^T & \\mathcal{I}_{\\phi,\\phi}\n\\end{bmatrix}\\] The inverse can also be partitioned into a 2 by 2 block matrix: \\[\\mathcal{I}(\\psi, \\phi)^{-1} =\n\\begin{bmatrix}\n\\mathcal{I}^{\\psi,\\psi} & \\mathcal{I}^{\\psi,\\phi} \\\\\n\\left(\\mathcal{I}^{\\psi,\\phi}\\right)^T & \\mathcal{I}^{\\phi,\\phi}\n\\end{bmatrix}\\] The expression for \\(\\mathcal{I}^{\\psi,\\psi}\\) can be found from the block matrix inversion formula: \\[\\begin{align}\n\\mathcal{I}^{\\psi,\\psi} & = \\mathcal{I}_{\\psi,\\psi}^{-1} + \\mathcal{I}_{\\psi,\\psi}^{-1}\\mathcal{I}_{\\psi,\\phi}\\left(\\mathcal{I}_{\\phi,\\phi} -\\mathcal{I}_{\\psi,\\phi}^T\\mathcal{I}_{\\psi,\\psi}^{-1} \\mathcal{I}_{\\psi,\\phi}\\right)^{-1}\\mathcal{I}_{\\psi,\\phi}^T\\mathcal{I}_{\\psi,\\psi}^{-1} \\\\\n& = \\left(\\mathcal{I}_{\\psi,\\psi} - \\mathcal{I}_{\\psi,\\phi} \\mathcal{I}_{\\phi,\\phi}^{-1}\\mathcal{I}_{\\psi,\\phi}^T\\right)^{-1}\\label{eq:inv-fisher-block}\n\\end{align}\\]\nAll of these results hold for the observed information, \\(i(\\psi, \\phi)\\).\n\n2.1 Composite Wald test\nAgain using normal distribution theory, we can derive the Wald test with the observed information: \\[\\sqrt{n}(\\hat{\\psi}_n - \\psi_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}^{\\psi,\\psi}).\\] The Wald test statistic is then: \\[\\begin{align*}\nT_W = \\sqrt{n}(\\hat{\\psi}_n - \\psi_0) ^T   \\left(\\mathcal{I}^{\\psi,\\psi}\\mid_{\\psi = \\psi_0, \\phi = \\phi_0}\\right)^{-1}(\\hat{\\psi}_n - \\psi_0)\\sqrt{n}\n\\end{align*}\\] Using the appropriate transformation for the observed information in place of the Fisher information, we get \\[\\begin{align}\n\\label{eq:wald-composite}\nT_W = (\\hat{\\psi}_n - \\psi_0) ^T \\left(i^{\\psi,\\psi}\\mid_{\\psi = \\hat{\\psi}_n, \\phi = \\hat{\\pi}_n}\\right)^{-1}(\\hat{\\psi}_n - \\psi_0) \\overset{d}{\\to} \\chi^2_k\n\\end{align}\\]\n\n\n2.2 Composite Score test\nThe composite score test is a bit more complicated. The joint asymptotic distribution of the score is: \\[\\sqrt{n} \\frac{1}{n} \\nabla_{(\\psi,\\phi)} \\ell(\\psi, \\phi)\\mid_{\\psi=\\psi_0, \\phi = \\hat{\\phi}(\\psi_0)} \\overset{d}{\\to} \\mathcal{N}\\left(0,\n\\begin{bmatrix}\n\\mathcal{I}_{\\psi,\\psi} & \\mathcal{I}_{\\psi,\\phi} \\\\\n\\mathcal{I}_{\\psi,\\phi}^T & \\mathcal{I}_{\\phi,\\phi}\n\\end{bmatrix}\\right)\\] But when we have a nuisance parameter, under the null distribution we solve the score equations \\[\\nabla_{\\phi} \\ell(\\psi_0, \\phi) = 0,\\] leading to an MLE for \\(\\phi\\), \\(\\hat{\\phi}(\\psi_0)\\), that is dependent on \\(\\psi_0\\). This means the distribution for \\(\\sqrt{n} \\frac{1}{n} \\nabla_{\\psi} \\ell(\\psi, \\phi)\\mid_{\\psi=\\psi_0, \\phi = \\hat{\\phi}(\\psi_0)}\\) needs to condition on the score equations for \\(\\psi\\) being zero. If the score equations are asymptotically normally distributed, then the score equations for \\(\\psi\\) are conditionally normal. Recall that if vectors \\(X, Y\\) are multivariate normal with marginal variance covariance matrices \\(\\Sigma_X, \\Sigma_Y\\) and \\(\\Sigma_{X,Y}\\) is the covariance matrix of \\(X\\) with \\(Y\\), then \\(X \\mid Y\\) is multivariate normal with parameters \\[\\Exp{X} + \\Sigma_{X,Y} \\Sigma_Y^{-1}(Y - \\Exp{Y}),\\quad \\Sigma_X - \\Sigma_{X,Y} \\Sigma_Y^{-1}\\Sigma_{X,Y}^T.\\] In our case, the marginal mean of the score equations are zero, and \\(Y \\equiv \\nabla_{\\phi} \\ell(\\psi_0, \\hat{\\phi}(\\psi_0))\\) is zero, so the conditional distribution of the score of \\(\\psi\\) is \\[\\sqrt{n}\\frac{1}{n} \\nabla_{\\psi} \\ell(\\theta)\\mid_{\\psi=\\psi_0, \\phi = \\hat{\\phi}(\\psi_0)} \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}_{\\psi,\\psi} - \\mathcal{I}_{\\psi,\\phi} \\mathcal{I}_{\\phi,\\phi}^{-1}\\mathcal{I}_{\\phi,\\psi}^T).\\] The test statistic is then \\[\\begin{align*}\n   n^{-1/2} \\nabla_{\\psi} \\ell(\\theta)\\mid_{\\psi=\\psi_0, \\phi = \\hat{\\phi}(\\psi_0)} \\left(\\mathcal{I}_{\\psi,\\psi} - \\mathcal{I}_{\\psi,\\phi} \\mathcal{I}_{\\phi,\\phi}^{-1}\\mathcal{I}_{\\phi,\\psi}^T\\right)^{-1} n^{-1/2} \\nabla_{\\psi} \\ell(\\theta)\\mid_{\\psi=\\psi_0, \\phi = \\hat{\\phi}(\\psi_0)}\n\\end{align*}\\] as we showed in [eq:inv-fisher-block], the inverse matrix is the same as \\(\\mathcal{I}^{\\psi,\\psi}\\), so, subbing in our observed information matrix again, we get the final \\[\\begin{align*}\nT_S =  \\nabla_{\\psi} \\ell(\\theta)\\mid_{\\psi=\\psi_0, \\phi = \\hat{\\phi}(\\psi_0)} i(\\psi_0, \\hat{\\phi}(\\psi_0))^{\\psi, \\psi} \\nabla_{\\psi} \\ell(\\theta)\\mid_{\\psi=\\psi_0, \\phi = \\hat{\\phi}(\\psi_0)}\n\\end{align*}\\] which is asymptotically distributed as \\(\\chi^2_k\\).\n\n\n2.3 Composite likelihood ratio test\nThe composite likelihood ratio test is similar to the likelihood ratio test: \\[T_{LR} = 2(\\ell(\\hat{\\psi},\\hat{\\phi}) - \\ell(\\psi_0,\\hat{\\phi}(\\psi_0)))\\] and this is again asymptotically distributed as \\(\\chi^2_k\\)\n\nExample 1. Continued relative risk example Suppose we are interested in testing the hypothesis \\(H_0: \\beta = 0\\) vs \\(H_a: \\beta \\neq 0\\).\nRecall the definitions of \\(r_1, r_2, T_1, T_2\\): \\[\\begin{alignat*}\n{2}\n    r_1 & = \\sum_{i=1}^n (1 - z_i) \\delta_i\\quad &&\n    T_1 = \\sum_{i=1}^n (1 - z_i) t_i \\\\\n    r_2 & = \\sum_{i=1}^n z_i \\delta_i\\quad &&\n    T_2 = \\sum_{i=1}^n z_i t_i\n\\end{alignat*}\\] We showed in [exmp:simple-exp-reg] that the log-likelihood was: \\[\\begin{align}\n   \\ell(\\lambda, \\beta) =  (r_1 + r_2)\\log\\lambda -\\lambda T_1 + r_2 \\beta -\\lambda e^\\beta T_2\n\\end{align}\\] The score equations are \\[\\begin{align*}\n\\frac{\\partial}{\\partial \\lambda} \\ell(\\lambda, \\beta) &: \\frac{r_1 + r_2}{\\lambda} - T_1 - e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta} \\ell(\\lambda, \\beta) &: r_2 - \\lambda e^\\beta T_2\n\\end{align*}\\] and the matrix of second derivatives of the log-likelihood with respect to \\(\\lambda, \\beta\\), also known as the observed information, is \\[\\begin{align}\n  \\nabla^2_{\\lambda, \\beta} \\ell(\\lambda, \\beta) = \\begin{bmatrix}\n        \\frac{r_1 + r_2}{\\lambda^2} & e^\\beta T_2 \\\\\n        e^\\beta T_2 & \\lambda e^\\beta T_2\n    \\end{bmatrix}\n\\end{align}\\] The unrestricted MLE, (i.e. the MLE under the alternative hypothesis), is: \\[\\begin{align*}\n\\hat{\\lambda} & = \\frac{r_1}{T_1} \\\\\n\\hat{e^\\beta} & = \\frac{r_2}{T_2}\\frac{T_1}{r_1}\n\\end{align*}\\] Under the null hypothesis that \\(\\beta = 0\\), we have the restricted likelihood: \\[\\begin{align}\n   \\ell(\\lambda, \\beta=0) =  (r_1 + r_2)\\log\\lambda -\\lambda T_1 -\\lambda T_2\n\\end{align}\\] which can be differentiated with respect to \\(\\lambda\\), set to zero, and solved for \\(\\lambda\\): \\[\\begin{align}\n\\hat{\\lambda}_0 & = \\frac{r_1 + r_2}{T_1 + T_2}\n\\end{align}\\] The inverse of the observed information evaluated at the unrestricted MLE was shown to be \\[\\begin{align}\n\\frac{r_1 + r_2}{r_1 r_2}\n\\end{align}\\] The inverse of the observed information is: \\[\\begin{align}\n\\hat{\\mathcal{I}}^{-1}(\\lambda, \\beta) =  \\frac{1}{\\frac{(r_1 + r_2)e^\\beta T_2}{\\lambda} - e^{2 \\beta} T_2^2}\\begin{bmatrix}\n      \\lambda e^\\beta T_2  & -e^\\beta T_2 \\\\\n        -e^\\beta T_2 & \\frac{r_1 + r_2}{\\lambda^2}\n    \\end{bmatrix}\n\\end{align}\\] which when the \\(2,2\\) element is evaluated at the \\(\\hat{\\lambda}_0\\), or \\[\\hat{\\mathcal{I}}^{-1}(\\hat{\\lambda}_0, 0)_{2,2} = \\frac{(T_1 + T_2)^2}{(r_1 + r_2) T_1 T_2}\\] Now for the test statistics:\n\nLikelihood ratio test: After some algebra, we get \\[T_{LR} = 2 r_1 \\left(\\log\\left(\\frac{r_1}{T_1}\\right)- \\log\\left(\\frac{r_1 + r_2}{T_1 + T_2}\\right)\\right)+ 2 r_2 \\left(\\log\\left(\\frac{r_2}{T_2}\\right)- \\log\\left(\\frac{r_1 + r_2}{T_1 + T_2}\\right)\\right)\\]\nWald test: The test statistic is: \\[T_W = \\left(\\log\\frac{r_2 / T_2}{r_1/T_1}\\right)^2 \\frac{r_1 r_2}{r_1 + r_2}.\\]\nScore test The starting test statistic is: \\[T_S = \\left(r_2 - (r_1+r_2)\\frac{T_2 }{T_1 + T_2} \\right)^2 \\frac{(T_1 + T_2)^2}{(r_1 + r_2) T_1 T_2}.\\] This is sort of interesting because it looks a bit like the log-rank statistic! \\(\\frac{T_2}{T_1 + T_2}\\) is a bit like the proportion of time at risk the second group experienced, and the expected total failures in the second group is this proportion multiplied by the total failures in both groups. It’s not too hard to see why you might want to reject the null that \\(\\beta=0\\) if this statistic were large. This simplifies to \\[T_S = \\frac{(T_1 r_2  - T_2 r_1)^2}{(r_1 + r_2)T_1 T_2 }.\\]\n\nFor an observed dataset of \\(r_1 = 10, r_2 = 12, T_1 = 25, T_2 = 27\\), they all yield values around \\(0.06\\), which is far below the critical value of \\(3.84\\), which is the \\(95^\\mathrm{th}\\) quantile from a \\(\\chi^2_1\\)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/MCMC-lecture-1.html",
    "href": "missing-data-material-W-26/notes/MCMC-lecture-1.html",
    "title": "MCMC and diagnostics",
    "section": "",
    "text": "See Tanner and Wong (1987) for more details; what follows is an abbreviated presentation of their work, with bits of Gelfand and Smith (1990) thrown in.\nLet \\(p(\\theta_1, \\theta_2 \\mid y)\\) be the posterior of interest.\nWe can represent the marginal posterior for \\(\\theta_1\\) as: \\[\np(\\theta_1 \\mid y) = \\int_{\\Omega_{\\theta_2}} p(\\theta_1 \\mid y, \\theta_2) p(\\theta_2 \\mid y) d\\theta_2,\n\\] and similarly for \\(\\theta_2\\): \\[\np(\\theta_2 \\mid y) = \\int_{\\Omega_{\\theta_1}} p(\\theta_2 \\mid y, \\theta_1) p(\\theta_1 \\mid y) d\\theta_1\n\\]\nPlugging the second expression into the first yields:\n\\[\np(\\theta_1 \\mid y) = \\int_{\\Omega_{\\theta_2}} p(\\theta_1 \\mid y, \\theta_2) \\int_{\\Omega_{\\theta_1}} p(\\theta_2 \\mid y, \\theta_1^\\prime) p(\\theta_1^\\prime \\mid y) d\\theta^\\prime_1 d\\theta_2,\n\\]\nwhich we can rearrange into:\n\\[\np(\\theta_1 \\mid y) = \\int_{\\Omega_{\\theta_1}} \\int_{\\Omega_{\\theta_2}} p(\\theta_1 \\mid y, \\theta_2) p(\\theta_2 \\mid y, \\theta_1^\\prime) d\\theta_2 p(\\theta_1^\\prime \\mid y) d\\theta^\\prime_1,\n\\] Let \\(P(\\theta \\mid \\theta^prime)\\) be the proposal distribution, which conditions on \\(\\theta^\\prime\\): \\[\nP(\\theta \\mid \\theta^\\prime) = \\int_{\\Omega_{\\theta_2}} p(\\theta_1 \\mid y, \\theta_2) p(\\theta_2 \\mid y, \\theta_1^\\prime) d\\theta_2\n\\] Then the equation \\[\ng(\\theta_1) = \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  g(\\theta_1^\\prime) d\\theta^\\prime_1,\n\\] is an integral equation. Given weak conditions on \\(P(\\theta_1 \\mid \\theta^\\prime_1)\\), \\(p(\\theta_1 \\mid y)\\) is a unique solution for the equation.\nWe can define the integral transformation: \\(T g = \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  g(\\theta_1^\\prime) d\\theta^\\prime_1\\), which takes an \\(L_1\\) integrable function, \\(\\lVert g \\rVert_1 = \\int_{\\Omega_{\\theta_1}} \\abs{g(\\theta_1)} d\\theta_1 &lt; \\infty\\) and yields another \\(L_1\\) integrable function.\nOne thing to note is that \\(\\lVert T g \\rVert_1 = \\lVert g \\rVert_1\\), and, because \\(P(\\theta \\mid \\theta^\\prime)\\) is a probability density in \\(\\theta\\) for each \\(\\theta^\\prime\\), it integrates to \\(1\\), \\[\n\\begin{aligned}\n\\int_{\\Omega_{\\theta_1}} \\abs{(T g)(\\theta_1)} d\\theta_1 & = \\int_{\\Omega_{\\theta_1}} \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  g(\\theta_1^\\prime) d\\theta^\\prime_1 d\\theta_1 \\\\\n& = \\int_{\\Omega_{\\theta_1}} \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)d\\theta_1  g(\\theta_1^\\prime) d\\theta^\\prime_1  \\\\\n& = \\int_{\\Omega_{\\theta_1}} 1  g(\\theta_1^\\prime) d\\theta^\\prime_1  \\\\\n& = \\int_{\\Omega_{\\theta_1}} \\abs{g(\\theta_1^\\prime)} d\\theta^\\prime_1  \\\\\n& = \\lVert g \\rVert_1\n\\end{aligned}\n\\] Also note that if \\(f(\\theta) \\geq g(\\theta)\\) for all \\(\\theta\\), then \\(T f \\geq T g\\) for all \\(\\theta\\).\n\\[\n\\begin{aligned}\nT f - T g & =  \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  f(\\theta_1^\\prime) d\\theta^\\prime_1 - \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  g(\\theta_1^\\prime) d\\theta^\\prime_1 \\\\\n& = \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime) (f(\\theta_1^\\prime) - g(\\theta_1^\\prime)) d\\theta^\\prime_1 \\\\\n& \\geq 0\n\\end{aligned}\n\\] With these two facts, we can show that given a function \\(g_i\\), \\(g_{i+1} = T g_i\\) and a solution, which we’ll call \\(g^\\star \\equiv p(\\theta_1 \\mid y)\\), which solves the integral equation, that \\(\\lVert g_{i+1} - g^\\star \\rVert \\leq  \\lVert g_{i} - g^\\star \\rVert\\).\n\\[\n\\begin{aligned}\n\\norm{g_{i+1} - g^\\star} & = \\norm{T (g_{i} - g^\\star)} \\\\\n& \\leq \\norm{T \\abs{g_{i} - g^\\star}} \\\\\n& = \\norm{g_{i} - g^\\star}\n\\end{aligned}\n\\] There are two more conditions that lead to \\(g^\\star\\) being the unique solution to the equation, and to another desirable characteristic for our computational schemes, which is called geometric ergodicity: \\[\n\\norm{g_{i+1} - g^\\star} \\leq \\alpha^{i}(g_0) \\norm{g_{0} - g^\\star}, \\, \\alpha \\in (0, 1)\n\\] as long as \\(\\sup_\\theta g_0(\\theta) / g^\\star(\\theta) &lt; \\infty\\).\nThis condition is the following: For every \\(\\theta_0 \\in \\Omega_{\\theta}\\) there is an open neighborhood \\(U\\) of \\(\\theta_0\\) so that: \\[\nP(\\theta \\mid \\theta^\\prime) &gt; 0, \\forall (\\theta, \\theta^\\prime) \\in U\n\\]\n\\[\nP(\\theta \\mid \\theta^\\prime) \\leq M &lt; \\infty \\,\\, \\forall (\\theta, \\theta^\\prime) \\in \\Omega_\\theta\n\\]\nWe can use this fact to show that the scheme will converge to \\(p(\\theta_1 \\mid y)\\), the unique posterior for \\(\\theta_1\\), and the stationary distribution for the proposal \\(P(\\theta \\mid \\theta^\\prime\\). I won’t go over the proof, but you can see it in section 6 of Tanner and Wong (1987).\nThis suggests the following algorithm, which we’ll call Gibbs sampling, to generate draws from an unknown posterior:\n\nDetermine starting values \\(\\theta_1^0, \\theta_2^0\\)\nFor \\(t=1, \\dots, S\\)\n\nDraw \\(\\theta_1^{t+1} \\sim p(\\theta_1 \\mid \\theta_2^t, y)\\)\nDraw \\(\\theta_2^{t+1} \\sim p(\\theta_2 \\mid \\theta_1^{t+1}, y)\\)\n\nDiscard \\(S/2\\) iterations, and keep the set of draws \\(\\{ (\\theta_1^s, \\theta_2^s) \\mid s = S/2+1, ,\\dots, S\\}\\).\n\nThe final set of draws \\(S/2\\) draws are approximately distributed according to \\(p(\\theta_1, \\theta_2 \\mid y)\\)\nThe assumptions that make the algorithm work are key. Tanner and Wong (1987) note that \\(\\alpha\\) is dependent on the starting distribution, and \\(\\alpha\\) can be arbitrarily close to \\(1\\) for unbounded parameter spaces.\nThe condition on the starting distribution mean that a \\(g_0\\) with compact support might be a good choice, because we avoid a situation where \\(\\sup_\\theta g_0(\\theta) / g^\\star(\\theta)\\) is large because \\(g_0\\) has tails that are heavier than those of \\(g^\\star(\\theta)\\) as \\(\\norm{\\theta} \\to \\infty\\).\nThe problem with Gibbs is that it often moves slowly in high dimensions when there is high correlation between components.\n\n\nGiven a set of draws \\(\\{\\theta_s, s = 1, \\dots, S\\}\\) from a Gibbs sampler, we want a way to assess how well our empirical average: \\[\n\\bar{f} = \\frac{1}{S} \\sum_{s = 1}^S f(\\theta_s)\n\\] approximates the true expectation: \\[\n\\Exp{f(\\theta)}{p(\\theta \\mid y)}\n\\] Under fairly general conditions, \\[\n\\sqrt{n}(\\bar{f} - \\Exp{f}) \\overset{d}{\\to} N(0, \\sigma^2)\n\\] The wrinkle is that \\(\\sigma^2\\) wont equal \\(\\text{Var}_{p(\\theta \\mid y)}(f(\\theta))\\) because our draws aren’t independent. In fact, as shown in Geyer (2005), we can compute the variance of the left-hand side: \\[\n\\begin{aligned}\n\\text{Var}(\\sqrt{n}(\\bar{f} - \\Exp{f})) & = n \\text{Var}(\\bar{f}) \\\\\n& = \\frac{1}{n}\\sum_{s=1}^n \\text{Var}(f(\\theta_s)) + \\frac{2}{n} \\sum_{i=1}^{n-1} \\sum_{j={i+1}}^{n} \\text{Cov}(f(\\theta_i),f(\\theta_j))\n\\end{aligned}\n\\] We’ll assume we have a stationary chain, so \\(\\text{Var}(f(\\theta_s)) = \\gamma_0\\), and \\(\\gamma_k = \\text{Cov}(f(\\theta_{i}),f(\\theta_{i+k}))\\), which leads to\n\\[\n\\begin{aligned}\n\\frac{1}{n}\\sum_{s=1}^n \\text{Var}(f(\\theta_s)) + \\frac{2}{n} \\sum_{i=1}^{n-1} \\sum_{j={i+1}}^{n} \\text{Cov}(f(\\theta_i),f(\\theta_j)) & = \\gamma_0 + \\frac{2}{n} \\sum_{i=1}^{n-1} \\sum_{j={i+1}}^{n} \\gamma_{j - i} \\\\\n& = \\gamma_0 + \\frac{2}{n} \\sum_{i=1}^{n-1} (n - i) \\gamma_i\n\\end{aligned}\n\\] If things are well-behaved, the series on the right converges to: \\[\n\\begin{aligned}\n\\sigma^2 = \\gamma_0 + 2 \\sum_{i=1}^{\\infty} \\gamma_i\n\\end{aligned}\n\\] Thus, \\[\n\\sqrt{n}(\\bar{f} - \\Exp{f}) \\overset{d}{\\to} N(0, \\gamma_0 + 2 \\sum_{i=1}^{\\infty} \\gamma_i)\n\\] If we had independent draws of \\(\\theta_i\\), we would instead have \\[\n\\sqrt{n}(\\bar{f} - \\Exp{f}) \\overset{d}{\\to} N(0, \\gamma_0)\n\\] The ratio of these variances: \\[\n\\frac{\\gamma_0}{\\gamma_0 + 2 \\sum_{i=1}^{\\infty} \\gamma_i} = \\frac{1}{1 + 2 \\sum_{i=1}^{\\infty} \\frac{\\gamma_i}{\\gamma_0}}\n\\] Can be used to compute something called the effective sample size from an MCMC sample of \\(S\\) draws: \\[\nn_\\text{eff} = \\frac{n}{1 + 2 \\sum_{i=1}^{\\infty} \\rho_k}\n\\] where \\(\\rho_k\\) is the autocorrelation at the \\(k^\\text{th}\\) lag. The effective sample size represents the number of independent draws with an equivalent variance to the samples from an MCMC chain.\nThese figures can be used to benchmark MCMC algorithms by computing \\(n_\\text{eff}/\\text{sec}\\) of computing time, or \\(n_\\text{eff}/\\text{flop}\\), where flop is floating point operation.\n\n\n\nOne question we might have is how to assess if we’ve run enough iterations so that we’re drawing from the stationary distribution. We won’t ever truly reach the stationary distribution, because, as we can see above, we only ``reach” the stationary distribution asymptotically.\nHowever, we can use diagnostics to determine if there is any evidence that we haven’t converged. The most common heuristic for this is called \\(\\hat{R}\\), which was proposed in Gelman and Rubin (1992), and is thus called the Gelman-Rubin statistic, or R-hat.\nThe statistic relies on the fact that asymptotically, running the MCMC chain long enough from any starting distribution will yield samples from the stationary distribution. Thus, if we choose many well-dispersed starting points, and run our sampler for each starting point, the samples generated from each starting point should be indistinguishable from each other.\nTK: draw one example on the board of chains that would look stationary for a single chain, but are not if we have two\n\n\n\n\n\n\n\n\n\nOne way to assess differences between chains is to compare the within-chain variance for a parameter of interest, say \\(\\theta\\) to the combined variance of all the chains. Let \\(\\theta^{(sm)}\\) be the draw for \\(\\theta\\) from the \\(m^\\mathrm{th}\\) chain out of \\(M\\) chains at the \\(s^\\mathrm{th}\\) iteration. Let \\(\\bar{\\theta}^{(.m)} = \\frac{1}{S}\\sum_{s=1}^S \\theta^{(sm)}\\), \\(\\bar{\\theta}^{(..)} = \\frac{1}{M}\\sum_{m=1}^M \\bar{\\theta}^{(.m)}\\). Let \\(v_m = \\frac{1}{S-1}\\sum_{s=1}^S (\\theta^{(sm)} - \\bar{\\theta}^{(.m)})^2\\), and let \\(W = \\frac{1}{M}\\sum_{m=1}^M v_m\\). We also compute the between-chain variance, so \\(B = \\frac{S}{M-1}\\sum_{m=1}^M (\\bar{\\theta}^{(.m)} - \\bar{\\theta}^{(..)})^2\\)\nThen we estimate the variance of the posterior with the following consistent but biased estimator: \\[\n\\widehat{\\text{var}}(\\theta \\mid y) = \\frac{S-1}{S} W + \\frac{B}{S}\n\\] We can see that this is likely an oversetimate of the variance because as \\(S\\to\\infty\\), \\(v_m \\to \\text{var}(\\theta \\mid y)\\), and the first factor converges to the same, while the second factor goes to zero.\nThe term \\(W\\) underestimates the posterior variance for any finite \\(S\\) because each chain hasn’t explored the full extent of the tails of the distribution, and thus will have a smaller variance than the true variance.\nThe final expression for \\(\\hat{R}\\) is \\[\n\\hat{R} = \\sqrt{\\frac{\\widehat{\\text{var}}(\\theta \\mid y)}{W}}\n\\] The statistic is calculated so that each chain in the above calculation is the first or second half of a single chain. Thus, if we ran two MCMC chains, \\(M=4\\) above.\nThe reason for this is to catch scenarios where each chain is nonstationary, but the chains are marginally indistinguishable.\n\n\n\n\n\n\n\n\n\nThe version that Stan uses is outlined here: Vehtari et al. (2021). It uses an \\(\\hat{R}\\) that is computed using normlized ranks, which means we compute the rank \\(r^{(sm)}\\) of each draw of a parameter with respect to the pooled draws from all the chains. Then the ranks are trasnformed to z-scores using: \\[\nz^{(sm)} = \\Phi^{-1}\\lp\\frac{r^{(sm)} - 3/8}{S M + 1/4}\\rp\n\\] Then the \\(\\hat{R}\\) is computed using \\(z^{(sm)}\\) in place of \\(\\theta^{(sm)}\\) above. This has the effect of making the statistic useful in scenarios where there is an infinite mean (e.g. when we’re sampling from a Cauchy distribution, or something with similarly heavy tails)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/MCMC-lecture-1.html#gibbs-sampling",
    "href": "missing-data-material-W-26/notes/MCMC-lecture-1.html#gibbs-sampling",
    "title": "MCMC and diagnostics",
    "section": "",
    "text": "See Tanner and Wong (1987) for more details; what follows is an abbreviated presentation of their work, with bits of Gelfand and Smith (1990) thrown in.\nLet \\(p(\\theta_1, \\theta_2 \\mid y)\\) be the posterior of interest.\nWe can represent the marginal posterior for \\(\\theta_1\\) as: \\[\np(\\theta_1 \\mid y) = \\int_{\\Omega_{\\theta_2}} p(\\theta_1 \\mid y, \\theta_2) p(\\theta_2 \\mid y) d\\theta_2,\n\\] and similarly for \\(\\theta_2\\): \\[\np(\\theta_2 \\mid y) = \\int_{\\Omega_{\\theta_1}} p(\\theta_2 \\mid y, \\theta_1) p(\\theta_1 \\mid y) d\\theta_1\n\\]\nPlugging the second expression into the first yields:\n\\[\np(\\theta_1 \\mid y) = \\int_{\\Omega_{\\theta_2}} p(\\theta_1 \\mid y, \\theta_2) \\int_{\\Omega_{\\theta_1}} p(\\theta_2 \\mid y, \\theta_1^\\prime) p(\\theta_1^\\prime \\mid y) d\\theta^\\prime_1 d\\theta_2,\n\\]\nwhich we can rearrange into:\n\\[\np(\\theta_1 \\mid y) = \\int_{\\Omega_{\\theta_1}} \\int_{\\Omega_{\\theta_2}} p(\\theta_1 \\mid y, \\theta_2) p(\\theta_2 \\mid y, \\theta_1^\\prime) d\\theta_2 p(\\theta_1^\\prime \\mid y) d\\theta^\\prime_1,\n\\] Let \\(P(\\theta \\mid \\theta^prime)\\) be the proposal distribution, which conditions on \\(\\theta^\\prime\\): \\[\nP(\\theta \\mid \\theta^\\prime) = \\int_{\\Omega_{\\theta_2}} p(\\theta_1 \\mid y, \\theta_2) p(\\theta_2 \\mid y, \\theta_1^\\prime) d\\theta_2\n\\] Then the equation \\[\ng(\\theta_1) = \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  g(\\theta_1^\\prime) d\\theta^\\prime_1,\n\\] is an integral equation. Given weak conditions on \\(P(\\theta_1 \\mid \\theta^\\prime_1)\\), \\(p(\\theta_1 \\mid y)\\) is a unique solution for the equation.\nWe can define the integral transformation: \\(T g = \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  g(\\theta_1^\\prime) d\\theta^\\prime_1\\), which takes an \\(L_1\\) integrable function, \\(\\lVert g \\rVert_1 = \\int_{\\Omega_{\\theta_1}} \\abs{g(\\theta_1)} d\\theta_1 &lt; \\infty\\) and yields another \\(L_1\\) integrable function.\nOne thing to note is that \\(\\lVert T g \\rVert_1 = \\lVert g \\rVert_1\\), and, because \\(P(\\theta \\mid \\theta^\\prime)\\) is a probability density in \\(\\theta\\) for each \\(\\theta^\\prime\\), it integrates to \\(1\\), \\[\n\\begin{aligned}\n\\int_{\\Omega_{\\theta_1}} \\abs{(T g)(\\theta_1)} d\\theta_1 & = \\int_{\\Omega_{\\theta_1}} \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  g(\\theta_1^\\prime) d\\theta^\\prime_1 d\\theta_1 \\\\\n& = \\int_{\\Omega_{\\theta_1}} \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)d\\theta_1  g(\\theta_1^\\prime) d\\theta^\\prime_1  \\\\\n& = \\int_{\\Omega_{\\theta_1}} 1  g(\\theta_1^\\prime) d\\theta^\\prime_1  \\\\\n& = \\int_{\\Omega_{\\theta_1}} \\abs{g(\\theta_1^\\prime)} d\\theta^\\prime_1  \\\\\n& = \\lVert g \\rVert_1\n\\end{aligned}\n\\] Also note that if \\(f(\\theta) \\geq g(\\theta)\\) for all \\(\\theta\\), then \\(T f \\geq T g\\) for all \\(\\theta\\).\n\\[\n\\begin{aligned}\nT f - T g & =  \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  f(\\theta_1^\\prime) d\\theta^\\prime_1 - \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime)  g(\\theta_1^\\prime) d\\theta^\\prime_1 \\\\\n& = \\int_{\\Omega_{\\theta_1}} P(\\theta_1 \\mid \\theta_1^\\prime) (f(\\theta_1^\\prime) - g(\\theta_1^\\prime)) d\\theta^\\prime_1 \\\\\n& \\geq 0\n\\end{aligned}\n\\] With these two facts, we can show that given a function \\(g_i\\), \\(g_{i+1} = T g_i\\) and a solution, which we’ll call \\(g^\\star \\equiv p(\\theta_1 \\mid y)\\), which solves the integral equation, that \\(\\lVert g_{i+1} - g^\\star \\rVert \\leq  \\lVert g_{i} - g^\\star \\rVert\\).\n\\[\n\\begin{aligned}\n\\norm{g_{i+1} - g^\\star} & = \\norm{T (g_{i} - g^\\star)} \\\\\n& \\leq \\norm{T \\abs{g_{i} - g^\\star}} \\\\\n& = \\norm{g_{i} - g^\\star}\n\\end{aligned}\n\\] There are two more conditions that lead to \\(g^\\star\\) being the unique solution to the equation, and to another desirable characteristic for our computational schemes, which is called geometric ergodicity: \\[\n\\norm{g_{i+1} - g^\\star} \\leq \\alpha^{i}(g_0) \\norm{g_{0} - g^\\star}, \\, \\alpha \\in (0, 1)\n\\] as long as \\(\\sup_\\theta g_0(\\theta) / g^\\star(\\theta) &lt; \\infty\\).\nThis condition is the following: For every \\(\\theta_0 \\in \\Omega_{\\theta}\\) there is an open neighborhood \\(U\\) of \\(\\theta_0\\) so that: \\[\nP(\\theta \\mid \\theta^\\prime) &gt; 0, \\forall (\\theta, \\theta^\\prime) \\in U\n\\]\n\\[\nP(\\theta \\mid \\theta^\\prime) \\leq M &lt; \\infty \\,\\, \\forall (\\theta, \\theta^\\prime) \\in \\Omega_\\theta\n\\]\nWe can use this fact to show that the scheme will converge to \\(p(\\theta_1 \\mid y)\\), the unique posterior for \\(\\theta_1\\), and the stationary distribution for the proposal \\(P(\\theta \\mid \\theta^\\prime\\). I won’t go over the proof, but you can see it in section 6 of Tanner and Wong (1987).\nThis suggests the following algorithm, which we’ll call Gibbs sampling, to generate draws from an unknown posterior:\n\nDetermine starting values \\(\\theta_1^0, \\theta_2^0\\)\nFor \\(t=1, \\dots, S\\)\n\nDraw \\(\\theta_1^{t+1} \\sim p(\\theta_1 \\mid \\theta_2^t, y)\\)\nDraw \\(\\theta_2^{t+1} \\sim p(\\theta_2 \\mid \\theta_1^{t+1}, y)\\)\n\nDiscard \\(S/2\\) iterations, and keep the set of draws \\(\\{ (\\theta_1^s, \\theta_2^s) \\mid s = S/2+1, ,\\dots, S\\}\\).\n\nThe final set of draws \\(S/2\\) draws are approximately distributed according to \\(p(\\theta_1, \\theta_2 \\mid y)\\)\nThe assumptions that make the algorithm work are key. Tanner and Wong (1987) note that \\(\\alpha\\) is dependent on the starting distribution, and \\(\\alpha\\) can be arbitrarily close to \\(1\\) for unbounded parameter spaces.\nThe condition on the starting distribution mean that a \\(g_0\\) with compact support might be a good choice, because we avoid a situation where \\(\\sup_\\theta g_0(\\theta) / g^\\star(\\theta)\\) is large because \\(g_0\\) has tails that are heavier than those of \\(g^\\star(\\theta)\\) as \\(\\norm{\\theta} \\to \\infty\\).\nThe problem with Gibbs is that it often moves slowly in high dimensions when there is high correlation between components.\n\n\nGiven a set of draws \\(\\{\\theta_s, s = 1, \\dots, S\\}\\) from a Gibbs sampler, we want a way to assess how well our empirical average: \\[\n\\bar{f} = \\frac{1}{S} \\sum_{s = 1}^S f(\\theta_s)\n\\] approximates the true expectation: \\[\n\\Exp{f(\\theta)}{p(\\theta \\mid y)}\n\\] Under fairly general conditions, \\[\n\\sqrt{n}(\\bar{f} - \\Exp{f}) \\overset{d}{\\to} N(0, \\sigma^2)\n\\] The wrinkle is that \\(\\sigma^2\\) wont equal \\(\\text{Var}_{p(\\theta \\mid y)}(f(\\theta))\\) because our draws aren’t independent. In fact, as shown in Geyer (2005), we can compute the variance of the left-hand side: \\[\n\\begin{aligned}\n\\text{Var}(\\sqrt{n}(\\bar{f} - \\Exp{f})) & = n \\text{Var}(\\bar{f}) \\\\\n& = \\frac{1}{n}\\sum_{s=1}^n \\text{Var}(f(\\theta_s)) + \\frac{2}{n} \\sum_{i=1}^{n-1} \\sum_{j={i+1}}^{n} \\text{Cov}(f(\\theta_i),f(\\theta_j))\n\\end{aligned}\n\\] We’ll assume we have a stationary chain, so \\(\\text{Var}(f(\\theta_s)) = \\gamma_0\\), and \\(\\gamma_k = \\text{Cov}(f(\\theta_{i}),f(\\theta_{i+k}))\\), which leads to\n\\[\n\\begin{aligned}\n\\frac{1}{n}\\sum_{s=1}^n \\text{Var}(f(\\theta_s)) + \\frac{2}{n} \\sum_{i=1}^{n-1} \\sum_{j={i+1}}^{n} \\text{Cov}(f(\\theta_i),f(\\theta_j)) & = \\gamma_0 + \\frac{2}{n} \\sum_{i=1}^{n-1} \\sum_{j={i+1}}^{n} \\gamma_{j - i} \\\\\n& = \\gamma_0 + \\frac{2}{n} \\sum_{i=1}^{n-1} (n - i) \\gamma_i\n\\end{aligned}\n\\] If things are well-behaved, the series on the right converges to: \\[\n\\begin{aligned}\n\\sigma^2 = \\gamma_0 + 2 \\sum_{i=1}^{\\infty} \\gamma_i\n\\end{aligned}\n\\] Thus, \\[\n\\sqrt{n}(\\bar{f} - \\Exp{f}) \\overset{d}{\\to} N(0, \\gamma_0 + 2 \\sum_{i=1}^{\\infty} \\gamma_i)\n\\] If we had independent draws of \\(\\theta_i\\), we would instead have \\[\n\\sqrt{n}(\\bar{f} - \\Exp{f}) \\overset{d}{\\to} N(0, \\gamma_0)\n\\] The ratio of these variances: \\[\n\\frac{\\gamma_0}{\\gamma_0 + 2 \\sum_{i=1}^{\\infty} \\gamma_i} = \\frac{1}{1 + 2 \\sum_{i=1}^{\\infty} \\frac{\\gamma_i}{\\gamma_0}}\n\\] Can be used to compute something called the effective sample size from an MCMC sample of \\(S\\) draws: \\[\nn_\\text{eff} = \\frac{n}{1 + 2 \\sum_{i=1}^{\\infty} \\rho_k}\n\\] where \\(\\rho_k\\) is the autocorrelation at the \\(k^\\text{th}\\) lag. The effective sample size represents the number of independent draws with an equivalent variance to the samples from an MCMC chain.\nThese figures can be used to benchmark MCMC algorithms by computing \\(n_\\text{eff}/\\text{sec}\\) of computing time, or \\(n_\\text{eff}/\\text{flop}\\), where flop is floating point operation.\n\n\n\nOne question we might have is how to assess if we’ve run enough iterations so that we’re drawing from the stationary distribution. We won’t ever truly reach the stationary distribution, because, as we can see above, we only ``reach” the stationary distribution asymptotically.\nHowever, we can use diagnostics to determine if there is any evidence that we haven’t converged. The most common heuristic for this is called \\(\\hat{R}\\), which was proposed in Gelman and Rubin (1992), and is thus called the Gelman-Rubin statistic, or R-hat.\nThe statistic relies on the fact that asymptotically, running the MCMC chain long enough from any starting distribution will yield samples from the stationary distribution. Thus, if we choose many well-dispersed starting points, and run our sampler for each starting point, the samples generated from each starting point should be indistinguishable from each other.\nTK: draw one example on the board of chains that would look stationary for a single chain, but are not if we have two\n\n\n\n\n\n\n\n\n\nOne way to assess differences between chains is to compare the within-chain variance for a parameter of interest, say \\(\\theta\\) to the combined variance of all the chains. Let \\(\\theta^{(sm)}\\) be the draw for \\(\\theta\\) from the \\(m^\\mathrm{th}\\) chain out of \\(M\\) chains at the \\(s^\\mathrm{th}\\) iteration. Let \\(\\bar{\\theta}^{(.m)} = \\frac{1}{S}\\sum_{s=1}^S \\theta^{(sm)}\\), \\(\\bar{\\theta}^{(..)} = \\frac{1}{M}\\sum_{m=1}^M \\bar{\\theta}^{(.m)}\\). Let \\(v_m = \\frac{1}{S-1}\\sum_{s=1}^S (\\theta^{(sm)} - \\bar{\\theta}^{(.m)})^2\\), and let \\(W = \\frac{1}{M}\\sum_{m=1}^M v_m\\). We also compute the between-chain variance, so \\(B = \\frac{S}{M-1}\\sum_{m=1}^M (\\bar{\\theta}^{(.m)} - \\bar{\\theta}^{(..)})^2\\)\nThen we estimate the variance of the posterior with the following consistent but biased estimator: \\[\n\\widehat{\\text{var}}(\\theta \\mid y) = \\frac{S-1}{S} W + \\frac{B}{S}\n\\] We can see that this is likely an oversetimate of the variance because as \\(S\\to\\infty\\), \\(v_m \\to \\text{var}(\\theta \\mid y)\\), and the first factor converges to the same, while the second factor goes to zero.\nThe term \\(W\\) underestimates the posterior variance for any finite \\(S\\) because each chain hasn’t explored the full extent of the tails of the distribution, and thus will have a smaller variance than the true variance.\nThe final expression for \\(\\hat{R}\\) is \\[\n\\hat{R} = \\sqrt{\\frac{\\widehat{\\text{var}}(\\theta \\mid y)}{W}}\n\\] The statistic is calculated so that each chain in the above calculation is the first or second half of a single chain. Thus, if we ran two MCMC chains, \\(M=4\\) above.\nThe reason for this is to catch scenarios where each chain is nonstationary, but the chains are marginally indistinguishable.\n\n\n\n\n\n\n\n\n\nThe version that Stan uses is outlined here: Vehtari et al. (2021). It uses an \\(\\hat{R}\\) that is computed using normlized ranks, which means we compute the rank \\(r^{(sm)}\\) of each draw of a parameter with respect to the pooled draws from all the chains. Then the ranks are trasnformed to z-scores using: \\[\nz^{(sm)} = \\Phi^{-1}\\lp\\frac{r^{(sm)} - 3/8}{S M + 1/4}\\rp\n\\] Then the \\(\\hat{R}\\) is computed using \\(z^{(sm)}\\) in place of \\(\\theta^{(sm)}\\) above. This has the effect of making the statistic useful in scenarios where there is an infinite mean (e.g. when we’re sampling from a Cauchy distribution, or something with similarly heavy tails)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-9-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-9-notes.html",
    "title": "Missing data lecture 9: MAR vs. MAAR (again) and Data Coarsening",
    "section": "",
    "text": "Let there be \\(n\\) units, of which we’re interested in measuring \\(K\\) variables. Let the \\(n \\times K\\) matrix of observations be denoted \\(Y\\), with elements \\(y_{ij}\\) while a realization of this matrix is called \\(\\tilde{y}\\), with elements \\(\\tilde{y}_{ij}\\). Let the matrix of missingness indicators be denoted \\(M\\), elements \\(m_{ij}\\), with a particular realization \\(\\tilde{m}\\), with elements \\(\\tilde{m}_{ij}\\). Let \\(Y_{(0)} = \\{y_{ij} \\mid m_{ij} = 0, i = 1, \\dots, n, j = 1, \\dots, K \\}\\). Let \\(Y_{(1)} = \\{y_{ij} \\mid m_{ij} = 1, i = 1, \\dots, n, j = 1, \\dots, K \\}\\). Let \\(\\mathcal{Y}_{(1)}\\) be the sample space of the missing values. Let a realization of these sets of variables be \\(\\tilde{y}_{(0)}\\) and \\(\\tilde{y}_{(1)}\\).\nThe joint likelihood of the observed data and the missingness indicators is:\n\\[\nL_\\text{full}(\\theta, \\phi \\mid \\tilde{y}_{(0)}, \\tilde{m}) = \\int_{\\mathcal{Y}_{(1)}} f_Y(\\tilde{y}_{(0)}, y_{(1)} \\mid \\theta) P(M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}, \\phi) dy_{(1)}\n\\]\nThe definition of MAR from the book is as follows:\n\nDefinition 1 (Missing-at-random) \\[\nf_{M \\mid Y} (M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}, \\phi) = f_{M \\mid Y} (M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}^*, \\phi)\n\\] for all \\(y_{(1)}, y_{(1)}^*, \\phi\\) .\n\nThe definition of MAAR is:\n\nDefinition 2 (Missing-always-at-random) \\[\nf_{M \\mid Y} (M = m \\mid Y_{(0)} = y_{(0)}, Y_{(1)} = y_{(1)}, \\phi) = f_{M \\mid Y} (M = m \\mid Y_{(0)} = y_{(0)}, Y_{(1)} = y_{(1)}^*, \\phi)\n\\] for all \\(m, y_{(0)},y_{(1)}, y_{(1)}^*, \\phi\\).\n\n\nExample 1 (Example MAR vs. MAAR) The example given by Little (2021) is the following:\nSuppose we have observations \\((y_i, x_i)\\), where \\(y_i\\) is potentially missing and \\(x_i\\) is either \\(1\\) or \\(2\\), denoting group membership. Let \\(\\theta = (\\mu_1, \\mu_2, \\sigma^2)\\), and the model for the observations be \\[\ny_i \\mid x_i, \\theta \\sim \\text{Normal}(\\mu_{x_i}, \\sigma^2)\n\\] The standard confidence interval for the difference in means is: \\[\n\\bar{y}_2 - \\bar{y}_1 \\pm t_{\\nu, 0.975}(s\\sqrt{1/n_1 + 1/n2})\n\\] This also corresponds to the Bayesian credible interval when using a flat prior on \\(\\mu_1, \\mu_2\\) and \\(\\log \\sigma^2\\). Suppose the missingness mechanism is as follows: \\[\nP(m_i = 1 \\mid x_i, y_i, \\phi) =\n\\begin{cases}\n0 & x_i = 1\\\\\n0 & x_i = 2 \\text{ and } y_i \\leq \\phi\\\\\n1 & x_i = 2 \\text{ and } y_i &gt; \\phi\n\\end{cases}\n\\] That is, for group 2, if the observation is above an unknown cutoff value, the value is not recorded.\nSuppose that we have a dataset where there are no missing values. Then the data is MAR but not MAAR, because in repeated hypothetical samples there would be missing values that are MNAR. The Bayesian credible interval is still valid under MAR because we’re conditioning on the dataset we have, whereas the Frequentist interval isn’t valid because it couldn’t be repeated for datasets where \\(y_i\\) is missing for some group \\(2\\) observations."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-9-notes.html#clarifying-mar-vs.-unit-mar",
    "href": "missing-data-material-W-26/notes/lecture-9-notes.html#clarifying-mar-vs.-unit-mar",
    "title": "Missing data lecture 9: MAR vs. MAAR (again) and Data Coarsening",
    "section": "",
    "text": "Let there be \\(n\\) units, of which we’re interested in measuring \\(K\\) variables. Let the \\(n \\times K\\) matrix of observations be denoted \\(Y\\), with elements \\(y_{ij}\\) while a realization of this matrix is called \\(\\tilde{y}\\), with elements \\(\\tilde{y}_{ij}\\). Let the matrix of missingness indicators be denoted \\(M\\), elements \\(m_{ij}\\), with a particular realization \\(\\tilde{m}\\), with elements \\(\\tilde{m}_{ij}\\). Let \\(Y_{(0)} = \\{y_{ij} \\mid m_{ij} = 0, i = 1, \\dots, n, j = 1, \\dots, K \\}\\). Let \\(Y_{(1)} = \\{y_{ij} \\mid m_{ij} = 1, i = 1, \\dots, n, j = 1, \\dots, K \\}\\). Let \\(\\mathcal{Y}_{(1)}\\) be the sample space of the missing values. Let a realization of these sets of variables be \\(\\tilde{y}_{(0)}\\) and \\(\\tilde{y}_{(1)}\\).\nThe joint likelihood of the observed data and the missingness indicators is:\n\\[\nL_\\text{full}(\\theta, \\phi \\mid \\tilde{y}_{(0)}, \\tilde{m}) = \\int_{\\mathcal{Y}_{(1)}} f_Y(\\tilde{y}_{(0)}, y_{(1)} \\mid \\theta) P(M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}, \\phi) dy_{(1)}\n\\]\nThe definition of MAR from the book is as follows:\n\\[\nf_{M \\mid Y} (\\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}, \\phi) = f_{M \\mid Y} (\\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}^*, \\phi)\n\\] for all \\(y_{(1)}, y_{(1)}^*, \\phi\\).\nThe definition of MAAR is:\n\\[\nf_{M \\mid Y} (m \\mid Y_{(0)} = y_{(0)}, Y_{(1)} = y_{(1)}, \\phi) = f_{M \\mid Y} (m \\mid Y_{(0)} = y_{(0)}, Y_{(1)} = y_{(1)}^*, \\phi)\n\\] for all \\(m, y_{(0)},y_{(1)}, y_{(1)}^*, \\phi\\).\n\n\nWhen we have an assumption that observations and missingness for units can be considered conditionally independent given parameters \\(\\theta\\) and \\(\\phi\\), we get unit MAR instead of MAR.\nLet \\(Y_i\\) be the \\(i^\\mathrm{th}\\) row of the matrix \\(Y\\), or the length \\(K\\) random vector representing observations for unit \\(i\\), while \\(\\tilde{y}_i\\) is a particular realization of this random vector and \\(y_i\\) is a dummy vector. Similarly let \\(M_i\\) be the \\(i^\\mathrm{th}\\) row of the matrix \\(M\\), with particular realization \\(\\tilde{m}_i\\) and \\(m_i\\) a dummy vector. Furthermore, let \\(Y_{i(0)}, Y_{i(1)}\\) be the observed and missing random vectors of \\(y_i\\), while \\(\\tilde{y}_{i(0)}, \\tilde{y}_{i(1)}\\) are realizations of these vectors.\nThen the joint distribution of observations and missingness is \\[\nf_{Y, M}(y, m \\mid \\theta, \\phi) = \\prod_{i=1}^n f_{Y_i}(y_i \\mid \\theta) f_{M_i \\mid Y_i}(m_i \\mid y_i, \\theta)\n\\] For unit MAR, the condition becomes:\n\\[\nf_{M_i \\mid Y_i}(\\tilde{m}_i \\mid Y_{i(0)} = \\tilde{y}_{i(0)}, Y_{i(1)} = y_{i(1)}, \\phi) =\nf_{M_i \\mid Y_i}(\\tilde{m}_i \\mid Y_{i(0)} = \\tilde{y}_{i(0)}, Y_{i(1)} = y_{i(1)}^\\star, \\phi)\n\\] for all \\(y_{i(1)}, y_{i(1)}^\\star, \\phi\\) for all \\(i\\).\n\n\n\nThe example given by Little (2021) is the following:\nSuppose we have observations \\((y_i, x_i)\\), where \\(y_i\\) is potentially missing and \\(x_i\\) is either \\(1\\) or \\(2\\), denoting group membership. Let \\(\\theta = (\\mu_1, \\mu_2, \\sigma^2)\\), and the model for the observations be \\[\ny_i \\mid x_i, \\theta \\sim \\text{Normal}(\\mu_{x_i}, \\sigma^2)\n\\] The standard confidence interval for the difference in means is: \\[\n\\bar{y}_2 - \\bar{y}_1 \\pm t_{\\nu, 0.975}(s\\sqrt{1/n_1 + 1/n2})\n\\] This also corresponds to the Bayesian credible interval when using a flat prior on \\(\\mu_1, \\mu_2\\) and \\(\\log \\sigma^2\\). Suppose the missingness mechanism is as follows: \\[\nP(m_i = 1 \\mid x_i, y_i, \\phi) =\n\\begin{cases}\n0 & x_i = 1\\\\\n0 & x_i = 2 \\text{ and } y_i \\leq \\phi\\\\\n1 & x_i = 2 \\text{ and } y_i &gt; \\phi\n\\end{cases}\n\\] That is, for group 2, if the observation is above an unknown cutoff value, the value is not recorded.\nSuppose that we have a dataset where there are no missing values. Then the data is MAR but not MAAR, because in repeated hypothetical samples there would be missing values that are MNAR. The Bayesian credible interval is still valid under MAR because we’re conditioning on the dataset we have, whereas the Frequentist interval isn’t valid because it couldn’t be repeated for datasets where \\(y_i\\) is missing for some group \\(2\\) observations."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-9-notes.html#coarsened-data",
    "href": "missing-data-material-W-26/notes/lecture-9-notes.html#coarsened-data",
    "title": "Missing data lecture 9: MAR vs. MAAR (again) and Data Coarsening",
    "section": "Coarsened data",
    "text": "Coarsened data\nCoarsened data is a generalization of missing data that includes other ways in which the resolution of data can be reduced. Examples include censoring, grouping, rounding, or heaping. Heaping is the phenomenon where there are varying levels of resolution reported in the same dataset. For example, on a questionnaire that asks for the the number of cigarettes smoked per day, some people will report exact numbers, and others will report multiples of packs. With rounded data, the coarsening is more deterministic, namely we know that an observation is exactly within the interval, say between \\([\\text{floor}(y), \\text{floor}(y)+1]\\) With coarsened data, there is still the complete data matrix \\(y = (y_{ij})\\), but there is now a coarsening variable \\(c_{ij}\\) that interacts with the true value to return the observed data.\nLet \\(w_{ij}\\) be the observed data, and let \\(W(y_{ij}, c_{ij})\\) be the function of the true value and the coarsening variable that returns some subset of \\(\\mathcal{Y}_{ij}\\) to which \\(y_{ij}\\) belongs. Thus \\(w_{ij} = W(y_{ij}, c_{ij})\\) with the requirement that \\(y_{ij} \\in W(y_{ij}, c_{ij})\\). Let \\(g_{ij}\\) be the observed coarsening random variable that is governed by a function \\(G(y_{ij}, c_{ij})\\) such that \\(c_{ij} \\in G(y_{ij}, c_{ij})\\). Just as \\(w\\) is a coarsened version of \\(y\\), \\(g\\) is a coarsened version of \\(c\\).\nThe simplest nontrivial example is the censored exponential data from above, though we will modify the scenario so that each individual has a potentially different censoring time \\(c_i\\). Let \\(y_i\\) be the true time to failure, while \\(c_i\\) is the censoring time.\n\\[\nw_{i} = W(y_i, c_i) =\n\\begin{cases}\ny_i & y_i \\leq c_i \\\\\n(c_i, \\infty) & y_i &gt; c_i\n\\end{cases}\n\\] \\[\ng_{i} = G(y_i, c_i) =\n\\begin{cases}\n(y_i, \\infty) & y_i \\leq c_i \\\\\nc_i & y_i &gt; c_i\n\\end{cases}\n\\]  Let the realization of \\(g\\) and \\(w\\) be \\(\\tilde{g}\\) and \\(\\tilde{w}\\), with elements \\(\\tilde{g}_i\\) and \\(\\tilde{w}_i\\). Furthermore, let the distribution of interest for \\(y_i\\) be \\(f_Y(y_i \\mid \\theta)\\), while we let the coarsening distributuion be \\(f_{C \\mid Y}(c_i \\mid y_i, \\phi)\\). Then we can write: \\[\nL_{\\text{full}}(\\theta, \\phi \\mid \\tilde{g}, \\tilde{w}) = \\int \\int f_{C \\mid Y}(c \\mid y, \\phi) f_Y(y \\mid \\theta) \\ind{y \\in \\tilde{w}} \\ind{c \\in \\tilde{g}}dy\\, dc\n\\]\nAnother way to write this is by simplifying after the integration:\nLet the vector \\(c_{(0)} = \\{c_i \\mid g_i = c_i, i = 1, \\dots, n\\}\\) and let \\(c_{(1)} = \\{c_i \\mid g_i \\neq c_i, i = 1, \\dots, n\\}\\) Let \\(c = (c_{(0)}, c_{(1)})\\) be the vector of coarsening values. Let \\(y_{(0)}\\) be the set of values that we observe exactly, and \\(y_{(1)}\\) be the set of values that are censored. Let \\(\\tilde{w}_{(1)}\\) be the set of subsets corresponding to the coarsened \\(y\\)’s and the same for \\(\\tilde{g}_{(1)}\\).\nThen the integral can be rewritten in terms of these variables:\n\\[\n\\small\nL_{\\text{full}}(\\theta, \\phi \\mid \\tilde{c}_{(0)}, \\tilde{y}_{(0)}, \\tilde{g}_{(1)}, \\tilde{w}_{(1)}) = \\int \\int f_{C \\mid Y}(\\tilde{c}_{(0)}, c_{(1)} \\mid \\tilde{y}_{(0)}, y_{(1)},\\phi) f_Y(\\tilde{y}_{(0)}, y_{(1)} \\mid \\theta) \\ind{y_{(1)} \\in \\tilde{w}} \\ind{c_{(1)} \\in \\tilde{g}}dy_{(1)}\\, dc_{(1)}\n\\] The likelihood that ignores the coarsening process is:\n\\[\n\\small\nL_{\\text{ign}}(\\theta \\mid \\tilde{y}_{(0)}, \\tilde{w}_{(1)}) = \\int f_Y(\\tilde{y}_{(0)}, y_{(1)} \\mid \\theta) \\ind{y_{(1)} \\in \\tilde{w}_i} dy_{(1)}\n\\]\nThis leads to a definition of coarsening at random, or CAR, that relates to conditions on the coarsening distribution:\n\\[\nf_{C \\mid Y}(\\tilde{c}_{(0)}, c_{(1)} \\mid \\tilde{y}_{(0)}, y_{(1)}, \\phi) = f_{C \\mid Y}(\\tilde{c}_{(0)}, c_{(1)}^\\star \\mid \\tilde{y}_{(0)}, y_{(1)}^\\star, \\phi)\n\\] For all \\(c_{(1)}, c_{(1)}^\\star, y_{(1)}, y_{(1)}^\\star, \\phi\\).\nEach of these definitions has a unit-level variant, as MAR did above:\nIn the failure time example we have two contributions to the likelihood: \\[\nL_{\\text{full}}(\\theta, \\phi \\mid y_{(0)}, c_{(0)}) = \\prod_{i \\mid y_i \\leq c_i}  f(y_i \\mid \\theta) \\int_{y_i}^\\infty f_{C \\mid Y}(c_i \\mid y_i, \\phi) dc \\times \\prod_{i \\mid y_i &gt; c_i}  \\int_{c_i}^\\infty f_{C \\mid Y}(c_i \\mid y, \\phi)f(y \\mid \\theta) dy\n\\] If we have that \\(f(c_i \\mid y_i, \\phi) = f(c_i \\mid \\phi)\\) for all \\(i\\), and that \\(\\phi\\) and \\(\\theta\\) are variationally independent, we can write the likelihood as the product of \\(L_{\\text{ign}}(\\theta \\mid y_{(0)}, c_{(0)})\\) and \\(L_{\\text{rest}}(\\phi \\mid y_{(0)}, c_{(0)})\\) \\[\n\\prod_{i \\mid y_i \\leq c_i} f(y_i \\mid \\theta) \\prod_{i \\mid y_i &gt; c_i} \\int_{c_i}^\\infty f(y \\mid \\theta) dy \\times \\prod_{i \\mid y_i \\leq c_i} \\int_{y_i}^\\infty f_{C}(c_i \\mid \\phi) dc \\prod_{i \\mid y_i &gt; c_i} f(c_i \\mid \\phi)\n\\] In this case, the censoring mechanism is CAR, but not MAR, as we saw earlier.\nIn the cigarette smoking example, let \\(y_i\\) be the true number of cigarettes smoked per day, and let \\(c_i\\) be an indicator for the precision of reporting. Then define \\(w_{i}\\) to be: \\[\nw_{i} =\n\\begin{cases}\n[\\text{floor}(y_i), \\text{floor}(y_i)+1] & c_i = 0 \\\\\n[20 \\times \\text{floor}(y_i/20), 20  \\times\\text{floor}(y_i/20)+20] & c_i = 1\n\\end{cases}\n\\] This assumes that people round down the number of cigarettes they smoke, rather than rounding to the nearest integer, like you’d do if there weren’t a stigma around smoking.\nLet the lower bound of the interval \\(w_i\\) be \\(w_{iL}\\), then \\(g_i\\) be defined as: \\[\ng_i =\n\\begin{cases}\nc_i & w_{iL} \\bmod 20 \\neq 0 \\\\\n\\{0,1\\} & w_{iL} \\bmod 20 = 0\n\\end{cases}\n\\]\nSuppose that \\(f_{C \\mid Y}(c_i \\mid y_i, \\phi) = \\Phi(\\phi_1 + \\phi_2 y_i)^{c_i} (1 - \\Phi(\\phi_1 + \\phi_2 y_i)^{1 - c_i})\\).\nThen this example isn’t CAR, because the coarsening is dependent on the number of cigarettes smoked.\nWhat does the likelihood look like?\n\\[\n\\small\n\\begin{aligned}\nL(\\theta, \\phi \\mid \\tilde{w}, \\tilde{g}) & = \\prod_{i \\mid w_{iL} \\bmod 20 \\neq 0} \\int_{w_{iL}}^{w_{iL} + 1} f_Y(y \\mid \\theta) (1 - \\Phi(\\phi_1 + \\phi_2 y)) dy \\\\\n& \\times \\prod_{i \\mid w_i \\bmod 20 = 0} \\int_{w_{iL}}^{w_{iL} + 20} (f_Y(y \\mid \\theta)\\Phi(\\phi_1 + \\phi_2 y)) dy + \\int_{w_{iL}}^{w_{iL} + 1}(f_Y(y \\mid \\theta)(1 - \\Phi(\\phi_1 + \\phi_2 y)) dy\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/MCMC-lecture-2.html",
    "href": "missing-data-material-W-26/notes/MCMC-lecture-2.html",
    "title": "Metropolis and Hamiltonian Monte Carlo",
    "section": "",
    "text": "Suppose we want to sample from a distribution \\(\\pi(\\theta)\\) (for the rest of the lecture I’ll suppress the dependence on \\(y\\) unless otherwise noted), but we can’t easily do so, we might be able to create a Markov Chain whose stationary distribution is \\(\\pi(\\theta)\\).\nThe Markov Chain has the property that \\[\nP(\\theta^{(n)} \\in A \\mid \\theta^{(n-1)} = c_{n-1}, \\dots, \\theta^{(n-1)} = c_{1}) = P(\\theta^{(n)} \\in A \\mid \\theta^{(n-1)} = c_{n-1})\n\\] and that \\[\nP(\\theta^{(n)} \\in A \\mid \\theta^{(n-1)} = c)\n\\] doesn’t depend on \\(n\\).\nThis transition function has the property that for any value \\(c\\) of \\(\\theta^{(n-1)}\\), the function \\(P(\\theta^{n} \\in A \\mid \\theta^{(n-1)} = c)\\) is a probability measure over whatever space \\(A\\) is in, and for any fixed \\(A\\), \\(P(\\theta^{n} \\in A \\mid \\theta^{(n-1)} = x)\\) is a measurable function of \\(x\\).\nIn finite spaces, the transition function is just a matrix with \\((i,j)^\\mathrm{th}\\) entry\nWhat the proof from Tanner and Wong (1987) shows is that we can create a Markov Chain with the stationary distribution \\(p(\\theta)\\) if we have a transition function \\(P(\\theta \\mid \\theta^\\prime)\\) with the following properties:\n\n\\(\\pi(\\theta) = \\int_{\\theta^\\prime} P(\\theta \\mid \\theta^\\prime) \\pi(\\theta^\\prime) d\\theta^\\prime\\)\n\\(P(\\theta \\mid \\theta^\\prime) \\leq M &lt; \\infty\\) for all \\(\\theta, \\theta^\\prime\\).\nFor every \\(\\theta_0 \\in \\Omega_{\\theta}\\) there is an open neighborhood \\(U\\) of \\(\\theta_0\\) so that: \\[\nP(\\theta \\mid \\theta^\\prime) &gt; 0, \\forall (\\theta, \\theta^\\prime) \\in U\n\\]\n\nand an initial distribution \\(g(\\theta)\\) that satisfies:\n\\[\n\\sup_\\theta g(\\theta) / \\pi(\\theta) &lt; \\infty\n\\] One way of showing condition 1 is by showing that a chain is reversible, namely that for two sets \\(A\\) and \\(B\\): \\[\n\\int_{A} \\pi(\\theta^\\prime) \\int_B p(\\theta \\mid \\theta^\\prime) d\\theta\\, d\\theta^\\prime =  \\int_{B} \\pi(\\theta^\\prime) \\int_A p(\\theta \\mid \\theta^\\prime) d\\theta\\, d\\theta^\\prime\n\\] We can represent \\(\\int_B p(\\theta \\mid \\theta^\\prime) d\\theta\\) as \\(P(B \\mid \\theta^\\prime)\\) When \\(A\\) is the whole parameter space, \\(\\Omega_\\theta\\) this says something more interpretable: \\[\n\\int_{\\Omega_\\theta} \\pi(\\theta^\\prime) \\int_B p(\\theta \\mid \\theta^\\prime) d\\theta\\, d\\theta^\\prime =  \\int_{B} \\pi(\\theta^\\prime) d\\theta^\\prime\n\\] This says: If I draw a value from the stationary distribution, and then draw a value from the transition function, my probability of landing in the set \\(B\\) is the same as if I had just measured whether the first draw was in set \\(B\\).\n\n\nThis is all from Geyer’s notes on MCMC, Geyer (2005).\nOne way to construct a transition function that has this behavior is by using the Metropolis algorithm.\n\nSample a new point \\(\\theta^{(2)} \\mid \\theta^{(1)}\\) with a proposal distribution that we can draw from, \\(J(\\theta^{(2)} \\mid \\theta^{(1)})\\), such that \\(J(\\theta^{(1)} \\mid \\theta^{(2)})\\) = \\(J(\\theta^{(2)} \\mid \\theta^{(1)})\\).\nAccept the new point with probability \\(\\min(r,1)\\): \\[\nr = \\frac{\\pi(\\theta^{(2)})}{\\pi(\\theta^{(1)})}\n\\] else set the next step of the Markov Chain to \\(\\theta^{(1)}\\).\n\nCrucially, this acceptance probability, which is called the Metropolis acceptance rate, can be computed without regards to the normalizing constant of the probability density.\nWe can see this easily because it’s a ratio of the same density evaluated at two different parameters. Let \\(\\pi(\\theta) = p(\\theta \\mid y)\\) where \\(p(\\theta)\\) is the prior for \\(\\theta\\), \\(f_Y(y \\mid \\theta)\\) is the density of the observations, and \\(p(y) = \\int_\\theta p(\\theta) f_Y(y \\mid \\theta) d \\theta\\):\n\\[\n\\begin{aligned}\nr & = \\frac{p(\\theta^{(2)} \\mid y)}{p(\\theta^{(1)} \\mid y)} \\\\\n& = \\frac{p(\\theta^{(2)}, y)/p(y)}{p(\\theta^{(1)}, y)/p(y)} \\\\\n& = \\frac{p(\\theta^{(2)}, y)}{p(\\theta^{(1)}, y)} \\\\\n& = \\frac{p(\\theta^{(2)}) f_Y(y \\mid \\theta^{(2)})}{p(\\theta^{(1)}) f_Y(y \\mid \\theta^{(1)})}\n\\end{aligned}\n\\] This is helpful, because we don’t know the normalizing constant \\(p(y)\\) for most models we’re interested in fitting.\nWhat does the Metropolis algorithm imply for the transition density?\nWe need to compute the conditional measure \\(P(A \\mid \\theta^{(1)})\\), which gives the probability of landing in set \\(A\\), or of drawing a value \\(\\theta^{(2)}\\) that is in set \\(A\\) from the algorithm above.\nThere are two ways we can get to set \\(A\\). The first way is if the proposed point \\(\\theta^{(2)}\\) is in set \\(A\\) and the draw is accepted. The other way is if \\(\\theta^{(1)}\\) is in set \\(A\\) and we reject the proposal from \\(\\theta^{(1)} \\to \\theta^{(2)}\\).\nThe probability of acceptance for a single point \\(\\theta^{(2)}\\) given we started at \\(\\theta^{(1)}\\) is \\(h(\\theta^{(1)}, \\theta^{(2)}) = \\min(r(\\theta^{(1)},\\theta^{(2)}),1)\\). The probability that we transition from \\(\\theta^{(1)}\\to \\theta^{(2)}\\) is given by \\[\n\\int_A J(\\theta \\mid \\theta^{(1)}) h(\\theta^{(1)}, \\theta) d\\theta\n\\] The probability we accept any jump is the integral over the whole space, \\(\\Omega_\\theta\\): \\[\na(\\theta^{(1)}) = \\int_{\\Omega_\\theta} J(\\theta \\mid \\theta^{(1)}) h(\\theta^{(1)}, \\theta) d\\theta\n\\] Then the probability that we reject the proposal at \\(\\theta^{(1)}\\) is \\(1 - a(\\theta^{(1)})\\). The total probability of landing in set \\(A\\) if we reject the draw is \\(1\\) if \\(\\theta^{(1)} \\in A\\), or 0 if it isn’t in \\(A\\), which we can represent as \\(\\ind{\\theta^{(1)} \\in A}\\).\nThis means that \\[\nP(A \\mid \\theta^{(1)}) = (1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in A} + \\int_A h(\\theta^{(1)}, \\theta) J(\\theta \\mid \\theta^{(1)}) d\\theta\n\\] Now we need to show that this is a reversible transition, namely:\n\\[\n\\int_B \\pi(\\theta^{(1)}) P(A \\mid \\theta^{(1)}) d\\theta^{(1)}  = \\int_A \\pi(\\theta^{(1)}) P(B \\mid \\theta^{(1)}) d\\theta^{(1)}\n\\] crucially, for a density \\(\\pi(\\theta) \\equiv p(\\theta \\mid y)\\) with an unnormalized joint density \\(p(\\theta, y)\\) with the property:\n\\[\np(\\theta^{(1)}, y)h(\\theta^{(1)}, \\theta) J(\\theta \\mid \\theta^{(1)}) = p(\\theta, y)h(\\theta, \\theta^{(1)}) J(\\theta^{(1)} \\mid \\theta)\n\\] This is true because, assuming \\(p(\\theta^{(1)}, y) \\leq p(\\theta, y)\\), \\[\n\\begin{aligned}\np(\\theta^{(1)}, y)h(\\theta^{(1)}, \\theta) J(\\theta \\mid \\theta^{(1)}) & = p(\\theta, y)\\frac{p(\\theta^{(1)}, y)}{p(\\theta, y)} J(\\theta^{(1)} \\mid \\theta) \\\\\n& = p(\\theta^{(1)}, y) J(\\theta \\mid \\theta^{(1)}) \\\\\n\\end{aligned}\n\\] \\(h(\\theta^{(1)}, \\theta) = \\min(p(\\theta, y)/p(\\theta^{(1)}, y), 1) = 1\\). We’ll start with the LHS above and we can show that it equals the RHS. First we start with the first term on the LHS:\n\\[\n\\small\n\\begin{aligned}\n\\int_B p(\\theta^{(1)} \\mid y) (1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in A}d\\theta^{(1)} & = \\int_{\\Omega_\\theta} \\ind{\\theta^{(1)} \\in B} p(\\theta^{(1)} \\mid y) (1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in A}d\\theta^{(1)} \\\\\n& = \\int_{\\Omega_\\theta} \\ind{\\theta^{(1)} \\in A} p(\\theta^{(1)} \\mid y) (1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in B}d\\theta^{(1)} \\\\\n& = \\int_Ap(\\theta^{(1)} \\mid y)(1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in B} d \\theta^{(1)}.\n\\end{aligned}\n\\]\n\\[\n\\small\n\\begin{aligned}\n\\int_B p(\\theta^{(1)} \\mid y) \\int_{A} h(\\theta,\\theta^{(1)}) J(\\theta \\mid \\theta^{(1)})d\\theta d\\theta^{(1)} & = \\int_{\\Omega_\\theta}\\ind{\\theta^{(1)} \\in B} p(\\theta^{(1)} \\mid y)  \\int_{\\Omega_\\theta}\\ind{\\theta \\in A} h(\\theta,\\theta^{(1)}) J(\\theta \\mid \\theta^{(1)}) d\\theta d\\theta^{(1)} \\\\\n& = \\int_{\\Omega_\\theta}\\int_{\\Omega_\\theta}\\ind{\\theta \\in A} \\ind{\\theta^{(1)} \\in B} p(\\theta^{(1)} \\mid y)  h(\\theta^{(1)},\\theta) J(\\theta \\mid \\theta^{(1)}) d\\theta d\\theta^{(1)} \\\\\n& = \\frac{1}{p(y)}\\int_{\\Omega_\\theta}\\int_{\\Omega_\\theta}\\ind{\\theta \\in A} \\ind{\\theta^{(1)} \\in B} p(\\theta^{(1)}, y)  h(\\theta^{(1)},\\theta) J(\\theta \\mid \\theta^{(1)}) d\\theta d\\theta^{(1)} \\\\\n& = \\frac{1}{p(y)}\\int_{\\Omega_\\theta}\\int_{\\Omega_\\theta}\\ind{\\theta \\in A} \\ind{\\theta^{(1)} \\in B} p(\\theta, y)  h(\\theta,\\theta^{(1)}) J(\\theta^{(1)}\\mid \\theta) d\\theta^{(1)}d\\theta  \\\\\n& = \\frac{1}{p(y)}\\int_{\\Omega_\\theta}\\ind{\\theta \\in A}p(\\theta, y)  \\int_{\\Omega_\\theta} \\ind{\\theta^{(1)} \\in B}  h(\\theta,\\theta^{(1)}) J(\\theta^{(1)}\\mid \\theta) d\\theta^{(1)}d\\theta  \\\\\n& = \\int_{A} p(\\theta \\mid y)  \\int_{B} h(\\theta,\\theta^{(1)}) J(\\theta^{(1)}\\mid \\theta) d\\theta^{(1)}d\\theta  \n\\end{aligned}\n\\]\nPutting these together, we’ve shown that: \\[\n\\int_B p(\\theta^{(1)} \\mid y) P(A \\mid \\theta^{(1)}) d\\theta^{(1)}  = \\int_A \\pi(\\theta^{(1)} \\mid y) P(B \\mid \\theta^{(1)}) d\\theta^{(1)}\n\\] A default Metropolis sampler can be generated using a multivariate normal distribution for \\(J_t(\\theta^b \\mid \\theta^a)\\) \\[\n\\theta^b \\sim N(\\theta^a, \\Sigma)\n\\]\nwhere we tune \\(\\Sigma\\) to be about the scale we expect the posterior to be. That means that when we’re in regions of high density, we’ll have a good chance of jumping to a point that has reasonable posterior density, which means we won’t reject the proposal with high probability.\n\n\n\nOne way to generate a proposal distribution with this property is with an idea from physics using parameter expansion, namely if we have a distribution we’d like to sample from, \\(\\pi(\\theta)\\), we can introduce 1-to-1 auxiliary variables \\(\\varphi\\) (i.e. if we have \\(d\\) \\(\\theta\\), we’ll have \\(d\\) \\(\\varphi\\)) with a multivariate normal distribution so our joint target density is \\(\\pi(\\theta)\\mathcal{N}(\\varphi \\mid 0, M)\\).\nIf we represent the marginal target density as \\(\\exp(- (-\\log\\pi(\\theta) - \\log\\mathcal{N}(\\varphi \\mid 0, M)))\\), and call \\(U(\\theta) -\\log\\pi(\\theta)\\), \\(K(\\varphi) = - \\log\\mathcal{N}(\\varphi \\mid 0, M)))\\), we get the following representation: \\[\n\\exp(-(U(\\theta) + K(\\varphi))) \\equiv \\pi(\\theta)\\mathcal{N}(\\varphi \\mid 0, M)\n\\] We can think of \\(\\theta\\) as representing the positions of \\(d\\) particles and \\(\\varphi\\) as representing the momentum. In this sense, \\(U(\\theta)\\) is a potential energy, and \\(K(\\varphi)\\) is a kinetic energy term. The total energy in the system is \\(U(\\theta) + K(\\varphi)\\) and this is called the Hamiltonian, or \\(H(\\theta, \\varphi)\\). It turns out that given an initial starting point \\((\\theta_0, \\varphi_0)\\) we can simulate the trajectories of these particles for any time \\(t\\) in the future using the Hamiltonian and what are called Hamilton’s system of equations:\n\\[\n\\begin{aligned}\n\\frac{d \\theta}{d t} & = \\frac{\\partial H(\\theta, \\varphi)}{\\partial \\varphi} \\\\\n\\frac{d \\varphi}{d t} & = -\\frac{\\partial H(\\theta, \\varphi)}{\\partial \\theta}\n\\end{aligned}\n\\] We can write this in matrix notation if we define the matrix \\(J^{-1}\\) as: \\[\n\\begin{bmatrix}\n0 & I_d\\\\\n-I_d & 0\n\\end{bmatrix}\n\\]\n\\[\n\\nabla_t \\begin{bmatrix} \\theta \\\\\n\\varphi\n\\end{bmatrix} = J \\nabla_{\\theta,\\varphi} H(\\theta, \\varphi)\n\\]\nThen for a small time step \\(\\Delta t\\) we get \\[\n\\begin{aligned}\n\\theta_{\\Delta t} & = \\theta_{0} + \\frac{d \\theta}{d t}(\\theta, \\varphi) \\Delta t\\\\\n\\varphi_{\\Delta t} & = \\varphi_{0} + \\frac{d \\varphi}{d t}(\\theta, \\varphi)\\Delta dt\\\\\n\\end{aligned}\n\\] This seems straightforward, but \\[\n\\begin{bmatrix}\n0 & -I_d\\\\\nI_d & 0\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix} \\theta_{\\Delta t} \\\\\n\\varphi_{\\Delta t}\n\\end{bmatrix}  =\n\\begin{bmatrix} \\theta \\\\\n\\varphi\n\\end{bmatrix} +  \n\\begin{bmatrix}\n0 & I_d\\\\\n-I_d & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\nabla_\\theta H(\\theta, \\varphi) \\\\\n\\nabla_\\varphi H(\\theta, \\varphi)\n\\end{bmatrix}  \\Delta t\n\\] What’s the Jacobian of this transformation?\n\\[\n\\nabla_{\\theta,\\varphi} \\begin{bmatrix} \\theta_{\\Delta t} \\\\\n\\varphi_{\\Delta t}\n\\end{bmatrix}  =\n\\begin{bmatrix} I_d & 0\\\\\n0 & I_d\n\\end{bmatrix} +  \n\\begin{bmatrix}\n0 & I_d\\\\\n-I_d & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\nabla^2_{\\theta} H(\\theta, \\varphi) & \\nabla^2_{\\theta, \\varphi} H(\\theta, \\varphi)\\\\\n\\nabla^2_{\\varphi, \\theta} H(\\theta, \\varphi) & \\nabla^2_\\varphi H(\\theta, \\varphi)\n\\end{bmatrix} \\Delta t\n\\] This simplifies to \\[\n\\nabla_{\\theta,\\varphi} \\begin{bmatrix} \\theta_{\\Delta t} \\\\\n\\varphi_{\\Delta t}\n\\end{bmatrix}  =\n\\begin{bmatrix} I_d + \\Delta t \\nabla^2_{\\theta, \\varphi} H(\\theta, \\varphi)  & \\Delta t \\nabla^2_{\\theta} H(\\theta, \\varphi)\\\\\n-\\Delta t \\nabla^2_{\\varphi} H(\\theta, \\varphi) & I_d- \\Delta t \\nabla^2_{\\theta, \\varphi} H(\\theta, \\varphi)\n\\end{bmatrix}\n\\]\nIt turns out that the determinant of this matrix is \\(I_d\\) plus terms that involve \\((\\Delta t)^2\\). If we make \\(\\Delta t\\) small, this means that the determinant of the transformation is \\(1\\).\nAnother nice property of these equations is that the Hamiltonian is constant in time: \\[\n\\begin{aligned}\n\\frac{d H(\\theta, \\phi)}{d t} & = \\sum_j \\frac{d \\theta}{dt} \\frac{\\partial H}{\\partial \\theta} + \\frac{d \\varphi}{dt} \\frac{\\partial H}{\\partial \\varphi}  \\\\\n& = \\sum_j \\frac{\\partial H}{\\partial \\varphi} \\frac{\\partial H}{\\partial \\theta} - \\frac{\\partial H}{\\partial \\theta} \\frac{\\partial H}{\\partial \\varphi} \\\\\n& = 0\n\\end{aligned}\n\\] This means that if we sample \\((\\theta_0, \\varphi_0)\\) from the density \\(\\exp(-H(\\theta, \\varphi))\\) and compute the final position and momentum of the particles after \\(t\\) time \\((\\theta_t, \\varphi_t)\\), we’ll get the same density over \\(\\theta_0, \\varphi_0\\) that we started with: Let \\(F_t(\\theta_0, \\varphi_0) = (\\theta_t, \\varphi_t)\\). This has an inverse, such that \\(F^{-1}_t(\\theta_t, \\varphi_t) = (\\theta_0, \\varphi_0)\\). In fact, this inverse is equal to \\(F^{-1}_t(\\theta_t, \\varphi_t) = F_{-t}(\\theta_t, \\varphi_t)\\) Let’s compute the density under this transformation, starting with the density:\n\\[\n\\exp(-H(\\theta_0, \\varphi_0)) d\\theta_0 d\\varphi_0\n\\]\n\\[\n\\begin{aligned}\n\\exp(-H(F^{-1}_{t}(\\theta_t \\varphi_t))) \\det \\nabla_{\\theta_t, \\varphi_t} F^{-1}_{t}(\\theta_t \\varphi_t) & = \\exp(-H(F_{-t}(\\theta_t \\varphi_t))) \\det \\nabla_{\\theta_t, \\varphi_t} F^{-1}_{t}(\\theta_t \\varphi_t) \\\\\n& = \\exp(-H(\\theta_t \\varphi_t)) d\\theta_t  d\\varphi_t\n\\end{aligned}\n\\] where the second line follows from the fact that the change in time for the Hamiltonian is zero, and that the determinant of the transformation is 1. Thus, plugging in \\(\\theta_t, \\varphi_t\\) to the Hamiltonian has no effect on the value of the function; the Hamiltonian is constant.\nThis means that the starting values define the total energy for the system.\nThe algorithm for ex\nOne key point from above is that we can take gradients without worrying about the normalizing constant! \\[\n\\begin{aligned}\n\\frac{\\partial H(\\theta, \\varphi)}{\\partial \\theta} & = -\\frac{d }{d \\theta}\\log(p(\\theta \\mid y)) \\\\\n& = -\\frac{d}{d\\theta} \\lp \\log p(\\theta) + \\log(f_Y(y \\mid \\theta)) - \\log(f_Y(y))\\rp \\\\\n& = -\\frac{d}{d\\theta} \\lp \\log p(\\theta) + \\log(f_Y(y \\mid \\theta))\\rp \\\\\n\\end{aligned}\n\\] because the marginal density of the data is not dependent on \\(\\theta\\). Thus we can use this algorithm to sample from densities with intractable normalizing constants, which is pretty much any interesting statitsical model.\nThe idea is to draw an initial value for \\(\\varphi\\) from a multivariate normal distribution, and then to run Hamilton’s equations to get draws for the final \\(\\theta^t\\) and \\(\\varphi^t\\)\nThe problem with this idea is that we can’t solve Hamilton’s equations for any non-trivial problem. What we do instead is to discretize the equations and solve them approximately. If our Hamiltonian allows for \\(p(\\varphi \\mid \\theta)\\), we’ll need to use complex numerical integration schemes. If, as is typical, we use a distribution for \\(\\varphi\\) that is independent of \\(\\theta\\) (something like a multivariate normal distribution with a fixed covariance matrix, \\(M\\), for the density \\(p(\\varphi)\\)), we can use the leapfrog integrator to approximately solve the equations of motion.\nLet’s define \\(L\\) as the number of steps of the integrator, and \\(\\epsilon\\) as the step-size, or how finely discretized our equations of motion are. Start with \\(\\theta^{(0)}\\), and \\(\\varphi^{(0)} \\sim \\text{Normal}(0, M)\\) and Then for each step \\(l = 1, \\dots, L\\), repeat: \\[\n\\begin{aligned}\n\\varphi^{(\\epsilon(l - 1/2))} & = \\varphi^{(\\epsilon(l-1))} + \\frac{\\epsilon}{2} \\frac{d \\log(p(\\theta \\mid y))}{d \\theta} \\\\\n\\theta^{(\\epsilon l)} & = \\theta^{(\\epsilon(l-1))} + \\epsilon M^{-1} \\varphi^{(\\epsilon(l-1))} \\\\\n\\varphi^{(\\epsilon l)} & = \\varphi^{(\\epsilon (l-1/2))} + \\frac{\\epsilon}{2} \\frac{d \\log(p(\\theta \\mid y))}{d \\theta} \\\\\n\\end{aligned}\n\\] Here is the expression for the final proposal for \\(\\theta^{(L\\epsilon)}\\): \\[\n\\theta^{(L\\epsilon)} = \\theta^{(0)} + \\frac{L \\epsilon^2}{2} \\nabla_\\theta U(\\theta^{(0)}) - \\epsilon^2 \\sum_{l=1}^L (L - l) \\nabla_\\theta U(\\theta^{(\\epsilon l)}) + L \\epsilon \\varphi^{(0)}\n\\] where \\(\\varphi^{(0)} \\sim \\text{Normal}(0, M)\\).\nOne problem with this implementation is that the Hamiltonian isn’t exactly conserved, so we do have to do a Metropolis step at the end of the \\(L\\) steps to determine if we accept the proposal.\nFinally, after we run the algorithm, we have a new set of parameters \\(\\theta^{(L\\epsilon)}, \\varphi^{(L\\epsilon)}\\). We then compute the ratio of the exponentiated Hamiltonian at the start and end of the algorithm: \\[\nr = \\frac{p(\\theta^{(0)} \\mid y) p(\\varphi^{(0)})}{p(\\theta^{(L \\epsilon)} \\mid y) p(\\varphi^{(L \\epsilon)})}\n\\] and set \\(\\theta^{(1)} = \\theta^{(L \\epsilon)}\\) with probability \\(\\min(r, 1)\\), or \\(\\theta^{(1)} = \\theta^{(0)}\\) otherwise.\nThis algorithm has three tuning parameters, \\(L,\\epsilon, M\\). One way to set \\(L\\) is to run the algorithm until you detect that the particles have begun to move back towards the starting point, \\(\\theta^{(0)}\\). That way, you’ll minimize the autocorrelation between draws, and boost your effective sample size.\nThat suggests a heuristic to measure the dot-product of \\((\\theta^{(l\\epsilon)} - \\theta^{(0)})\\) and \\(\\varphi\\). When this becomes negative, it means that the momentum is pointing in a different direction than the difference between the current step and the initial point.\nThis is the idea behind the No-U-Turn-Sampler, which stops the leapfrog integrator when \\[\n\\sum_j (\\theta^{(l\\epsilon)}_j - \\theta^{(0)}_j) \\varphi^{(l\\epsilon)}_j &lt; 0\n\\] We can’t exactly use this as an exact stopping rule because just choosing \\(\\theta^{(l\\epsilon)}\\) as the final draw in the trajectory would not lead to a sampler with detailed balance.\nSee Hoffman, Gelman, et al. (2014) for more information.\nAnother parameter that needs to be set is \\(\\epsilon\\), which is the discretization size of the numerical integrator.\nIf this is set to be too small, you’ll need many leapfrog steps to make measurable progress. If it is set too large, the numerical error in the integrator will add up too quickly and you won’t be approximating the solution to the diff-eqs well anymore. This can lead to something called divergences, where the integrator diverges."
  },
  {
    "objectID": "missing-data-material-W-26/notes/MCMC-lecture-2.html#mcmc-and-gibbs-sampling-recap",
    "href": "missing-data-material-W-26/notes/MCMC-lecture-2.html#mcmc-and-gibbs-sampling-recap",
    "title": "Metropolis and Hamiltonian Monte Carlo",
    "section": "",
    "text": "Suppose we want to sample from a distribution \\(\\pi(\\theta)\\) (for the rest of the lecture I’ll suppress the dependence on \\(y\\) unless otherwise noted), but we can’t easily do so, we might be able to create a Markov Chain whose stationary distribution is \\(\\pi(\\theta)\\).\nThe Markov Chain has the property that \\[\nP(\\theta^{(n)} \\in A \\mid \\theta^{(n-1)} = c_{n-1}, \\dots, \\theta^{(n-1)} = c_{1}) = P(\\theta^{(n)} \\in A \\mid \\theta^{(n-1)} = c_{n-1})\n\\] and that \\[\nP(\\theta^{(n)} \\in A \\mid \\theta^{(n-1)} = c)\n\\] doesn’t depend on \\(n\\).\nThis transition function has the property that for any value \\(c\\) of \\(\\theta^{(n-1)}\\), the function \\(P(\\theta^{n} \\in A \\mid \\theta^{(n-1)} = c)\\) is a probability measure over whatever space \\(A\\) is in, and for any fixed \\(A\\), \\(P(\\theta^{n} \\in A \\mid \\theta^{(n-1)} = x)\\) is a measurable function of \\(x\\).\nIn finite spaces, the transition function is just a matrix with \\((i,j)^\\mathrm{th}\\) entry\nWhat the proof from Tanner and Wong (1987) shows is that we can create a Markov Chain with the stationary distribution \\(p(\\theta)\\) if we have a transition function \\(P(\\theta \\mid \\theta^\\prime)\\) with the following properties:\n\n\\(\\pi(\\theta) = \\int_{\\theta^\\prime} P(\\theta \\mid \\theta^\\prime) \\pi(\\theta^\\prime) d\\theta^\\prime\\)\n\\(P(\\theta \\mid \\theta^\\prime) \\leq M &lt; \\infty\\) for all \\(\\theta, \\theta^\\prime\\).\nFor every \\(\\theta_0 \\in \\Omega_{\\theta}\\) there is an open neighborhood \\(U\\) of \\(\\theta_0\\) so that: \\[\nP(\\theta \\mid \\theta^\\prime) &gt; 0, \\forall (\\theta, \\theta^\\prime) \\in U\n\\]\n\nand an initial distribution \\(g(\\theta)\\) that satisfies:\n\\[\n\\sup_\\theta g(\\theta) / \\pi(\\theta) &lt; \\infty\n\\] One way of showing condition 1 is by showing that a chain is reversible, namely that for two sets \\(A\\) and \\(B\\): \\[\n\\int_{A} \\pi(\\theta^\\prime) \\int_B p(\\theta \\mid \\theta^\\prime) d\\theta\\, d\\theta^\\prime =  \\int_{B} \\pi(\\theta^\\prime) \\int_A p(\\theta \\mid \\theta^\\prime) d\\theta\\, d\\theta^\\prime\n\\] We can represent \\(\\int_B p(\\theta \\mid \\theta^\\prime) d\\theta\\) as \\(P(B \\mid \\theta^\\prime)\\) When \\(A\\) is the whole parameter space, \\(\\Omega_\\theta\\) this says something more interpretable: \\[\n\\int_{\\Omega_\\theta} \\pi(\\theta^\\prime) \\int_B p(\\theta \\mid \\theta^\\prime) d\\theta\\, d\\theta^\\prime =  \\int_{B} \\pi(\\theta^\\prime) d\\theta^\\prime\n\\] This says: If I draw a value from the stationary distribution, and then draw a value from the transition function, my probability of landing in the set \\(B\\) is the same as if I had just measured whether the first draw was in set \\(B\\).\n\n\nThis is all from Geyer’s notes on MCMC, Geyer (2005).\nOne way to construct a transition function that has this behavior is by using the Metropolis algorithm.\n\nSample a new point \\(\\theta^{(2)} \\mid \\theta^{(1)}\\) with a proposal distribution that we can draw from, \\(J(\\theta^{(2)} \\mid \\theta^{(1)})\\), such that \\(J(\\theta^{(1)} \\mid \\theta^{(2)})\\) = \\(J(\\theta^{(2)} \\mid \\theta^{(1)})\\).\nAccept the new point with probability \\(\\min(r,1)\\): \\[\nr = \\frac{\\pi(\\theta^{(2)})}{\\pi(\\theta^{(1)})}\n\\] else set the next step of the Markov Chain to \\(\\theta^{(1)}\\).\n\nCrucially, this acceptance probability, which is called the Metropolis acceptance rate, can be computed without regards to the normalizing constant of the probability density.\nWe can see this easily because it’s a ratio of the same density evaluated at two different parameters. Let \\(\\pi(\\theta) = p(\\theta \\mid y)\\) where \\(p(\\theta)\\) is the prior for \\(\\theta\\), \\(f_Y(y \\mid \\theta)\\) is the density of the observations, and \\(p(y) = \\int_\\theta p(\\theta) f_Y(y \\mid \\theta) d \\theta\\):\n\\[\n\\begin{aligned}\nr & = \\frac{p(\\theta^{(2)} \\mid y)}{p(\\theta^{(1)} \\mid y)} \\\\\n& = \\frac{p(\\theta^{(2)}, y)/p(y)}{p(\\theta^{(1)}, y)/p(y)} \\\\\n& = \\frac{p(\\theta^{(2)}, y)}{p(\\theta^{(1)}, y)} \\\\\n& = \\frac{p(\\theta^{(2)}) f_Y(y \\mid \\theta^{(2)})}{p(\\theta^{(1)}) f_Y(y \\mid \\theta^{(1)})}\n\\end{aligned}\n\\] This is helpful, because we don’t know the normalizing constant \\(p(y)\\) for most models we’re interested in fitting.\nWhat does the Metropolis algorithm imply for the transition density?\nWe need to compute the conditional measure \\(P(A \\mid \\theta^{(1)})\\), which gives the probability of landing in set \\(A\\), or of drawing a value \\(\\theta^{(2)}\\) that is in set \\(A\\) from the algorithm above.\nThere are two ways we can get to set \\(A\\). The first way is if the proposed point \\(\\theta^{(2)}\\) is in set \\(A\\) and the draw is accepted. The other way is if \\(\\theta^{(1)}\\) is in set \\(A\\) and we reject the proposal from \\(\\theta^{(1)} \\to \\theta^{(2)}\\).\nThe probability of acceptance for a single point \\(\\theta^{(2)}\\) given we started at \\(\\theta^{(1)}\\) is \\(h(\\theta^{(1)}, \\theta^{(2)}) = \\min(r(\\theta^{(1)},\\theta^{(2)}),1)\\). The probability that we transition from \\(\\theta^{(1)}\\to \\theta^{(2)}\\) is given by \\[\n\\int_A J(\\theta \\mid \\theta^{(1)}) h(\\theta^{(1)}, \\theta) d\\theta\n\\] The probability we accept any jump is the integral over the whole space, \\(\\Omega_\\theta\\): \\[\na(\\theta^{(1)}) = \\int_{\\Omega_\\theta} J(\\theta \\mid \\theta^{(1)}) h(\\theta^{(1)}, \\theta) d\\theta\n\\] Then the probability that we reject the proposal at \\(\\theta^{(1)}\\) is \\(1 - a(\\theta^{(1)})\\). The total probability of landing in set \\(A\\) if we reject the draw is \\(1\\) if \\(\\theta^{(1)} \\in A\\), or 0 if it isn’t in \\(A\\), which we can represent as \\(\\ind{\\theta^{(1)} \\in A}\\).\nThis means that \\[\nP(A \\mid \\theta^{(1)}) = (1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in A} + \\int_A h(\\theta^{(1)}, \\theta) J(\\theta \\mid \\theta^{(1)}) d\\theta\n\\] Now we need to show that this is a reversible transition, namely:\n\\[\n\\int_B \\pi(\\theta^{(1)}) P(A \\mid \\theta^{(1)}) d\\theta^{(1)}  = \\int_A \\pi(\\theta^{(1)}) P(B \\mid \\theta^{(1)}) d\\theta^{(1)}\n\\] crucially, for a density \\(\\pi(\\theta) \\equiv p(\\theta \\mid y)\\) with an unnormalized joint density \\(p(\\theta, y)\\) with the property:\n\\[\np(\\theta^{(1)}, y)h(\\theta^{(1)}, \\theta) J(\\theta \\mid \\theta^{(1)}) = p(\\theta, y)h(\\theta, \\theta^{(1)}) J(\\theta^{(1)} \\mid \\theta)\n\\] This is true because, assuming \\(p(\\theta^{(1)}, y) \\leq p(\\theta, y)\\), \\[\n\\begin{aligned}\np(\\theta^{(1)}, y)h(\\theta^{(1)}, \\theta) J(\\theta \\mid \\theta^{(1)}) & = p(\\theta, y)\\frac{p(\\theta^{(1)}, y)}{p(\\theta, y)} J(\\theta^{(1)} \\mid \\theta) \\\\\n& = p(\\theta^{(1)}, y) J(\\theta \\mid \\theta^{(1)}) \\\\\n\\end{aligned}\n\\] \\(h(\\theta^{(1)}, \\theta) = \\min(p(\\theta, y)/p(\\theta^{(1)}, y), 1) = 1\\). We’ll start with the LHS above and we can show that it equals the RHS. First we start with the first term on the LHS:\n\\[\n\\small\n\\begin{aligned}\n\\int_B p(\\theta^{(1)} \\mid y) (1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in A}d\\theta^{(1)} & = \\int_{\\Omega_\\theta} \\ind{\\theta^{(1)} \\in B} p(\\theta^{(1)} \\mid y) (1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in A}d\\theta^{(1)} \\\\\n& = \\int_{\\Omega_\\theta} \\ind{\\theta^{(1)} \\in A} p(\\theta^{(1)} \\mid y) (1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in B}d\\theta^{(1)} \\\\\n& = \\int_Ap(\\theta^{(1)} \\mid y)(1 - a(\\theta^{(1)})) \\ind{\\theta^{(1)} \\in B} d \\theta^{(1)}.\n\\end{aligned}\n\\]\n\\[\n\\small\n\\begin{aligned}\n\\int_B p(\\theta^{(1)} \\mid y) \\int_{A} h(\\theta,\\theta^{(1)}) J(\\theta \\mid \\theta^{(1)})d\\theta d\\theta^{(1)} & = \\int_{\\Omega_\\theta}\\ind{\\theta^{(1)} \\in B} p(\\theta^{(1)} \\mid y)  \\int_{\\Omega_\\theta}\\ind{\\theta \\in A} h(\\theta,\\theta^{(1)}) J(\\theta \\mid \\theta^{(1)}) d\\theta d\\theta^{(1)} \\\\\n& = \\int_{\\Omega_\\theta}\\int_{\\Omega_\\theta}\\ind{\\theta \\in A} \\ind{\\theta^{(1)} \\in B} p(\\theta^{(1)} \\mid y)  h(\\theta^{(1)},\\theta) J(\\theta \\mid \\theta^{(1)}) d\\theta d\\theta^{(1)} \\\\\n& = \\frac{1}{p(y)}\\int_{\\Omega_\\theta}\\int_{\\Omega_\\theta}\\ind{\\theta \\in A} \\ind{\\theta^{(1)} \\in B} p(\\theta^{(1)}, y)  h(\\theta^{(1)},\\theta) J(\\theta \\mid \\theta^{(1)}) d\\theta d\\theta^{(1)} \\\\\n& = \\frac{1}{p(y)}\\int_{\\Omega_\\theta}\\int_{\\Omega_\\theta}\\ind{\\theta \\in A} \\ind{\\theta^{(1)} \\in B} p(\\theta, y)  h(\\theta,\\theta^{(1)}) J(\\theta^{(1)}\\mid \\theta) d\\theta^{(1)}d\\theta  \\\\\n& = \\frac{1}{p(y)}\\int_{\\Omega_\\theta}\\ind{\\theta \\in A}p(\\theta, y)  \\int_{\\Omega_\\theta} \\ind{\\theta^{(1)} \\in B}  h(\\theta,\\theta^{(1)}) J(\\theta^{(1)}\\mid \\theta) d\\theta^{(1)}d\\theta  \\\\\n& = \\int_{A} p(\\theta \\mid y)  \\int_{B} h(\\theta,\\theta^{(1)}) J(\\theta^{(1)}\\mid \\theta) d\\theta^{(1)}d\\theta  \n\\end{aligned}\n\\]\nPutting these together, we’ve shown that: \\[\n\\int_B p(\\theta^{(1)} \\mid y) P(A \\mid \\theta^{(1)}) d\\theta^{(1)}  = \\int_A \\pi(\\theta^{(1)} \\mid y) P(B \\mid \\theta^{(1)}) d\\theta^{(1)}\n\\] A default Metropolis sampler can be generated using a multivariate normal distribution for \\(J_t(\\theta^b \\mid \\theta^a)\\) \\[\n\\theta^b \\sim N(\\theta^a, \\Sigma)\n\\]\nwhere we tune \\(\\Sigma\\) to be about the scale we expect the posterior to be. That means that when we’re in regions of high density, we’ll have a good chance of jumping to a point that has reasonable posterior density, which means we won’t reject the proposal with high probability.\n\n\n\nOne way to generate a proposal distribution with this property is with an idea from physics using parameter expansion, namely if we have a distribution we’d like to sample from, \\(\\pi(\\theta)\\), we can introduce 1-to-1 auxiliary variables \\(\\varphi\\) (i.e. if we have \\(d\\) \\(\\theta\\), we’ll have \\(d\\) \\(\\varphi\\)) with a multivariate normal distribution so our joint target density is \\(\\pi(\\theta)\\mathcal{N}(\\varphi \\mid 0, M)\\).\nIf we represent the marginal target density as \\(\\exp(- (-\\log\\pi(\\theta) - \\log\\mathcal{N}(\\varphi \\mid 0, M)))\\), and call \\(U(\\theta) -\\log\\pi(\\theta)\\), \\(K(\\varphi) = - \\log\\mathcal{N}(\\varphi \\mid 0, M)))\\), we get the following representation: \\[\n\\exp(-(U(\\theta) + K(\\varphi))) \\equiv \\pi(\\theta)\\mathcal{N}(\\varphi \\mid 0, M)\n\\] We can think of \\(\\theta\\) as representing the positions of \\(d\\) particles and \\(\\varphi\\) as representing the momentum. In this sense, \\(U(\\theta)\\) is a potential energy, and \\(K(\\varphi)\\) is a kinetic energy term. The total energy in the system is \\(U(\\theta) + K(\\varphi)\\) and this is called the Hamiltonian, or \\(H(\\theta, \\varphi)\\). It turns out that given an initial starting point \\((\\theta_0, \\varphi_0)\\) we can simulate the trajectories of these particles for any time \\(t\\) in the future using the Hamiltonian and what are called Hamilton’s system of equations:\n\\[\n\\begin{aligned}\n\\frac{d \\theta}{d t} & = \\frac{\\partial H(\\theta, \\varphi)}{\\partial \\varphi} \\\\\n\\frac{d \\varphi}{d t} & = -\\frac{\\partial H(\\theta, \\varphi)}{\\partial \\theta}\n\\end{aligned}\n\\] We can write this in matrix notation if we define the matrix \\(J^{-1}\\) as: \\[\n\\begin{bmatrix}\n0 & I_d\\\\\n-I_d & 0\n\\end{bmatrix}\n\\]\n\\[\n\\nabla_t \\begin{bmatrix} \\theta \\\\\n\\varphi\n\\end{bmatrix} = J \\nabla_{\\theta,\\varphi} H(\\theta, \\varphi)\n\\]\nThen for a small time step \\(\\Delta t\\) we get \\[\n\\begin{aligned}\n\\theta_{\\Delta t} & = \\theta_{0} + \\frac{d \\theta}{d t}(\\theta, \\varphi) \\Delta t\\\\\n\\varphi_{\\Delta t} & = \\varphi_{0} + \\frac{d \\varphi}{d t}(\\theta, \\varphi)\\Delta dt\\\\\n\\end{aligned}\n\\] This seems straightforward, but \\[\n\\begin{bmatrix}\n0 & -I_d\\\\\nI_d & 0\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix} \\theta_{\\Delta t} \\\\\n\\varphi_{\\Delta t}\n\\end{bmatrix}  =\n\\begin{bmatrix} \\theta \\\\\n\\varphi\n\\end{bmatrix} +  \n\\begin{bmatrix}\n0 & I_d\\\\\n-I_d & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\nabla_\\theta H(\\theta, \\varphi) \\\\\n\\nabla_\\varphi H(\\theta, \\varphi)\n\\end{bmatrix}  \\Delta t\n\\] What’s the Jacobian of this transformation?\n\\[\n\\nabla_{\\theta,\\varphi} \\begin{bmatrix} \\theta_{\\Delta t} \\\\\n\\varphi_{\\Delta t}\n\\end{bmatrix}  =\n\\begin{bmatrix} I_d & 0\\\\\n0 & I_d\n\\end{bmatrix} +  \n\\begin{bmatrix}\n0 & I_d\\\\\n-I_d & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\nabla^2_{\\theta} H(\\theta, \\varphi) & \\nabla^2_{\\theta, \\varphi} H(\\theta, \\varphi)\\\\\n\\nabla^2_{\\varphi, \\theta} H(\\theta, \\varphi) & \\nabla^2_\\varphi H(\\theta, \\varphi)\n\\end{bmatrix} \\Delta t\n\\] This simplifies to \\[\n\\nabla_{\\theta,\\varphi} \\begin{bmatrix} \\theta_{\\Delta t} \\\\\n\\varphi_{\\Delta t}\n\\end{bmatrix}  =\n\\begin{bmatrix} I_d + \\Delta t \\nabla^2_{\\theta, \\varphi} H(\\theta, \\varphi)  & \\Delta t \\nabla^2_{\\theta} H(\\theta, \\varphi)\\\\\n-\\Delta t \\nabla^2_{\\varphi} H(\\theta, \\varphi) & I_d- \\Delta t \\nabla^2_{\\theta, \\varphi} H(\\theta, \\varphi)\n\\end{bmatrix}\n\\]\nIt turns out that the determinant of this matrix is \\(I_d\\) plus terms that involve \\((\\Delta t)^2\\). If we make \\(\\Delta t\\) small, this means that the determinant of the transformation is \\(1\\).\nAnother nice property of these equations is that the Hamiltonian is constant in time: \\[\n\\begin{aligned}\n\\frac{d H(\\theta, \\phi)}{d t} & = \\sum_j \\frac{d \\theta}{dt} \\frac{\\partial H}{\\partial \\theta} + \\frac{d \\varphi}{dt} \\frac{\\partial H}{\\partial \\varphi}  \\\\\n& = \\sum_j \\frac{\\partial H}{\\partial \\varphi} \\frac{\\partial H}{\\partial \\theta} - \\frac{\\partial H}{\\partial \\theta} \\frac{\\partial H}{\\partial \\varphi} \\\\\n& = 0\n\\end{aligned}\n\\] This means that if we sample \\((\\theta_0, \\varphi_0)\\) from the density \\(\\exp(-H(\\theta, \\varphi))\\) and compute the final position and momentum of the particles after \\(t\\) time \\((\\theta_t, \\varphi_t)\\), we’ll get the same density over \\(\\theta_0, \\varphi_0\\) that we started with: Let \\(F_t(\\theta_0, \\varphi_0) = (\\theta_t, \\varphi_t)\\). This has an inverse, such that \\(F^{-1}_t(\\theta_t, \\varphi_t) = (\\theta_0, \\varphi_0)\\). In fact, this inverse is equal to \\(F^{-1}_t(\\theta_t, \\varphi_t) = F_{-t}(\\theta_t, \\varphi_t)\\) Let’s compute the density under this transformation, starting with the density:\n\\[\n\\exp(-H(\\theta_0, \\varphi_0)) d\\theta_0 d\\varphi_0\n\\]\n\\[\n\\begin{aligned}\n\\exp(-H(F^{-1}_{t}(\\theta_t \\varphi_t))) \\det \\nabla_{\\theta_t, \\varphi_t} F^{-1}_{t}(\\theta_t \\varphi_t) & = \\exp(-H(F_{-t}(\\theta_t \\varphi_t))) \\det \\nabla_{\\theta_t, \\varphi_t} F^{-1}_{t}(\\theta_t \\varphi_t) \\\\\n& = \\exp(-H(\\theta_t \\varphi_t)) d\\theta_t  d\\varphi_t\n\\end{aligned}\n\\] where the second line follows from the fact that the change in time for the Hamiltonian is zero, and that the determinant of the transformation is 1. Thus, plugging in \\(\\theta_t, \\varphi_t\\) to the Hamiltonian has no effect on the value of the function; the Hamiltonian is constant.\nThis means that the starting values define the total energy for the system.\nThe algorithm for ex\nOne key point from above is that we can take gradients without worrying about the normalizing constant! \\[\n\\begin{aligned}\n\\frac{\\partial H(\\theta, \\varphi)}{\\partial \\theta} & = -\\frac{d }{d \\theta}\\log(p(\\theta \\mid y)) \\\\\n& = -\\frac{d}{d\\theta} \\lp \\log p(\\theta) + \\log(f_Y(y \\mid \\theta)) - \\log(f_Y(y))\\rp \\\\\n& = -\\frac{d}{d\\theta} \\lp \\log p(\\theta) + \\log(f_Y(y \\mid \\theta))\\rp \\\\\n\\end{aligned}\n\\] because the marginal density of the data is not dependent on \\(\\theta\\). Thus we can use this algorithm to sample from densities with intractable normalizing constants, which is pretty much any interesting statitsical model.\nThe idea is to draw an initial value for \\(\\varphi\\) from a multivariate normal distribution, and then to run Hamilton’s equations to get draws for the final \\(\\theta^t\\) and \\(\\varphi^t\\)\nThe problem with this idea is that we can’t solve Hamilton’s equations for any non-trivial problem. What we do instead is to discretize the equations and solve them approximately. If our Hamiltonian allows for \\(p(\\varphi \\mid \\theta)\\), we’ll need to use complex numerical integration schemes. If, as is typical, we use a distribution for \\(\\varphi\\) that is independent of \\(\\theta\\) (something like a multivariate normal distribution with a fixed covariance matrix, \\(M\\), for the density \\(p(\\varphi)\\)), we can use the leapfrog integrator to approximately solve the equations of motion.\nLet’s define \\(L\\) as the number of steps of the integrator, and \\(\\epsilon\\) as the step-size, or how finely discretized our equations of motion are. Start with \\(\\theta^{(0)}\\), and \\(\\varphi^{(0)} \\sim \\text{Normal}(0, M)\\) and Then for each step \\(l = 1, \\dots, L\\), repeat: \\[\n\\begin{aligned}\n\\varphi^{(\\epsilon(l - 1/2))} & = \\varphi^{(\\epsilon(l-1))} + \\frac{\\epsilon}{2} \\frac{d \\log(p(\\theta \\mid y))}{d \\theta} \\\\\n\\theta^{(\\epsilon l)} & = \\theta^{(\\epsilon(l-1))} + \\epsilon M^{-1} \\varphi^{(\\epsilon(l-1))} \\\\\n\\varphi^{(\\epsilon l)} & = \\varphi^{(\\epsilon (l-1/2))} + \\frac{\\epsilon}{2} \\frac{d \\log(p(\\theta \\mid y))}{d \\theta} \\\\\n\\end{aligned}\n\\] Here is the expression for the final proposal for \\(\\theta^{(L\\epsilon)}\\): \\[\n\\theta^{(L\\epsilon)} = \\theta^{(0)} + \\frac{L \\epsilon^2}{2} \\nabla_\\theta U(\\theta^{(0)}) - \\epsilon^2 \\sum_{l=1}^L (L - l) \\nabla_\\theta U(\\theta^{(\\epsilon l)}) + L \\epsilon \\varphi^{(0)}\n\\] where \\(\\varphi^{(0)} \\sim \\text{Normal}(0, M)\\).\nOne problem with this implementation is that the Hamiltonian isn’t exactly conserved, so we do have to do a Metropolis step at the end of the \\(L\\) steps to determine if we accept the proposal.\nFinally, after we run the algorithm, we have a new set of parameters \\(\\theta^{(L\\epsilon)}, \\varphi^{(L\\epsilon)}\\). We then compute the ratio of the exponentiated Hamiltonian at the start and end of the algorithm: \\[\nr = \\frac{p(\\theta^{(0)} \\mid y) p(\\varphi^{(0)})}{p(\\theta^{(L \\epsilon)} \\mid y) p(\\varphi^{(L \\epsilon)})}\n\\] and set \\(\\theta^{(1)} = \\theta^{(L \\epsilon)}\\) with probability \\(\\min(r, 1)\\), or \\(\\theta^{(1)} = \\theta^{(0)}\\) otherwise.\nThis algorithm has three tuning parameters, \\(L,\\epsilon, M\\). One way to set \\(L\\) is to run the algorithm until you detect that the particles have begun to move back towards the starting point, \\(\\theta^{(0)}\\). That way, you’ll minimize the autocorrelation between draws, and boost your effective sample size.\nThat suggests a heuristic to measure the dot-product of \\((\\theta^{(l\\epsilon)} - \\theta^{(0)})\\) and \\(\\varphi\\). When this becomes negative, it means that the momentum is pointing in a different direction than the difference between the current step and the initial point.\nThis is the idea behind the No-U-Turn-Sampler, which stops the leapfrog integrator when \\[\n\\sum_j (\\theta^{(l\\epsilon)}_j - \\theta^{(0)}_j) \\varphi^{(l\\epsilon)}_j &lt; 0\n\\] We can’t exactly use this as an exact stopping rule because just choosing \\(\\theta^{(l\\epsilon)}\\) as the final draw in the trajectory would not lead to a sampler with detailed balance.\nSee Hoffman, Gelman, et al. (2014) for more information.\nAnother parameter that needs to be set is \\(\\epsilon\\), which is the discretization size of the numerical integrator.\nIf this is set to be too small, you’ll need many leapfrog steps to make measurable progress. If it is set too large, the numerical error in the integrator will add up too quickly and you won’t be approximating the solution to the diff-eqs well anymore. This can lead to something called divergences, where the integrator diverges."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-9-notes.html#clarifying-mar-vs.-maar",
    "href": "missing-data-material-W-26/notes/lecture-9-notes.html#clarifying-mar-vs.-maar",
    "title": "Missing data lecture 9: MAR vs. MAAR (again) and Data Coarsening",
    "section": "",
    "text": "Let there be \\(n\\) units, of which we’re interested in measuring \\(K\\) variables. Let the \\(n \\times K\\) matrix of observations be denoted \\(Y\\), with elements \\(y_{ij}\\) while a realization of this matrix is called \\(\\tilde{y}\\), with elements \\(\\tilde{y}_{ij}\\). Let the matrix of missingness indicators be denoted \\(M\\), elements \\(m_{ij}\\), with a particular realization \\(\\tilde{m}\\), with elements \\(\\tilde{m}_{ij}\\). Let \\(Y_{(0)} = \\{y_{ij} \\mid m_{ij} = 0, i = 1, \\dots, n, j = 1, \\dots, K \\}\\). Let \\(Y_{(1)} = \\{y_{ij} \\mid m_{ij} = 1, i = 1, \\dots, n, j = 1, \\dots, K \\}\\). Let \\(\\mathcal{Y}_{(1)}\\) be the sample space of the missing values. Let a realization of these sets of variables be \\(\\tilde{y}_{(0)}\\) and \\(\\tilde{y}_{(1)}\\).\nThe joint likelihood of the observed data and the missingness indicators is:\n\\[\nL_\\text{full}(\\theta, \\phi \\mid \\tilde{y}_{(0)}, \\tilde{m}) = \\int_{\\mathcal{Y}_{(1)}} f_Y(\\tilde{y}_{(0)}, y_{(1)} \\mid \\theta) P(M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}, \\phi) dy_{(1)}\n\\]\nThe definition of MAR from the book is as follows:\n\nDefinition 1 (Missing-at-random) \\[\nf_{M \\mid Y} (M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}, \\phi) = f_{M \\mid Y} (M = \\tilde{m} \\mid Y_{(0)} = \\tilde{y}_{(0)}, Y_{(1)} = y_{(1)}^*, \\phi)\n\\] for all \\(y_{(1)}, y_{(1)}^*, \\phi\\) .\n\nThe definition of MAAR is:\n\nDefinition 2 (Missing-always-at-random) \\[\nf_{M \\mid Y} (M = m \\mid Y_{(0)} = y_{(0)}, Y_{(1)} = y_{(1)}, \\phi) = f_{M \\mid Y} (M = m \\mid Y_{(0)} = y_{(0)}, Y_{(1)} = y_{(1)}^*, \\phi)\n\\] for all \\(m, y_{(0)},y_{(1)}, y_{(1)}^*, \\phi\\).\n\n\nExample 1 (Example MAR vs. MAAR) The example given by Little (2021) is the following:\nSuppose we have observations \\((y_i, x_i)\\), where \\(y_i\\) is potentially missing and \\(x_i\\) is either \\(1\\) or \\(2\\), denoting group membership. Let \\(\\theta = (\\mu_1, \\mu_2, \\sigma^2)\\), and the model for the observations be \\[\ny_i \\mid x_i, \\theta \\sim \\text{Normal}(\\mu_{x_i}, \\sigma^2)\n\\] The standard confidence interval for the difference in means is: \\[\n\\bar{y}_2 - \\bar{y}_1 \\pm t_{\\nu, 0.975}(s\\sqrt{1/n_1 + 1/n2})\n\\] This also corresponds to the Bayesian credible interval when using a flat prior on \\(\\mu_1, \\mu_2\\) and \\(\\log \\sigma^2\\). Suppose the missingness mechanism is as follows: \\[\nP(m_i = 1 \\mid x_i, y_i, \\phi) =\n\\begin{cases}\n0 & x_i = 1\\\\\n0 & x_i = 2 \\text{ and } y_i \\leq \\phi\\\\\n1 & x_i = 2 \\text{ and } y_i &gt; \\phi\n\\end{cases}\n\\] That is, for group 2, if the observation is above an unknown cutoff value, the value is not recorded.\nSuppose that we have a dataset where there are no missing values. Then the data is MAR but not MAAR, because in repeated hypothetical samples there would be missing values that are MNAR. The Bayesian credible interval is still valid under MAR because we’re conditioning on the dataset we have, whereas the Frequentist interval isn’t valid because it couldn’t be repeated for datasets where \\(y_i\\) is missing for some group \\(2\\) observations."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-9-notes.html#unit-missingness",
    "href": "missing-data-material-W-26/notes/lecture-9-notes.html#unit-missingness",
    "title": "Missing data lecture 9: MAR vs. MAAR (again) and Data Coarsening",
    "section": "Unit missingness",
    "text": "Unit missingness\nWhen we have an assumption that observations and missingness for units can be considered conditionally independent given parameters \\(\\theta\\) and \\(\\phi\\), we get unit MAR instead of MAR.\nLet \\(Y_i\\) be the \\(i^\\mathrm{th}\\) row of the matrix \\(Y\\), or the length \\(K\\) random vector representing observations for unit \\(i\\), while \\(\\tilde{y}_i\\) is a particular realization of this random vector and \\(y_i\\) is a dummy vector. Similarly let \\(M_i\\) be the \\(i^\\mathrm{th}\\) row of the matrix \\(M\\), with particular realization \\(\\tilde{m}_i\\) and \\(m_i\\) a dummy vector. Furthermore, let \\(Y_{i(0)}, Y_{i(1)}\\) be the observed and missing random vectors of \\(y_i\\), while \\(\\tilde{y}_{i(0)}, \\tilde{y}_{i(1)}\\) are realizations of these vectors.\nThen the joint distribution of observations and missingness is \\[\nf_{Y, M}(y, M = m \\mid \\theta, \\phi) = \\prod_{i=1}^n f_{Y_i}(y_i \\mid \\theta) f_{M_i \\mid Y_i}(M_i = m_i \\mid Y_i =  y_i, \\phi)\n\\] For unit MAR, the condition becomes:\n\nDefinition 3 (Unit missing-at-random) \\[\nf_{M_i \\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{i(0)} = \\tilde{y}_{i(0)}, Y_{i(1)} = y_{i(1)}, \\phi) =\nf_{M_i \\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{i(0)} = \\tilde{y}_{i(0)}, Y_{i(1)} = y_{i(1)}^\\star, \\phi)\n\\] for all \\(y_{i(1)}, y_{i(1)}^\\star, \\phi\\) for all \\(i\\).\n\nNote the dependence on all \\(i\\) in Definition 3 vs. Definition 1."
  },
  {
    "objectID": "posts/higher-order-asymptotics.html",
    "href": "posts/higher-order-asymptotics.html",
    "title": "Higher-order expansion of MLE for multivariate parameters",
    "section": "",
    "text": "These notes are an expansion of § 5.1 in Severini (2000) to multivariate parameters \\(\\theta \\in \\R^p\\); § 5.3.2 does derive higher-order cumulants for \\(\\sqrt{n}(\\hat{\\psi} - \\psi^\\dagger)\\) in the presence of nuisance parameters \\(\\lambda\\), but the resulting approximations incur extra error due to the use of asymptotic expansions of the profile likelihood function, which is not a proper likelihood (at least not to first order).\n\n1 Higher-order Taylor series of score function\nLet \\(\\theta \\in \\R^p\\).\n\\[\n\\begin{aligned}\n\\ell_\\theta(\\hat{\\theta}_n)_j & = \\ell_\\theta(\\theta^\\dagger)_j + \\ell_{\\theta\\theta}(\\theta^\\dagger)_j(\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j(\\hat{\\theta}_n - \\theta^\\dagger) \\\\\n& \\quad + \\frac{1}{6}\\sum_{ikm} (\\ell_{\\theta\\theta\\theta\\theta}(\\tilde{\\theta}_{nj})_j)_{ikm}(\\hat{\\theta}_{ni} - \\theta^\\dagger_i)(\\hat{\\theta}_{nk} - \\theta^\\dagger_k)(\\hat{\\theta}_{nm} - \\theta^\\dagger_m)  \n\\end{aligned}\n\\] where \\[\n\\tilde{\\theta}_{nj} = \\hat{\\theta}_{n} \\pi_j + (1 - \\pi_j) \\theta^\\dagger, \\pi_j \\in (0, 1)\n\\]\nWe assume standard regularity conditions set out in lecture 8. Under these conditions it can be shown that the last term is \\(O_p(n^{-1/2})\\), so we can rewrite this as \\[\n\\begin{aligned}\n\\ell_\\theta(\\hat{\\theta}_n)_j & = \\ell_\\theta(\\theta^\\dagger)_j + \\ell_{\\theta\\theta}(\\theta^\\dagger)_j(\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j(\\hat{\\theta}_n - \\theta^\\dagger) + O_p(n^{-1/2})\n\\end{aligned}\n\\] Multiply each side by \\(\\frac{\\sqrt{n}}{n}\\): \\[\n\\begin{aligned}\n0  & = \\sqrt{n}\\frac{\\ell_\\theta(\\theta^\\dagger)_j}{n} + \\frac{\\ell_{\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) / \\sqrt{n} + O_p(n^{-1})\n\\end{aligned}\n\\] To make the above more amenable to matrix algebra, let’s use the trace trick and some properties of the vec operator to rewrite the quadratic term. Note that the matrix \\(\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j\\) is symmetric for each \\(j\\). \\[\n\\begin{aligned}\n\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) & = \\tr(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)) \\\\\n& = \\tr(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T) \\\\\n& = \\tr(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)^T_j}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T) \\\\\n& = \\asvec(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n})^T\\asvec(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T) \\\\\n\\end{aligned}\n\\] This will allow us to write our system of equations in matrix form with the matrix \\(D\\) as: \\[\nD =\n\\begin{bmatrix}\n\\asvec(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_1}{n})^T \\\\\n\\asvec(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_2}{n})^T \\\\\n\\vdots \\\\\n\\asvec(\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_p}{n})^T\n\\end{bmatrix}\n\\] Then we write the system of equations: \\[\n0  = \\sqrt{n}\\frac{\\ell_\\theta(\\theta^\\dagger)}{n} + \\frac{\\ell_{\\theta\\theta}(\\theta^\\dagger)}{n}\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) + \\frac{1}{2}D\\asvec(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)^T) / \\sqrt{n} + O_p(n^{-1})\n\\tag{1}\\]\nWe’ll assume the following asymptotic representations of the sample means of the matrices of derivatives, where \\(Z_j = O_p(1)\\), \\(Z_{1j} \\in \\R, Z_{2j} \\in \\R^p, Z_{3j} \\in \\R^{p\\times p}\\) .\n\\[\n\\begin{aligned}\n\\frac{\\ell_\\theta(\\theta^\\dagger)_j}{n} & = \\frac{Z_{1j}}{\\sqrt{n}} \\\\\n\\frac{\\ell_{\\theta\\theta}(\\theta^\\dagger)_j}{n} & = -\\mathcal{I}_j(\\theta^\\dagger) + \\frac{Z_{2j}}{\\sqrt{n}} \\\\\n\\frac{\\ell_{\\theta\\theta\\theta}(\\theta^\\dagger)_j}{n} & = V_j + \\frac{Z_{3j}}{\\sqrt{n}} \\\\\n\\end{aligned}\n\\]\nFinally, we would like to find an asymptotic expansion of the form: \\[\n\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) = B + C / \\sqrt{n} + O_p(n^{-1})\n\\]\nFirst, we substitute the asymptotic representations for the likelihood derivatives into Equation 1 and keep track of the orders. In order to economize on notation, let \\(X = \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\):\n\\[\n\\begin{aligned}\n& Z_1 + (-\\mathcal{I}(\\theta^\\dagger) + \\frac{Z_{2}}{\\sqrt{n}})X + \\frac{1}{2}\n\\begin{bmatrix}\n\\asvec\\lp V_1 + \\frac{Z_{31}}{\\sqrt{n}}\\rp \\\\\n\\asvec\\lp V_2 + \\frac{Z_{32}}{\\sqrt{n}}\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p + \\frac{Z_{3p}}{\\sqrt{n}}\\rp \\\\\n\\end{bmatrix}\n\\asvec(XX^T) / \\sqrt{n} + O_p(n^{-1})  \\\\\n\\end{aligned}\n\\] Then we plug in \\(X = B + C / \\sqrt{n})\\). We can ignore the \\(O_p(n^{-1})\\) term because everything that multiplies \\(X\\) is at most \\(O_p(1)\\), so the terms multiplied by \\(O_p(n^{-1})\\) get relegated to the \\(O_p(n^{-1})\\) term:\n\\[\n\\begin{aligned}\n& Z_1 + (-\\mathcal{I}(\\theta^\\dagger) + \\frac{Z_{2}}{\\sqrt{n}})(B + C / \\sqrt{n}) \\\\\n& \\quad + \\frac{1}{2}\n\\begin{bmatrix}\n\\asvec\\lp V_1 + \\frac{Z_{31}}{\\sqrt{n}}\\rp \\\\\n\\asvec\\lp V_2 + \\frac{Z_{32}}{\\sqrt{n}}\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p + \\frac{Z_{3p}}{\\sqrt{n}}\\rp \\\\\n\\end{bmatrix}\n\\asvec(((B + C / \\sqrt{n})((B + C / \\sqrt{n})^T) / \\sqrt{n} + O_p(n^{-1})  \\\\\n\\end{aligned}\n\\]\nRearranging and grouping by orders of \\(n\\) gives:\n\\[\n\\begin{align}\n& Z_1 -\\mathcal{I}(\\theta^\\dagger) B + \\frac{Z_{2}}{\\sqrt{n}} B  -\\frac{\\mathcal{I}(\\theta^\\dagger) C}{\\sqrt{n}} + \\frac{Z_2 C}{n} \\tag{A}\\\\\n& \\quad + \\frac{1}{2}\n\\begin{bmatrix}\n\\asvec\\lp V_1\\rp \\\\\n\\asvec\\lp V_2\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p\\rp \\\\\n\\end{bmatrix}\n\\asvec(((B + C / \\sqrt{n})((B + C / \\sqrt{n})^T) / \\sqrt{n}  \\tag{B}\\\\\n& \\quad + \\frac{1}{2}\n\\begin{bmatrix}\n\\asvec\\lp\\frac{Z_{31}}{\\sqrt{n}}\\rp \\\\\n\\asvec\\lp\\frac{Z_{32}}{\\sqrt{n}}\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp\\frac{Z_{3p}}{\\sqrt{n}}\\rp \\\\\n\\end{bmatrix}\n\\asvec(((B + C / \\sqrt{n})((B + C / \\sqrt{n})^T) / \\sqrt{n} + O_p(n^{-1}) \\tag{C} \\\\\n\\end{align}\n\\] Before going further, we can see some simplifications. The first \\(Z_2 C / n\\) term in line (A) is \\(O_p(n^{-1})\\) so we can ignore it as we move forward. Further, in line (B), we can ignore the terms involving \\(C/\\sqrt{n}\\) because distributing the \\(n^{-1/2}\\) at the end of the line will move each term to \\(O_p{n^{-1}}\\). Finally, the entirety of line (C) is \\(O_p(n^{-1})\\) because the left-hand matrix is \\(O_p(n^{-1/2})\\) and will move to \\(O_p(n^{-1})\\) after we multiply by the right-hand \\(n^{-1/2}\\).\nThis yields, after collecting terms together: \\[\n\\begin{align}\n& Z_1 -\\mathcal{I}(\\theta^\\dagger) B + \\frac{1}{\\sqrt{n}}\\lp Z_{2}B  -\\mathcal{I}(\\theta^\\dagger) C\n+ \\frac{1}{2}\n\\begin{bmatrix}\n\\asvec\\lp V_1\\rp \\\\\n\\asvec\\lp V_2\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p\\rp \\\\\n\\end{bmatrix}\n\\asvec(B B^T) \\rp  + O_p(n^{-1})\n\\end{align}\n\\] Setting each coefficient for the order equal to zero and solving for the unknown \\(B,C\\) will give the higher-order expansion. \\[\nB = \\mathcal{I}^{-1}(\\theta^\\dagger)Z_1\n\\]\n\\[\n\\begin{aligned}\n0 & = Z_{2}B  -\\mathcal{I}(\\theta^\\dagger) C\n+ \\frac{1}{2}\n\\begin{bmatrix}\n\\asvec\\lp V_1\\rp \\\\\n\\asvec\\lp V_2\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p\\rp \\\\\n\\end{bmatrix}\n\\asvec(B B^T) \\\\\n& =  Z_{2}\\mathcal{I}^{-1}(\\theta^\\dagger)Z_1  -\\mathcal{I}(\\theta^\\dagger) C\n+ \\frac{1}{2}\n\\begin{bmatrix}\n\\asvec\\lp V_1\\rp \\\\\n\\asvec\\lp V_2\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p\\rp \\\\\n\\end{bmatrix}\n\\asvec((\\mathcal{I}^{-1}(\\theta^\\dagger)Z_1Z_1^T \\mathcal{I}^{-1}(\\theta^\\dagger))\n\\end{aligned}\n\\] This gives: \\[\nC = \\mathcal{I}^{-1}(\\theta^\\dagger)Z_{2}\\mathcal{I}^{-1}(\\theta^\\dagger)Z_1 + \\frac{1}{2}\\mathcal{I}^{-1}(\\theta^\\dagger)\n\\begin{bmatrix}\n\\asvec\\lp V_1\\rp \\\\\n\\asvec\\lp V_2\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p\\rp \\\\\n\\end{bmatrix}\n\\asvec((\\mathcal{I}^{-1}(\\theta^\\dagger)Z_1Z_1^T \\mathcal{I}^{-1}(\\theta^\\dagger))\n\\] Finally, we get the asymptotic expansion for \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\) to \\(O_p(n^{-1})\\):\n\\[\n\\begin{aligned}\n\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) & = \\mathcal{I}^{-1}(\\theta^\\dagger)Z_1 \\\\\n& + \\frac{1}{\\sqrt{n}}\\mathcal{I}^{-1}(\\theta^\\dagger)Z_{2}\\mathcal{I}^{-1}(\\theta^\\dagger)Z_1 \\\\\n& + \\frac{1}{2\\sqrt{n}}\\mathcal{I}^{-1}(\\theta^\\dagger)\n\\begin{bmatrix}\n\\asvec\\lp V_1\\rp \\\\\n\\asvec\\lp V_2\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p\\rp \\\\\n\\end{bmatrix}\n\\asvec((\\mathcal{I}^{-1}(\\theta^\\dagger)Z_1Z_1^T \\mathcal{I}^{-1}(\\theta^\\dagger)) + O_p(n^{-1})\n\\end{aligned}\n\\] In order to get the asymptotic bias for \\(\\hat{\\theta}_n\\), we need to take the expectation of the RHS.\nIn the expression above, we have that \\[\n\\begin{aligned}\nZ_1 & = \\ell_{\\theta}(\\theta^\\dagger) / \\sqrt{n} \\\\\nZ_2 & = (\\ell_{\\theta\\theta}(\\theta^\\dagger) - \\ExpA{\\ell_{\\theta\\theta}(\\theta^\\dagger)}{\\theta^\\dagger}) / \\sqrt{n}\n\\end{aligned}\n\\] Let’s vectorize \\(Z_2\\) to get a better handle on it’s asymptotic distribution: \\[\n\\begin{aligned}\n\\asvec(Z_2) & = \\sqrt{n}\\frac{1}{n} \\asvec\\lp (\\ell_{\\theta\\theta}(\\theta^\\dagger) - \\ExpA{\\ell_{\\theta\\theta}(\\theta^\\dagger)}{\\theta^\\dagger})\\rp\n\\end{aligned}\n\\]\nBy the multivariate CLT, we get the following limiting distribution: \\[\n\\begin{align}\n\\asvec(Z_2) & \\overset{d}{\\to} \\mathcal{N}(0, \\Sigma) \\\\\n\\Sigma & = \\Exp{\\asvec\\lp (\\ell_{\\theta\\theta}(\\theta^\\dagger) - \\ExpA{\\ell_{\\theta\\theta}(\\theta^\\dagger)}{\\theta^\\dagger})\\rp\\asvec\\lp (\\ell_{\\theta\\theta}(\\theta^\\dagger) - \\ExpA{\\ell_{\\theta\\theta}(\\theta^\\dagger)}{\\theta^\\dagger})\\rp^T}\n\\end{align}\n\\]\nOf course, both of these have mean zero, so our asymptotic bias can be derived from \\[\n\\begin{aligned}\n\\Exp{\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)}  & = \\frac{1}{\\sqrt{n}}\\mathcal{I}^{-1}(\\theta^\\dagger)\\Exp{Z_{2}\\mathcal{I}^{-1}(\\theta^\\dagger)Z_1} \\\\\n& + \\frac{1}{2\\sqrt{n}}\\mathcal{I}^{-1}(\\theta^\\dagger)\n\\begin{bmatrix}\n\\asvec\\lp V_1\\rp \\\\\n\\asvec\\lp V_2\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p\\rp \\\\\n\\end{bmatrix}\n\\asvec((\\mathcal{I}^{-1}(\\theta^\\dagger)\\Exp{Z_1Z_1^T} \\mathcal{I}^{-1}(\\theta^\\dagger)) + O_p(n^{-1})\n\\end{aligned}\n\\]\nGiven that \\(Z_1 \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger))\\), we have that \\[\n\\begin{aligned}\n\\Exp{Z_1 Z_1^T} & = \\mathcal{I}(\\theta^\\dagger)\n\\end{aligned}\n\\] Leading to the simplified\n\\[\n\\begin{aligned}\n\\Exp{\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)}  & = \\frac{1}{\\sqrt{n}}\\mathcal{I}^{-1}(\\theta^\\dagger)\\Exp{Z_{2}\\mathcal{I}^{-1}(\\theta^\\dagger)Z_1} \\\\\n& + \\frac{1}{2\\sqrt{n}}\\mathcal{I}^{-1}(\\theta^\\dagger)\n\\begin{bmatrix}\n\\asvec\\lp V_1\\rp \\\\\n\\asvec\\lp V_2\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p\\rp \\\\\n\\end{bmatrix}\n\\asvec(\\mathcal{I}^{-1}(\\theta^\\dagger)) + O_p(n^{-1})\n\\end{aligned}\n\\]\nWriting out \\(Z_2 I^{-1} Z_1\\) gives:\n\\[\n\\begin{aligned}\n& (\\ell_{\\theta\\theta}(\\theta^\\dagger) - \\ExpA{\\ell_{\\theta\\theta}(\\theta^\\dagger)}{\\theta^\\dagger}) / \\sqrt{n} \\mathcal{I}^{-1}(\\theta^\\dagger) \\ell_{\\theta}(\\theta^\\dagger) / \\sqrt{n} \\\\\n& =(\\ell_{\\theta\\theta}(\\theta^\\dagger)\\mathcal{I}^{-1}(\\theta^\\dagger)\\ell_{\\theta}(\\theta^\\dagger) / n + \\ell_{\\theta}(\\theta^\\dagger) / n\n\\end{aligned}\n\\]\nWhich leads to calculating the following expectation, noting that \\(\\Exp{\\ell_\\theta(\\theta^\\dagger)} = 0\\):\n\\[\n\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger)\\mathcal{I}^{-1}(\\theta^\\dagger)\\ell_{\\theta}(\\theta^\\dagger)} / n\n\\] We can rewrite this in terms of a Kronecker product:\n\\[\n\\Exp{\\ell_{\\theta}(\\theta^\\dagger)^T \\otimes \\ell_{\\theta\\theta}(\\theta^\\dagger)}\\asvec(\\mathcal{I}^{-1}(\\theta^\\dagger)) / n\n\\]\nThis yields a somewhat tidy expression for the bias correction:\n\\[\n\\begin{aligned}\n\\Exp{\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)}  & = \\frac{1}{\\sqrt{n}}\\mathcal{I}^{-1}(\\theta^\\dagger) \\Exp{\\ell_{\\theta}(\\theta^\\dagger)^T \\otimes \\ell_{\\theta\\theta}(\\theta^\\dagger)}\\asvec(\\mathcal{I}^{-1}(\\theta^\\dagger)) / n\\\\\n& + \\frac{1}{2\\sqrt{n}}\\mathcal{I}^{-1}(\\theta^\\dagger)\n\\begin{bmatrix}\n\\asvec\\lp V_1\\rp \\\\\n\\asvec\\lp V_2\\rp \\\\\n\\vdots  \\\\\n\\asvec\\lp V_p\\rp \\\\\n\\end{bmatrix}\n\\asvec(\\mathcal{I}^{-1}(\\theta^\\dagger)) + O_p(n^{-1})\n\\end{aligned}\n\\]\n\n\n\n\n\nReferences\n\nSeverini, Thomas A. 2000. Likelihood Methods in Statistics. Oxford University Press."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Random thoughts",
    "section": "",
    "text": "Higher order asymptotics for MLEs"
  },
  {
    "objectID": "blog.html#posts",
    "href": "blog.html#posts",
    "title": "Random thoughts",
    "section": "",
    "text": "Higher order asymptotics for MLEs"
  },
  {
    "objectID": "survival-material/lecture-10.html",
    "href": "survival-material/lecture-10.html",
    "title": "Lecture 10",
    "section": "",
    "text": "Information is from (Collett 1994), (Harrell et al. 2001), (O. O. Aalen 1988), (O. Aalen, Borgan, and Gjessing 2008)."
  },
  {
    "objectID": "survival-material/lecture-10.html#parametric-proportional-hazards-models",
    "href": "survival-material/lecture-10.html#parametric-proportional-hazards-models",
    "title": "Lecture 10",
    "section": "2.1 Parametric proportional hazards models",
    "text": "2.1 Parametric proportional hazards models\nRecall our definition of proportional hazards employing an exponential function with \\(\\mathbf{z}_i \\in \\R^k\\): \\[\\begin{align}\n    \\lambda(t \\mid \\mathbf{z}_i) = \\lambda_0(t \\mid \\boldsymbol{\\theta}) \\exp(\\boldsymbol{\\beta}^T \\mathbf{z}_i)\n\\end{align}\\] This implies the following properties for our model: \\[\\begin{align}\n   \\log \\lambda(t \\mid \\mathbf{z}_i) & = \\log \\lambda_0(t \\mid \\boldsymbol{\\theta}) + \\boldsymbol{\\beta}^T \\mathbf{z}_i \\\\\n   \\log \\Lambda(t \\mid \\mathbf{z}_i) & = \\log \\Lambda_0(t \\mid \\boldsymbol{\\theta}) + \\boldsymbol{\\beta}^T \\mathbf{z}_i\n\\end{align}\\] This means that the predictors act linearly on the log scale for both the hazard ratio and the cumulative hazard, and that the effect of the predictors is constant over time.\nThe interpretation of coefficients is as the change in the log hazard, or log cumulative hazard: \\[\\beta_j = \\log \\lambda(t\\mid z_1, \\dots z_{j-1}, z_j + 1, z_{j+1}, \\dots z_k) - \\log \\lambda(t\\mid z_1, \\dots z_{j-1}, z_j, z_{j+1}, \\dots z_k).\\] Alternatively, we have \\[e^{\\beta_j} =  \\frac{\\lambda(t\\mid z_1, \\dots z_{j-1}, z_j + 1, z_{j+1}, \\dots z_k)}{\\lambda(t\\mid z_1, \\dots z_{j-1}, z_j, z_{j+1}, \\dots z_k)}.\\] Increasing \\(z_j\\) by \\(1\\) has the effect of increasing the hazard of an event by \\(e^{\\beta_j}\\).\nAs discussed previously and shown in [fig:prop-hazard], When we have a single categorical predictor, we can assess the validity of proportional hazards by plotting the \\(\\log(-\\log)\\) of the KM estimate of survival within each subgroup, and determining if the lines are roughly linear in \\(\\log t\\) and if they are parallel. If they are not parallel, but are straight, this may be an indication that one could fit separate the groups with separate shape, or \\(\\alpha\\), parameters."
  },
  {
    "objectID": "survival-material/lecture-10.html#testing-for-proportional-hazards",
    "href": "survival-material/lecture-10.html#testing-for-proportional-hazards",
    "title": "Lecture 10",
    "section": "2.2 Testing for proportional hazards",
    "text": "2.2 Testing for proportional hazards\nFollowing (Collett 1994), in the Weibull model we may test the proportional hazards assumption by fitting a more flexible model and using a composite likelihood ratio test. Suppose we have patients categorized into 3 age groups, and we use dummy coding for our design matrix: \\[\\begin{alignat*}\n{2}\n& \\text{{\\bf Group}} \\quad && \\text{{\\bf Predictors}}\\\\\n& \\text{Youngest group} \\quad  && \\mathbf{z}_i = (0, 0)^T \\\\\n& \\text{Middle group} \\quad  && \\mathbf{z}_i  = (1, 0)^T \\\\\n& \\text{Oldest group} \\quad  && \\mathbf{z}_i  = (0, 1)^T\n\\end{alignat*}\\] and we want to test whether fitting the following proportional hazards Weibull regression model: \\[X_i \\sim \\text{Weibull}(\\gamma e^{\\boldsymbol{\\beta}^T \\mathbf{z}_i},\\alpha)\\] is sufficient. An alternative model that allows for hazards that are not proportional is \\[X_i \\sim \\text{Weibull}(\\gamma e^{\\boldsymbol{\\beta}^T \\mathbf{z}_i},\\alpha e^{\\boldsymbol{\\theta}^T \\mathbf{z}_i})\\] Note that this alternative model is equivalent to fitting separate Weibull models to each group. Then the null hypothesis we’d like to test is whether \\(\\boldsymbol{\\theta}_1 = \\boldsymbol{\\theta}_2 = 0\\). We can use the composite likelihood ratio test to determine whether the data contradict this null hypothesis. The test statistic would be distributed as \\(\\chi^2_2\\) given the constraints in the null hypothesis.\nThe test statistic in the case where we fit separate models to each subgroup is \\[2(\\ell_1(\\hat{\\psi}_1,\\hat{\\phi}_1) + \\ell_2(\\hat{\\psi}_2,\\hat{\\phi}_2) + \\ell_3(\\hat{\\psi}_3,\\hat{\\phi}_3) - \\ell(\\psi_0,\\hat{\\phi}(\\psi_0)))\\] where \\(\\ell_j(\\hat{\\psi}_j,\\hat{\\phi}_j), j = 1,2,3\\) is the log-likelihood from the fitted Weibull model to each age group."
  },
  {
    "objectID": "survival-material/lecture-10.html#accelerated-failure-time-formulation",
    "href": "survival-material/lecture-10.html#accelerated-failure-time-formulation",
    "title": "Lecture 10",
    "section": "2.3 Accelerated failure time formulation",
    "text": "2.3 Accelerated failure time formulation\nThere is an alternative way to specify the Weibull model, wherein we model the log of the survival times as being a linear function of covariates. \\[\\log (X_i) = \\mu + \\mathbf{z}_i^T \\boldsymbol{\\eta} + \\sigma \\epsilon_i\\] Let \\(\\epsilon_i\\) be Gumbel distributed with a probability density function \\[f(\\epsilon) = \\exp(\\epsilon - e^\\epsilon)\\] If we let \\(\\nu = e^\\epsilon\\), then we can compute the density over \\(\\nu\\). \\(\\epsilon(\\nu) = \\log(\\nu)\\). \\(f(\\nu) = f(\\epsilon(\\nu)) \\frac{d}{d\\nu} \\epsilon(\\nu)\\) \\[\\begin{align}\n    \\exp(\\log(\\nu) - e^{\\log(\\nu)}) / \\nu = e^{-\\nu}\n\\end{align}\\] This shows that \\(e^{\\epsilon} \\sim \\text{Exponential}(1)\\). Now we can write the survival function of \\(X_i\\): \\[\\begin{align*}\n   S(t) & = P(X_i &gt; t) \\\\\n   & = P(\\log(X_i) &gt; \\log(t)) \\\\\n   & = P(\\mu + \\mathbf{z}_i^T \\boldsymbol{\\eta} + \\sigma \\epsilon_i &gt; \\log(t)) \\\\\n   & = P(\\epsilon_i &gt; (\\log(t) - \\mu - \\mathbf{z}_i^T \\boldsymbol{\\eta})/\\sigma) \\\\\n   & = P(e^{\\epsilon_i} &gt; \\exp(\\log(t) - \\mu - \\mathbf{z}_i^T \\boldsymbol{\\eta})^{1/\\sigma}) \\\\\n   & = \\exp\\left(-\\exp(\\log(t) - \\mu - \\mathbf{z}_i^T \\boldsymbol{\\eta})^{1/\\sigma}\\right)\\\\\n   & = \\exp\\left(-e^{-\\mu/\\sigma} t^{1/\\sigma}  \\exp(\\mathbf{z}_i^T (-\\boldsymbol{\\eta}/\\sigma)) \\right)\n\\end{align*}\\] Recall the survival function of a Weibull with hazard function \\(\\gamma \\alpha t^{\\alpha - 1} \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\): \\[S(t) = \\exp(-\\gamma t^\\alpha\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})).\\] Then there are the following correspondences between our parameters for the log-linear model and the original proportional hazards model: \\[\\begin{align*}\n    \\alpha & = \\frac{1}{\\sigma} \\\\\n    \\gamma & = e^{-\\mu / \\sigma} \\\\\n    \\boldsymbol{\\beta} & = -\\boldsymbol{\\eta} / \\sigma\n\\end{align*}\\]\nIn general, the correspondence between the model for the log-failure time and the proportional hazards will not hold, but it does in the Weibull model."
  },
  {
    "objectID": "survival-material/lecture-10.html#model-checking-in-aft-models",
    "href": "survival-material/lecture-10.html#model-checking-in-aft-models",
    "title": "Lecture 10",
    "section": "3.1 Model checking in AFT models",
    "text": "3.1 Model checking in AFT models\nThe relationships that held for the Weibull regressions can be ported to other AFT models. (Klein, Moeschberger, et al. 2003) suggest checking a function of the cumulative hazard against a function of \\(t\\) to assess adequacy of model fit. We can use the (tie-corrected) Nelson-Aalen estimator of the cumulative hazard function: \\[\\begin{align*}\n   \\hat{\\Lambda}^\\text{NA}(t) = \\sum_{i \\mid t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t)}\n\\end{align*}\\] and examine transformations thereof against appropriate transformations of \\(t\\).\nFor the log-logistic model, \\(\\Lambda(t) = \\log(1 + \\lambda t^\\alpha)\\). This implies that \\[\\log(\\exp(\\hat{\\Lambda}^\\text{NA}(t)) - 1) \\approx \\log \\lambda + \\alpha \\log t\\] We can compute similar expressions for the Weibull and the log-normal model."
  },
  {
    "objectID": "survival-material/lecture-10.html#cox-snell-residuals",
    "href": "survival-material/lecture-10.html#cox-snell-residuals",
    "title": "Lecture 10",
    "section": "3.2 Cox-Snell residuals",
    "text": "3.2 Cox-Snell residuals\nRecall from [sec:cumu-haz] that the following relationship holds: When \\(X_i \\sim F\\) with cumulative hazard function \\(\\Lambda(t)\\) \\[\\Lambda(X_i) \\sim \\text{Exp}(1).\\] We can use this idea to generate graphical checks for our models.\nContinuing with the log-logistic model, we could graphically assess whether the following Cox-Snell residual, denoted \\(r^C_i\\): \\[r^C_i = \\log(1 + e^{\\mathbf{z}_i^T \\hat{\\boldsymbol{\\theta}}} \\hat{\\lambda} t_i^{\\hat{\\alpha}})\\] is exponentially distributed with unit rate. The issue with plotting these residuals directly against the quantiles of an exponential distribution is that for the censored observations, \\(\\Lambda(C_i)\\) won’t be exponentially distributed. But we can use the properties of the cumulative hazard function to our advantage, namely that it is nondecreasing in \\(t\\). Thus for censored observations where \\(t_i = c_i\\), this implies that \\(x_i \\geq t_i\\). Thus, \\(\\Lambda(t_i) \\leq \\Lambda(x_i)\\), so we can say that when \\(\\delta_i = 0\\), \\(\\Lambda(x_i)\\) is censored at \\(\\Lambda(t_i)\\).\nThe solution is to use the Kaplan-Meier estimator again! We can form the censored cumulative hazard sample: \\[\\begin{align}\n    \\{(\\tilde{t}_i = \\min(\\Lambda(x_i), \\Lambda(c_i))&, \\delta_i = \\mathbbm{1}\\left(x_i \\leq c_i\\right)), i = 1, \\dots, n\\} =  \\\\\n   & \\{(\\tilde{t}_i = \\Lambda(t_i), \\delta_i = \\mathbbm{1}\\left(x_i \\leq c_i\\right)), i = 1, \\dots, n\\}\n\\end{align} \\tag{2}\\] where the second line follows from the nondecreasing characteristic of \\(\\Lambda(t)\\).\nThen we can fit the Kaplan Meier estimator to the dataset \\((\\tilde{t}_i, \\delta_i)\\) observations to infer the non-censored distribution of \\(\\Lambda(x_i)\\). The procedure is as outlined below:\n\nFit a parametric survival model to \\(\\{(t_i, \\delta_i, \\mathbf{z}_i), i = 1, \\dots, n\\}\\)\nCalculate the Cox-Snell residuals using the estimated survival model: \\(\\{(\\tilde{t}_i = \\hat{\\Lambda}(t_i), \\delta_i = \\mathbbm{1}\\left(x_i \\leq c_i\\right)), i = 1, \\dots, n\\}\\)\nFit a Kaplan-Meier estimator to the datatset comprising observations of Equation 2\nPlot the \\(\\log(-\\log(\\hat{S}^\\text{KM}(t)))\\) vs. \\(\\log t\\) to see whether a line with zero intercept and slope \\(1\\) fits in the confidence intervals"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-10-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-10-notes.html",
    "title": "Missing data lecture 10: Flawed approach to missing data and EM",
    "section": "",
    "text": "One generally flawed approach to inference in missing data problems is to treat the missing values as unknown parameters and to maximize the following function of parameters and missing values: \\[\nL_{\\text{mispar}}(\\theta, y_{(1)} \\mid y_{(0)}) = f_{Y}(y_{(1)}, y_{(0)} \\mid \\theta)\n\\] The MLE using this distribution would require you to jointly maximize the likelihood with respect to \\(\\theta\\) and \\(y_{(1)}\\). Let’s take this approach with the censored exponential samples and see what we get. We had that the likelihood was\n\\[\n\\prod_{i=1}^r \\theta^{-1} e^{-y_i / \\theta} \\ind{y_i &lt; c} \\prod_{i={r+1}}^n \\theta^{-1} e^{-y_i / \\theta} \\ind{y_i \\geq c} f(y \\mid \\theta)\n\\] Because \\(e^{-y_i/\\theta}\\) is monotonically decreasing in \\(y_i\\) for any \\(\\theta\\), \\(\\hat{y}_i\\) is \\(c\\) for the missing observations.\nPlugging this into the log-likelihood gives \\[\n-r \\log\\theta -(n-r) \\log \\theta -\\frac{\\sum_{i=1}^r y_i + (n - r)c}{\\theta}\n\\] Taking derivatives and setting this equal to zero gives: \\[\n\\hat{\\theta}_{\\text{mispar}} = \\frac{\\sum_{i=1}^r y_i + (n - r)c}{n}  = \\frac{r}{n}\\hat{\\theta}\n\\] This understates the true value \\(\\theta\\), and one can show that this estimator isn’t consistent for \\(\\theta\\). The only way this estimator is consistent for \\(\\theta\\) is if \\(r / n \\to 1\\).\nThis example shows that the goal in missing data analysis isn’t to predict missing values, it is to account for the uncertainty in missing values by integrating over the distribution of missing values.\nHere is another example.\nLet \\(y_i\\) be normally distributed with unknown mean \\(\\mu\\) and variance \\(\\sigma^2\\). Suppose there are \\(r\\) observed values and \\(n - r\\) missing values. We assume that the data are MAR and the parameters \\(\\mu\\) and \\(\\sigma^2\\) are distinct from the parameters of the missingness distribution.\nThe MLE from the ignorable likelihood is just the MLE on the complete cases: \\[\n\\hat{\\mu} = \\sum_{i=1}^r \\frac{y_i}{r}, \\hat{\\sigma}^2 = \\sum_{i=1}^r \\frac{(y_i - \\hat{\\mu}^2)}{r}\n\\] If we write down the mistaken likelihood for the data we get:\n\\[\n\\ell_{\\text{mispar}}(\\mu, \\sigma^2, y_{r+1},\\dots, y_n \\mid y_{(0)}) =\n-\\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^r (y_i - \\mu)^2 - \\frac{1}{2\\sigma^2} \\sum_{i=r+1}^n (y_i - \\mu)^2\n\\] We can maximize this by setting \\(y_{r+1}, \\dots, y_n\\) to \\(\\mu\\), thereby eliminating the second sum. This leads to an MLE for \\(\\mu\\) that is equal to \\(\\hat{\\mu}\\) above. The variance, however, is incorrectly estimated. Taking gradients and solving for \\(\\sigma^2\\) gives \\[\n\\hat{\\sigma}^2 = \\sum_{i=1}^r \\frac{(y_i - \\hat{\\mu})^2}{n}\n\\] Why is this wrong? Intuitively, we can see that the expected value of the second term isn’t zero, and thus we’ll understate the variance Thus, the MLE in the misparametrized model yields a variance estimate that is too low.\nAgain, we want to integrate over our uncertainty in the missing values, rather than predict missing values."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-10-notes.html#flawed-approach-to-missing-data",
    "href": "missing-data-material-W-26/notes/lecture-10-notes.html#flawed-approach-to-missing-data",
    "title": "Missing data lecture 10: Flawed approach to missing data and EM",
    "section": "",
    "text": "One generally flawed approach to inference in missing data problems is to treat the missing values as unknown parameters and to maximize the following function of parameters and missing values: \\[\nL_{\\text{mispar}}(\\theta, y_{(1)} \\mid y_{(0)}) = f_{Y}(y_{(1)}, y_{(0)} \\mid \\theta)\n\\] The MLE using this distribution would require you to jointly maximize the likelihood with respect to \\(\\theta\\) and \\(y_{(1)}\\). Let’s take this approach with the censored exponential samples and see what we get. We had that the likelihood was\n\\[\n\\prod_{i=1}^r \\theta^{-1} e^{-y_i / \\theta} \\ind{y_i &lt; c} \\prod_{i={r+1}}^n \\theta^{-1} e^{-y_i / \\theta} \\ind{y_i \\geq c} f(y \\mid \\theta)\n\\] Because \\(e^{-y_i/\\theta}\\) is monotonically decreasing in \\(y_i\\) for any \\(\\theta\\), \\(\\hat{y}_i\\) is \\(c\\) for the missing observations.\nPlugging this into the log-likelihood gives \\[\n-r \\log\\theta -(n-r) \\log \\theta -\\frac{\\sum_{i=1}^r y_i + (n - r)c}{\\theta}\n\\] Taking derivatives and setting this equal to zero gives: \\[\n\\hat{\\theta}_{\\text{mispar}} = \\frac{\\sum_{i=1}^r y_i + (n - r)c}{n}  = \\frac{r}{n}\\hat{\\theta}\n\\] This understates the true value \\(\\theta\\), and one can show that this estimator isn’t consistent for \\(\\theta\\). The only way this estimator is consistent for \\(\\theta\\) is if \\(r / n \\to 1\\).\nThis example shows that the goal in missing data analysis isn’t to predict missing values, it is to account for the uncertainty in missing values by integrating over the distribution of missing values.\nHere is another example.\nLet \\(y_i\\) be normally distributed with unknown mean \\(\\mu\\) and variance \\(\\sigma^2\\). Suppose there are \\(r\\) observed values and \\(n - r\\) missing values. We assume that the data are MAR and the parameters \\(\\mu\\) and \\(\\sigma^2\\) are distinct from the parameters of the missingness distribution.\nThe MLE from the ignorable likelihood is just the MLE on the complete cases: \\[\n\\hat{\\mu} = \\sum_{i=1}^r \\frac{y_i}{r}, \\hat{\\sigma}^2 = \\sum_{i=1}^r \\frac{(y_i - \\hat{\\mu}^2)}{r}\n\\] If we write down the mistaken likelihood for the data we get:\n\\[\n\\ell_{\\text{mispar}}(\\mu, \\sigma^2, y_{r+1},\\dots, y_n \\mid y_{(0)}) =\n-\\frac{n}{2} \\log \\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^r (y_i - \\mu)^2 - \\frac{1}{2\\sigma^2} \\sum_{i=r+1}^n (y_i - \\mu)^2\n\\] We can maximize this by setting \\(y_{r+1}, \\dots, y_n\\) to \\(\\mu\\), thereby eliminating the second sum. This leads to an MLE for \\(\\mu\\) that is equal to \\(\\hat{\\mu}\\) above. The variance, however, is incorrectly estimated. Taking gradients and solving for \\(\\sigma^2\\) gives \\[\n\\hat{\\sigma}^2 = \\sum_{i=1}^r \\frac{(y_i - \\hat{\\mu})^2}{n}\n\\] Why is this wrong? Intuitively, we can see that the expected value of the second term isn’t zero, and thus we’ll understate the variance Thus, the MLE in the misparametrized model yields a variance estimate that is too low.\nAgain, we want to integrate over our uncertainty in the missing values, rather than predict missing values."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-10-notes.html#em-algorithm",
    "href": "missing-data-material-W-26/notes/lecture-10-notes.html#em-algorithm",
    "title": "Missing data lecture 10: Flawed approach to missing data and EM",
    "section": "EM algorithm",
    "text": "EM algorithm\nThe last section discussed how maximizing the likelihood as a function of the missing data points was an incorrect way to go about doing likelihood inference when there is missing data.\nThe ``right” way to do so is to integrate over the uncertainty in your likelihood stemming from the missing observations, and then maximize this object.\n\\[\nL_\\text{ign}(\\theta \\mid Y_{(0)} = \\tilde{y}_{(0)}) \\propto \\int_{\\mathcal{Y}_{(1)}} f_Y(Y_{(0)} = \\tilde{y}_{(0)}, Y_{(0)} = y_{(1)} \\mid \\theta) dy_{(1)}\n\\] We’ll call the log-likelihood above: \\[\n\\ell(\\theta \\mid  Y_{(0)} = \\tilde{y}_{(0)})\n\\]\nSometimes we can’t do this maximization directly in one step because it is tricky. Instead, of doing the maximization directly we do so iteratively, and this is the motivation for the Expectation-Maximization algorithm: Let \\(\\ell_Y(\\theta \\mid Y)\\) be the complete data log-likelihood, and \\(f(Y_{(1)} \\mid Y_{(0)}, \\theta^{(t)})\\) be the distribution of missing values given the observed values and the current best guess at the parameters \\(\\theta^{(t)}\\). The object, which we’ll call \\(Q(\\theta \\mid \\theta^{(t)})\\), we want to maximize is the expected log-likelihood, where we take the expectation over the conditional distribution of the missing values\n\\[\nQ(\\theta \\mid \\theta^{(t)}) = \\int_{\\mathcal{Y}_{(1)}}\\ell_Y(\\theta \\mid Y_{(1)}, Y_{(0)} = \\tilde{y}_{(0)})f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta^{(t)})dY_{(1)}\n\\] ### Normal example, again\nContinuing the normal example from above, we have that our complete data log-likelihood is: \\[\n\\ell_Y(\\mu, \\sigma^2 \\mid Y_{(1)}, Y_{(0)}) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^r (y_i - \\mu)^2 -\\frac{1}{2\\sigma^2} \\sum_{i=r+1}^n (y_i - \\mu)^2\n\\]\nBecause we’ve assumed that the data are MAR (which in univariate settings equals MCAR), we have that \\[\ny_i \\sim \\text{Normal}(\\mu^{(t)}, (\\sigma^2)^{(t)})\n\\] Then for each \\(y_i\\), \\[\n\\Exp{y_i^2 - 2\\mu y_i + \\mu^2} = (\\mu^{(t)})^2 + (\\sigma^{t})^2  - 2 \\mu \\mu^{(t)} + \\mu^2\n\\] Then the expected log-likelihood is: \\[\nQ(\\theta \\mid \\theta^{(t)}) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^r (y_i - \\mu)^2 -\\frac{1}{2\\sigma^2} (n-r) ((\\mu^{(t)})^2 + (\\sigma^{t})^2  - 2 \\mu \\mu^{(t)} + \\mu^2)\n\\] Taking partial derivatives with respect to \\(\\mu\\) gives: \\[\n\\begin{aligned}\n0 & = \\frac{1}{\\sigma^2} \\sum_{i=1}^r (y_i - \\mu) -\\frac{1}{\\sigma^2} (n-r) (-2 \\mu^{(t)} + 2 \\mu)  \\\\\n& = \\frac{1}{\\sigma^2} (\\sum_{i=1}^r y_i  + (n - r) \\mu^{(t)}) - n\\frac{\\mu}{\\sigma^2}\n\\end{aligned}\n\\] Solving for \\(\\mu\\) gives the update rule for \\(\\mu^{(t)}\\):\n\\[\n\\begin{aligned}\n\\mu^{(t+1)} = \\frac{\\sum_{i=1}^r y_i  + (n - r) \\mu^{(t)}}{n}\n\\end{aligned}\n\\] Taking partials with respect to \\(\\sigma^2\\) \\[\n\\frac{\\partial Q(\\theta \\mid \\theta^{(t)})}{\\partial \\sigma^2} = -\\frac{n}{2 \\sigma^2} +\\frac{1}{2\\sigma^4} (\\sum_{i=1}^r (y_i - \\mu)^2 + (n-r) ((\\mu^{(t)})^2 + (\\sigma^{(t)})^2  - 2 \\mu \\mu^{(t)} + \\mu^2))\n\\] Setting this equal to zero and simplifying gives: \\[\n\\begin{aligned}\n0 & = -\\frac{n}{2 \\sigma^2} +\\frac{1}{2\\sigma^4} (\\sum_{i=1}^r (y_i - \\mu^{(t+1)})^2 + (n-r) ((\\mu^{(t)})^2 + (\\sigma^{(t)})^2  - 2 \\mu^{(t+1)} \\mu^{(t)} + (\\mu^{(t+1)})^2))\n\\end{aligned}\n\\] This yields: \\[\n\\begin{aligned}\n(\\sigma^2)^{(t+1)} & = \\frac{(\\sum_{i=1}^r (y_i - \\mu^{(t+1)})^2 + (n-r) ((\\mu^{(t)})^2 + (\\sigma^{(t)})^2  - 2 \\mu^{(t+1)} \\mu^{(t)} + (\\mu^{(t+1)})^2))}{n}\n\\end{aligned}\n\\] which simplifies to \\[\n\\begin{aligned}\n(\\sigma^2)^{(t+1)} & = \\frac{(\\sum_{i=1}^r y_i^2 + (n-r) ((\\mu^{(t)})^2 + (\\sigma^{(t)})^2)}{n} - (\\mu^{(t+1)})^2\n\\end{aligned}\n\\] This shows why the prior procedure didn’t work; namely it ignores the extra variability in imputations for the missing \\(y_i^2\\) terms.\nTo see why this simplifies, expand out \\[\n\\sum_{i=1}^r (y_i - \\mu^{(t+1)})^2 = \\sum_{i=1}^r y_i^2 - 2 \\mu^{(t+1)} \\sum_{i=1}^r y_i +  r (\\mu^{(t+1)})^2\n\\] and sub in the following expression for \\(\\sum_i y_i\\) \\[\n\\begin{aligned}\n\\sum_{i=1}^r y_i = n \\mu^{(t+1)}  - (n - r)\\mu^{(t)}\n\\end{aligned}\n\\]\nPlugging in our estimate for \\(\\mu^{(t+1)}\\) gives the solution above.\nOne can show that the EM solution converges to the complete data result.\n\nNontrivial example\nHere’s an example: Let’s say we have two outcomes, so \\(Y\\) is an \\(n \\times 2\\) matrix. For simplicity’s sake, we assume the missingness is ignorable, and assume we have a general missingness pattern, i.e. there are 3 patterns of missingness, assuming all participants had at least one measurement. The parameters of interest are \\(\\mu_1, \\mu_2, \\Sigma\\), and we’d like to use all the available data to do inference. If we had a complete dataset, we know from an earlier lecture the ML solutions for these quantities: \\[\n\\hat{\\mu}_1 = \\bar{y}_1,\\,\\hat{\\mu}_2 = \\bar{y}_2,\\, \\hat{\\Sigma} = 1/n\\sum_i y_i y_i^T - \\hat{\\mu}\\hat{\\mu}^T\n\\] Let \\[\nr_1 = \\# (y_{1i} \\text{ obs, }, y_{2i} \\text{ missing }), r_2 = \\# (y_{2i} \\text{ obs, }, y_{1i} \\text{ missing }), n - r_1 - r_2 = \\# (y_{2i} \\text{ obs, }, y_{1i} \\text{ obs })\n\\], and suppose we have arranged our indices \\(i\\) so \\(i \\in \\{1, \\dots, r_1\\}\\) have \\(y_1\\) observed, \\(i \\in \\{r_1 + 1, \\dots, r_1 + r_2\\}\\) have \\(y_2\\) observed and \\(i \\in \\{r_1+r_2+1, \\dots, n\\}\\) have all data observed. Let \\(f_{Y_j}(y_{ji} \\mid \\mu_j, \\Sigma_{j,j}), j = 1, 2\\) be the univariate normal density, while \\(f_{Y}(y_{i} \\mid \\mu_1, \\mu_2, \\Sigma)\\) is the bivariate normal density. The observed data likelihood is \\[\nL(\\mu_1, \\mu_2, \\Sigma \\mid Y_{(0)}) = \\prod_{i=1}^{r_1} f_{Y_1}(y_{1i} \\mid \\mu_1, \\Sigma_{1,1})\\prod_{i=r_1+1}^{r_1+r_2} f_{Y_2}(y_{2i} \\mid \\mu_2, \\Sigma_{2,2})\\prod_{i=r_1+r_2+1}^{n} f_{Y_2}(y_{i} \\mid \\mu_1, \\mu_2, \\Sigma)\n\\] We can find the MLEs for this expression, but it isn’t standard, and it’ll involve some thinking, whereas if we had a complete dataset we could just do the maximization very easily. The two variable dataset seems pretty tractable, but imagine we had many variables with lots of missingness patterns, and then the maximization would be hard."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-10-notes.html#why-does-em-work",
    "href": "missing-data-material-W-26/notes/lecture-10-notes.html#why-does-em-work",
    "title": "Missing data lecture 10: Flawed approach to missing data and EM",
    "section": "Why does EM work?",
    "text": "Why does EM work?\nWhy does this work? We want to show that maximizing \\(Q\\) is equivalent to maximizing \\(L_\\text{ign}\\).\nWe can write the complete data distribution as the product of two factors: \\[\nf_Y(Y_{(1)}, Y_{(0)} \\mid \\theta) = f(Y_{(0)} \\mid \\theta)f(Y_{(1)}\\mid Y_{(0)}, \\theta)\n\\] Taking logs gives us the complete data likelihood in terms of the observe data likelihood and the log likelihood of the conditional distribution of the missing data given the observed data and a parameter value. \\[\n\\ell_Y(Y_{(1)}, Y_{(0)} \\mid \\theta) = \\log(f(Y_{(0)} \\mid \\theta)) + \\log(f(Y_{(1)}\\mid Y_{(0)}, \\theta))\n\\] The first term is the observed data likelihood, which is what we want to maximize. Rearranging gives: \\[\n\\log(f(Y_{(0)} \\mid \\theta)) = \\ell_Y(Y_{(1)}, Y_{(0)} \\mid \\theta) - \\log(f(Y_{(1)}\\mid Y_{(0)}, \\theta))\n\\] Taking expectations for a current iterate \\(\\theta^r\\) over \\(f(Y_{(1)}\\mid Y_{(0)}, \\theta^t)\\), gives \\[\n\\Exp{\\log(f(Y_{(0)} \\mid \\theta))}{f(Y_{(1)}\\mid Y_{(0)}, \\theta^t)} = Q(\\theta \\mid \\theta^t) - H(\\theta \\mid \\theta^t)\n\\] where \\(Q\\) is as above and \\(H(\\theta \\mid \\theta^t)\\) is: \\[\nH(\\theta \\mid \\theta^{(t)}) = \\int_{\\mathcal{Y}_{(1)}} \\log f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta)f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta^t)dY_{(1)}\n\\] We can show that \\(H(\\theta \\mid \\theta^t) \\leq H(\\theta^t \\mid \\theta^t)\\) for all \\(\\theta\\).\n\\[\n\\begin{aligned}\n    H(\\theta \\mid \\theta^t)  - H(\\theta^t \\mid \\theta^t) & = \\int_{\\mathcal{Y}_{(1)}} \\log \\frac{f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta)}{f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta^t)}f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta^t)dY_{(1)}  \\\\\n& = \\Exp{\\log \\frac{f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta)}{f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}}}{f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta^t)} \\\\\n& \\leq\n\\log \\Exp{\\frac{f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta)}{f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}}}{f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta^t)} \\\\\n& = \\log 1 = 0\n\\end{aligned}\n\\] This is the same proof that shows that the KL divergence is always positive! \\[\n\\begin{aligned}\n\\text{KL}(f \\mid g) = \\int_{\\mathcal{Y}} \\log \\frac{f(y)}{g(y)} f(y) dy\n\\end{aligned}\n\\]\nNow we look at the value of the maximized oberved data likelihood: \\[\n\\Exp{\\log(f(Y_{(0)} \\mid \\theta^{t}))}{f(Y_{(1)} \\mid Y_{(0)}, \\theta^t)}\n\\] and how this changes between steps \\(t\\) and \\(t+1\\) so that \\(\\theta^{t+1}\\) as \\(\\theta^{t+1} = \\text{argmax}_\\theta Q(\\theta \\mid \\theta^{t})\\).\nThe difference between the observed likelihoods\n\\[\n\\begin{aligned}\n\\Exp{\\log(f(Y_{(0)} \\mid \\theta^{t+1}))}{f(Y_{(1)} \\mid Y_{(0)}, \\theta^{t+1})} - & \\Exp{\\log(f(Y_{(0)} \\mid \\theta^{t}))}{{f(Y_{(1)} \\mid Y_{(0)}, \\theta^{t})}} \\\\ & =  Q(\\theta^{t+1} \\mid \\theta^t) - Q(\\theta^{t} \\mid \\theta^t) - (H(\\theta^{t+1} \\mid \\theta^t) - H(\\theta^{t} \\mid \\theta^t))\n\\end{aligned}\n\\] Thus, by maximizing the expected complete data likelihood, we in turn maximize the function we really want, namely the observed likelihood.\nAnother result is that if \\(\\theta^{t+1}\\) is chosen such that: 1. \\(\\frac{\\partial}{\\partial \\theta} Q(\\theta \\mid \\theta^t) \\mid_{\\theta = \\theta^{t+1}} = 0\\)\n\n\\(\\theta^{t+1} \\to \\theta^\\star\\)\n\\(f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta)\\) is sufficiently smooth in \\(\\theta\\)\n\nThen\n\\(\\frac{\\partial}{\\partial \\theta} \\ell(\\theta \\mid Y_{(0)} = \\tilde{y}_{(0)}) \\mid_{\\theta = \\theta^\\star} = 0\\)\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta} \\ell(\\theta \\mid Y_{(0)} = \\tilde{y}_{(0)}) \\mid_{\\theta = \\theta^\\star} = \\frac{\\partial}{\\partial \\theta} Q(\\theta \\mid \\theta^{t}) \\mid_{\\theta = \\theta^\\star} - \\frac{\\partial}{\\partial \\theta} H(\\theta \\mid \\theta^{t}) \\mid_{\\theta = \\theta^\\star}\n\\end{aligned}\n\\] The first quantity on the RHS is zero by the condition that we pick \\(\\theta^{t+1}\\) as that which leads to \\(\\frac{\\partial}{\\partial \\theta} Q(\\theta \\mid \\theta^{t}) \\mid_{\\theta = \\theta^\\star}=0\\), so if \\(\\theta^t \\to \\theta^\\star\\), we must have gotten there by setting these gradients equal to zero. The second quantity on the RHS is \\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta} H(\\theta \\mid \\theta^{t}) \\mid_{\\theta = \\theta^\\star} & = \\frac{\\partial}{\\partial \\theta} \\int_{\\mathcal{Y}_{(1)}} \\log f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta) f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta^t) dY_{(1)} \\mid_{\\theta=\\theta^\\star} \\\\\n& = \\int_{\\mathcal{Y}_{(1)}} \\frac{\\frac{\\partial}{\\partial \\theta}f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta)\\mid_{\\theta=\\theta^\\star}}{f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta^\\star)}  f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta^t) dY_{(1)} \\mid_{\\theta=\\theta^\\star} \\\\\n\\end{aligned}\n\\] as \\(\\theta^t \\to \\theta^\\star\\) the denominators cancel, leaving\n\\[\n\\int_{\\mathcal{Y}_{(1)}} \\frac{\\partial}{\\partial \\theta}f(Y_{(1)} \\mid Y_{(0)} = \\tilde{y}_{(0)}, \\theta)\\mid_{\\theta=\\theta^\\star}dY_{(1)}\n\\] which, if we pull the derivative out of the integral again, equals zero because we’re differentiating a constant.\n\nMultivariate normal example again\nIn the multivariate normal example we know how to maximize the likelihood, but how do we do the conditional expectation?\n\\[\n\\Exp{\\ell_{Y}(\\mu, \\Sigma \\mid Y) \\mid Y_{(0)}, \\mu^t, \\Sigma^t} = \\frac{1}{2} \\log (\\det\\Sigma^{-1}) -\\frac{1}{2}\\text{tr}\\sum_i\\Exp{\\lp(y_i - \\mu) (y_i - \\mu)^T  \\mid Y_{(0)}, \\mu^t, \\Sigma^t}\\Sigma^{-1}\\rp\n\\] If we expand out the cross product, we see we need \\[\n\\Exp{(y_i - \\mu) (y_i - \\mu)^T  \\mid Y_{(0)}, \\mu^t, \\Sigma^t} = \\Exp{y_i y_i^T \\mid Y_{(0)}, \\mu^t, \\Sigma^t} - \\Exp{\\mu y_i^T - y_i^T \\mu \\mid Y_{(0)}, \\mu^t, \\Sigma^t}  - \\mu\\mu^T\n\\] The first term is : \\[\n\\Exp{y_i y_i^T \\mid Y_{(0)}, \\mu^t, \\Sigma^t} = \\text{Cov}(y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t) + \\Exp{y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t}\\Exp{y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t}^T\n\\] Plugging this back in above gives:\n\\[\n\\text{Cov}(y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t) + \\Exp{y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t}\\Exp{y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t}^T - \\Exp{\\mu y_i^T - y_i^T \\mu \\mid Y_{(0)}, \\mu^t, \\Sigma^t}  - \\mu\\mu^T\n\\] simplifying to \\[\n\\text{Cov}(y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t) + (\\Exp{y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t} - \\mu )(\\Exp{y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t} - \\mu )^T\n\\] Pluggig this back in above, we get the expected log-likelihood \\[\n\\begin{aligned}\n& \\Exp{\\ell_{Y}(\\mu, \\Sigma \\mid Y) \\mid Y_{(0)}, \\mu^t, \\Sigma^t} = \\frac{1}{2} \\log (\\det\\Sigma^{-1}) \\\\\n& -\\frac{1}{2}\\text{tr}\\sum_i\\lp\\text{Cov}(y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t) + (\\Exp{y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t} - \\mu )(\\Exp{y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t} - \\mu )^T\\rp\\Sigma^{-1}\n\\end{aligned}\n\\] This leads to the M step estimates:\n\\[\n\\mu^{t+1} = \\frac{1}{n} \\sum_i \\Exp{y_i \\mid Y_{(0)}, \\mu^t, \\Sigma^t},\n\\]\nand for\n\\[\n\\Sigma^{t+1} = \\frac{1}{n}\\sum_i\\text{Cov}(y_i \\mid y_{i(0)}, \\mu^t, \\Sigma^t) + (\\Exp{y_i \\mid y_{i(0)}, \\mu^t, \\Sigma^t} - \\mu^{t+1} )(\\Exp{y_i \\mid y_{i(0)}, \\mu^t, \\Sigma^t} - \\mu^{t+1} )^T\n\\]\nThe key is that the conditional expectation and covariances for \\(y_i\\) are informed by the observed data."
  },
  {
    "objectID": "survival-material/solution-for-question-1.html",
    "href": "survival-material/solution-for-question-1.html",
    "title": "Lecture 9",
    "section": "",
    "text": "For many of the prior examples, a convenient estimator for the Fisher information is the average of the observed information. The observed information is just the negative of the matrix of second derivatives of the log-likelihood: \\[\\begin{align}\n   -\\ell_{\\theta\\theta}(\\theta) =\n   -\\begin{bmatrix}\n       \\ddtA{\\theta_1}\\ell(\\theta) & \\ddtB{\\theta_1}{\\theta_2}\\ell(\\theta) & \\dots & \\ddtB{\\theta_1}{\\theta_p}\\ell(\\theta) \\\\\n       \\ddtB{\\theta_2}{\\theta_1} \\ell(\\theta) & \\ddtA{\\theta_2} \\ell(\\theta) & \\dots & \\ddtB{\\theta_2}{\\theta_p}\\ell(\\theta) \\\\\n       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n       \\ddtB{\\theta_p}{\\theta_1} \\ell(\\theta) & \\ddtB{\\theta_p}{\\theta_2} \\ell(\\theta) & \\dots & \\ddtA{\\theta_p}\\ell(\\theta) \\\\\n   \\end{bmatrix}\n\\end{align}\\] This is often denoted as \\[j(\\theta) \\equiv -\\ell_{\\theta\\theta}(\\theta).\\] Replacing \\(\\ell(\\theta) = \\sum_i \\log f_\\theta(X_i)\\) and using the fact that derivatives are linear operators: \\[\\begin{align}\nj(\\theta) =\n   -\\sum_i \\begin{bmatrix}\n       \\ddtA{\\theta_1}\\log f_\\theta(X_i) & \\ddtB{\\theta_1}{\\theta_2}\\log f_\\theta(X_i) & \\dots & \\ddtB{\\theta_1}{\\theta_p}\\log f_\\theta(X_i) \\\\\n       \\ddtB{\\theta_2}{\\theta_1} \\log f_\\theta(X_i) & \\ddtA{\\theta_2} \\log f_\\theta(X_i) & \\dots & \\ddtB{\\theta_2}{\\theta_p}\\log f_\\theta(X_i) \\\\\n       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n       \\ddtB{\\theta_p}{\\theta_1} \\log f_\\theta(X_i) & \\ddtB{\\theta_p}{\\theta_2} \\log f_\\theta(X_i) & \\dots & \\ddtA{\\theta_p}\\log f_\\theta(X_i) \\\\\n   \\end{bmatrix}\n\\end{align}\\] we can see that the natural estimator of \\(\\mathcal{I}(\\theta)\\) is the average observed information, which does indeed converge in probability to the Fisher information \\[\\frac{1}{n} j(\\theta) \\overset{p}{\\to} \\mathcal{I}(\\theta).\\] Of course, typically we won’t know \\(\\theta\\) (unless we’re evaluating \\(i(\\theta)\\) at \\(\\theta_0\\)), so we use the plug-in estimator, or \\(j(\\hat{\\theta}_n)\\) which still converges in probability to the Fisher information: \\[\\frac{1}{n} j(\\hat{\\theta}_n) \\overset{p}{\\to} \\mathcal{I}(\\theta).\\]"
  },
  {
    "objectID": "survival-material/solution-for-question-1.html#tests-in-terms-of-observed-information",
    "href": "survival-material/solution-for-question-1.html#tests-in-terms-of-observed-information",
    "title": "Lecture 9",
    "section": "1 Tests in terms of observed information",
    "text": "1 Tests in terms of observed information\nWhen we use observed information in place of the Fisher information, the Wald and Score tests look a bit different:\n\n1.1 Wald test with the observed information\n\\[n (\\hat{\\theta}_n - \\theta_0)^T \\frac{1}{n}j(\\hat{\\theta}_n) (\\hat{\\theta}_n - \\theta_0) = (\\hat{\\theta}_n - \\theta_0)^T j(\\hat{\\theta}_n) (\\hat{\\theta}_n - \\theta_0) \\overset{\\text{asympt.}}{\\sim} \\chi^2(p)\\]\n\n\n1.2 Score test with the observed information\n\\[\\begin{align*}\nT_S & = \\left(\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T (\\frac{1}{n}j(\\hat{\\theta}_0))^{-1}\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}  \\\\\n& = \\left(\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T n(j(\\hat{\\theta}_0))^{-1}\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}  \\\\\n& = \\left(\\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T j(\\hat{\\theta}_0)^{-1}\\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/solution-for-question-1.html#composite-tests",
    "href": "survival-material/solution-for-question-1.html#composite-tests",
    "title": "Lecture 9",
    "section": "2 Composite tests",
    "text": "2 Composite tests\nThis section is an expansion of Appendix B in (Klein, Moeschberger, et al. 2003).\nWe can modify all of our tests to accommodate testing a subset of the parameters. Typically we’ll have a subset of our parameter vector, let’s call it \\(\\psi\\), that we’re interested in, and we have another subset, \\(\\phi\\), that are nuisance parameters. In the our prior exponential regression example, we’ll likely be interested in testing if \\(\\beta\\neq 0\\), and thus we won’t care about testing \\(\\lambda\\).\nLet’s let \\(\\theta=(\\psi, \\phi)\\), and let \\(\\theta \\in \\R^p\\) so \\(\\psi\\in\\R^k\\), \\(k &lt; p\\), \\(\\phi\\in\\R^{p-k}\\). Our null hypothesis will be: \\[H_0: \\psi = \\psi_0.\\] Let \\(\\hat{\\phi}(\\psi_0)\\) be the MLE for the nuisance parameter with \\(\\psi\\) fixed under the null hypothesis. We’ll also partition the information matrix into a 2 by 2 block matrix: \\[\\mathcal{I}(\\psi, \\phi) =\n\\begin{bmatrix}\n\\Exp{-\\nabla^2_\\psi \\log f_\\theta(X_1)} & \\Exp{-\\nabla^2_{\\psi, \\phi} \\log f_\\theta(X_1)} \\\\\n\\Exp{-\\nabla^2_{\\psi, \\phi} \\log f_\\theta(X_1)} & \\Exp{-\\nabla^2_\\phi \\log f_\\theta(X_1)}\n\\end{bmatrix} = \\begin{bmatrix}\n\\mathcal{I}_{\\psi,\\psi} & \\mathcal{I}_{\\psi,\\phi} \\\\\n\\mathcal{I}_{\\psi,\\phi}^T & \\mathcal{I}_{\\phi,\\phi}\n\\end{bmatrix}\\] The observed information matrix can also be partioned the same way: \\[j(\\psi^\\prime, \\phi^\\prime) =\n\\begin{bmatrix}\n-\\ell_{\\psi\\psi}(\\psi^\\prime, \\phi^\\prime) & -\\ell_{\\psi\\phi}(\\psi^\\prime, \\phi^\\prime) \\\\\n-\\ell_{\\psi\\phi}(\\psi^\\prime, \\phi^\\prime)^T & -\\ell_{\\phi\\phi}(\\psi^\\prime, \\phi^\\prime)\n\\end{bmatrix} = \\begin{bmatrix}\nj_{\\psi,\\psi}(\\psi^\\prime,\\phi^\\prime) & j_{\\psi,\\phi}(\\psi^\\prime,\\phi^\\prime) \\\\\nj_{\\psi,\\phi}(\\psi^\\prime,\\phi^\\prime)^T & j_{\\phi,\\phi}(\\psi^\\prime,\\phi^\\prime)\n\\end{bmatrix}\\]\nThe inverse can also be partitioned into a 2 by 2 block matrix: \\[\\mathcal{I}(\\psi, \\phi)^{-1} =\n\\begin{bmatrix}\n\\mathcal{I}^{\\psi,\\psi} & \\mathcal{I}^{\\psi,\\phi} \\\\\n\\left(\\mathcal{I}^{\\psi,\\phi}\\right)^T & \\mathcal{I}^{\\phi,\\phi}\n\\end{bmatrix}\\] The expression for \\(\\mathcal{I}^{\\psi,\\psi}\\) can be found from the block matrix inversion formula: \\[\\begin{align}\n\\mathcal{I}^{\\psi,\\psi} & = \\mathcal{I}_{\\psi,\\psi}^{-1} + \\mathcal{I}_{\\psi,\\psi}^{-1}\\mathcal{I}_{\\psi,\\phi}\\left(\\mathcal{I}_{\\phi,\\phi} -\\mathcal{I}_{\\psi,\\phi}^T\\mathcal{I}_{\\psi,\\psi}^{-1} \\mathcal{I}_{\\psi,\\phi}\\right)^{-1}\\mathcal{I}_{\\psi,\\phi}^T\\mathcal{I}_{\\psi,\\psi}^{-1} \\\\\n& = \\left(\\mathcal{I}_{\\psi,\\psi} - \\mathcal{I}_{\\psi,\\phi} \\mathcal{I}_{\\phi,\\phi}^{-1}\\mathcal{I}_{\\psi,\\phi}^T\\right)^{-1}\n\\end{align} \\tag{1}\\]\nAll of the same notation will be used for the observed information, \\(j(\\psi, \\phi)\\):\n\\[j(\\psi^\\prime, \\phi^\\prime)^{-1} =\n\\begin{bmatrix}\nj^{\\psi,\\psi}(\\psi^\\prime,\\phi^\\prime) & j^{\\psi,\\phi}(\\psi^\\prime,\\phi^\\prime) \\\\\nj^{\\psi,\\phi}(\\psi^\\prime,\\phi^\\prime)^T & j^{\\phi,\\phi}(\\psi^\\prime,\\phi^\\prime)\n\\end{bmatrix}\\]\n\n2.1 Composite Wald test\nAgain using normal distribution theory, we can derive the Wald test with the observed information: \\[\\sqrt{n}(\\hat{\\psi}_n - \\psi_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}^{\\psi,\\psi}).\\] The Wald test statistic is then: \\[\\begin{align*}\nT_W = \\sqrt{n}(\\hat{\\psi}_n - \\psi_0) ^T   \\left(\\mathcal{I}^{\\psi,\\psi}\\mid_{\\psi = \\psi_0, \\phi = \\phi_0}\\right)^{-1}(\\hat{\\psi}_n - \\psi_0)\\sqrt{n}\n\\end{align*}\\] Using the appropriate transformation for the observed information in place of the Fisher information, we get \\[\\begin{align}\nT_W = (\\hat{\\psi}_n - \\psi_0) ^T \\left(j^{\\psi,\\psi}(\\hat{\\psi}_n, \\hat{\\phi}_n)\\right)^{-1}(\\hat{\\psi}_n - \\psi_0) \\overset{d}{\\to} \\chi^2_k\n\\end{align}\\] This simplifies by plugging in the Schur complement of \\(j_{\\phi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)\\), or \\(j_{\\psi,\\psi}(\\hat{\\psi}_n, \\hat{\\phi}_n) - j_{\\psi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n) j_{\\phi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)^{-1}j_{\\psi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)^T\\), which was the inverse of \\(j^{\\psi,\\psi}(\\hat{\\psi}_n, \\hat{\\phi}_n)\\), or \\[\nT_W = (\\hat{\\psi}_n - \\psi_0) ^T \\left(j_{\\psi,\\psi}(\\hat{\\psi}_n, \\hat{\\phi}_n) - j_{\\psi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n) j_{\\phi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)^{-1}j_{\\psi,\\phi}(\\hat{\\psi}_n, \\hat{\\phi}_n)^T\\right)(\\hat{\\psi}_n - \\psi_0)\n\\]\n\n\n2.2 Composite Score test\nThe composite score test is a bit more complicated. The joint asymptotic distribution of the score is: \\[\\sqrt{n} \\frac{1}{n} \\ell_{\\theta}(\\psi_0, \\hat{\\phi}(\\psi_0)) \\overset{d}{\\to} \\mathcal{N}\\left(0,\n\\begin{bmatrix}\n\\mathcal{I}_{\\psi,\\psi} & \\mathcal{I}_{\\psi,\\phi} \\\\\n\\mathcal{I}_{\\psi,\\phi}^T & \\mathcal{I}_{\\phi,\\phi}\n\\end{bmatrix}\\right)\\] But when we have a nuisance parameter, under the null distribution we solve the score equations \\[\\ell_{\\phi}(\\psi_0, \\phi) = 0,\\] leading to an MLE for \\(\\phi\\), \\(\\hat{\\phi}(\\psi_0)\\), that is dependent on \\(\\psi_0\\). This means the distribution for \\(\\sqrt{n} \\frac{1}{n} \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))\\) needs to condition on the score equations for \\(\\psi\\) being zero. If the score equations are asymptotically normally distributed, then the score equations for \\(\\psi\\) are conditionally normal. Recall that if vectors \\(X, Y\\) are multivariate normal with marginal variance covariance matrices \\(\\Sigma_X, \\Sigma_Y\\) and \\(\\Sigma_{X,Y}\\) is the covariance matrix of \\(X\\) with \\(Y\\), then \\(X \\mid Y\\) is multivariate normal with parameters \\[\\Exp{X} + \\Sigma_{X,Y} \\Sigma_Y^{-1}(Y - \\Exp{Y}),\\quad \\Sigma_X - \\Sigma_{X,Y} \\Sigma_Y^{-1}\\Sigma_{X,Y}^T.\\] In our case, the marginal mean of the score equations are zero, and \\(Y \\equiv \\ell_{\\phi}(\\psi_0, \\hat{\\phi}(\\psi_0))\\) is zero, so the conditional distribution of the score of \\(\\psi\\) is \\[\\sqrt{n}\\frac{1}{n} \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0)) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}_{\\psi,\\psi} - \\mathcal{I}_{\\psi,\\phi} \\mathcal{I}_{\\phi,\\phi}^{-1}\\mathcal{I}_{\\phi,\\psi}^T).\\] The test statistic is then \\[\\begin{align*}\n   n^{-1/2} \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))^T\\left(\\mathcal{I}_{\\psi,\\psi} - \\mathcal{I}_{\\psi,\\phi} \\mathcal{I}_{\\phi,\\phi}^{-1}\\mathcal{I}_{\\phi,\\psi}^T\\right)^{-1} n^{-1/2} \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))\n\\end{align*}\\] as we showed in Equation 1, the inverse matrix is the same as \\(\\mathcal{I}^{\\psi,\\psi}\\), so, subbing in our observed information matrix again, we get the final \\[\\begin{align*}\nT_S =  \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))^T j^{\\psi, \\psi}(\\psi_0, \\hat{\\phi}(\\psi_0)) \\ell_{\\psi}(\\psi_0, \\hat{\\phi}(\\psi_0))\n\\end{align*}\\] which is asymptotically distributed as \\(\\chi^2_k\\).\n\n\n2.3 Composite likelihood ratio test\nThe composite likelihood ratio test is similar to the likelihood ratio test: \\[T_{LR} = 2(\\ell(\\hat{\\psi},\\hat{\\phi}) - \\ell(\\psi_0,\\hat{\\phi}(\\psi_0)))\\] and this is again asymptotically distributed as \\(\\chi^2_k\\)\n\nExample 1. Continued relative risk example Suppose we are interested in testing the hypothesis \\(H_0: \\beta = 0\\) vs \\(H_a: \\beta \\neq 0\\).\nRecall the definitions of \\(r_1, r_2, T_1, T_2\\): \\[\\begin{alignat*}\n{2}\n    r_1 & = \\sum_{i=1}^n (1 - z_i) \\delta_i\\quad &&\n    T_1 = \\sum_{i=1}^n (1 - z_i) t_i \\\\\n    r_2 & = \\sum_{i=1}^n z_i \\delta_i\\quad &&\n    T_2 = \\sum_{i=1}^n z_i t_i\n\\end{alignat*}\\] We showed in our prior example that the log-likelihood was: \\[\\begin{align}\n   \\ell(\\beta, \\lambda) =  (r_1 + r_2)\\log\\lambda -\\lambda T_1 + r_2 \\beta -\\lambda e^\\beta T_2\n\\end{align}\\] The score equations are \\[\\begin{align*}\n\\frac{\\partial}{\\partial \\lambda} \\ell(\\beta,\\lambda) &: \\frac{r_1 + r_2}{\\lambda} - T_1 - e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta} \\ell(\\beta,\\lambda) &: r_2 - \\lambda e^\\beta T_2\n\\end{align*}\\] and the matrix of second derivatives of the log-likelihood with respect to \\(\\lambda, \\beta\\), also known as the observed information, is \\[\\begin{align}\n  \\nabla^2_{\\lambda, \\beta} \\ell(\\beta,\\lambda) = \\begin{bmatrix}\n        \\frac{r_1 + r_2}{\\lambda^2} & e^\\beta T_2 \\\\\n        e^\\beta T_2 & \\lambda e^\\beta T_2\n    \\end{bmatrix}\n\\end{align}\\] The unrestricted MLE, (i.e. the MLE under the alternative hypothesis), is: \\[\\begin{align*}\n\\hat{\\lambda} & = \\frac{r_1}{T_1} \\\\\n\\hat{e^\\beta} & = \\frac{r_2}{T_2}\\frac{T_1}{r_1}\n\\end{align*}\\] Under the null hypothesis that \\(\\beta = 0\\), we have the restricted likelihood: \\[\\begin{align}\n   \\ell(\\beta=0, \\lambda) =  (r_1 + r_2)\\log\\lambda -\\lambda T_1 -\\lambda T_2\n\\end{align}\\] which can be differentiated with respect to \\(\\lambda\\), set to zero, and solved for \\(\\lambda\\): \\[\\begin{align}\n\\hat{\\lambda}(\\beta = 0) & = \\frac{r_1 + r_2}{T_1 + T_2}\n\\end{align}\\] The inverse of the observed information evaluated at the unrestricted MLE was shown to be \\[\\begin{align}\n\\frac{r_1 + r_2}{r_1 r_2}\n\\end{align}\\] The inverse of the observed information is: \\[\\begin{align}\n\\hat{\\mathcal{I}}^{-1}(\\beta, \\lambda) =  \\frac{1}{\\frac{(r_1 + r_2)e^\\beta T_2}{\\lambda} - e^{2 \\beta} T_2^2}\\begin{bmatrix}\n      \\lambda e^\\beta T_2  & -e^\\beta T_2 \\\\\n        -e^\\beta T_2 & \\frac{r_1 + r_2}{\\lambda^2}\n    \\end{bmatrix}\n\\end{align}\\] which when the \\(2,2\\) element is evaluated at the \\(\\hat{\\lambda}(\\beta=0)\\), or \\[\\hat{\\mathcal{I}}^{-1}(0,\\hat{\\lambda}(\\beta=0))_{2,2} = \\frac{(T_1 + T_2)^2}{(r_1 + r_2) T_1 T_2}\\] Now for the test statistics:\n\nLikelihood ratio test: After some algebra, we get \\[T_{LR} = 2 r_1 \\left(\\log\\left(\\frac{r_1}{T_1}\\right)- \\log\\left(\\frac{r_1 + r_2}{T_1 + T_2}\\right)\\right)+ 2 r_2 \\left(\\log\\left(\\frac{r_2}{T_2}\\right)- \\log\\left(\\frac{r_1 + r_2}{T_1 + T_2}\\right)\\right)\\]\nWald test: The test statistic is: \\[T_W = \\left(\\log\\frac{r_2 / T_2}{r_1/T_1}\\right)^2 \\frac{r_1 r_2}{r_1 + r_2}.\\]\nScore test The starting test statistic is: \\[T_S = \\left(r_2 - (r_1+r_2)\\frac{T_2 }{T_1 + T_2} \\right)^2 \\frac{(T_1 + T_2)^2}{(r_1 + r_2) T_1 T_2}.\\] This is sort of interesting because it looks a bit like the log-rank statistic! \\(\\frac{T_2}{T_1 + T_2}\\) is a bit like the proportion of time at risk the second group experienced, and the expected total failures in the second group is this proportion multiplied by the total failures in both groups. It’s not too hard to see why you might want to reject the null that \\(\\beta=0\\) if this statistic were large. This simplifies to \\[T_S = \\frac{(T_1 r_2  - T_2 r_1)^2}{(r_1 + r_2)T_1 T_2 }.\\]\n\nFor an observed dataset of \\(r_1 = 10, r_2 = 12, T_1 = 25, T_2 = 27\\), they all yield values around \\(0.06\\), which is far below the critical value of \\(3.84\\), which is the \\(95^\\mathrm{th}\\) quantile from a \\(\\chi^2_1\\)."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-2.html",
    "href": "missing-data-material-W-26/homeworks/HW-2.html",
    "title": "HW 2",
    "section": "",
    "text": "Continuing with the PANSS score dataset from last HW, load the Surrogate package in R and load the dataset Schizo_PANSS:\n\nlibrary(Surrogate)\ndata(\"Schizo_PANSS\")\nhw_data &lt;- Schizo_PANSS[,c(\"Id\",\"Treat\",\"Week1\",\"Week4\",\"Week8\")]\n\n\n\nSubset the data to complete cases only and fit the following model to the complete case data:\n\\[\ny_i \\mid \\text{Treat}_i \\sim \\text{Normal}(\\mu + \\beta_1 \\text{Treat}_i + \\beta_2 t + \\beta_3 \\text{Treat}_i t, \\Sigma)\n\\]\nYou’ll do so using the iterative maximization algorithm we learned in class:\n\\[\n\\beta^{(t+1)} = \\lp \\textstyle\\sum_i X_i^T (\\Sigma^{(t)})^{-1} X_i \\rp^{-1} \\textstyle \\sum_i X_i^T (\\Sigma^{(t)})^{-1} y_i\n\\]\n\\[\n\\Sigma^{(t+1)} = \\frac{1}{n} \\textstyle\\sum_i (y_i - X_i \\beta^{(t)})(y_i - X_i \\beta^{(t)})^T\n\\]\nwhere \\(t\\) is the vector \\(1, 4, 8\\) indicating at what time points the measurements were taken and \\(\\mu\\) is a scalar mean.\nInclude your MLEs for \\(\\Sigma\\) and the vector \\((\\mu, \\beta_1, \\beta_2, \\beta_3)\\), and be sure to interpret your inferred coefficients in the context of the PANSS dataset.\nIt’ll help to reshape the data into long format from wide format:\n\ncomp_case &lt;- hw_data |&gt;\n  subset(\n    !is.na(Week1) &\n      !is.na(Week4) &\n      !is.na(Week8)\n  )\nlong_case &lt;- \n  stats::reshape(\n    comp_case, \n    direction = \"long\", \n    varying = 3:5, \n    sep = \"\"\n  )[,-5]\nnames(long_case) &lt;- c(\"Id\",\"Treat\",\"time\",\"panss\")\n\nIn order to make sure your algorithm is successful, include a test of your algorithm on on this simulated dataset, where you compare your algorithm’s inferences to the true values of \\(\\beta\\) and \\(\\Sigma\\):\n\nset.seed(123)\nn &lt;- 10000\np &lt;- 5\nK &lt;- 3\nX &lt;- list()\nbeta &lt;- rnorm(p)\nL &lt;- matrix(rnorm(K * K),K,K)\nSigma &lt;- L %*% t(L)\ny &lt;- list()\nfor (i in 1:n) {\n  X[[i]] &lt;- matrix(rnorm(p * K),K,p)\n  y[[i]] &lt;- X[[i]] %*% beta + MASS::mvrnorm(mu = rep(0,K), Sigma = Sigma)\n}\n\n\n\n\nHow might you expand this model based on your initial data analysis? There are no wrong answers."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-2.html#question-1a",
    "href": "missing-data-material-W-26/homeworks/HW-2.html#question-1a",
    "title": "HW 2",
    "section": "",
    "text": "Subset the data to complete cases only and fit the following model to the complete case data:\n\\[\ny_i \\mid \\text{Treat}_i \\sim \\text{Normal}(\\mu + \\beta_1 \\text{Treat}_i + \\beta_2 t + \\beta_3 \\text{Treat}_i t, \\Sigma)\n\\]\nYou’ll do so using the iterative maximization algorithm we learned in class:\n\\[\n\\beta^{(t+1)} = \\lp \\textstyle\\sum_i X_i^T (\\Sigma^{(t)})^{-1} X_i \\rp^{-1} \\textstyle \\sum_i X_i^T (\\Sigma^{(t)})^{-1} y_i\n\\]\n\\[\n\\Sigma^{(t+1)} = \\frac{1}{n} \\textstyle\\sum_i (y_i - X_i \\beta^{(t)})(y_i - X_i \\beta^{(t)})^T\n\\]\nwhere \\(t\\) is the vector \\(1, 4, 8\\) indicating at what time points the measurements were taken and \\(\\mu\\) is a scalar mean.\nInclude your MLEs for \\(\\Sigma\\) and the vector \\((\\mu, \\beta_1, \\beta_2, \\beta_3)\\), and be sure to interpret your inferred coefficients in the context of the PANSS dataset.\nIt’ll help to reshape the data into long format from wide format:\n\ncomp_case &lt;- hw_data |&gt;\n  subset(\n    !is.na(Week1) &\n      !is.na(Week4) &\n      !is.na(Week8)\n  )\nlong_case &lt;- \n  stats::reshape(\n    comp_case, \n    direction = \"long\", \n    varying = 3:5, \n    sep = \"\"\n  )[,-5]\nnames(long_case) &lt;- c(\"Id\",\"Treat\",\"time\",\"panss\")\n\nIn order to make sure your algorithm is successful, include a test of your algorithm on on this simulated dataset, where you compare your algorithm’s inferences to the true values of \\(\\beta\\) and \\(\\Sigma\\):\n\nset.seed(123)\nn &lt;- 10000\np &lt;- 5\nK &lt;- 3\nX &lt;- list()\nbeta &lt;- rnorm(p)\nL &lt;- matrix(rnorm(K * K),K,K)\nSigma &lt;- L %*% t(L)\ny &lt;- list()\nfor (i in 1:n) {\n  X[[i]] &lt;- matrix(rnorm(p * K),K,p)\n  y[[i]] &lt;- X[[i]] %*% beta + MASS::mvrnorm(mu = rep(0,K), Sigma = Sigma)\n}"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-2.html#question-1b",
    "href": "missing-data-material-W-26/homeworks/HW-2.html#question-1b",
    "title": "HW 2",
    "section": "",
    "text": "How might you expand this model based on your initial data analysis? There are no wrong answers."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-2.html#part-1",
    "href": "missing-data-material-W-26/homeworks/HW-2.html#part-1",
    "title": "HW 2",
    "section": "2.1 Part 1",
    "text": "2.1 Part 1\nBefore running your model, test your algorithm on the simulated dataset above, using the prior hyperparameters, with \\(B = 4\\) and \\(S = 500\\): \\[\n\\mu_0 = 0, \\Sigma_0 = I_p, \\nu_0 = 3, V_0 = I_p\n\\]\nIn order to run this model, you’ll need to be able to draw from an inverse Wishart distribution. Use the version that is implemented in the packages MCMCpack.\nWhen you run your chains, you’ll have 4 chains with 250 draws of each parameter per chain. You can use these draws to assess whether there is evidence against having reached the steady state distribution. Use the package posterior and rhat in the posterior package to report the univariate \\(\\hat{R}\\) statistics for each dimension of the beta vector.\nTo do so, you’ll need to arrange the draws for each dimension of beta into a matrix with rows corresponding to the 250 iterations and columns corresponding to the chains. Then you can pass that matrix to posterior::rhat to get an estimate of how much the scale of the posterior distribution would shrink as \\(S \\to \\infty\\). Numbers near \\(1\\), or \\(\\hat{R} \\leq 1.01\\) indicate that one may not have much to gain from running the chains for more iterations."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-2.html#part-2",
    "href": "missing-data-material-W-26/homeworks/HW-2.html#part-2",
    "title": "HW 2",
    "section": "2.2 Part 2",
    "text": "2.2 Part 2\nRun your model with the same priors as above on the comp_case data from above, and report 95% central posterior quantiles for each dimension of \\(\\beta\\).\nAlso report the 95% posterior quantiles for the parameters: \\(\\Sigma_{2,1} / \\sqrt{\\Sigma_{1,1}\\Sigma_{2,2}}\\), \\(\\Sigma_{3,1} / \\sqrt{\\Sigma_{1,1}\\Sigma_{3,3}}\\), and \\(\\Sigma_{3,2} / \\sqrt{\\Sigma_{2,2}\\Sigma_{3,3}}\\), or the correlation between the errors for each of the PANSS measurement occasions.\nThis may be done by defining the appropriate transformations of the parameter draws."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-2.html#question-2a",
    "href": "missing-data-material-W-26/homeworks/HW-2.html#question-2a",
    "title": "HW 2",
    "section": "2.1 Question 2a",
    "text": "2.1 Question 2a\nBefore running your model, test your algorithm on the simulated dataset above, using the prior hyperparameters, with \\(B = 4\\) and \\(S = 500\\): \\[\n\\mu_0 = 0, \\Sigma_0 = I_p, \\nu_0 = 3, V_0 = I_p\n\\]\nIn order to run this model, you’ll need to be able to draw from an inverse Wishart distribution. Use the version that is implemented in the packages MCMCpack.\nWhen you run your chains, you’ll have 4 chains with 250 draws of each parameter per chain. You can use these draws to assess whether there is evidence against having reached the steady state distribution. Use the package posterior and rhat in the posterior package to report the univariate \\(\\hat{R}\\) statistics for each dimension of the beta vector.\nTo do so, you’ll need to arrange the draws for each dimension of beta into a matrix with rows corresponding to the 250 iterations and columns corresponding to the chains. Then you can pass that matrix to posterior::rhat to get an estimate of how much the scale of the posterior distribution would shrink as \\(S \\to \\infty\\). Numbers near \\(1\\), or \\(\\hat{R} \\leq 1.01\\) indicate that one may not have much to gain from running the chains for more iterations."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-2.html#question-2b",
    "href": "missing-data-material-W-26/homeworks/HW-2.html#question-2b",
    "title": "HW 2",
    "section": "2.2 Question 2b",
    "text": "2.2 Question 2b\nRun your model with the same priors as above on the comp_case data from above, and report 95% central posterior quantiles for each dimension of \\(\\beta\\).\nAlso report the 95% posterior quantiles for the parameters: \\(\\Sigma_{2,1} / \\sqrt{\\Sigma_{1,1}\\Sigma_{2,2}}\\), \\(\\Sigma_{3,1} / \\sqrt{\\Sigma_{1,1}\\Sigma_{3,3}}\\), and \\(\\Sigma_{3,2} / \\sqrt{\\Sigma_{2,2}\\Sigma_{3,3}}\\), or the correlation between the errors for each of the PANSS measurement occasions.\nThis may be done by defining the appropriate transformations of the parameter draws."
  },
  {
    "objectID": "survival-material/lecture-11.html",
    "href": "survival-material/lecture-11.html",
    "title": "Lecture 11",
    "section": "",
    "text": "1 Influence of data points in likelihood equations\nThe material in this section is from (Collett 1994), (Cain and Lange 1984), and (Broderick, Giordano, and Meager 2023). Like in linear regression, we’d like to determine if some of our data points are influencing our conclusions; armed with this information, perhaps we can expand the model to incorporate these outliers, or perhaps there is a data processing error that we can rectify and re-run our analysis.\nOne idea is to determine whether omitting one data point appreciably changes our estimate of our parameter of interest. The simplest way to do this is to refit the data \\(n\\)-times, where each time we omit one data point. For small datasets, this is reasonable, but when we have large \\(n\\), or a very complex model, it may be infeasible to refit the model \\(n\\) times.\nInstead, we can cleverly use Taylor expansions to approximate the effect of small perturbations in the data on the estimated coefficient. If these small perturbations induce large changes in our estimated coefficients, then it stands to reason that the datapoints that have been perturbed are influential to our estimates.\nLet’s make things more concrete. Suppose we have a model with a parameter vector, \\(\\boldsymbol{\\theta} \\in \\R^k\\), and a maximum likelihood estimate thereof \\(\\hat{\\boldsymbol{\\theta}}\\). We’d like to understand how \\(\\hat{\\boldsymbol{\\theta}}\\) changes if we drop one datapoint. Let the index of this datapoint be \\(j\\). We can formalize the idea of dropping a datapoint by examining the score equations. Recall our typical problem setup: We have \\(n\\) observations, each of which is a triplet of the time to failure or the time to censoring, \\(t_i\\), an indicator \\(\\delta_i\\) that \\(t_i\\) is the time to failure, and \\(\\mathbf{z}_i \\in \\R^k\\), the covariate vector associated with each unit. Let this triplet of information associated with each unit \\(i\\) be denoted \\(\\mathbf{y}_i = (t_i, \\delta_i, \\mathbf{z}_i)\\). As in previous lectures, let the log-likelihood be defined as: \\[\n\\ell(\\boldsymbol{\\theta}; \\mathbf{y}_i) \\equiv f_{\\boldsymbol{\\theta}}(t_i, \\delta_i, \\mathbf{z}_i),\n\\] and let \\[\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\ell(\\boldsymbol{\\theta}; \\mathbf{y}_i).\\] Recall the notation for the score : \\[\n\\ell_\\boldsymbol{\\theta}(\\boldsymbol{\\theta}') =  \\lp\\nabla_{\\boldsymbol{\\theta}} \\ell(\\boldsymbol{\\theta})\\rp \\Big|_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}'}\n\\] Let \\(\\hat{\\boldsymbol{\\theta}}\\) be the value for \\(\\boldsymbol{\\theta}\\) that solves the score equations \\[\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}) =  \\mathbf{0}\\]\nWe now introduce the variable \\(\\mathbf{w} = (1, \\dots, 1)^T\\) into the log-likelihood, \\[\n\\ell(\\boldsymbol{\\theta},\\mathbf{w}) = \\sum_{i=1}^n w_i \\ell(\\boldsymbol{\\theta}; \\mathbf{y}_i),\n\\] and by linearity of the gradient \\[\\begin{align}\n    \\ell_\\boldsymbol{\\theta}(\\boldsymbol{\\theta}, \\mathbf{w}) = \\sum_{i=1}^n w_i \\ell_\\boldsymbol{\\theta}(\\boldsymbol{\\theta}; \\mathbf{y}_i)\n\\end{align}\\] Now denote the vector \\(\\hat{\\boldsymbol{\\theta}}(\\mathbf{w})\\) which solves the equations \\[\n\\ell_\\boldsymbol{\\theta}(\\boldsymbol{\\theta}, \\mathbf{w}) = \\mathbf{0}.\n\\]\nNote that our original MLE, \\(\\hat{\\boldsymbol{\\theta}} \\equiv \\hat{\\boldsymbol{\\theta}}(\\mathbf{1})\\). This notation now gives us a way to delete the \\(j^\\mathrm{th}\\) datapoint, namely by setting \\(w_j = 0\\).\nThen we’ll approximate \\(\\hat{\\boldsymbol{\\theta}}(\\mathbf{w}')\\) for \\(\\mathbf{w}'\\) “near” the vector \\(\\mathbf{1}\\).\nFor the \\(m^\\mathrm{th}\\) element of \\(\\hat{\\boldsymbol{\\theta}}(\\mathbf{w})\\), \\(\\hat{\\boldsymbol{\\theta}}(\\mathbf{w})_m\\), this is: \\[\\begin{align}\n    \\hat{\\boldsymbol{\\theta}}(\\mathbf{w})_m \\approx \\hat{\\boldsymbol{\\theta}}(\\mathbf{1})_m + \\sum_{i=1}^n (w_i - 1) \\left(\\frac{\\partial}{\\partial w_i} \\hat{\\boldsymbol{\\theta}}(\\mathbf{w})_m\\right)\\Bigg|_{\\mathbf{w} = \\mathbf{1}}\n\\end{align}\\] When all but one of these \\(w_i\\) is equal to \\(1\\), namely \\(w_j = 0\\), let \\[\\mathbf{w}_{(j)} = (\\mathbf{1}_{j-1}^T, 0, \\mathbf{1}_{n - j}^T)^T.\\] Then we get \\[\\begin{align}\n    \\hat{\\boldsymbol{\\theta}}(\\mathbf{w}_{(j)})_m \\approx \\hat{\\boldsymbol{\\theta}}(\\mathbf{1})_m - \\left(\\frac{\\partial}{\\partial w_j} \\hat{\\boldsymbol{\\theta}}(\\mathbf{w})_m\\right)\\Bigg|_{\\mathbf{w} = \\mathbf{1}}\n\\end{align}.\\] For the whole vector \\(\\hat{\\boldsymbol{\\theta}}(\\mathbf{w}_{(j)})\\) we get, as in (Cain and Lange 1984), \\[\\begin{align}\n    \\hat{\\boldsymbol{\\theta}}(\\mathbf{w}_{(j)}) \\approx \\hat{\\boldsymbol{\\theta}}(\\mathbf{1}) - \\left(\\frac{\\partial}{\\partial w_j} \\hat{\\boldsymbol{\\theta}}(\\mathbf{w})\\right)\\Bigg|_{\\mathbf{w} = \\mathbf{1}}\n\\end{align}\\] where \\[\\frac{\\partial}{\\partial w_j} \\hat{\\boldsymbol{\\theta}}(\\mathbf{w}) = (\\frac{\\partial}{\\partial w_j} \\hat{\\boldsymbol{\\theta}}(\\mathbf{w})_1, \\dots, \\frac{\\partial}{\\partial w_j} \\hat{\\boldsymbol{\\theta}}(\\mathbf{w})_k)^T.\\] The question remains how to calculate \\(\\frac{\\partial}{\\partial w_j} \\hat{\\boldsymbol{\\theta}}(\\mathbf{w})\\) evaluated at \\(\\mathbf{w}=1\\)?\nNote that the score equations are a function of the parameter vector and the vector of weights. The MLE given a set of weights \\(\\mathbf{w}\\), \\(\\hat{\\boldsymbol{\\theta}}(\\mathbf{w})\\) solves the system of equations: \\[\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{w}), \\mathbf{w}) = \\mathbf{0}.\\] Then the implicit function theorem allows us to differentiate the expression above with respect to \\(w_j\\) and solve for the derivative of interest, \\(\\frac{\\partial}{\\partial w_j} \\hat{\\boldsymbol{\\theta}}(\\mathbf{w})\\).\nRecall the chain rule for multivariate functions: Let \\(g(t) = v(x(t), y(t))\\) and calculate \\(\\frac{\\partial}{\\partial t} g(t)\\): \\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial t} v(x(t), y(t)) & = \\frac{\\partial v(x, y)}{\\partial x}\\Bigg|_{x=x(t), y=y(t)} \\frac{\\partial x(u)}{\\partial u}\\Bigg|_{u = t} \\\\\n& +  \\frac{\\partial v(x, y)}{\\partial y}\\Bigg|_{x = x(t), y = y(t)} \\frac{\\partial y(u)}{\\partial u}\\Bigg|_{u = t}\\end{aligned}.\\] We can differentiate the expression for the score function: \\[\\begin{align*}\n\\frac{\\partial}{\\partial w_j} \\mathbf{0} & = \\frac{\\partial}{\\partial w_j} \\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{w}), \\mathbf{w}) \\\\\n\\mathbf{0} & =\n\\frac{\\partial \\ell_\\boldsymbol{\\theta}(\\boldsymbol{\\theta},\\mathbf{w})}{\\partial \\boldsymbol{\\theta}^T}\\Bigg|_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}(\\mathbf{1}), \\mathbf{w}=\\mathbf{1}} \\frac{\\hat{\\boldsymbol{\\theta}}(\\mathbf{w})}{\\partial w_j}\\Bigg|_{\\mathbf{w}=\\mathbf{1}} + \\frac{\\partial \\ell_\\boldsymbol{\\theta}(\\boldsymbol{\\theta}, \\mathbf{w})}{\\partial w_j}\\Bigg|_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}(\\mathbf{1}), \\mathbf{w}=\\mathbf{1}} \\\\\n\\mathbf{0} & = \\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}),\\mathbf{1}) \\frac{\\hat{\\boldsymbol{\\theta}}(\\mathbf{w})}{\\partial w_j}\\Bigg|_{\\mathbf{w}=\\mathbf{1}} + \\frac{\\partial \\ell_\\boldsymbol{\\theta}(\\boldsymbol{\\theta}, \\mathbf{w})}{\\partial w_j}\\Bigg|_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}(\\mathbf{1}), \\mathbf{w}=\\mathbf{1}}\n\\end{align*}\\] Assuming that the observed information matrix for the complete dataset, or: \\[\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}),\\mathbf{1})\n\\] is invertible, we can solve the equation for the quantity of interest, \\(\\frac{\\partial}{\\partial w_j} \\hat{\\boldsymbol{\\theta}}(\\mathbf{w})\\) evaluated at \\(\\mathbf{w}=1\\). Note that \\[\n\\frac{\\partial \\ell_\\boldsymbol{\\theta}(\\boldsymbol{\\theta}, \\mathbf{w})}{\\partial w_j}\\Bigg|_{\\boldsymbol{\\theta} = \\hat{\\boldsymbol{\\theta}}(\\mathbf{1}), \\mathbf{w}=\\mathbf{1}} = \\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}), \\mathbf{1};\\mathbf{y}_i)\n\\] so\n\\[\\begin{align*}\n\\frac{\\hat{\\boldsymbol{\\theta}}(\\mathbf{w})}{\\partial w_j}\\mid_{\\mathbf{w}=\\mathbf{1}} = \\left(-\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}),\\mathbf{1})\\right)^{-1} \\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}), \\mathbf{1};\\mathbf{y}_i)\n\\end{align*}\\] Finally, we get the general equation for the sensitivity of the MLE to the deletion of the \\(j^\\mathrm{th}\\) data point: \\[\\begin{align}\n\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}) - \\hat{\\boldsymbol{\\theta}}(\\mathbf{w}_{(j)}) \\approx \\left(-\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\right)^{-1} \\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)\n\\end{align} \\tag{1}\\] This makes a good bit of sense; if the gradient of the log-likelihood function at a point lies along a direction of large uncertainty, this datapoint will have a large influence on the MLE.\nThe expression in Equation 1 also makes sense when viewed through the lens of the limiting distribution for the MLE. Recall that from a previous lecture we have that a Taylor expansion for \\(\\ell_{\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\) Using the Taylor expansion formula with remainders yields \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}) - \\boldsymbol{\\theta}^\\dagger) & = \\left(-\\frac{1}{n}  \\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\boldsymbol{\\theta}^\\dagger) \\right)^{-1} \\frac{1}{\\sqrt{n}}  \\ell_\\boldsymbol{\\theta}(\\boldsymbol{\\theta}^\\dagger) + o_p(1)\n\\end{align*}\\] The plug-in estimator for the right-hand side at \\(\\boldsymbol{\\theta}^\\dagger = \\hat{\\boldsymbol{\\theta}}(\\mathbf{1})\\) yields: \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}) - \\boldsymbol{\\theta}^\\dagger) & = \\left(-\\frac{1}{n} \\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}))\\right)^{-1} \\frac{1}{\\sqrt{n}}  \\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1})) + o_p(1) \\\\\n& = \\left(-\\frac{1}{n} \\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}))\\right)^{-1} \\frac{1}{\\sqrt{n}}  \\sum_{i=1}^n\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}); \\mathbf{y}_i) + o_p(1)\n\\end{align*}\\] Dividing each side by \\(\\sqrt{n}\\) yields \\[\\begin{align*}\n\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}) - \\boldsymbol{\\theta}^\\dagger = \\left(-\\frac{1}{n} \\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}))\\right)^{-1} \\frac{1}{\\sqrt{n}}  \\sum_{i=1}^n\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}); \\mathbf{y}_i) + o_p(1/\\sqrt{n})\n\\end{align*}\\] Thus, asymptotically, each observation \\((t_i, \\delta_i, \\mathbf{z}_i)\\) perturbs the deviation between the MLE and the true value by approximately: \\[\\begin{align*}\n\\left(-\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}))\\right)^{-1} \\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}); \\mathbf{y}_i).\n\\end{align*}\\]\n\n1.0.1 Linear regression\nIn the case of the linear regression model with normally distributed errors and known variance, we have the following results: \\[\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}); \\mathbf{y}_i) =\n\\mathbf{z}_i (y_i - \\hat{\\boldsymbol{\\beta}} \\mathbf{z}_i )\\] and \\[\\left(-\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}))\\right)^{-1} = -(\\mathbf{Z}^T \\mathbf{Z})^{-1}.\\] Assuming that \\(\\mathbf{Z}\\) is full column rank, we can decompose the variance covariance matrix as: \\[-(\\mathbf{Z}^T \\mathbf{Z})^{-1} = -\\mathbf{Q} A \\mathbf{Q}^T\\] where \\(\\mathbf{Q}\\) is a matrix with the orthonormal eigenvectors of \\(\\mathbf{Z}^T \\mathbf{Z}\\) as columns. Thus the influence of the \\(j^\\mathrm{th}\\) datapoint is \\[-\\mathbf{Q} A \\mathbf{Q}^T \\mathbf{z}_i (y_i - \\hat{\\boldsymbol{\\beta}} \\mathbf{z}_i).\\] If \\(\\mathbf{z}_i\\) lies in a direction of large uncertainty for the variance-covariance matrix (i.e. the vector is aligned with the eigenvector associated with a large eigenvalue), and there is a large fitted residual, the datapoint will have a lot of influence on at least one of the coefficients.\nCompare this to the exact calculation of the influence on \\(\\hat{\\beta}\\) from removing one point in a linear regression model: \\[\\hat{\\beta}(\\mathbf{w}) - \\hat{\\beta}(\\mathbf{1}) = \\frac{-\\mathbf{Q} A \\mathbf{Q}^T \\mathbf{z}_i (y_i - \\hat{\\boldsymbol{\\beta}} \\mathbf{z}_i)}{1 - \\mathbf{z}_i^T\\mathbf{Q} A \\mathbf{Q}^T \\mathbf{z}_i}\\]\n\n\n\n\n\n\nReferences\n\nBroderick, Tamara, Ryan Giordano, and Rachael Meager. 2023. “An Automatic Finite-Sample Robustness Metric: When Can Dropping a Little Data Make a Big Difference?” arXiv.\n\n\nCain, Kevin C., and Nicholas T. Lange. 1984. “Approximate Case Influence for the Proportional Hazards Regression Model with Censored Data.” Biometrics 40 (2): 493. https://doi.org/10.2307/2531402.\n\n\nCollett, David. 1994. Modelling Survival Data in Medical Research. Chapman & Hall."
  },
  {
    "objectID": "survival-material/lecture-12.html",
    "href": "survival-material/lecture-12.html",
    "title": "Lecture 12",
    "section": "",
    "text": "Another thing to note is that if the information matrix is block diagonal, then the gradients corresponding to one parameter block can’t influence the MLE of the opposing parameter block.\nFor notational ease: \\[\\Delta \\hat{\\boldsymbol{\\theta}}_j = \\hat{\\boldsymbol{\\theta}}(\\mathbf{1}) - \\hat{\\boldsymbol{\\theta}}(\\mathbf{w}_{(j)}).\\]\n(Collett 1994), citing (Hall, Rogers, and Pregibon 1982), suggests standardizing the sensitivity to the control for the inverse of the variance-covariance of the estimated \\(\\hat{\\boldsymbol{\\theta}}\\), namely: \\[(\\Delta \\hat{\\boldsymbol{\\theta}}_j)^T j(\\hat{\\boldsymbol{\\theta}}) (\\Delta \\hat{\\boldsymbol{\\theta}}_j).\\] Recall that \\[j(\\hat{\\boldsymbol{\\theta}}) = -\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\] and from the last lecture:\n\\[\n\\Delta \\hat{\\boldsymbol{\\theta}}_j \\approx \\left(-\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\right)^{-1} \\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)\n\\]\nThis leads to the tidy expression: \\[\\begin{align}\n\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)^T \\left(-\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\right)^{-1} \\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i).\n\\end{align} \\tag{1}\\] Alternatively, we can use the sandwich estimator for the asymptotic variance covariance matrix: \\[\\begin{align*}\n\\hat{\\Sigma}_{R} & = \\left(-\\frac{1}{n}\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\right)^{-1} \\left(\\frac{1}{n}\\sum_{i=1}^n\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)^T\\right)\\left(-\\frac{1}{n}\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\right)^{-1}\n\\end{align*}\\] Note that this is variance/covariance matrix for \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\). We instead want to get a sense for the variance/covariance matrix for \\(\\hat{\\theta}_n\\), so we divide the expression by \\(\\sqrt{n}\\), leading to a variance estimate that is scaled by \\(n^{-1}\\). Using the statistic \\[(\\Delta \\hat{\\boldsymbol{\\theta}}_j)^T (n^{-1}\\hat{\\Sigma}_{R})^{-1} (\\Delta \\hat{\\boldsymbol{\\theta}}_j)\\] and noting the following equality: \\[\\begin{align*}\n(n^{-1} \\hat{\\Sigma}_{R})^{-1} & = -\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}}) \\left(\\sum_{i=1}^n\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)^T\\right)^{-1}\\left(-\\ell_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}}(\\hat{\\boldsymbol{\\theta}})\\right)\n\\end{align*}\\] yields: \\[\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)^T\\left(\\sum_{i=1}^n\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i)^T\\right)^{-1}\\ell_\\boldsymbol{\\theta}(\\hat{\\boldsymbol{\\theta}};\\mathbf{y}_i).\\] Let’s do an example where we can analytically calculate the influence score for a single observation on the parameter vector:\n\nExample 1. Influence of datapoints in exponential regression model The inverse of the observed information is: \\[\\begin{align}\n\\hat{\\mathcal{I}}^{-1}(\\hat{\\lambda}, \\hat{\\beta}) =  \\frac{1}{\\frac{(r_1 + r_2)e^{\\hat{\\beta}} T_2}{\\hat{\\lambda}} - e^{2 \\hat{\\beta}} T_2^2}\\begin{bmatrix}\n      \\hat{\\lambda} e^{\\hat{\\beta}} T_2  & -e^{\\hat{\\beta}} T_2 \\\\\n        -e^{\\hat{\\beta}} T_2 & \\frac{r_1 + r_2}{\\hat{\\lambda}^2}\n    \\end{bmatrix}\n\\end{align}\\] which simplifies to: \\[\\begin{align}\n    \\begin{bmatrix}\n        \\frac{r_1}{T_1^2} & -\\frac{1}{T_1}\\\\\n        -\\frac{1}{T_1} & \\frac{r_1 + r_2}{r_1 r_2}\n    \\end{bmatrix}\n\\end{align}\\] and the score equations are: \\[\\begin{align}\n\\frac{\\partial}{\\partial \\lambda} \\ell(\\lambda, \\beta) & = \\frac{\\delta_i}{\\lambda} - e^{z_i \\beta} t_i\\\\\n\\frac{\\partial}{\\partial \\beta} \\ell(\\lambda, \\beta) & = \\delta_i z_i  - z_i \\lambda e^{z_i \\beta} t_i\n\\end{align}\\] which we evaluate at the MLE: \\[\\begin{align*}\n\\hat{\\lambda} & = \\frac{r_1}{T_1} \\\\\n\\hat{e^\\beta} & = \\frac{r_2}{T_2}\\frac{T_1}{r_1}\n\\end{align*}\\] to yield \\[\\begin{align}\n\\frac{\\partial}{\\partial \\lambda} \\ell(\\lambda, \\beta) & = \\frac{\\delta_i T_1 }{r_1} - \\left(\\frac{r_2}{T_2}\\frac{T_1}{r_1}\\right)^{z_i} t_i\\\\\n\\frac{\\partial}{\\partial \\beta} \\ell(\\lambda, \\beta) & = \\delta_i z_i  -\\left(\\frac{r_2}{T_2}\\right)z_i t_i\n\\end{align}\\]\nFor an individual with \\(z_i = 0\\), this gives the sensitivities: \\[\\begin{align}\n\\begin{bmatrix}\n\\frac{\\delta_i - t_i \\frac{r_1}{T_1}}{T_1}\\\\\n    \\frac{t_i}{T_1} - \\frac{\\delta_i}{r_1}\n\\end{bmatrix}  =\n\\begin{bmatrix}\n\\frac{\\delta_i - t_i \\frac{r_1}{T_1}}{T_1}\\\\\n    \\frac{t_i \\frac{r_1}{T_1} - \\delta_i}{r_1}\n\\end{bmatrix}\n\\end{align}\\] These expressions make sense. At a mathematical level, they agree with the total derivatives for each function: \\(\\frac{r_1}{T_1}\\) and \\(-\\log(r_1 / T_1)\\). Our expression for the sensitivity of the MLE to the omission of one datapoint is in terms of the difference between the MLE of the full model and the MLE of the leave-one-observation-out model: \\[\\hat{\\boldsymbol{\\theta}}(\\mathbf{1}) - \\hat{\\boldsymbol{\\theta}}(\\mathbf{w}_{(j)})\\] This means that the change in total time at risk for a group \\(j = 1, 2\\), or \\(T_j\\), is positive, as is the change in total failures for each group: \\[\\begin{align}\nT_j - (T_j)_{(i)} & = t_i \\\\\nr_j - (r_j)_{(i)} & = \\delta_i\n\\end{align}\\]\nThen the expression the \\[\\begin{align}\n    \\mathrm{d} \\frac{r_1}{T_1} & = \\frac{\\partial}{\\partial r_1} \\frac{r_1}{T_1} \\mathrm{d}r_1 + \\frac{\\partial}{\\partial T_1} \\frac{r_1}{T_1} \\mathrm{d}T_1 \\\\\n    & = \\frac{\\mathrm{d}r_1}{T_1} - \\mathrm{d}T_1\\frac{r_1}{T_1^2} \\\\\n    & \\approx \\frac{\\delta_i - t_i\\frac{r_1}{T_1}}{T_1}\n\\end{align}\\] and \\[\\begin{align}\n    \\mathrm{d} -\\log(r_1 / T_1) & = -\\frac{\\partial}{\\partial r_1} \\log(r_1 / T_1) \\mathrm{d}r_1 - \\frac{\\partial}{\\partial T_1} \\log(r_1 / T_1) \\mathrm{d}T_1 \\\\\n    & = \\frac{\\mathrm{d}T_1}{T_1} - \\frac{\\mathrm{d}r_1}{r_1} \\\\\n    & \\approx \\frac{t_i\\frac{r_1}{T_1}-\\delta_i}{r_1}\n\\end{align}\\]\nIt helps to think about the units of the parameter estimates. \\(\\lambda\\) measures the rate of failures per unit time, while \\(\\beta\\) measures the log of the relative rates of failure. Thus \\(\\beta\\) is unitless. Remember that \\[\\delta_i - t_i \\frac{r_j}{T_j}\\] is the residual for an individual \\(i\\) in group \\(j\\). It compares the observed failure to the expected failure rate, which in the exponential model is just the estimated rate of failure times the time at risk for \\(i\\), or \\(t_i\\). When one removes an individual from group 1 the estimate for the rate of failure in group 1 declines by the residual expected failure per unit time. At the same time, the log relative rate of failure must increase by the residual failure per unit failure because the estimator for \\(\\beta\\) is \\(\\log(r_2 / T_2) - \\log(\\hat{\\lambda})\\). Thus any change in \\(\\hat{\\lambda}\\) has an opposite change for \\(\\hat{\\beta}\\).\nFor an individual with \\(z_i = 1\\), the sensitivities are: \\[\\begin{align}\n\\begin{bmatrix}\n0\\\\\n  \\frac{\\delta_i}{r_2} -  \\frac{t_i}{T_2}\n\\end{bmatrix} =\n\\begin{bmatrix}\n0\\\\\n    \\frac{\\delta_i - t_i \\frac{r_2}{T_2}}{r_2}\n\\end{bmatrix}\n\\end{align}\\] Again, this makes sense; \\(\\hat{\\lambda} = \\frac{r_1}{T_1}\\), so omitting an individual in group \\(2\\) can’t change the MLE for \\(\\lambda\\). Finally, given \\(\\hat{\\beta} = \\log(r_2 / T_2) - \\log(\\hat{\\lambda})\\), omitting a datapoint will decrease the failure rate estimate within group \\(2\\) by the residual scaled by the failure rate. Note the total derivative of \\(\\log(r_2 / T_2)\\), as above, is: \\[\\begin{align}\n    \\mathrm{d} \\log(r_2 / T_2) & = \\frac{\\mathrm{d}r_2}{r_2} - \\frac{\\mathrm{d}T_2}{T_2}\n\\end{align}\\]\nWe can also calculate the scaled total deviation. For \\(z_i = 0\\) we have: \\[\\begin{align}\n   \\frac{\\left(t_i \\frac{r_1}{T_1} - \\delta_i\\right)^2}{r_1}\n\\end{align}\\] and for \\(z_i = 1\\) we have \\[\\begin{align}\n   \\frac{\\left(t_i \\frac{r_2}{T_2} - \\delta_i\\right)^2}{r_2}\n\\end{align}\\]\n\n\n\n\n\nReferences\n\nCollett, David. 1994. Modelling Survival Data in Medical Research. Chapman & Hall.\n\n\nHall, Gaineford J, William H Rogers, and Daryl Pregibon. 1982. Outliers Matter in Survival Analysis. Vol. 6761. Rand Corporation."
  },
  {
    "objectID": "survival-material/lecture-13.html",
    "href": "survival-material/lecture-13.html",
    "title": "Lecture 13: Cox model",
    "section": "",
    "text": "The Cox proportional hazards model is one of the most widely-used statistical models. It is a semiparametric model for the hazard ratio, which means we will avoid specifying a parameteric form for the baseline, time-varying hazard rate, while specifying a parametric model for the influence of covariates on the hazard rate: \\[\\lambda(t \\mid \\mathbf{z}) = \\lambda_0(t) \\exp(\\mathbf{z}^T \\boldsymbol{\\beta})\\]\nIn the following section we’ll derive the likelihood for the model in a similar way to our NPMLE derivation of the hazard rate.\n\n\nSuppose we have the standard survival-analysis triplet of observable random variables for each unit \\(i\\) under study: \\[\\{(T_i = \\min(X_i, C_i), \\Delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right), \\mathbf{z}_i), i = 1, \\dots, n\\}\\] where \\(X_i\\) is the absolutely continuous time to failure for unit \\(i\\), \\(C_i\\) is the absolutely continuous time to censoring, and \\(\\mathbf{z}_i\\) is a length-\\(p\\) vector of time-invariant covariates that we are conditioning on.\nAs stated above, we assume that the hazard function for the distribution of \\(X_i\\) is: \\[\\lambda_i(t) = \\lambda_0(t) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\] and we’ll leave the function \\(\\lambda_0(t)\\) unspecified. As in our lecture on the Kaplan-Meier estimator, we’ll derive an estimator for \\(\\lambda_0(t)\\) at the event times \\(\\{t_i, i = 1, \\dots, n\\}\\). Let \\(\\Lambda(t)\\) be the right-continuous-with-left-hand-limits (cádlág for short, in French) cumulative hazard function with mass points at \\(t_i\\). Note that \\(\\lambda(t_i) = \\Lambda(t_i) - \\Lambda(t_i-)\\), and we define the integral \\[\\int_B F(t) \\mathrm{d}\\Lambda(t) = \\sum_{i \\mid t_i \\in B} F(t_i) \\lambda(t_i).\\] This is a nonparametric estimator because as the data grow, so too does the dimension of the parameter space. This is akin to the Theoretical note in (Klein, Moeschberger, et al. 2003), and an exercise in (Aalen 1988).\nThe joint likelihood for the model for set of observed data \\(\\{(t_i, \\delta_i, \\mathbf{z}_i), i = 1, \\dots, n\\}\\), which is assumed to have no ties in event times, is: \\[\\begin{align}\n    L(\\lambda_0, \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)^{\\delta_i} \\exp(-\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\textstyle\\int_0^{t_i} \\lambda_0(u) du).\n\\end{align} \\tag{1}\\] Recall the definition of \\(Y_i(u)\\): \\[Y_i(u) = \\mathbbm{1}\\left(t_i \\geq u\\right).\\] The function is left continuous with a jump from 1 to 0 at \\(t_i\\). Then we can rewrite the model as \\[\\begin{align}\n    L(\\lambda_0, \\boldsymbol{\\beta}) & = \\prod_{i=1}^n \\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)^{\\delta_i} \\exp(-\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\textstyle\\int_0^\\infty Y_i(u) \\lambda_0(u) du) \\\\\n    & = \\left(\\prod_{i=1}^n \\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)^{\\delta_i}\\right)\\exp(-\\textstyle\\int_0^\\infty \\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(u) \\lambda_0(u) du) \\\\\n    & = \\left(\\prod_{i=1}^n \\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)^{\\delta_i}\\right)\\exp(-\\textstyle\\sum_{j=1}^n \\left(\\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)\\right)\\lambda_0(t_j))\n\\end{align} \\tag{2}\\] Let’s fix a value of \\(\\boldsymbol{\\beta}\\) and compute the NPMLE for \\(\\lambda_0(t_j)\\). The log-likelihood for \\(\\lambda_0\\) is \\[\\begin{align*}\n    \\ell(\\lambda_0, \\boldsymbol{\\beta}) & = \\sum_{i=1}^n \\delta_i \\log\\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)-\\textstyle\\sum_{j=1}^n \\left(\\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)\\right)\\lambda_0(t_j)\n\\end{align*}\\] Differentiating with respect to \\(\\lambda_0(t_j)\\), provided \\(\\delta_j = 1\\), gives \\[\\begin{align*}\n  \\frac{\\partial}{\\partial \\lambda_0(t_j)} \\ell(\\lambda_0, \\boldsymbol{\\beta}) = \\frac{1}{\\lambda_0(t_j)} - \\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)\n\\end{align*}\\] We note that the second derivative with respect to \\(\\lambda_0(t_j)\\) is strictly negative for all positive \\(\\lambda_0(t_j)\\) Setting this expression equal to zero and solving for \\(\\hat{\\lambda_0(t_j)}\\) will give the unique NPMLE \\[\\begin{align}\n   \\hat{\\lambda}_0(t_j) = \\frac{1}{\\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)}\n\\end{align}\\] The denominator can be simplified if we define the set \\(R(t_j): \\{i \\mid Y_i(t_j) = 1, i = 1, \\dots, n\\}\\). This is called the risk set at time \\(t_j\\). \\[\\begin{align}\n   \\hat{\\lambda}_0(t_j) = \\frac{1}{\\sum_{i \\in R(t_j)} \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}\n\\end{align}\\]\nLet’s substitute this NPMLE into the likelihood in Equation 1. Recognize that by definition of \\(\\delta_j\\) and \\(\\lambda_0(t_j)\\) only jumping at event-of-interest times, the estimator is equivalently defined as \\[\\begin{align}\n   \\hat{\\lambda}_0(t_j) = \\frac{\\delta_j}{\\sum_{i \\in R(t_j)} \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}\n\\end{align}\\] Subbing this back into the last line in Equation 2 gives \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & = \\left(\\prod_{i=1}^n \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)^{\\delta_i} \\right)\\exp\\left(-\\sum_{j=1}^n \\delta_j \\frac{\\left(\\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)\\right)}{\\sum_{i \\in R(t_j)} \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}\\right)\\\\\n     & = \\left(\\prod_{i=1}^n \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)^{\\delta_i} \\right)\\exp\\left(-\\sum_{j=1}^n \\delta_j\\right)\n\\end{align} \\tag{3}\\] Then we get the final term for the Cox model partial likelihood: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\prod_{i=1}^n \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)^{\\delta_i}\\\\\n    & =\\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)\n\\end{align} \\tag{4}\\] where we note that maximizing Equation 4 will maximize Equation 3.\nLet \\(\\hat{\\boldsymbol{\\beta}}\\) be the MLE of the expression \\(L(\\boldsymbol{\\beta})\\). Then the estimator for the cumulative hazard is defined as: \\[\\begin{align}\n   \\hat{\\Lambda}_0(t) = \\sum_{t_i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\hat{\\boldsymbol{\\beta})}}\n\\end{align}\\] This is known as the Breslow estimator for the cumulative hazard.\n\n\n\nThe final form of the partial likelihood for the Cox model is: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)\n\\end{align}\\] If we multiply top and bottom by \\(\\lambda_0(t_i)\\) (essentially we’ll be multiplying by \\(1\\) a bunch of times), we get: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\lambda_0(t_i) \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)\n\\end{align}\\] If we write out the hazard function explicitly we get the following probability distribution: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\lim_{dt \\searrow 0} P(t_i \\leq X_i &lt; t_i + dt \\mid X_i \\geq t_i)}{\\sum_{j \\in R(t_i)} \\lim_{dt \\searrow 0} P(t_i \\leq X_j &lt; t_i + dt \\mid X_j \\geq t_i)}\\right)\n\\end{align}\\] Under the assumption of independent event times, we can interpret this probability function as the probability the \\(i^\\mathrm{th}\\) participant surviving to time \\(t_i\\) and failing just after \\(t_i\\) conditional on exactly 1 death occurring just after time \\(t_i\\) among those surviving to time \\(t_i\\). Finally, under noninformative censoring, and noting that this is \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\lim_{dt \\searrow 0} P(t_i \\leq X_i &lt; t_i + dt \\mid X_i \\geq t_i, C_i \\geq t_i)}{\\sum_{j=1}^n \\lim_{dt \\searrow 0} P(t_i \\leq X_j &lt; t_i + dt \\mid X_j \\geq t_i, C_i \\geq t_i)}\\right)\n\\end{align}\\] Where the sum in the denominator follows because if \\(C_i &lt; t_i\\) or \\(X_i &lt; t_i\\), then \\(P(t_i \\leq X_j &lt; t_i + dt \\mid X_j \\geq t_i, C_i \\geq t_i) = 0\\).\nThe no ties assumption is not problematic with absolutely continuous data. In practice there will be ties in the data.\n\n\nThe simplest approximation when there are ties in the data is to consider the times at which the ties occurred to be distinct, but mismeasured. We define \\(\\{\\tau_j, j = 1, \\dots r = \\sum_i \\delta_i\\}\\) to be the distinct times at which failures occur. We can expand the dataset with \\[\\{(\\tau_j, d_j = \\sum_{i\\mid t_i = \\tau_j} \\delta_i, \\mathbf{s}_j = \\sum_{i\\mid t_i = \\tau_j}\\mathbf{z}_i), j = 1, \\dots, r\\}.\\] We still need access to the risk set function \\(R(t) = \\sum_i Y_i(t)\\) and \\(\\mathbf{z}_i\\). The partial likelihood can be written in terms of the new dataset: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\prod_{i=1}^n \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)^{\\delta_i}\\\\\n    & =\\prod_{j=1}^r \\prod_{i \\mid t_i = \\tau_j} \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{k \\in R(t_i)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})}\\right)\\\\\n    & = \\prod_{j=1}^r \\frac{\\exp(\\mathbf{s}_j^T \\boldsymbol{\\beta})}{\\left(\\sum_{k \\in R(\\tau_j)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})\\right)^{d_j}}\n\\end{align}\\]\nThis is not quite right because we’ve ignored the fact that when failure time is continuous, all true failure times are ordered in time. Let \\(\\tau_j\\) be a time interval in which several units failed, and let the set of units that fail at \\(\\tau_j\\) be \\(D(\\tau)\\), defined as: \\[D(\\tau_j) = \\{i \\mid t_i = \\tau_j, i = 1, \\dots, n\\}.\\] The proper way to handle ties would be to integrate over all possible permutations of failure times.\n\n\nThis section follows (Kalbfleisch and Prentice 2002). Let the indices of the set of units failing at time \\(\\tau_j\\) be \\(\\{1, \\dots, d_j\\}\\). Let \\(Q_j\\) be the set of permutations of \\(D(\\tau_j)\\) and let \\(P = (p_1, \\dots, p_{d_j})\\) be an element of this permutation. Finally, let the extended at risk set be \\(R(\\tau_j, P, m) = R(\\tau_j) \\setminus  \\{p_1, \\dots, p_{m-1}\\}\\) where we let \\(p_0\\) be the empty set. For a single term in the likelihood at time \\(\\tau_j\\), we need to integrate the term \\[\\begin{align}\n    \\prod_{i \\mid \\delta_i = 1, t_i = \\tau_j} \\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{k \\in R(\\tau_j)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})}\n\\end{align}\\] over all permutations of \\(D(\\tau_j)\\). We have no extra information to weight the orderings, so we give them all equal weight as \\(\\frac{1}{d_j!}\\). This integral is \\[\\begin{align}\n  \\frac{\\exp(\\mathbf{s}_j^T \\boldsymbol{\\beta})}{d_j!} \\sum_{P \\in Q_j} \\prod_{m=1}^{d_j} \\frac{1}{\\sum_{k \\in R(\\tau_j, P, m)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})}\n\\end{align}\\] Not surprisingly, this calculation is prohibitively computationally intensive. The part that is computationally intensive is the sum.\nLet’s look at at a simple example to see one way we might approximate this integral:\n\nExample 1. Suppose we have \\(d_j = 2\\) for some \\(\\tau_j\\). Let the risk set at \\(\\tau_j\\) be \\(R(\\tau_j)\\), and let \\(i\\) and \\(m\\) be the indices such that \\(t_i = t_{m} = \\tau_j\\). Let \\(c = \\sum_{k \\in R(\\tau_j)} \\exp(\\mathbf{z}_k^T \\beta)\\), \\(a = \\exp(\\mathbf{z}_i^T \\beta)\\), \\(b = \\exp(\\mathbf{z}_m^T \\beta)\\). If \\(t_i \\leq t_m\\), the terms in the Cox model should be: \\[\\frac{a}{c}\\times\\frac{b}{c - a}\\] and if \\(t_m &lt; t_i\\), we would instead have \\[\\frac{b}{c}\\times\\frac{a}{c - b}\\] Because we have no information about which event is more probable, we weight them equally with \\(\\frac{1}{2}\\): \\[\\frac{1}{2}\\frac{a}{c}\\times\\frac{b}{c - a} + \\frac{1}{2}\\frac{b}{c}\\times\\frac{a}{c - b} = \\frac{ab}{2}(\\frac{1}{c}\\times \\frac{1}{c - a} + \\frac{1}{c}\\times \\frac{1}{c - b})\\] We could approximate this expression instead by: \\[\\frac{ab}{2c}(\\frac{1}{c - a} + \\frac{1}{c - b}) \\approx \\frac{ab}{2c}\\left(\\frac{2}{c - (a + b)/2}\\right)\\]\n\nIf we note that we have \\(d_j!\\) terms in the sum, we can approximate the sum by Efron’s approximation: \\[\\begin{align}\n   \\frac{d_j!}{\\prod_{m=0}^{d_j-1}\\left(\\sum_{k \\in R(\\tau_j)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta}) - \\frac{m}{d_j} \\sum_{k \\in D(\\tau_j)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})\\right)}\n\\end{align}\\] The intuition for this method is that you approximate the decrement in the risk set by the average of the relative risk of failure of the failed units at time \\(\\tau_j\\).\n\n\n\n\n\nThe key idea for the Cox regression model is that we can maximize the partial likelihood without worrying about specifying any form for the baseline hazard rate \\(\\lambda_0(t)\\). Informally, this allows us to use standard asymptotic tests and confidence intervals for \\(\\boldsymbol{\\beta}\\) without worrying about the infinite dimensional (i.e. unknown function) baseline hazard rate. Thus we can use all the asymptotic likelihood techniques we developed for parametric models for the Cox model."
  },
  {
    "objectID": "survival-material/lecture-13.html#cox-model-likelihood-derivation-and-the-breslow-estimator-without-ties",
    "href": "survival-material/lecture-13.html#cox-model-likelihood-derivation-and-the-breslow-estimator-without-ties",
    "title": "Lecture 13: Cox model",
    "section": "",
    "text": "Suppose we have the standard survival-analysis triplet of observable random variables for each unit \\(i\\) under study: \\[\\{(T_i = \\min(X_i, C_i), \\Delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right), \\mathbf{z}_i), i = 1, \\dots, n\\}\\] where \\(X_i\\) is the absolutely continuous time to failure for unit \\(i\\), \\(C_i\\) is the absolutely continuous time to censoring, and \\(\\mathbf{z}_i\\) is a length-\\(p\\) vector of time-invariant covariates that we are conditioning on.\nAs stated above, we assume that the hazard function for the distribution of \\(X_i\\) is: \\[\\lambda_i(t) = \\lambda_0(t) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\] and we’ll leave the function \\(\\lambda_0(t)\\) unspecified. As in our lecture on the Kaplan-Meier estimator, we’ll derive an estimator for \\(\\lambda_0(t)\\) at the event times \\(\\{t_i, i = 1, \\dots, n\\}\\). Let \\(\\Lambda(t)\\) be the right-continuous-with-left-hand-limits (cádlág for short, in French) cumulative hazard function with mass points at \\(t_i\\). Note that \\(\\lambda(t_i) = \\Lambda(t_i) - \\Lambda(t_i-)\\), and we define the integral \\[\\int_B F(t) \\mathrm{d}\\Lambda(t) = \\sum_{i \\mid t_i \\in B} F(t_i) \\lambda(t_i).\\] This is a nonparametric estimator because as the data grow, so too does the dimension of the parameter space. This is akin to the Theoretical note in (Klein, Moeschberger, et al. 2003), and an exercise in (Aalen 1988).\nThe joint likelihood for the model for set of observed data \\(\\{(t_i, \\delta_i, \\mathbf{z}_i), i = 1, \\dots, n\\}\\), which is assumed to have no ties in event times, is: \\[\\begin{align}\n    L(\\lambda_0, \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)^{\\delta_i} \\exp(-\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\textstyle\\int_0^{t_i} \\lambda_0(u) du).\n\\end{align} \\tag{1}\\] Recall the definition of \\(Y_i(u)\\): \\[Y_i(u) = \\mathbbm{1}\\left(t_i \\geq u\\right).\\] The function is left continuous with a jump from 1 to 0 at \\(t_i\\). Then we can rewrite the model as \\[\\begin{align}\n    L(\\lambda_0, \\boldsymbol{\\beta}) & = \\prod_{i=1}^n \\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)^{\\delta_i} \\exp(-\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\textstyle\\int_0^\\infty Y_i(u) \\lambda_0(u) du) \\\\\n    & = \\left(\\prod_{i=1}^n \\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)^{\\delta_i}\\right)\\exp(-\\textstyle\\int_0^\\infty \\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(u) \\lambda_0(u) du) \\\\\n    & = \\left(\\prod_{i=1}^n \\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)^{\\delta_i}\\right)\\exp(-\\textstyle\\sum_{j=1}^n \\left(\\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)\\right)\\lambda_0(t_j))\n\\end{align} \\tag{2}\\] Let’s fix a value of \\(\\boldsymbol{\\beta}\\) and compute the NPMLE for \\(\\lambda_0(t_j)\\). The log-likelihood for \\(\\lambda_0\\) is \\[\\begin{align*}\n    \\ell(\\lambda_0, \\boldsymbol{\\beta}) & = \\sum_{i=1}^n \\delta_i \\log\\left(\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})\\right)-\\textstyle\\sum_{j=1}^n \\left(\\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)\\right)\\lambda_0(t_j)\n\\end{align*}\\] Differentiating with respect to \\(\\lambda_0(t_j)\\), provided \\(\\delta_j = 1\\), gives \\[\\begin{align*}\n  \\frac{\\partial}{\\partial \\lambda_0(t_j)} \\ell(\\lambda_0, \\boldsymbol{\\beta}) = \\frac{1}{\\lambda_0(t_j)} - \\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)\n\\end{align*}\\] We note that the second derivative with respect to \\(\\lambda_0(t_j)\\) is strictly negative for all positive \\(\\lambda_0(t_j)\\) Setting this expression equal to zero and solving for \\(\\hat{\\lambda_0(t_j)}\\) will give the unique NPMLE \\[\\begin{align}\n   \\hat{\\lambda}_0(t_j) = \\frac{1}{\\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)}\n\\end{align}\\] The denominator can be simplified if we define the set \\(R(t_j): \\{i \\mid Y_i(t_j) = 1, i = 1, \\dots, n\\}\\). This is called the risk set at time \\(t_j\\). \\[\\begin{align}\n   \\hat{\\lambda}_0(t_j) = \\frac{1}{\\sum_{i \\in R(t_j)} \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}\n\\end{align}\\]\nLet’s substitute this NPMLE into the likelihood in Equation 1. Recognize that by definition of \\(\\delta_j\\) and \\(\\lambda_0(t_j)\\) only jumping at event-of-interest times, the estimator is equivalently defined as \\[\\begin{align}\n   \\hat{\\lambda}_0(t_j) = \\frac{\\delta_j}{\\sum_{i \\in R(t_j)} \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}\n\\end{align}\\] Subbing this back into the last line in Equation 2 gives \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & = \\left(\\prod_{i=1}^n \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)^{\\delta_i} \\right)\\exp\\left(-\\sum_{j=1}^n \\delta_j \\frac{\\left(\\sum_{i=1}^n \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})Y_i(t_j)\\right)}{\\sum_{i \\in R(t_j)} \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}\\right)\\\\\n     & = \\left(\\prod_{i=1}^n \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)^{\\delta_i} \\right)\\exp\\left(-\\sum_{j=1}^n \\delta_j\\right)\n\\end{align} \\tag{3}\\] Then we get the final term for the Cox model partial likelihood: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\prod_{i=1}^n \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)^{\\delta_i}\\\\\n    & =\\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)\n\\end{align} \\tag{4}\\] where we note that maximizing Equation 4 will maximize Equation 3.\nLet \\(\\hat{\\boldsymbol{\\beta}}\\) be the MLE of the expression \\(L(\\boldsymbol{\\beta})\\). Then the estimator for the cumulative hazard is defined as: \\[\\begin{align}\n   \\hat{\\Lambda}_0(t) = \\sum_{t_i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\hat{\\boldsymbol{\\beta})}}\n\\end{align}\\] This is known as the Breslow estimator for the cumulative hazard."
  },
  {
    "objectID": "survival-material/lecture-13.html#alternative-view-of-the-cox-model",
    "href": "survival-material/lecture-13.html#alternative-view-of-the-cox-model",
    "title": "Lecture 13: Cox model",
    "section": "",
    "text": "The final form of the partial likelihood for the Cox model is: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)\n\\end{align}\\] If we multiply top and bottom by \\(\\lambda_0(t_i)\\) (essentially we’ll be multiplying by \\(1\\) a bunch of times), we get: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\lambda_0(t_i) \\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\lambda_0(t_i) \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)\n\\end{align}\\] If we write out the hazard function explicitly we get the following probability distribution: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\lim_{dt \\searrow 0} P(t_i \\leq X_i &lt; t_i + dt \\mid X_i \\geq t_i)}{\\sum_{j \\in R(t_i)} \\lim_{dt \\searrow 0} P(t_i \\leq X_j &lt; t_i + dt \\mid X_j \\geq t_i)}\\right)\n\\end{align}\\] Under the assumption of independent event times, we can interpret this probability function as the probability the \\(i^\\mathrm{th}\\) participant surviving to time \\(t_i\\) and failing just after \\(t_i\\) conditional on exactly 1 death occurring just after time \\(t_i\\) among those surviving to time \\(t_i\\). Finally, under noninformative censoring, and noting that this is \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\left(\\prod_{i \\mid \\delta_i = 1} \\frac{\\lim_{dt \\searrow 0} P(t_i \\leq X_i &lt; t_i + dt \\mid X_i \\geq t_i, C_i \\geq t_i)}{\\sum_{j=1}^n \\lim_{dt \\searrow 0} P(t_i \\leq X_j &lt; t_i + dt \\mid X_j \\geq t_i, C_i \\geq t_i)}\\right)\n\\end{align}\\] Where the sum in the denominator follows because if \\(C_i &lt; t_i\\) or \\(X_i &lt; t_i\\), then \\(P(t_i \\leq X_j &lt; t_i + dt \\mid X_j \\geq t_i, C_i \\geq t_i) = 0\\).\nThe no ties assumption is not problematic with absolutely continuous data. In practice there will be ties in the data.\n\n\nThe simplest approximation when there are ties in the data is to consider the times at which the ties occurred to be distinct, but mismeasured. We define \\(\\{\\tau_j, j = 1, \\dots r = \\sum_i \\delta_i\\}\\) to be the distinct times at which failures occur. We can expand the dataset with \\[\\{(\\tau_j, d_j = \\sum_{i\\mid t_i = \\tau_j} \\delta_i, \\mathbf{s}_j = \\sum_{i\\mid t_i = \\tau_j}\\mathbf{z}_i), j = 1, \\dots, r\\}.\\] We still need access to the risk set function \\(R(t) = \\sum_i Y_i(t)\\) and \\(\\mathbf{z}_i\\). The partial likelihood can be written in terms of the new dataset: \\[\\begin{align}\n    L(\\boldsymbol{\\beta}) & \\propto \\prod_{i=1}^n \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{j \\in R(t_i)} \\exp(\\mathbf{z}_j^T \\boldsymbol{\\beta})}\\right)^{\\delta_i}\\\\\n    & =\\prod_{j=1}^r \\prod_{i \\mid t_i = \\tau_j} \\left(\\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{k \\in R(t_i)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})}\\right)\\\\\n    & = \\prod_{j=1}^r \\frac{\\exp(\\mathbf{s}_j^T \\boldsymbol{\\beta})}{\\left(\\sum_{k \\in R(\\tau_j)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})\\right)^{d_j}}\n\\end{align}\\]\nThis is not quite right because we’ve ignored the fact that when failure time is continuous, all true failure times are ordered in time. Let \\(\\tau_j\\) be a time interval in which several units failed, and let the set of units that fail at \\(\\tau_j\\) be \\(D(\\tau)\\), defined as: \\[D(\\tau_j) = \\{i \\mid t_i = \\tau_j, i = 1, \\dots, n\\}.\\] The proper way to handle ties would be to integrate over all possible permutations of failure times.\n\n\nThis section follows (Kalbfleisch and Prentice 2002). Let the indices of the set of units failing at time \\(\\tau_j\\) be \\(\\{1, \\dots, d_j\\}\\). Let \\(Q_j\\) be the set of permutations of \\(D(\\tau_j)\\) and let \\(P = (p_1, \\dots, p_{d_j})\\) be an element of this permutation. Finally, let the extended at risk set be \\(R(\\tau_j, P, m) = R(\\tau_j) \\setminus  \\{p_1, \\dots, p_{m-1}\\}\\) where we let \\(p_0\\) be the empty set. For a single term in the likelihood at time \\(\\tau_j\\), we need to integrate the term \\[\\begin{align}\n    \\prod_{i \\mid \\delta_i = 1, t_i = \\tau_j} \\frac{\\exp(\\mathbf{z}_i^T \\boldsymbol{\\beta})}{\\sum_{k \\in R(\\tau_j)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})}\n\\end{align}\\] over all permutations of \\(D(\\tau_j)\\). We have no extra information to weight the orderings, so we give them all equal weight as \\(\\frac{1}{d_j!}\\). This integral is \\[\\begin{align}\n  \\frac{\\exp(\\mathbf{s}_j^T \\boldsymbol{\\beta})}{d_j!} \\sum_{P \\in Q_j} \\prod_{m=1}^{d_j} \\frac{1}{\\sum_{k \\in R(\\tau_j, P, m)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})}\n\\end{align}\\] Not surprisingly, this calculation is prohibitively computationally intensive. The part that is computationally intensive is the sum.\nLet’s look at at a simple example to see one way we might approximate this integral:\n\nExample 1. Suppose we have \\(d_j = 2\\) for some \\(\\tau_j\\). Let the risk set at \\(\\tau_j\\) be \\(R(\\tau_j)\\), and let \\(i\\) and \\(m\\) be the indices such that \\(t_i = t_{m} = \\tau_j\\). Let \\(c = \\sum_{k \\in R(\\tau_j)} \\exp(\\mathbf{z}_k^T \\beta)\\), \\(a = \\exp(\\mathbf{z}_i^T \\beta)\\), \\(b = \\exp(\\mathbf{z}_m^T \\beta)\\). If \\(t_i \\leq t_m\\), the terms in the Cox model should be: \\[\\frac{a}{c}\\times\\frac{b}{c - a}\\] and if \\(t_m &lt; t_i\\), we would instead have \\[\\frac{b}{c}\\times\\frac{a}{c - b}\\] Because we have no information about which event is more probable, we weight them equally with \\(\\frac{1}{2}\\): \\[\\frac{1}{2}\\frac{a}{c}\\times\\frac{b}{c - a} + \\frac{1}{2}\\frac{b}{c}\\times\\frac{a}{c - b} = \\frac{ab}{2}(\\frac{1}{c}\\times \\frac{1}{c - a} + \\frac{1}{c}\\times \\frac{1}{c - b})\\] We could approximate this expression instead by: \\[\\frac{ab}{2c}(\\frac{1}{c - a} + \\frac{1}{c - b}) \\approx \\frac{ab}{2c}\\left(\\frac{2}{c - (a + b)/2}\\right)\\]\n\nIf we note that we have \\(d_j!\\) terms in the sum, we can approximate the sum by Efron’s approximation: \\[\\begin{align}\n   \\frac{d_j!}{\\prod_{m=0}^{d_j-1}\\left(\\sum_{k \\in R(\\tau_j)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta}) - \\frac{m}{d_j} \\sum_{k \\in D(\\tau_j)} \\exp(\\mathbf{z}_k^T \\boldsymbol{\\beta})\\right)}\n\\end{align}\\] The intuition for this method is that you approximate the decrement in the risk set by the average of the relative risk of failure of the failed units at time \\(\\tau_j\\)."
  },
  {
    "objectID": "survival-material/lecture-13.html#interpretation-of-the-cox-regression-model",
    "href": "survival-material/lecture-13.html#interpretation-of-the-cox-regression-model",
    "title": "Lecture 13: Cox model",
    "section": "",
    "text": "The key idea for the Cox regression model is that we can maximize the partial likelihood without worrying about specifying any form for the baseline hazard rate \\(\\lambda_0(t)\\). Informally, this allows us to use standard asymptotic tests and confidence intervals for \\(\\boldsymbol{\\beta}\\) without worrying about the infinite dimensional (i.e. unknown function) baseline hazard rate. Thus we can use all the asymptotic likelihood techniques we developed for parametric models for the Cox model."
  }
]