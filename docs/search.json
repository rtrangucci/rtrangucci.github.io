[
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "My papers",
    "section": "",
    "text": "Here’s a selection of my published papers:\n\nModeling Racial/ethnic Differences in COVID-19 Incidence with Covariates Subject to Non-random Missingness (Trangucci, Chen, and Zelner 2023)\nRacial Disparities in Coronavirus Disease 2019 (COVID-19) Mortality Are Driven by Unequal Infection Risks (Zelner et al. 2021)\nQuantifying Observed Prior Impact (Jones, Trangucci, and Chen 2021)\nModeling Spatial Risk of Diarrheal Disease Associated with Household Proximity to Untreated Wastewater Used for Irrigation in the Mezquital Valley, Mexico (Contreras Jesse D. et al. 2020)\nBayesian Hierarchical Weighting Adjustment and Survey Inference. (Si 2020)\nEffects of Sequential Influenza A(H1N1)Pdm09 Vaccination on Antibody Waning (Zelner et al. 2019)"
  },
  {
    "objectID": "papers.html#preprints",
    "href": "papers.html#preprints",
    "title": "My papers",
    "section": "Preprints",
    "text": "Preprints\n\nIdentified vaccine efficacy for binary post-infection outcomes under misclassification without monotonicity (Trangucci, Chen, and Zelner 2022)\nBayesian Methods for Modeling Cumulative Exposure to Extensive Environmental Health Hazards (Submitted) (Trangucci et al. 2024)"
  },
  {
    "objectID": "papers.html#conference-papers",
    "href": "papers.html#conference-papers",
    "title": "My papers",
    "section": "Conference papers",
    "text": "Conference papers\n\nHierarchical Gaussian Processes in Stan (Trangucci 2017)\nPrior formulation for Gaussian process hyperparameters. (Trangucci, Betancourt, and Vehtari 2016)"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "STAT 625 - Winter 2024\n\n\n\n\n\nSTAT 625 - Winter 2025\nSTAT 599 - Winter 2025\n\n\n\n\n\nSTAT 625 - Winter 2026\nSTAT 599 - Winter 2026"
  },
  {
    "objectID": "posts/heckman-in-stan.html",
    "href": "posts/heckman-in-stan.html",
    "title": "Heckman models in Stan",
    "section": "",
    "text": "This is cmdstanr version 0.8.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/robertntrangucci/.cmdstan/cmdstan-2.36.0\n\n\n- CmdStan version: 2.36.0\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\nI took a class in the first year of my Master’s program about missing data from Ben Goodrich. The class was all about what to do when you encountered some sort of missing observations when analyzing a dataset. Missingness can arise for many reasons when analyzing data: it can arise from people declining to participate in a survey, survey respondents refusing to answer a question, from patients who drop out of a randomized study, or from the fact that an outcome of interest is only observable for a subset of patients who experience (or don’t experience) an intermediate event.\nWhen dealing with missing data, it is crucial that we understand why an observation, be it an outcome, a covariate, or a unit, is missing. If the\nThe Heckman selection model was one of the first examples we were shown in class of a missing data problem where the probability that an individual had a missing observation depended on the value of the missing observation (Heckman 1979). These are the hardest problems to address in missing data because\nA standard survey example would be that people with very high and those with very low incomes may decline to report income on a survey more than those with moderate incomes. In the vaccine example, the selection effect we are concerned with is that people with weaker immune systems may become infected even with a vaccine and thus have higher viral loads. A naive comparison of viral loads between vaccinated and unvaccinated people might show that the vaccine increases viral loads in those who are vaccinated (Hudgens, Hoering, and Self 2003).\nTo make things a little more concrete, suppose we’re interested in learning about the how vaccines moderate post-infection viral loads. The precise quantity we’re interested in is the change in post-infection viral load for people who would be infected with the pathogen no matter their vaccination status. These unlucky people are referred to as the ``Always-Infected’’ group. We might be tempted to get a rough estimate of this quantity by comparing viral loads in infected people in the vaccinated group vs. those in the unvaccinated group. The problem with this analysis is that if the vaccine has any causal effect on infection the individual characteristics of the two groups, infected-vaccinated people and infected-unvaccinated people, may differ. In this problem missingness arises in two forms. The first is that we observe only one outcome for each participant in our trial: the outcome corresponding to the treatment arm assignment. This is what Holland called the fundamental problem of causal inference. The second is more unique to this setting: we observe viral loads only in those who are infected.\nLet \\(Z_i\\) be the vaccine treatment, with \\(Z_i = 1\\) indicating treatment and \\(Z_i = 0\\) indicating placebo. For the ease of exposition, let’s suppose that treatment is randomized. We don’t need more selection bias (though selection into treatment could very well be a source of additional selection bias). Let \\(S_i(z)\\) be the infection status of individual \\(i\\) with vaccination status \\(z\\), and let \\(Y_i(z)\\) be the viral load of individual \\(i\\). If \\(S_i(z) = 0\\), we set \\(Y_i(z) = *\\).\nSuppose we also observe the study site, denoted as \\(R_i\\). This is the hospital or health clinic that enrolled patient \\(i\\) into the study. This study site is assumed to impact that probability of infection, but it is assumed to be independent of post-infection viral load. The standard term for a variable like \\(R_i\\) is an instrumental variable.\nThe model we’ll simulate data from is the following: \\[\n\\begin{aligned}\n\\log Y_i(z) & = \\mu_Y(z) + \\sigma_z\\, U_i(z) \\\\\n\\tilde{S}_i(z) \\mid R_i & = \\mu_S(R_i) + z \\,\\gamma_S + W_i \\\\\nS_i(z) & = 1(\\tilde{S}_i(z) > 0) \\\\\n(U_i(0),U_i(1),W_i) & \\sim \\text{Normal}(\\mathbf{0}, \\boldsymbol{\\Omega})\n\\end{aligned}\n\\] The individuals who are always infected are identified as those with \\(\\{S_i(1) = 1,S_i(0) = 1\\}\\). One key assumption of this model is that these individuals can be identified with certainty. This is also known as a assumption. To see why, let’s see what our model implies about the always-infected event: \\[\n\\begin{aligned}\n\\{S_i(1) = 1, S_i(0) = 1\\} & = \\{\\tilde{S}_i(1) > 0, \\tilde{S}_i(0) > 0\\} \\\\\n& = \\{\\mu_S(R_i) + \\gamma_S > -W_i, \\mu_S(R_i) > -W_i\\} \\\\\n& = \\{\\min(\\mu_S(R_i) + \\gamma_S,\\mu_S(R_i))  > -W_i\\}\n\\end{aligned}\n\\] The event that \\(S_i(z) = 1\\) is equivalent to \\[\n\\{\\mu_S(R_i) + \\gamma_S > -W_i\\}\n\\] Thus, if \\(\\gamma_S\\) is less than zero, as we would hope in a vaccine efficacy trial, then those who are infected in the vaccine arm are also those who would be infected in the placebo arm. The monotonicity assumption is a consequence of two assumptions of the model: - there is one error term for the probit regression - there is no individual heterogeneity in the coefficient \\(\\gamma_S\\).\nThe likelihood for the model is fairly straightforward: For individuals who remain uninfected, the likelihood term corresponds to \\(P(\\tilde{S}_i(z) \\leq 0)\\), which corresponds to the univariate standard normal CDF evaluated at \\(-\\mu_S(z,r)\\), or: \\[\nP(\\mu_S(z,r) + W_i \\leq 0) = \\Phi(-\\mu_S(z,r))\n\\]\nThe likelihood for an individual \\(i\\) who is infected corresponds to observing \\(\\log Y_i(z) = y_i\\) and \\(\\tilde{S}_i(z) > 0 \\mid \\log Y_i(z) = y_i, R_i = r_i\\): \\[\n\\begin{aligned}\nL_i(\\theta) & = \\frac{1}{\\sqrt{2\\pi} \\sigma_z}\\exp\\left(-\\frac{1}{2\\sigma_z^2}(y_i - \\mu_Y(z_i))^2\\right) \\\\\n& \\Phi\\left(\\frac{\\mu_S(z_i) + \\frac{\\rho_{z_i}}{\\sigma_z}(y_i - \\mu_Y(z_i))}{\\sqrt{1 - \\rho_{z_i}^2}}\\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(\\rho_z = \\text{Cor}(W_i,U_i(z))\\).\nTo get a sense for what this model implies about the treatment effect, let’s examine the conditional expectation of \\(Y_i(z)\\) given \\(\\tilde{S}_i(z) > 0\\) when a patient is vaccinated vs. when the patient is unvaccinated:\n\\[\n\\tau^\\text{naive} = \\mathbb{E} \\left[Y_i(1) \\mid \\tilde{S}_i(1) > 0 \\right] - \\mathbb{E} \\left[Y_i(0) \\mid \\tilde{S}_i(0) > 0 \\right]\n\\] This is what we’ll call the naive estimand, because it conditions on a post-treatment outcome, namely infection, which can induce selection bias, as we’ll see.\nThe expectation \\(\\mathbb{E} \\left[Y_i(z) \\mid \\tilde{S}_i(z) > 0 \\right]\\) is the following:\n\\[\n\\begin{aligned}\n\\frac{(2 \\pi\\sigma_z^2)^{-1/2}}{\\Phi(\\mu_S(z_i))}\\int_{-\\infty}^{\\infty}  \\exp\\left(y-\\frac{1}{2\\sigma_z^2}(y - \\mu_Y(z_i))^2\\right)\n\\Phi\\left(\\frac{\\mu_S(z_i) + \\frac{\\rho_{z_i}}{\\sigma_z}(y - \\mu_Y(z_i))}{\\sqrt{1 - \\rho_{z_i}^2}}\\right) dy\n\\end{aligned}\n\\] To my knowledge, this doesn’t have a closed form expression because of the exponentiation of \\(y\\), though there might be a clever way to evaluate this integral. We can, however, use R’s 1-d numerical integration routine to approximate this expectation.\nThe code below evaluates \\(\\tau^\\text{naive}\\), as well as\n\ncond_exp <- function(y, mu_y, mu_s, sd_y, rho) {\n  exp(y + dnorm(y, mu_y, sd_y, log = TRUE) + pnorm((mu_s + rho / sd_y * (y - mu_y))/sqrt(1 - rho^2),log.p = TRUE))\n}\ntau_naive <- function(rho_0, rho_1, sd_y, mu_y, mu_s, s_eff, y_eff) {\n  I1 <- integrate(cond_exp, -Inf, Inf, \n                  mu_y = mu_y + y_eff, \n                  mu_s = mu_s + s_eff, \n                  sd_y = sd_y, \n                  rho = rho_1, \n                  abs.tol = 0L, \n                  rel.tol = .Machine$double.eps^0.85\n  )\n  stopifnot(I1$subdivisions > 3)\n  D1 <- pnorm(mu_s + s_eff)\n  I2 <- integrate(cond_exp, -Inf, Inf, \n                  mu_y = mu_y, \n                  mu_s = mu_s, \n                  sd_y = sd_y, \n                  rho = rho_0, \n                  abs.tol = 0L, \n                  rel.tol = .Machine$double.eps^0.85)\n  stopifnot(I2$subdivisions > 3)\n  D2 <- pnorm(mu_s)\n  return(I1$value / D1 - I2$value / D2)\n}\ntrue_eff <- function(sd_y, mu_y, y_eff) {\n  mu_0 <- exp(mu_y + sd_y^2 / 2)\n  mu_1 <- exp(mu_y + y_eff + sd_y^2 / 2)\n  return(mu_1 - mu_0)\n}\ndelta_s <- function(mu_z, s_eff) {\n  p_0 <- pnorm(mu_z)\n  p_1 <- pnorm(mu_z + s_eff)\n  return(p_1 - p_0)\n}\n\nFirst we’ll explore a scenario where there is high positive correlation between the errors in the viral load equation and the latent variable related to infection. We set \\(\\rho_1 = \\rho_0 = 0.9\\), \\(\\sigma_0 = \\sigma_1 = 0.5\\), \\(\\mu_Y(0) = 10\\) and \\(\\mu_S(0) = -1.5\\). Let the true effect of the vaccine on mean log-viral load be \\(\\delta_y = -0.25\\).\nWe will plot how the naive estimand of viral load effect changes with the change in infection probability.\n\neffs <- seq(-3, 3, by = 0.01)\np_s <- sapply(effs, \\(x) delta_s(-0.5, x)) \nestimand <- sapply(effs, \\(x) f_naive_est(rho = 0.9, sd_y = 0.5, mu_y = 10, mu_s = -1.5, s_eff = x, y_eff = -0.25))\np_change_in_sign <- p_s[which.min(abs(estimand))]\nplot(p_s, \n     estimand,\n     main = \"Naive effect estimand vs. infection effect\",\n     ylab = bquote(E(y ~ \"|\" ~ S == 1, Z == 1) - E(y ~ \"|\" ~ S == 1, Z == 0)),\n     xlab = bquote(P(S == 1 ~ \"|\" ~ Z == 1) - P(S == 1 ~ \"|\" ~ Z == 0)),\n     type=\"l\")\nabline(v = p_change_in_sign, col = \"red\")\nabline(h = 0, col = \"red\")\n\n\n\n\nThis graph shows that as the vaccine becomes more effective at preventing infection, the apparent benefit of the vaccine on viral load decreases. In fact, at around a -0.18 decrease in the probability of infection, the naive estimand shows that the vaccine increases viral load, which is not the case.\nIf we fix a scenario and\n\neffs <- seq(-3, 3, by = 0.01)\np_s <- sapply(effs, \\(x) delta_s(-0.5, x)) \nestimand <- sapply(effs, \\(x) f_naive_est(rho = 0.9, sd_y = 0.5, mu_y = 10, mu_s = -1.5, s_eff = x, y_eff = -0.25))\np_change_in_sign <- p_s[which.min(abs(estimand))]\nplot(p_s, \n     estimand,\n     main = \"Naive effect estimand vs. infection effect\",\n     ylab = bquote(E(y ~ \"|\" ~ S == 1, Z == 1) - E(y ~ \"|\" ~ S == 1, Z == 0)),\n     xlab = bquote(P(S == 1 ~ \"|\" ~ Z == 1) - P(S == 1 ~ \"|\" ~ Z == 0)),\n     type=\"l\")\nabline(v = p_change_in_sign, col = \"red\")\nabline(h = 0, col = \"red\")\n\n\n\n\nNote that \\(\\rho_{01} = \\text{Cor}(U_i(0),U_i(1))\\) does not enter into the likelihood, which makes sense because we’ll never observe both \\(S_i(0)\\) and \\(S_i(1)\\) for the same individual. This does not mean that we won’t have any information about \\(\\rho_{01}\\); because \\(\\boldsymbol{\\Omega}\\) must be positive definite, the \\(\\det \\boldsymbol{\\Omega} > 0\\). This means that what we do learn about \\(\\rho_0\\) and \\(\\rho_1\\) will constrain the domain of \\(\\rho_{01}\\) so that \\(\\det \\boldsymbol{\\Omega} > 0\\). This translates to the following bounds on \\(\\rho_{01}\\) \\[\n\\rho_{01} \\in \\left(\\rho_0\\rho_1 - \\sqrt{1 - \\rho_0^2 - \\rho_1^2 + \\rho_0^2\\rho_1^2},\\, \\rho_0\\rho_1 + \\sqrt{1 - \\rho_0^2 - \\rho_1^2 + \\rho_0^2\\rho_1^2}\\right)\n\\] If we incorporate prior information, the asymptotic posterior for \\(\\rho_{01}\\) will take on the shape of the conditional prior for \\(\\rho_{01}\\) given the values for \\(\\rho_0\\) and \\(\\rho_1\\).\nGiven our focus on comparing the impact of vaccination on viral load, the target causal estimand is a measure of the average decrease (hopefully) in viral load caused by vaccination in individuals who are always infected. Mathematically, this is: \\[\\mathbb{E}[Y_i(1) - Y_i(0) \\mid S_i(1) = S_i(0) = 1].\\] This estimand is somewhat odd in that we can never observe both infection outcomes, so we can never calculate this estimand exactly from observed data without extra assumptions. I explained above why we can’t equate this target estimand with the ``naive’’ version, the difference in viral load between vaccinated and unvaccinated people: \\[\n\\mathbb{E}[Y_i(1) \\mid S_i(1) = 1] - \\mathbb{E}[Y_i(0) \\mid S_i(0) = 1].\n\\] Logically, it makes sense, because the naive estimand is a mixture over several groups, instead of just the always-infected group, but we’ll simulate some data to get a better sense of how different the two estimands can be.\nIn our simulated example, we’ll suppose we have a randomized vaccine trial with 20,000 participants spread evenly across 10 study sites. We’ll set \\(\\rho_0\\) and \\(\\rho_1\\) to \\(0.75\\), which equates to strong residual correlation between infection risk and viral load; that might be reasonable if there is an unobserved factor impacting both infection and viral load. Maybe this is prior exposure to the virus plus genetic factors related to immune system health.\nThe covariates we’re including in the infection risk equation, \\(\\mu_S(z)\\) are age and study site, while the only covariates we’re including in the viral load model are categorical age predictors.\n\nset.seed(12222)\nn <- 20000\n\nsigma_0 <- 0.7\nsigma_1 <- 0.7\nsigma_W <- 1\nrho_0 <- 0.9\nrho_1 <- 0.9\nlb <- rho_0*rho_1 - sqrt(1 - rho_0^2 - rho_1^2 + rho_0^2*rho_1^2)\nub <- rho_0*rho_1 + sqrt(1 - rho_0^2 - rho_1^2 + rho_0^2*rho_1^2)\nrho_01 <- 0.7\ndiag_sigs <- diag(c(sigma_0,sigma_1,sigma_W))\nOmega <- matrix(c(1,rho_01,rho_0,rho_01,1,rho_1,rho_0,rho_1,1),3,3)\nSigma <- diag_sigs %*% Omega  %*% diag_sigs\nU0_U1_W <- mvrnorm(n = n, mu = c(0, 0, 0), Sigma = Sigma)\n\nThe plot of \\(U_0\\) vs. \\(W\\) gives a sense of how related the two error terms are:\n\n\n\n\n\nThe covariates we will include in the model are age, which we treat as a categorical variable, the study site, which is also treated as a categorical variable, and the binary treatment variable, which is assumed to be balanced within study sites.\n\nN_age <- 4\nN_sites <- 10\nage_vec <- sample(N_age, n, TRUE) |> factor()\nsite_vec <- rep(1:N_sites,each=n / N_sites) |> factor()\ntreat <- rep(c(rep(0,n/20),rep(1,n/20)),10)\n\nWe’ll create the model matrices from these variables, along with the coefficients for each model equation:\n\nX_S <- model.matrix(~ age_vec + site_vec)\nX_Y <- model.matrix(~ age_vec)\nd_S <- ncol(X_S)\nd_Y <- ncol(X_Y)\ngamma_Y <- -0.25\ngamma_S <- -0.85\n\nbeta_S <- c(-1.5, 0.1, 0.2, 0.3, rnorm(N_sites - 1) * 0.1)\nbeta_Y <- c(10.3, 0.5, 0.5, 0.5)\n\nS_0  <- (X_S %*% beta_S + U0_U1_W[,3]) >= 0\nS_1  <- (X_S %*% beta_S + gamma_S + U0_U1_W[,3]) >= 0\nS <- cbind(as.integer(S_0), as.integer(S_1))\nY_0  <- X_Y %*% beta_Y + U0_U1_W[,1]\nY_1  <- X_Y %*% beta_Y + gamma_Y + U0_U1_W[,2]\nY <- cbind(ifelse(S_0 == 1, Y_0, NA),\n           ifelse(S_1 == 1, Y_1, NA))\n\nsel <- as.vector(rbind(1-treat, treat))\nS_o <- as.vector(t(S))[sel == 1]\nY_o <- as.vector(t(Y))[sel == 1]\n\nidx_S_o <- which(S_o == 1)\nidx_AI <- which(S_0 == 1 & S_1 == 1)\n\nY_o_sel <- Y_o[idx_S_o]\nX_Y_sel <- X_Y[idx_S_o, ]\n\n\nnaive_estimand <- mean(exp(Y_o_sel[treat[idx_S_o] == 1])) -\n                  mean(exp(Y_o_sel[treat[idx_S_o] == 0]))\n\nY_estimand <- mean(exp(Y_1[idx_AI])) -\n               mean(exp(Y_0[idx_AI]))\n\nn_s_0 <- sum(treat[idx_S_o] == 1)\nn_s_1 <- sum(treat[idx_S_o] == 0)\n\nn_AI <- length(idx_AI)\n\nAfter all is said and done, the finite-sample estimator for the population estimand is -63,883, with a standard error of 6888.1207534 while the naive estimator for the population estimand is 41,600 with a standard error of 8335.141496. Note that these standard errors are understated a bit because the selection indicators are treated as fixed. In reality, both the selection and the outcome are random, so a more complete standard error calculation would account for the randomness in both sets of outcomesboth sets of outcomes.\nThis means that the naive estimator understates the benefit of the vaccine for people who are always infected by about 165%.\nThe approximate standard error for the naive estimator is 6888.1207534, while the approximate standard error .\nLet’s build the Stan model block by block. First, the data block needs to take in the sizes and types of all the matrices and vectors that we’ll need to fit the model.\n\nData block\n\n\ndata {\n  int<lower=1> N;     \n  int<lower=1> N_neg; \n  int<lower=1> N_pos; \n  int<lower=1> D_S;   \n  int<lower=1> D_Y;   \n  vector[N_pos] Y;    \n  array[N] int<lower=0,upper=1> S;  \n  array[N] int<lower=0,upper=1> Z;  \n  matrix[N_pos,D_Y] X_Y; \n  matrix[N,D_S] X_S;  \n}\n\nWe need the number of observations, N, the number of negative and positive cases (N_neg and N_pos). We also need the dimensions of the predictors for the \\(S\\) equation and the \\(Y\\) equation, D_S and D_Y. The observed viral loads are collected into a length-N_pos vector Y, while the observed infection status and treatment assignment are collected into length-N binary integer arrays S and Z. Finally, we have the predictors for the \\(Y\\) equation, X_Y, which is an N_pos by D_Y matrix, and the predictors for the \\(S\\) equation, which is an N by D_S matrix.\n\nTransformed data block\n\nThe transformed data block is needed to collect the indices, measured from \\(1,\\dots,N\\), of the negative cases (\\(S_i = 0\\)) and positive cases (\\(S_i = 1\\)). The reason we need this information is because the likelihood contribution to the infection status probit model, shown in Equation 1, requires the value of the log-viral load, so we’ll need to match the infection status outcomes to the viral load outcomes. We could do this matching in the R code outside the Stan model, but I prefer to do it within the Stan code so everything is in one place. We’ll collect the indices of the positive cases in n_pos, which is an array of length N_pos and holds integers between 1 and N, while negative case indices are collected in n_neg, defined similarly.\nThe other piece of information the infection status likelihood will need for positive cases is the value \\(\\rho_z\\), which depends on the individual treatment assignment. This information is collected in the vector Z_idx.\n\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\n\n\nParameters block\n\nThe parameter block is where we define which unknown parameters we need to estimate with our model. Our unknown parameters is the set: - \\(\\boldsymbol{\\Omega}\\): correlation matrix between the error terms for the regressions - \\(\\sigma_0\\): error standard deviation for log-viral load regression in placebo group - \\(\\sigma_1\\): error standard deviation for log-viral load regression in treatment group - \\(\\beta_y\\): regression coefficients for the log-viral load regression - \\(\\beta_s\\) - \\(\\gamma_s\\) _y)$, the correlation matrix for the errors. In our case, we have a\n\nparameters {\n  cholesky_factor_corr[3] L_Omega;\n  vector<lower=0>[2] sd_Y;\n  vector[D_Y] b_Y;\n  vector[D_S] b_S;\n  real gamma_S;\n  real gamma_Y;\n}\n\nThe Stan model is written like so:\n\ndata {\n  int<lower=1> N;     \n  int<lower=1> N_neg; \n  int<lower=1> N_pos; \n  int<lower=1> D_S;   \n  int<lower=1> D_Y;   \n  vector[N_pos] Y;    \n  array[N] int<lower=0,upper=1> S;  \n  array[N] int<lower=0,upper=1> Z;  \n  matrix[N_pos,D_Y] X_Y; \n  matrix[N,D_S] X_S;  \n}\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\nparameters {\n  cholesky_factor_corr[3] L_Omega;\n  vector<lower=0>[2] sd_Y;\n  vector[D_Y] b_Y;\n  vector[D_S] b_S;\n  real gamma_S;\n  real gamma_Y;\n}\ntransformed parameters {\n  matrix[3,3] Omega = L_Omega * L_Omega';\n}\nmodel {\n  vector[N] mu_S = X_S * b_S + to_vector(Z) * gamma_S;\n  vector[N_pos] mu_Y = X_Y * b_Y + to_vector(Z[n_pos]) * gamma_Y;\n  array[2] real rho = {Omega[1,3], Omega[2,3]};\n\n  b_Y ~ normal(0, 1);\n  b_S ~ normal(0, 1);\n  sd_Y ~ normal(0, 2);\n  gamma_S ~ normal(0, 5);\n  gamma_Y ~ normal(0, 5);\n\n  for (n in 1:N_neg) {\n    target += log(Phi(-mu_S[n_neg[n]]));\n  }\n  for (n in 1:N_pos) {\n    int treat_idx = Z_idx[n_pos[n]];\n    target += log(Phi(sqrt((1 - rho[treat_idx]) * (1 + rho[treat_idx]))^(-1)*(mu_S[n_pos[n]]\n                               + rho[treat_idx] / sd_Y[treat_idx]\n                               * (Y[n] - mu_Y[n]))));\n    Y[n] ~ normal(mu_Y[n], sd_Y[treat_idx]);\n  }\n}\ngenerated quantities {\n  array[2] real rho = {Omega[1,3], Omega[2,3]};\n  real Y_estimand = 0;\n  {\n    vector[N] mu_S = X_S * b_S;\n    matrix[3,3] Sigma = quad_form_diag(Omega, append_row(sd_Y, [1]'));\n    int tot_11 = 0;\n    for (n in 1:N) {\n      vector[3] errors = multi_normal_rng(rep_vector(0,3), Sigma);\n      if (errors[3] > -mu_S[n] - gamma_S && errors[3] > -mu_S[n]) {\n        real mu_Y_cf = X_S[n,1:D_Y] * b_Y;\n        real Y_0 = errors[1] + mu_Y_cf;\n        real Y_1 = errors[2] + mu_Y_cf + gamma_Y;\n        Y_estimand += Y_1 -  Y_0;\n        tot_11 += 1;\n      }\n    }\n    Y_estimand /= tot_11;\n  }\n}\n\n\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\n\nSome comments on the Stan code are in order. The data block is pretty straightforward, though it’s worth keeping in mind that we’ll need to keep track of three dimensions: the total number of data points, the number of infected and uninfected patients. This is because we’ll only have access to viral loads in the infected patients.\nLet’s generate some data to see how the errors change how the .\nFirst, let’s see whether the\n\n\n\n\nReferences\n\nHeckman, James J. 1979. “Sample Selection Bias as a Specification Error.” Econometrica 47 (1): 153. https://doi.org/10.2307/1912352.\n\n\nHudgens, Michael G., Antje Hoering, and Steven G. Self. 2003. “On the Analysis of Viral Load Endpoints in HIV Vaccine Trials.” Statistics in Medicine 22 (14): 2281–98. https://doi.org/10.1002/sim.1394."
  },
  {
    "objectID": "posts/beta-processes-in-stan.html",
    "href": "posts/beta-processes-in-stan.html",
    "title": "Gamma and Beta Processes in Stan",
    "section": "",
    "text": "rdirichlet <- function(n, alpha) {\n  d <- length(alpha)\n  rates <- rep(1, d)\n  gammas <- replicate(n = n,\n                      rgamma(d, shape = alpha,\n                             rate = rates)\n                      )\n  draws <- sweep(gammas, MARGIN = 2, STATS = colSums(gammas), FUN = \"/\")\n  return(t(draws))\n}\n\nI taught the PhD-level survival analysis course in the 2024 Winter quarter at Oregon State (website). The prior years’ courses in survival analysis were exclusively focused on Frequentist nonparametric and semiparametrci methods for inference: Kaplan-Meier, Cox proportional hazards model, etc. I wanted to see if I could bring a bit more Bayesian inference into the course. Ultimately, I didn’t include any Bayesian nonparametrics, but it gave me the opportunity to try some simple nonparametric methods in Stan (Stan is always my first choice to develop bespoke models, it’s second nature at this point).\nMy starting point to learn more about Bayesian nonparametric survival analysis modeling was our textbook for the course Klein and Moeschberger (Klein and Moeschberger 2003). They review several nonparameteric Bayesian models for survival analysis in 6.4: Dirichlet processes, and beta processes. Both methods put a nonparametric prior over the survival function, or \\(S(t)\\), but they do so in different ways.\nThese methods are used to model time-to-event data. Let’s call the time-to-event random variable \\(T_i\\) for an individual \\(i\\). The survival function for \\(T_i\\) is the complement of the cumulative distribution function, \\(P(T_i \\leq t)\\):\n\\[\nS(t) = P(T_i > t) = 1 - P(T_i \\leq t)\n\\]\nTime-to-event data is complicated by the fact that we often can’t observe all times to failure, but instead we observe only a subset of these failures corresponding to those times that occur prior to a censoring time. In the simplest case, called Type I censoring, we observe only \\(T_i\\) that are less than a constant \\(C\\).\nThe standard set-up for survival analysis data is to define two new random variables for each observation \\(i\\):\n\\[\nX_i = 1(T_i \\leq C) T_i + C 1(T_i > C), \\Delta_i = 1(T_i \\leq C).\n\\]\nNow we have a single time-to-event random variable \\(X_i\\) which is equal to \\(T_i\\) if \\(T_i\\) occurs prior to the censoring time \\(C\\). In this case, \\(\\Delta_i = 1\\). If \\(T_i\\) occurs after the censoring time, we don’t observe \\(T_i\\) but rather we observe only the censoring time. This sort of censoring set up occurs if we enroll patients in a randomized controlled trial, administer a treatment, and follow each patient for, say, 10 months. If the event of interest occurs within the 10-month window, we will have observed \\(T_i\\), otherwise, our only knowledge of the event time is that it is greater than 10-months.\nBack to our modeling strategy to learn about \\(S(t)\\) from a set of data, \\(\\{(X_i, \\Delta_i), i.= 1, \\dots, n\\}\\). One strategy to model the survival function nonparametrically is to define a partition of the positive real line, \\(\\{t_j, j = 0, \\dots, J\\}\\) with \\(t_0 = 0, t_J = \\infty\\)to model increments of the survival function \\(S(t_j) - S(t_{j-1})\\) over this partition. These increments will be between \\((0,1)\\) and they will sum to \\(1\\). A natural distribution for these random variables is a Dirichlet distribution, which respects the natural constraints of the increments process:\n\\[\n(S(t_0) - S(t_1), \\dots, S(t_j) - S(t_{j-1}), S(t_{J-1}) - S(t_{J})) \\sim \\text{Dirichlet}(c\\,\\boldsymbol{\\alpha}).\n\\]\nTypically, \\(\\boldsymbol{\\alpha}\\) is chosen so that it too is a function of time \\(t\\) and that it corresponds to a known parametric survival function, like say the exponential function: \\(S(t) = \\exp(-\\lambda t)\\).\nWhen we combine the prior over the survival function with counts of failures within an interval\n, but the function that we’re modeling is different between the models.\nThe Dirichlet process directly models the unknown survival function, while the beta process models the unknown cumulative hazard function.\nAnother popular choice for Bayesian nonparametric survival analysis, not covered by KM for some reason, is a gamma process. My first choice was a gamma process.\nOne of the proposed methods is the Beta process, which is rigorously developed in (Hjort 1990).\n\n\n\n\nReferences\n\nHjort, Nils Lid. 1990. “Nonparametric Bayes Estimators Based on Beta Processes in Models for Life History Data.” The Annals of Statistics 18 (3). https://doi.org/10.1214/aos/1176347749.\n\n\nKlein, John P., and Melvin L. Moeschberger. 2003. Survival Analysis: Techniques for Censored and Truncated Data. 2nd ed. Statistics for Biology and Health. New York: Springer."
  },
  {
    "objectID": "st_625_w_24.html",
    "href": "st_625_w_24.html",
    "title": "ST 625 - W 24",
    "section": "",
    "text": "Required: Survival Analysis: Techniques for censored and truncated data, Klein and Moeschberger\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nNotes"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rob Trangucci",
    "section": "",
    "text": "I am an assistant professor of Statistics at Oregon State University. My research focuses on novel statistical methodology in missing data analysis and causal inference for problems in epidemiology, designing Bayesian methods for survey inference, and creating tools to quantify how priors impact posterior inferences. Before I returned to academia to pursue a doctorate in statistics, I worked as a data scientist for a fintech startup, a statistical consultant for a Big Five publisher, and a core developer for the Stan statistical modeling and inference platform (https://mc-stan.org/). I earned a PhD in Statistics from the University of Michigan, an M.A. in Quantitative Methods in the Social Sciences from Columbia University, and a B.A. in Physics from Bucknell University."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Rob Trangucci",
    "section": "Interests",
    "text": "Interests\n\nCausal inference for vaccine efficacy\nMissing data\nPrincipal stratification\nPrior influence\nMultilevel regression and poststratification (MRP)\nBayesian inference"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rob Trangucci",
    "section": "Education",
    "text": "Education\n\nPhD in Statistics, 2023\n\nUniversity of Michigan\n\nMA in Quantitative Methods in the Social Science, 2014\n\nColumbia University\n\nBA in Physics, 2009\n\nBucknell University"
  },
  {
    "objectID": "st_625_w_25.html",
    "href": "st_625_w_25.html",
    "title": "ST 625 - W 25",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nNotes\n\n\n\n\n\nHW 1\nHW 2\nHW 3\nHW 4\nHW 5\nHW 6"
  },
  {
    "objectID": "st_599_w_25.html",
    "href": "st_599_w_25.html",
    "title": "ST 599 - W 25: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are identifiable for a given data generating process.\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes\nLecture 3 notes\nLecture 4 notes\nLecture 5 notes\nLecture 6 notes\nLecture 7 notes\nLecture 8 notes\nLecture 9 notes\nLecture 10 notes\nLecture 11 notes\nLecture 12 notes\nLecture 13 notes\nLecture 14 notes\nLecture 15 notes\nLecture 16 notes\n\n\n\n\n\nHW 1\nHW 2\nHW 3"
  },
  {
    "objectID": "courses.html#academic-courses",
    "href": "courses.html#academic-courses",
    "title": "Courses",
    "section": "",
    "text": "STAT 625 - Winter 2024\n\n\n\n\n\nSTAT 625 - Winter 2025\nSTAT 599 - Winter 2025\n\n\n\n\n\nSTAT 625 - Winter 2026\nSTAT 599 - Winter 2026"
  },
  {
    "objectID": "st_599_w_26.html",
    "href": "st_599_w_26.html",
    "title": "ST 599 - W 26: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are appropriately estimated under missingness for a given missing data technique (complete case analysis, multiple imputation, data augmentation, etc.).\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReading\nLO\nAssignments\n\n\n\n\n1/6/2026\nMissingness mechanisms and patterns\nLR 1.1-1.4\n1, 2\nNA\n\n\n1/8/2026\nMissing data techniques Complete case analysis  Weighting\nLR Ch. 3\n1, 2\nNA\n\n\n1/13/2026\nSingle imputation techniques\nLR Ch. 4\n1, 2\nNA\n\n\n1/15/2026\nMultiple imputation techniques\nLR Ch. 5\n1, 2\nNA\n\n\n1/20/2026\nLikelihood theory\nLR Ch. 6\n1, 5\nNA\n\n\n1/22/2026\nLikelihood theory\nLR Ch. 6\n1, 5\n1\n\n\n\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes\nLecture 3 notes\nLecture 4 notes\nLecture 5 notes\nLecture 6 notes\nLecture 7 notes\n\n\n\n\n\nHW 1"
  },
  {
    "objectID": "st_599_w_26.html#syllabus",
    "href": "st_599_w_26.html#syllabus",
    "title": "ST 599 - W 26: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are appropriately estimated under missingness for a given missing data technique (complete case analysis, multiple imputation, data augmentation, etc.).\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReading\nLO\nAssignments\n\n\n\n\n1/6/2026\nMissingness mechanisms and patterns\nLR 1.1-1.4\n1, 2\nNA\n\n\n1/8/2026\nMissing data techniques Complete case analysis  Weighting\nLR Ch. 3\n1, 2\nNA\n\n\n1/13/2026\nSingle imputation techniques\nLR Ch. 4\n1, 2\nNA\n\n\n1/15/2026\nMultiple imputation techniques\nLR Ch. 5\n1, 2\nNA\n\n\n1/20/2026\nLikelihood theory\nLR Ch. 6\n1, 5\nNA\n\n\n1/22/2026\nLikelihood theory\nLR Ch. 6\n1, 5\n1\n\n\n\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes\nLecture 3 notes\nLecture 4 notes\nLecture 5 notes\nLecture 6 notes\nLecture 7 notes\n\n\n\n\n\nHW 1"
  },
  {
    "objectID": "st_625_w_26.html",
    "href": "st_625_w_26.html",
    "title": "ST 625 - W 26",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\n\n\n\n\n\nMaps between Weibull parameterizations\nHW 1\nHW 2\nHW 3"
  },
  {
    "objectID": "st_625_w_26.html#syllabus",
    "href": "st_625_w_26.html#syllabus",
    "title": "ST 625 - W 26",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1\nLecture 2\nLecture 3\nLecture 4\nLecture 5\nLecture 6\nLecture 7\nLecture 8\n\n\n\n\n\nMaps between Weibull parameterizations\nHW 1\nHW 2\nHW 3"
  },
  {
    "objectID": "papers.html#published-papers",
    "href": "papers.html#published-papers",
    "title": "My papers",
    "section": "",
    "text": "Here’s a selection of my published papers:\n\nModeling Racial/ethnic Differences in COVID-19 Incidence with Covariates Subject to Non-random Missingness (Trangucci, Chen, and Zelner 2023)\nRacial Disparities in Coronavirus Disease 2019 (COVID-19) Mortality Are Driven by Unequal Infection Risks (Zelner et al. 2021)\nQuantifying Observed Prior Impact (Jones, Trangucci, and Chen 2021)\nModeling Spatial Risk of Diarrheal Disease Associated with Household Proximity to Untreated Wastewater Used for Irrigation in the Mezquital Valley, Mexico (Contreras Jesse D. et al. 2020)\nBayesian Hierarchical Weighting Adjustment and Survey Inference. (Si 2020)\nEffects of Sequential Influenza A(H1N1)Pdm09 Vaccination on Antibody Waning (Zelner et al. 2019)"
  },
  {
    "objectID": "survival-material/lecture-1.html",
    "href": "survival-material/lecture-1.html",
    "title": "Lecture 1",
    "section": "",
    "text": "This introduction is based in part on Klein, Moeschberger, et al. (2003), and in part on Aalen, Borgan, and Gjessing (2008) plus Fleming and Harrington (2005).\nSurvival analysis is the modeling and analysis of time-to-event data; this means we will be studying how to model nonnegative random variables (time will always be measured in such a way so that the observations are nonnegative). Think about a clinical trial for a new COVID vaccine and how you might model the length of time between study entry and infection in each arm of the trial. Let \\(X_i\\) be the time from trial entry to infection for the \\(i\\)-th participant. These sorts of trials are typically run until a prespecified number of people have become infected. Let \\(n\\) be the total number of participants in the trial and let \\(r\\) be the prespecified number of infections. Let \\(T_i\\) be the observed infection time for the \\(i\\)-th participant. This means that for \\(r\\) participants, \\(T_i = X_i\\), but for \\(n-r\\) participants we know only that the time-to-infection is larger than the observed time. Let \\(C_i\\) denote the time from study entry for participant \\(i\\) to study end. Then \\(T_i = \\min (X_i, C_i)\\), and let \\(\\delta_i = \\ind{T_i = X_i}\\). The density of \\(T_i\\) is related to the joint probability for \\(X_i\\) and \\(C_i\\), which is indexed by a possibly infinite dimensional parameter \\(\\theta\\): \\(P_{\\theta}(X_i &gt; t, C_i &gt; c)\\). When \\(\\delta_i = 1\\), and \\(T_i = X_i\\), the likelihood of the observation is \\[\n\\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_i &gt; u, C_i &gt; t)\\rp\\right\\rvert_{u = t},\n\\] while the likelihood for \\(\\delta_i = 0\\) is \\[\n\\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_i &gt; t, C_i &gt; u)\\rp\\right\\rvert_{u = t},\n\\] Then \\(T_i = C_i\\) for the other \\(n-r\\) participants. Under the null hypothesis that the vaccine has no effect, the population distribution function for all \\(n\\) participants for \\(X_i, C_i\\) is \\(P_{\\theta}(X_1 &gt; x, C_1 &gt; c)\\) (i.e. the distribution for survival times in the treatment group and the placebo group is the same). Then the joint density for the observed infection times is as follows: \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\theta) & = n! \\prod_{i=1}^r \\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_1 &gt; u, C_1 &gt; t_{(i)})\\rp\\right\\rvert_{u = t_{(i)}} \\\\\n& \\times \\prod_{i=r+1}^n \\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_1 &gt; t_{(i)}, C_1 &gt; u)\\rp\\right\\rvert_{u = t_{(i)}},\n\\end{align*}\\] where \\(t_{(i)}\\) is the \\(i\\)-th order statistic of the set \\(\\{t_1, \\dots, t_n\\}\\). Note that this is different from most other data analysis where missing observations are not expected to occur with much frequency. On the contrary, in survival analysis, missingness, both truncation and censoring are expected to occur with nearly every dataset, so much of our time will be spent ensuring our methods work when data arise with these peculiarities.\n\n\nNow suppose that \\(X_1 \\indy C_1\\), and that \\(\\theta\\) partitions into \\(\\eta\\) and \\(\\phi\\), such that \\[\nP_{\\theta}(X_1 &gt; x, C_1 &gt; c) = P_{\\eta}(X_1 &gt; x)P_{\\phi}(C_1 &gt; c).\n\\] Then we can rewrite the joint observational density for \\(T_i\\) as: \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\theta) & = n! \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)}) \\\\\n& \\times \\lp \\prod_{i=1}^r P_{\\phi}(C_1 &gt; t_{(i)}) \\rp \\prod_{i=r+1}^n f_C(t_{(i)} ;\\, \\phi).\n\\end{align*}\\] If we are only interested about inference about \\(\\eta\\), the parameters that govern the distribution of the true time-to-infection random variables, we can ignore the the distribution for the censoring random variables \\(C_1\\), and maximize the likelihood because, in \\(\\eta\\): \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\eta) \\propto \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)})  \n\\end{align*}\\] We will talk in more detail about censoring in the coming lectures.\n\n\n\nAalen, Borgan, and Gjessing (2008) notes that we cannot even compute a simple mean in this situation, so something like a t-test will be useless. As an aside, let’s try to compute a mean from the data above. Let \\(\\bar{T} = \\frac{1}{n} \\sum_{i=1}^n T_i\\). We can show that \\(\\lim_{n \\to \\infty} \\bar{T} \\leq \\Exp{X_i}\\) with probability \\(1\\).\n\nProof. Let \\(T_i = X_i \\ind{X_i \\leq C_i} + C_i \\ind{X_i &gt; C_i}\\). Then by the SLLN \\(\\bar{T} \\overset{\\text{a.s.}}{\\to} \\Exp{T_i}\\). \\[\\begin{align*}\n\\Exp{T_i} & = \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{C_i \\ind{X_i &gt; C_i}} \\\\\n& \\leq \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{X_i \\ind{X_i &gt; C_i}} = \\Exp{X_i}\n\\end{align*}\\]\n\n\n\n\nHow can we compute the mean time to infection then? One way to estimate the mean time to infection is to first estimate the function \\(S_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i &gt; t)\\), which is also known as the survival function. Recall this fact about non-negative random variables \\(X_i \\geq 0\\) w.p. 1: \\[\\begin{align*}\n\\Exp{X_i} = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt    \n\\end{align*}\\] This follows from an application of Fubini’s theorem applied to the integral: \\[\\begin{align*}\n\\Exp{X_i} & = \\int_0^\\infty u dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dt \\, dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dP_{X_i}(u ;\\, \\theta) dt \\\\\n& = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt\n\\end{align*}\\]\n\n\nLet \\(F_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i \\leq t)\\). Then because the survival function is defined as \\(S_{X_i}(t ;\\, \\theta) = 1 - F_{X_i}(t ;\\, \\theta)\\) (also known as the complementary CDF) the survival function inherits its properties from the CDF. The survival function:\n\n\\(S_{X_i}(t ;\\, \\theta)\\) is a nonincreasing function\n\\(S_{X_i}(0 ;\\, \\theta) = 1\\)\n\\(\\lim_{t\\to\\infty} S_{X_i}(t ;\\, \\theta) = 0\\)\nHas lefthand limits: \\(\\lim_{s \\nearrow t} S_{X_i}(s ;\\, \\theta) = S_{X_i}(t-;\\, \\theta).\\)\nIs right continuous: \\(\\lim_{s \\searrow t} S_{X_i}(s;\\, \\theta) = S_{X_i}(t;\\, \\theta).\\)\n\nAn example of a discrete survival function is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Example plot of a survival function for a discrete survival time, bounded between \\([0,10]\\)"
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-indycensor",
    "href": "survival-material/lecture-1.html#sec-indycensor",
    "title": "Lecture 1",
    "section": "",
    "text": "Now suppose that \\(X_1 \\indy C_1\\), and that \\(\\theta\\) partitions into \\(\\eta\\) and \\(\\phi\\), such that \\[\nP_{\\theta}(X_1 &gt; x, C_1 &gt; c) = P_{\\eta}(X_1 &gt; x)P_{\\phi}(C_1 &gt; c).\n\\] Then we can rewrite the joint observational density for \\(T_i\\) as: \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\theta) & = n! \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)}) \\\\\n& \\times \\lp \\prod_{i=1}^r P_{\\phi}(C_1 &gt; t_{(i)}) \\rp \\prod_{i=r+1}^n f_C(t_{(i)} ;\\, \\phi).\n\\end{align*}\\] If we are only interested about inference about \\(\\eta\\), the parameters that govern the distribution of the true time-to-infection random variables, we can ignore the the distribution for the censoring random variables \\(C_1\\), and maximize the likelihood because, in \\(\\eta\\): \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\eta) \\propto \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)})  \n\\end{align*}\\] We will talk in more detail about censoring in the coming lectures."
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-prop-surv-fun",
    "href": "survival-material/lecture-1.html#sec-prop-surv-fun",
    "title": "Lecture 1",
    "section": "3.1 Properties of the survival function",
    "text": "3.1 Properties of the survival function\nLet \\(F_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i \\leq t)\\). Then because the survival function is defined as \\(S_{X_i}(t ;\\, \\theta) = 1 - F_{X_i}(t ;\\, \\theta)\\) (also known as the complementary CDF) the survival function inherits its properties from the CDF. The survival function:\n\n\\(S_{X_i}(t ;\\, \\theta)\\) is a nonincreasing function\n\\(S_{X_i}(0 ;\\, \\theta) = 1\\)\n\\(\\lim_{t\\to\\infty} S_{X_i}(t ;\\, \\theta) = 0\\)\nHas lefthand limits: \\(\\lim_{s \\nearrow t} S_{X_i}(s ;\\, \\theta) = S_{X_i}(t-;\\, \\theta).\\)\nIs right continuous: \\(\\lim_{s \\searrow t} S_{X_i}(s;\\, \\theta) = S_{X_i}(t;\\, \\theta).\\)\n\nAn example of a discrete survival function is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Example plot of a survival function for a discrete survival time, bounded between \\([0,10]\\)"
  },
  {
    "objectID": "survival-material/lecture-2.html",
    "href": "survival-material/lecture-2.html",
    "title": "Lecture 2",
    "section": "",
    "text": "Another way to characterize the random variable \\(X_i\\) is the hazard function, which is typically denoted as \\(\\lambda(t)\\) or \\(h(t)\\) and is defined as \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X_i &lt; t + \\Delta t \\mid X_i \\geq t}{\\theta} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t }{\\theta}}\n\\end{align*}\\] First, note that we can define \\(\\Prob{X_i \\geq t}{\\theta}\\) in terms of the survival function as: \\[\n\\Prob{X_i \\geq t}{\\theta} = \\lim_{s\\nearrow t} S_{X_i}(s ;\\, \\theta).\n\\] Using the notation introduced in the last lecture, we can write this as \\[\n\\Prob{X_i \\geq t}{\\theta} = S_{X_i}(t- ;\\, \\theta).\n\\] Of course, when \\(X_i\\) is absolutely continuous,\\(S_{X_i}(t- ;\\, \\theta) = S_{X_i}(t ;\\, \\theta)\\), but when \\(X_i\\) is discrete, or mixed discrete and continuous, as noted above, it is not true in general that the survival function is left-continuous.\nA few things to note about \\(\\lambda_{X_i}(t ;\\, \\theta)\\): when \\(X_i\\) is an absolutely continuous random variable, which occurs when we’re considering survival in continuous time, we can write this in terms of the probability density function \\(f_{X_i}(t ;\\, \\theta)\\) and the cumulative distribution function \\(F_{X_i}(t ;\\, \\theta)\\): \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{F_{X_i}(t + \\Delta t;\\, \\theta) - F_{X_i}(t;\\, \\theta)}{\\Delta t} \\times \\frac{1}{1 - F_{X_i}(t;\\, \\theta)} \\\\\n& = \\frac{f_{X_i}(t;\\, \\theta)}{1 - F_{X_i}(t;\\, \\theta)}.\n\\end{align*}\\] Let’s examine how the survival function and the hazard function fit together. \\[\n\\lambda_{X_i}(t) = \\frac{f_{X_i}(t ;\\, \\theta)}{S_{X_i}(t- ;\\, \\theta)}.\n\\] Note that we can write the hazard function in terms of the survival function instead of the density, when \\(X_i\\) is absolutely continuous: \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& =  \\frac{1}{S_{X_i}(t ;\\, \\theta)} \\times \\lim_{\\Delta t \\searrow 0}\\frac{S_{X_i}(t ;\\, \\theta)-S_{X_i}(t + \\Delta t ;\\, \\theta) }{\\Delta t} \\\\\n& = \\frac{1}{S_{X_i}(t ;\\, \\theta)} \\times -\\frac{d}{dt} S_{X_i}(t ;\\, \\theta).\n\\end{align*}\\] This implies that \\[\n\\lambda_{X_i}(t) = -\\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta).\n\\] If we integrate both sides, we get another important identity in survival analysis: \\[\\begin{align}\n   \\int_{0}^u \\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta) dt & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\\\\n   \\log S_{X_i}(u ;\\, \\theta) - \\log S_{X_i}(0 ;\\, \\theta) & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\quad \\text{note}\\,\\,\\, S_{X_i}(0 ;\\, \\theta) = 1\\\\\n   S_{X_i}(u ;\\, \\theta) & = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp \\label{eq:exp-hazard}\n\\end{align}\\]\n\n\nThe relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-2.html#properties-of-the-hazard-function",
    "href": "survival-material/lecture-2.html#properties-of-the-hazard-function",
    "title": "Lecture 2",
    "section": "",
    "text": "The relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-1.html#properties-of-the-hazard-function",
    "href": "survival-material/lecture-1.html#properties-of-the-hazard-function",
    "title": "Lecture 1",
    "section": "4.1 Properties of the hazard function",
    "text": "4.1 Properties of the hazard function\nThe relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-mttf",
    "href": "survival-material/lecture-1.html#sec-mttf",
    "title": "Lecture 1",
    "section": "",
    "text": "Aalen, Borgan, and Gjessing (2008) notes that we cannot even compute a simple mean in this situation, so something like a t-test will be useless. As an aside, let’s try to compute a mean from the data above. Let \\(\\bar{T} = \\frac{1}{n} \\sum_{i=1}^n T_i\\). We can show that \\(\\lim_{n \\to \\infty} \\bar{T} \\leq \\Exp{X_i}\\) with probability \\(1\\).\n\nProof. Let \\(T_i = X_i \\ind{X_i \\leq C_i} + C_i \\ind{X_i &gt; C_i}\\). Then by the SLLN \\(\\bar{T} \\overset{\\text{a.s.}}{\\to} \\Exp{T_i}\\). \\[\\begin{align*}\n\\Exp{T_i} & = \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{C_i \\ind{X_i &gt; C_i}} \\\\\n& \\leq \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{X_i \\ind{X_i &gt; C_i}} = \\Exp{X_i}\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-survfun",
    "href": "survival-material/lecture-1.html#sec-survfun",
    "title": "Lecture 1",
    "section": "",
    "text": "How can we compute the mean time to infection then? One way to estimate the mean time to infection is to first estimate the function \\(S_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i &gt; t)\\), which is also known as the survival function. Recall this fact about non-negative random variables \\(X_i \\geq 0\\) w.p. 1: \\[\\begin{align*}\n\\Exp{X_i} = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt    \n\\end{align*}\\] This follows from an application of Fubini’s theorem applied to the integral: \\[\\begin{align*}\n\\Exp{X_i} & = \\int_0^\\infty u dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dt \\, dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dP_{X_i}(u ;\\, \\theta) dt \\\\\n& = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt\n\\end{align*}\\]\n\n\nLet \\(F_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i \\leq t)\\). Then because the survival function is defined as \\(S_{X_i}(t ;\\, \\theta) = 1 - F_{X_i}(t ;\\, \\theta)\\) (also known as the complementary CDF) the survival function inherits its properties from the CDF. The survival function:\n\n\\(S_{X_i}(t ;\\, \\theta)\\) is a nonincreasing function\n\\(S_{X_i}(0 ;\\, \\theta) = 1\\)\n\\(\\lim_{t\\to\\infty} S_{X_i}(t ;\\, \\theta) = 0\\)\nHas lefthand limits: \\(\\lim_{s \\nearrow t} S_{X_i}(s ;\\, \\theta) = S_{X_i}(t-;\\, \\theta).\\)\nIs right continuous: \\(\\lim_{s \\searrow t} S_{X_i}(s;\\, \\theta) = S_{X_i}(t;\\, \\theta).\\)\n\nAn example of a discrete survival function is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Example plot of a survival function for a discrete survival time, bounded between \\([0,10]\\)"
  },
  {
    "objectID": "survival-material/lecture-1.html#hazard-function",
    "href": "survival-material/lecture-1.html#hazard-function",
    "title": "Lecture 1",
    "section": "",
    "text": "Another way to characterize the random variable \\(X_i\\) is the hazard function, which is typically denoted as \\(\\lambda(t)\\) or \\(h(t)\\) and is defined as \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X_i &lt; t + \\Delta t \\mid X_i \\geq t}{\\theta} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t }{\\theta}}\n\\end{align*}\\] First, note that we can define \\(\\Prob{X_i \\geq t}{\\theta}\\) in terms of the survival function as: \\[\n\\Prob{X_i \\geq t}{\\theta} = \\lim_{s\\nearrow t} S_{X_i}(s ;\\, \\theta).\n\\] Using the notation introduced in Section 1.3.1, we can write this as \\[\n\\Prob{X_i \\geq t}{\\theta} = S_{X_i}(t- ;\\, \\theta).\n\\] Of course, when \\(X_i\\) is absolutely continuous,\\(S_{X_i}(t- ;\\, \\theta) = S_{X_i}(t ;\\, \\theta)\\), but when \\(X_i\\) is discrete, or mixed discrete and continuous, as noted above, it is not true in general that the survival function is left-continuous.\nA few things to note about \\(\\lambda_{X_i}(t ;\\, \\theta)\\): when \\(X_i\\) is an absolutely continuous random variable, which occurs when we’re considering survival in continuous time, we can write this in terms of the probability density function \\(f_{X_i}(t ;\\, \\theta)\\) and the cumulative distribution function \\(F_{X_i}(t ;\\, \\theta)\\): \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{F_{X_i}(t + \\Delta t;\\, \\theta) - F_{X_i}(t;\\, \\theta)}{\\Delta t} \\times \\frac{1}{1 - F_{X_i}(t;\\, \\theta)} \\\\\n& = \\frac{f_{X_i}(t;\\, \\theta)}{1 - F_{X_i}(t;\\, \\theta)}.\n\\end{align*}\\] Let’s examine how the survival function and the hazard function fit together. \\[\n\\lambda_{X_i}(t) = \\frac{f_{X_i}(t ;\\, \\theta)}{S_{X_i}(t- ;\\, \\theta)}.\n\\] Note that we can write the hazard function in terms of the survival function instead of the density, when \\(X_i\\) is absolutely continuous: \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{S_{X_i}(t ;\\, \\theta)-S_{X_i}(t + \\Delta t ;\\, \\theta) }{\\Delta t} \\times \\frac{1}{S_{X_i}(t ;\\, \\theta)} \\\\\n& = -\\frac{d}{dt} S_{X_i}(t ;\\, \\theta)/S_{X_i}(t ;\\, \\theta).\n\\end{align*}\\] This implies that \\[\n\\lambda_{X_i}(t) = -\\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta).\n\\] If we integrate both sides, we get another important identity in survival analysis: \\[\\begin{align}\n   \\int_{0}^u \\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta) dt & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\\\\n   \\log S_{X_i}(u ;\\, \\theta) - \\log S_{X_i}(0 ;\\, \\theta) & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\quad \\text{note}\\,\\,\\, S_{X_i}(0 ;\\, \\theta) = 1\\\\\n   S_{X_i}(u ;\\, \\theta) & = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp \\label{eq:exp-hazard}\n\\end{align}\\]\n\n\nThe relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-1.html#density-function-for-survival-time",
    "href": "survival-material/lecture-1.html#density-function-for-survival-time",
    "title": "Lecture 1",
    "section": "",
    "text": "Given that we have \\(S_{X_i}(t;\\,\\theta)\\) and \\(\\lambda(t) = \\frac{f_{X_i}(t;\\,\\theta)}{S_{X_i}(t-;\\,\\theta)}\\), we can recover the density, \\(f_{X_i}(t;\\,\\theta)\\) easily: \\[\nf_{X_i}(t;\\,\\theta) = \\lambda_{X_i}(t) S_{X_i}(t-;\\,\\theta)\n\\]"
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-cumu-haz",
    "href": "survival-material/lecture-1.html#sec-cumu-haz",
    "title": "Lecture 1",
    "section": "",
    "text": "One final important quantity that describes a survival distribution is that of , which we’ll denote as \\(\\Lambda_{X_i}(t)\\), though it is also denoted as \\(H(t)\\) in . This is defined as you might expect: \\[\n\\Lambda_{X_i}(t) = \\int_{0}^t \\lambda_{X_i}(u) du.\n\\] It has the important property that for any absolutely continuous failure time \\(X_i\\) with a given cumulative hazard function, the random variable \\(Y_i = \\Lambda_{X_i}(X_i)\\) is exponentially distributed with rate \\(1\\). The derivation is straightforward. Remember that \\(P(X_i &gt; t) = \\exp\\lp-\\Lambda_{X_i}(t)\\rp\\) \\[\\begin{align*}\nP(\\Lambda_{X_i}(X_i) &gt; t) & = P(X_i &gt; \\Lambda_{X_i}^{-1}(t)) \\\\\n& = \\exp\\lp-\\Lambda_{X_i}(\\Lambda_{X_i}^{-1}(t))\\rp \\\\\n& = \\exp\\lp-t\\rp\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-1.html#discrete-survival-time",
    "href": "survival-material/lecture-1.html#discrete-survival-time",
    "title": "Lecture 1",
    "section": "",
    "text": "We’ve been working with continuous survival times until now. If \\(X_i\\) is a discrete random variable with support on \\(\\{t_1, t_2, \\dots\\}\\), we lose some of the tidyness of the previous derivations. We can define the distribution of \\(X_i\\) in terms of the survival function, \\(P_{\\theta}(X_i &gt; t)\\). First let \\(p_j = P_{\\theta}(X_i = t_j)\\), so \\[\nS_{X_i}(t;\\,\\theta) = P_{\\theta}(X_i &gt; t) = \\sum_{j \\mid t_j &gt; t} p_j\n\\] We can also define the hazard function for a discrete random variable: \\[\n\\lambda_{X_i}(t_j) = \\frac{p_j}{S_{X_i}(t_{j-1};\\,\\theta)} = \\frac{p_j}{p_j + p_{j+1} + \\dots}\n\\] Note that \\(p_j = S_{X_i}(t_{j-1};\\,\\theta) - S_{X_i}(t_{j};\\,\\theta)\\), then \\[\n\\lambda_{X_i}(t_j) = 1 - \\frac{S_{X_i}(t_j;\\,\\theta)}{S_{X_i}(t_{j-1};\\,\\theta)}.\n\\] If we let \\(t_0 = 0\\) then \\(S_{X_i}(t_0;\\,\\theta) = 1\\). This allows us to write the survival function in a sort of telescoping product: \\[\\begin{align*}\nP_{\\theta}(X_i &gt; t_j) & = P_{\\theta}(X_i &gt; t_0) \\frac{P_{\\theta}(X_i &gt; t_1)}{P_{\\theta}(X_i &gt; t_0)} \\frac{P_{\\theta}(X_i &gt; t_2)}{P_{\\theta}(X_i &gt; t_1)} \\dots \\frac{P_{\\theta}(X_i &gt; t_j)}{P_{\\theta}(X_i &gt; t_{j-1})} \\\\\n   & =  1 \\frac{S_{X_i}(t_1;\\,\\theta)}{S_{X_i}(t_0;\\,\\theta)}\\frac{S_{X_i}(t_2;\\,\\theta)}{S_{X_i}(t_1;\\,\\theta)}  \\dots \\frac{S_{X_i}(t_j;\\,\\theta)}{S_{X_i}(t_{j-1};\\,\\theta)}\n\\end{align*}\\] This yields another way to write \\(S_{X_i}(t;\\,\\theta)\\): \\[\\begin{align}\\label{eq:discrete-survival}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j \\leq t} (1 - \\lambda_{X_i}(t_j)).\n\\end{align}\\] It turns out that we can write the survival function for continuous random variables in the same way. ## Connection between discrete and continuous survival functions {#sec:disc-continue} Recall the definition of the hazard function: \\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html",
    "title": "Lecture-1",
    "section": "",
    "text": "Defined in the book roughly as missing values that would be meaningful for you analysis if it had been observed\nMissing data is ubiquitous\nI’d bet everyone has analyzed data with missing values\nWhat did you do with the units that had missing data?"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#introduction",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#introduction",
    "title": "Lecture-1",
    "section": "",
    "text": "Defined in the book roughly as missing values that would be meaningful for you analysis if it had been observed\nMissing data is ubiquitous\nI’d bet everyone has analyzed data with missing values\nWhat did you do with the units that had missing data?"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#missing-data-patterns",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#missing-data-patterns",
    "title": "Lecture-1",
    "section": "Missing data patterns",
    "text": "Missing data patterns\n\n\nCourse will focus on rectangular or tabular datasets\nAssume we have \\(p\\) measurements on \\(n\\) units, arranged into matrix \\(\\mathbf{Y}\\)\nWe can create an \\(n \\times p\\) matrix \\(\\mathbf{M}\\), where \\(\\mathbf{M}_{ij} = 1\\) if \\(\\mathbf{Y}_{ij}\\) is missing and \\(\\mathbf{M}_{ij} = 0\\) if \\(\\mathbf{Y}_{ij}\\) is observed\n\nFor the next few slides, we’ll use \\(n=10\\) and \\(p=3\\)"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#univariate-missingness",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#univariate-missingness",
    "title": "Lecture-1",
    "section": "Univariate missingness",
    "text": "Univariate missingness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Graph with M\nplot(1, type = \"n\", xlim = c(0.5, 5.5), ylim = c(0, 1), axes = FALSE, xlab = \"\", ylab = \"\")\n\n# Define the bar positions and heights\nblue_heights &lt;- c(1, 1, 0.4)\nred_heights &lt;- c(0, 0, 0.6)\n\n# Adjusted bar positions for no spacing\nx_positions &lt;- c(1, 1.5, 2)\nbar_width &lt;- 0.5\n\n# Draw the blue bars\nrect(x_positions[1] - bar_width / 2, 0, x_positions[1] + bar_width / 2, blue_heights[1], col = \"#ADD8E6\", border = \"blue\", lwd = 2)\nrect(x_positions[2] - bar_width / 2, red_heights[2], x_positions[2] + bar_width / 2, red_heights[2] + blue_heights[2], col = \"#ADD8E6\", border = \"blue\", lwd = 2)\nrect(x_positions[3] - bar_width / 2, red_heights[3], x_positions[3] + bar_width / 2, red_heights[3] + blue_heights[3], col = \"#ADD8E6\", border = \"blue\", lwd = 2)\n\n# Draw the red bars\nrect(x_positions[3] - bar_width / 2, 0, x_positions[3] + bar_width / 2, red_heights[3], col = \"#FFCCCB\", border = \"red\", lwd = 2)\n\n# Add annotations\ntext(x_positions[2], red_heights[2] - 0.2, \"?\", col = \"red\", cex = 1.5)\ntext(x_positions[3], red_heights[3] - 0.2, \"?\", col = \"red\", cex = 1.5)\n\n# Add category labels\ntext(x_positions[1], par(\"usr\")[4] + 0.05, labels = bquote(Y[1]), xpd = TRUE)\ntext(x_positions[2], par(\"usr\")[4] + 0.05, labels = bquote(Y[2]), xpd = TRUE)\ntext(x_positions[3], par(\"usr\")[4] + 0.05, labels = bquote(Y[3]), xpd = TRUE)\n\n\nx_positions &lt;- c(2.5, 3, 3.5)\nbar_width &lt;- 0.5\n\n# Draw the M columns\nrect(x_positions[1] - bar_width / 2, 0, x_positions[1] + bar_width / 2, 1, col = \"white\", border = \"blue\", lwd = 2)\nrect(x_positions[2] - bar_width / 2, 0, x_positions[2] + bar_width / 2, 1, col = \"white\", border = \"blue\", lwd = 2)\nrect(x_positions[3] - bar_width / 2, 0, x_positions[3] + bar_width / 2, 1, col = \"white\", border = \"blue\", lwd = 2)\ndf &lt;- data.frame(y = seq(0.05,0.95, length.out = 10),\n                 m1 = rep(0,10),\n                 m2 = rep(0,10),\n                 m3 = c(rep(1,6),rep(0,4)))\ntext(rep(x_positions[1],10), df$y, label = df$m1)\ntext(rep(x_positions[2],10), df$y, label = df$m2)\ntext(rep(x_positions[3],6), df$y[1:6], label = df$m3[1:6], col = \"red\")\ntext(rep(x_positions[3],4), df$y[7:10], label = df$m3[7:10])\n\n# Add category labels\ntext(x_positions[1], par(\"usr\")[4] + 0.05, labels = bquote(M[1]), xpd = TRUE)\ntext(x_positions[2], par(\"usr\")[4] + 0.05, labels = bquote(M[2]), xpd = TRUE)\ntext(x_positions[3], par(\"usr\")[4] + 0.05, labels = bquote(M[3]), xpd = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nExample for this sort of missingness would be unit nonresponse on a survey; \\(Y_1, Y_2\\) would be design variables that are known for all potential survey respondenents, while \\(Y_3\\) would be measurement of interest"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#does-missingness-matter",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#does-missingness-matter",
    "title": "Lecture-1",
    "section": "Does missingness matter?",
    "text": "Does missingness matter?\n\n\nThe book’s definition of missing data implies that knowing the missing value would meaningfully change our inferences\nWe can bound this"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html",
    "title": "Missing data lecture 1",
    "section": "",
    "text": "At first glance the title of our textbook, Statistical Analysis with Missing Data, seems redundant. What is statistics but the study of drawing conclusions from limited data? One of the most basic applications of statistics is about how to make inferences about a population quantity from a simple random sample of that population. The measurements from those who were not sampled are, by definition, missing. Because we have a simple random sample from our population, however, we can be sure that the sample mean of the measurements from the sampled units will be an unbiased estimator of the population-level mean. What if some of the sampled units refuse to participate in the survey? Suppose the survey asks about income, and some respondents refuse to report income?\nOur definition of missing data for this course will condition on the sample drawn; in other words, we will focus on values that haven’t been recorded for the sample of data we observe. Suppose we have a sample of \\(n\\) units, on which we have \\(K\\) measurements, collected into a \\(n \\times K\\) matrix \\(Y\\) with elements \\(y_{ij}\\).\nPaired with this matrix of measurements is another \\(n \\times K\\) matrix \\(M\\) with elements \\(m_{ij}\\), called the missingness indicator matrix. This matrix encodes the information about whether the \\((i,j)^\\mathrm{th}\\) element of \\(Y\\) is missing or not. Let \\(m_{ij} = 1\\) if \\(y_{ij}\\) is missing, and \\(0\\) if \\(y_{ij}\\) is observed.\nThroughout the course we’ll keep in mind that we’re never looking to explicitly fill in the missing values with a single ``best” value. Instead, we’re going to consider the distribution of possible values that could be filled in and look at how our estimates change for each filled-in dataset.\n\n\nThe textbook defines missing data roughly as missing values that would be meaningful for your analysis if it had been observed (Roderick JA Little and Rubin 2019). The word ``meaningful” is doing a lot of work here; we’ll need to define meaningful for ourselves. Conceptually, we need to define why data is missing in the first place. For example, let’s say we’re analyzing the results from a longitudinal trial comparing Infliximab for severe Crohn’s disease, which is an autoinflammatory disease and a category of inflammatory bowel disease (IBD) (note this is not IBS, or irritable bowel syndrome, which is fairly common), to a new treatment. The investigators are interested comparing Crohn’s Disease Activity Index (CDAI) between the two arms, which is a measure of the severity of symptoms in Crohn’s patients. Imagine two scenarios: one in which an enrolled patient subsequently drops out of the study after several infusions of the new treatment, and one in which an enrolled patient dies prior to the end of the study. In the first scenario, it makes sense to consider that patient’s measure of CDAI to be missing, whereas in the second scenario, it doesn’t make much sense to think about imputing a CDAI for someone who has died.\nLet’s say we’re in the first scenario, and we’re confronted with some proportion of patients that have dropped out of the study. Dropout is common in longitudinal studies; in one dataset we’ll encounter later investigating the efficacy of a treatment for schizophrenia, about 37% of patients dropped out by the end of the study (Van Der Elst et al. 2024). The default way to deal with missing data in R is to use na.omit. This is also known as complete case analysis (CC analysis). How much would just using the complete cases impact our inferences?\nBeing statisticians, we’ll focus on the bias and variance of our estimates. Let’s make things more concrete. Suppose in our Crohn’s trial the outcome \\(Y_{i}\\) is the change from baseline CDAI to CDAI at the final visit. Assume that all participants have an initial CDAI, so \\(M_i\\) is \\(1\\) if the individual dropped out prior the final visit. As for bias, one can show (read: you’ll show on HW 1) that the following relationship holds: \\[\n\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y} = \\frac{\\text{Cov}(Y, \\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] We can bound the magnitude of this expression by using Cauchy-Schwarz:\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\frac{\\text{SD}(Y) \\text{SD}(\\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] which simplifies to \\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}}\\text{SD}(Y)\n\\] This makes sense; for a given proportion of missing values, the larger the variance of \\(Y\\) the larger the potential bias will be by excluding some of them.\nThis standard deviation is of course not estimable because we don’t have the missing values of \\(Y\\), but for variables with bounded support \\(Y \\in [a, b]\\), we can get a further upper bound on the standard deviation using Popoviciu’s inequality: \\[\n\\text{Var}(Y) \\leq \\frac{(b - a)^2}{4}\n\\] In the next code cell, I’ve written an expression for the upper bound of CDAI.\n\nupper_bound_CDAI &lt;- \n  7 * 20 * 2 +  \n  1 * 30 +\n  7 * 3 * 5 +\n  7 * 4 * 7 +\n  6 * 20 +\n  5 * 10 +\n  (42 - 3) * 6 +\n  50\n\nThe upper bound is approximately 1100 (see (Best 2006) for more details, the only value that may not have a hard upper bound is the number of stools per day, which I’ve set to \\(20\\) above, but may be higher)\nThis is useful in our hypothetical example because the CDAI scale runs from \\([0,1100]\\)\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}} \\times 550\n\\]\n\n\n\n\n\n\n\n\n\nWhile these are worst-case bounds, this shows that even small proportions of missing values can impact inferences if the missingness is correlated with the outcome value.\nTo give a sense of how large a proportion of patient might have missing values, a recent clinical trial evaluating Skyrizi, a popular treatment for Crohn’s, had three treatment groups (1 placebo and two active treatment arms), of which 12%, 8% and 15% had missing values for CDAI at week 52.\nThere is also the variance to consider. Even if the covariance between the missing values is zero, we will lose efficiency by dropping observations that have missing values. In the case where our estimator is a sample mean, and there are \\(n\\) units with \\(m\\) missing values, the variance of the CC estimator will be larger by \\(1 + \\frac{m}{n - m}\\).\nThus, in many cases, even if there are only small proportions of missing values, it can make sense to use partial information from incomplete cases to improve our estimators."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#does-missingness-matter",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#does-missingness-matter",
    "title": "Missing data lecture 1",
    "section": "",
    "text": "The textbook defines missing data roughly as missing values that would be meaningful for your analysis if it had been observed (Roderick JA Little and Rubin 2019). The word ``meaningful” is doing a lot of work here; we’ll need to define meaningful for ourselves. Conceptually, we need to define why data is missing in the first place. For example, let’s say we’re analyzing the results from a longitudinal trial comparing Infliximab for severe Crohn’s disease, which is an autoinflammatory disease and a category of inflammatory bowel disease (IBD) (note this is not IBS, or irritable bowel syndrome, which is fairly common), to a new treatment. The investigators are interested comparing Crohn’s Disease Activity Index (CDAI) between the two arms, which is a measure of the severity of symptoms in Crohn’s patients. Imagine two scenarios: one in which an enrolled patient subsequently drops out of the study after several infusions of the new treatment, and one in which an enrolled patient dies prior to the end of the study. In the first scenario, it makes sense to consider that patient’s measure of CDAI to be missing, whereas in the second scenario, it doesn’t make much sense to think about imputing a CDAI for someone who has died.\nLet’s say we’re in the first scenario, and we’re confronted with some proportion of patients that have dropped out of the study. Dropout is common in longitudinal studies; in one dataset we’ll encounter later investigating the efficacy of a treatment for schizophrenia, about 37% of patients dropped out by the end of the study (Van Der Elst et al. 2024). The default way to deal with missing data in R is to use na.omit. This is also known as complete case analysis (CC analysis). How much would just using the complete cases impact our inferences?\nBeing statisticians, we’ll focus on the bias and variance of our estimates. Let’s make things more concrete. Suppose in our Crohn’s trial the outcome \\(Y_{i}\\) is the change from baseline CDAI to CDAI at the final visit. Assume that all participants have an initial CDAI, so \\(M_i\\) is \\(1\\) if the individual dropped out prior the final visit. As for bias, one can show (read: you’ll show on HW 1) that the following relationship holds: \\[\n\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y} = \\frac{\\text{Cov}(Y, \\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] We can bound the magnitude of this expression by using Cauchy-Schwarz:\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\frac{\\text{SD}(Y) \\text{SD}(\\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] which simplifies to \\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}}\\text{SD}(Y)\n\\] This makes sense; for a given proportion of missing values, the larger the variance of \\(Y\\) the larger the potential bias will be by excluding some of them.\nThis standard deviation is of course not estimable because we don’t have the missing values of \\(Y\\), but for variables with bounded support \\(Y \\in [a, b]\\), we can get a further upper bound on the standard deviation using Popoviciu’s inequality: \\[\n\\text{Var}(Y) \\leq \\frac{(b - a)^2}{4}\n\\] In the next code cell, I’ve written an expression for the upper bound of CDAI.\n\nupper_bound_CDAI &lt;- \n  7 * 20 * 2 +  \n  1 * 30 +\n  7 * 3 * 5 +\n  7 * 4 * 7 +\n  6 * 20 +\n  5 * 10 +\n  (42 - 3) * 6 +\n  50\n\nThe upper bound is approximately 1100 (see (Best 2006) for more details, the only value that may not have a hard upper bound is the number of stools per day, which I’ve set to \\(20\\) above, but may be higher)\nThis is useful in our hypothetical example because the CDAI scale runs from \\([0,1100]\\)\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}} \\times 550\n\\]\n\n\n\n\n\n\n\n\n\nWhile these are worst-case bounds, this shows that even small proportions of missing values can impact inferences if the missingness is correlated with the outcome value.\nTo give a sense of how large a proportion of patient might have missing values, a recent clinical trial evaluating Skyrizi, a popular treatment for Crohn’s, had three treatment groups (1 placebo and two active treatment arms), of which 12%, 8% and 15% had missing values for CDAI at week 52.\nThere is also the variance to consider. Even if the covariance between the missing values is zero, we will lose efficiency by dropping observations that have missing values. In the case where our estimator is a sample mean, and there are \\(n\\) units with \\(m\\) missing values, the variance of the CC estimator will be larger by \\(1 + \\frac{m}{n - m}\\).\nThus, in many cases, even if there are only small proportions of missing values, it can make sense to use partial information from incomplete cases to improve our estimators."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-patterns",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-patterns",
    "title": "Missing data lecture 1",
    "section": "Missingness patterns",
    "text": "Missingness patterns\nConsider three variables, \\(Y_1, Y_2, Y_3\\) that we’ve measured on a sample of \\(n\\) participants. Each variable has an associated binary vector: \\(M_1, M_2, M_3\\). Missingness patterns refer to the observed sample space for the vectors \\([m_{i1}, m_{i2}, m_{i3}]\\). The simplest missingness pattern is where only one of the variables is subject to missingness: \\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,0,1]\\}.\n\\] This is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Univariate missingness pattern (Credit goes in part to ChatGPT who wrote the initial version of this plot)\n\n\n\n\n\nWe could also have multivariate missingness with only two missingness patterns \\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,1,1]\\}\n\\] which is shown in Figure 2:\n\n\n\n\n\n\n\n\nFigure 2: Multivariate missingness pattern\n\n\n\n\n\nWe could have something called monotone missingness, where we can order the missingness matrix such that if \\(M_{i2} = 1\\) then so is \\(M_{i3} = 1\\): \\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,1,1], [0,0,1]\\}\n\\]\n\n\n\n\n\n\n\n\nFigure 3: Monotone missingness pattern\n\n\n\n\n\nThe least restricted missingness pattern is called a general pattern. This would be a case where there is no special structure. Of course, for \\(p\\) variables each subject to missingness, the general missingness has a sample space of size \\(2^p\\)\nIf we consider that only \\(Y_2, Y_3\\) are subject to missingness, then we have the following, depicted in :\n\\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,1,0], [0,0,1], [0,1,1]\\}\n\\]\n\n\n\n\n\n\n\n\nFigure 4: General missingness pattern for two variables\n\n\n\n\n\nNot surprisingly, the general missingness pattern is the most realistic. You might imagine these patterns occurring during a survey. The pattern \\([0,1,1]\\) represents unit nonresponse (where a person who is contacted declines to participate in the survey), while \\([0,1,0], [0,0,1]\\) would be item nonresponse.\nThe reason that categorizing patterns of missingness is useful is because it can suggest different methods for dealing with missing data. It’s also something that is observable; missing values are not observable, but the patterns are. Thus, in the survey unit and item nonresponse, we might consider different strategies for dealing with unit nonresponse and item nonresponse."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-mechanisms",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-mechanisms",
    "title": "Missing data lecture 1",
    "section": "Missingness mechanisms",
    "text": "Missingness mechanisms\nThe most important paper in missing data was published by Don Rubin in 1976 Rubin (1976). Somewhat surprisingly to me, this paper was rejected by many stats journals. Rod Little says that he was assigned to review the paper as a graduate student when it was submitted to Biometrika, and he was convinced the paper was wrong after writing a long review. Luckily Little was overridden by his advisor, David Cox, who thought the paper was right, and decided to accept the paper.\nThe paper was important because it formalized methods of modeling missingness indicators, or the \\(M\\) matrix from above. Prior to this paper, the \\(M\\) matrix was not considered an outcome that could be modeled. Rubin’s paper instead categorized \\(M\\) as a random variable, and determined how the conditional distribution \\(M \\mid Y\\) impacted inferences using only the observed values of \\(Y\\).\nThe various ways in which \\(M\\) can depend on \\(Y\\) is really an investigation of why a value is missing. Is a survey respondent unwilling to report their income because is high? Did the patient drop out of the study because of side-effects of a drug, or because the drug exacerbated their condition? Did a database error lead to the random dropping of records?\nThe crux of missing data analysis hinges in what we’re willing to believe about why data are missing. These beliefs aren’t typically testable, unless we have designed our study to have missingness1.\nAgain, let \\(M\\) be the matrix with \\((i,j)^\\text{th}\\) entry \\(M_{ij}\\). Further, let \\(M_i\\) be the \\(i^\\text{th}\\) row of \\(M\\), let \\(\\tilde{m}_i\\) be a particular realization of \\(M_i\\) and let \\(m_i\\) be a dummy value. Let \\(Y\\) be the \\(n \\times K\\) matrix of measurements of interest (possibly including covariates), with elements \\(Y_{ij}\\). Let \\(Y_i\\) be the \\(i^\\mathrm{th}\\) row of matrix \\(Y\\), while \\(\\tilde{y}_i\\) is a particular realization of \\(Y_i\\) and \\(y_i\\) is a dummy value. We assume for simplicity (and for much of the book) that \\((M_i, Y_i)\\) are independent between rows.\nTo put a finer point on it, there are generally three categories of missingness mechanisms. They each relate to the distribution: \\[\nf_{M_i \\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi),\n\\] where \\(\\phi\\) are the parameters that govern the missingness mechanism.\nIt will be useful in the next subsections to define the following partitions of \\(Y_i\\): Let \\[Y_{(0)i} = (Y_{ij} : M_{ij} = 0)\\] be the vector of components of \\(Y_i\\) that are observed for unit \\(i\\) and let \\[Y_{(1)i} = (Y_{ij} : M_{ij} = 1)\\] denote the vector of \\(Y_i\\) components that are missing for \\(Y_i\\). The corresponding values, \\(\\tilde{y}_{(0)i}, \\tilde{y}_{(1)i}\\). For example: \\[\\tilde{y}_{(0)i} = (\\tilde{y}_{ij} : \\tilde{m}_{ij} = 0)\\] are particular realizations of these variables, while \\(y_{(0)i}, y_{(1)i}\\) will be dummy values. For example: \\[y_{(0)i} = (y_{ij} : m_{ij} = 0)\\]\nWe can see that \\(Y_{(0)i}, Y_{(1)i}, \\tilde{y}_{(0)i}, y_{(1)i}, y_{(0)i}, y_{(1)i}\\) depend on the missingness indicator variables, \\(M_i, \\tilde{m}_i, m_i\\), respectively.\n\nMissing completely at random (MCAR)\nThe simplest mechanism is called missing-completely-at-random (MCAR). This is where the missingness is unrelated to the outcome. Data are said to MCAR if the following holds for all \\(i\\), \\(y_i\\), \\(y^\\star_i\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi) = f_{M\\mid Y}(M_i = \\tilde{m}_i \\mid Y_i = y^*_i, \\phi).\n\\] An imporant clarification is that this NOT a conditional independence assumption. It is an assumption about the evaluation of the conditional mass function \\(f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi)\\) at a specific \\(m_i\\) (Mealli and Rubin 2015). This point is often misunderstood (including by me).\nConditional independence, \\(M_i \\indy Y_i\\) would be characterized as missing-always-completely-at-random (MACAR): Data are said to MACAR if the following holds for all \\(i\\), \\(m_i\\), \\(y_i\\), \\(y^\\star_i\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = m_i \\mid Y_i = y_i, \\phi) = f_{M\\mid Y}(M_i = m_i \\mid Y_i = y^*_i, \\phi).\n\\] The next mechanism is less restrictive than MCAR.\n\n\nMissing at random (MAR)\nMissing at random data are characterized by the following equality for all \\(i\\), \\(y_{(1)i}\\), \\(y^*_{(1)i}\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i} \\phi) = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i} \\phi)  \n\\] Again, as with MCAR, this is a statement about the evaluation of the function \\(f_{M\\mid Y}(m_i \\mid y_i, \\phi)\\). We can define a missing-at-random variant, missing-always-at-random (MAAR) that is equivalent to \\(M \\indy Y_{(1)i} \\mid Y_{(0)i}\\)2.\nThe following example is adapted from Mealli and Rubin (2015): Suppose we’re analyzing data from that Crohn’s disease trial and \\(Y_i\\) has two components: \\(Y_{i1}\\) is CDAI at visit 1 and \\(Y_{i2}\\) is CDAI at visit 2. For patient \\(i\\) suppose that \\(m_i = (0, 1)\\), or that \\(Y_{i2}\\) is missing but \\(Y_{i1}\\) is observed.\nBringing this example into line with our previous notation, \\(Y_{(0)i} \\equiv Y_{i1}\\) and \\(Y_{(1)i} \\equiv Y_{i2}\\). Then \\(\\tilde{y}_{(0)i} \\equiv \\tilde{y}_{i1}\\).\nConsider two scenarios:\n\n\\(Y_{i2}\\) is missing because \\(Y_{i1} &gt; \\phi\\). Given that \\(0^0 = 1\\), in this scenario the missingness mechanism can be translated as: \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{\\tilde{y}_{(0)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{\\tilde{y}_{(0)i} &gt; \\phi})^{1 - m_{i2}}\n\\]\n\\(Y_{i2}\\) is missing because \\(Y_{i2} &gt; \\phi\\). This mechanism is translated as \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{y_{(1)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{y_{(1)i} &gt; \\phi})^{1 - m_{i2}}\n\\] In scenario 1 the data are MAR because the mass function is a function of \\(\\tilde{y}_{(0)i}\\) only, (i.e it depends only on \\(Y_{i1}\\)), while in scenario 2 the data do not satisfy the definition of MAR."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#footnotes",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#footnotes",
    "title": "Missing data lecture 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne way that can happen is in a univariate missingness setting where \\(Y_3\\) respresents a hard-to-measure quantity (say number of REM cycles per night) and \\(Y_1, Y_2\\) are proxies for this quantity. If we randomly select a subset of our participants in which to measure \\(Y_3\\) then we know that missingness \\(M\\) is not related to \\(Y\\) (assuming that none of our selected participants refuse to participate!).↩︎\nMissing always at random (MAAR) Missing always at random data are characterized by the following equality for all \\(i\\), \\(m_i\\) \\(y_{(1)i}\\), \\(y^*_{(1)i}\\): \\[\nf_{M_i\\mid Y_i}(M_i = m_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = f_{M_i\\mid Y_i}(M_i = m_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i}, \\phi)  \n\\] This is a more restrictive assumption than MAR alone, though one could see why MAAR might be invoked for asymptotic arguments (Roderick J. Little 2021).↩︎"
  },
  {
    "objectID": "survival-material/lecture-2.html#connection-between-discrete-and-continuous-survival-functions-secdisc-continue-recall-the-definition-of-the-hazard-function",
    "href": "survival-material/lecture-2.html#connection-between-discrete-and-continuous-survival-functions-secdisc-continue-recall-the-definition-of-the-hazard-function",
    "title": "Lecture 2",
    "section": "4.1 Connection between discrete and continuous survival functions {#sec:disc-continue} Recall the definition of the hazard function:",
    "text": "4.1 Connection between discrete and continuous survival functions {#sec:disc-continue} Recall the definition of the hazard function:\n\\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use Equation 1 to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "survival-material/lecture-2.html#sec-disc-continue",
    "href": "survival-material/lecture-2.html#sec-disc-continue",
    "title": "Lecture 2",
    "section": "4.1 Connection between discrete and continuous survival functions",
    "text": "4.1 Connection between discrete and continuous survival functions\nRecall the definition of the hazard function:\n\\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that \\(\\lambda_{X_i}(t) \\,\\Delta t\\) is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\), \\(t_j = t_{j-1} + \\Delta t\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use Equation 1 to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html",
    "title": "Missing data lecture 2",
    "section": "",
    "text": "Notation\nMeaning\nDomain\n\n\n\n\n\\(Y\\)\n\\(n\\times K\\) matrix, collection of measurements of interest\n\\(\\R^{n \\times K}\\)\n\n\n\\(\\tilde{y}\\)\nRealization of \\(Y\\)\n\\(\\R^{n \\times K}\\)\n\n\n\\(Y_{ij}\\)\nElement of \\(Y\\), random variables\n\\(\\R\\)\n\n\n\\(\\tilde{y}_{ij}\\)\nParticular realization of \\(Y_{ij}\\)\n\\(\\R\\)\n\n\n\\(Y_{i}\\)\n\\(i^\\mathrm{th}\\) row of \\(Y\\), random variables\n\\(\\R^K\\)\n\n\n\\(\\tilde{y}_{i}\\)\nParticular realization of \\(Y_{i}\\)\n\\(\\R^K\\)\n\n\n\\(y_{i}\\)\nArbitrary element of \\(\\R^K\\), for use in density functions related to \\(Y_i\\)\n\\(\\R^K\\)\n\n\n\\(M\\)\n\\(n\\times K\\) binary matrix of missingness indicators\n\\(\\{0,1\\}^{n \\times K}\\)\n\n\n\\(\\tilde{m}\\)\nRealization of \\(M\\)\n\\(\\{0,1\\}^{n \\times K}\\)\n\n\n\\(M_{ij}\\)\nElement of \\(M\\), random variable\n\\(\\{0,1\\}\\)\n\n\n\\(M_{i}\\)\n\\(i^\\mathrm{th}\\) row of \\(M\\), random variables\n\\(\\{0,1\\}^{K}\\)\n\n\n\\(\\tilde{m}_{ij}\\)\nRealization of \\(M_{ij}\\)\n\\(\\{0,1\\}\\)\n\n\n\\(\\tilde{m}_{i}\\)\nRealization of \\(M_{i}\\)\n\\(\\{0,1\\}^K\\)\n\n\n\\(m_{i}\\)\nArbitrary element of \\(\\{0,1\\}^K\\) for use in PMFs for \\(M_i\\)\n\\(\\{0,1\\}^K\\)\n\n\n\\(R_i\\)\nNumber of observed values for row \\(i\\) in \\(Y\\), \\(\\sum_{j=1}^K (1 - M_{ij})\\)\n\\(\\mathbb{N}\\)\n\n\n\\(Y_{(0)i}\\)\nObserved elements of \\(Y_i\\)\n\\(\\R^{R_i}\\)\n\n\n\\(\\tilde{y}_{(0)i}\\)\nRealization of \\(Y_{(0)i}\\)\n\\(\\R^{R_i}\\)\n\n\n\\(y_{(0)i}\\)\nArbitrary element of \\(\\R^{R_i}\\)\n\\(\\R^{R_i}\\)\n\n\n\\(Y_{(1)i}\\)\nUnobserved or missing elements of \\(Y_i\\)\n\\(\\R^{K - R_i}\\)\n\n\n\\(\\tilde{y}_{(1)i}\\)\nRealization of \\(Y_{(1)i}\\)\n\\(\\R^{K - R_i}\\)\n\n\n\\(y_{(1)i}, y^\\star_{(1)i}\\)\nArbitrary elements of \\(\\R^{K - R_i}\\)\n\\(\\R^{K - R_i}\\)"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#footnotes",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#footnotes",
    "title": "Missing data lecture 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne exception is regression with missing predictors; scenarios in which the predictors have MNAR missingness that doesn’t depend on the outcome can be analyzed with CCA.↩︎"
  },
  {
    "objectID": "survival-material/lecture-3.html",
    "href": "survival-material/lecture-3.html",
    "title": "Lecture 3",
    "section": "",
    "text": "Now let’s delve into more detail about censoring, and how the likelihood can be built up from the hazard function and the survival function. define censoring as imprecise knowledge about an event time. If we observe a failure or an event exactly, the observation is not censored, but if we know only that an observation occurred within a range of values, we say the observation is censored. Let \\(X_i\\), as usual, be our failure time, which is not completely observed. Instead if:\n\n\\(X_i \\in [C, \\infty)\\), the observation is right censored\n\\(X_i \\in [0, U)\\), the observation is left censored\n\\(X_i \\in [C, U)\\), the observation is interval censored"
  },
  {
    "objectID": "survival-material/lecture-3.html#properties-of-the-hazard-function",
    "href": "survival-material/lecture-3.html#properties-of-the-hazard-function",
    "title": "Lecture 2",
    "section": "",
    "text": "The relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-3.html#sec-disc-continue",
    "href": "survival-material/lecture-3.html#sec-disc-continue",
    "title": "Lecture 2",
    "section": "4.1 Connection between discrete and continuous survival functions",
    "text": "4.1 Connection between discrete and continuous survival functions\nRecall the definition of the hazard function:\n\\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use Equation 1 to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#missingness-mechanisms",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#missingness-mechanisms",
    "title": "Missing data lecture 2",
    "section": "Missingness mechanisms",
    "text": "Missingness mechanisms\nMechanisms relate to the distribution: \\[\nf_{M_i \\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi),\n\\] where \\(\\phi\\) are the parameters that govern the missingness mechanism.\n\nMissing completely at random (MCAR)\nData are said to be MCAR if the following holds for all \\(i\\), \\(y_i\\), \\(y^\\star_i\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi) = f_{M\\mid Y}(M_i = \\tilde{m}_i \\mid Y_i = y^\\star_i, \\phi).\n\\]\n\n\nMissing at random (MAR)\nMissing at random data are characterized by the following equality for all \\(i\\), \\(y_{(1)i}\\), \\(y^\\star_{(1)i}\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i}, \\phi)  \n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#continuing-with-our-example-from-last-lecture",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#continuing-with-our-example-from-last-lecture",
    "title": "Missing data lecture 2",
    "section": "Continuing with our example from last lecture",
    "text": "Continuing with our example from last lecture\nThe following example is adapted from Mealli and Rubin (2015): Suppose we’re analyzing data from that Crohn’s disease trial and \\(Y_i\\) has two components: \\(Y_{i1}\\) is CDAI at visit 1 and \\(Y_{i2}\\) is CDAI at visit 2. For patient \\(i\\) suppose that \\(m_i = (0, 1)\\), or that \\(Y_{i2}\\) is missing but \\(Y_{i1}\\) is observed.\nBringing this example into line with our previous notation, \\(Y_{(0)i} \\equiv Y_{i1}\\) and \\(Y_{(1)i} \\equiv Y_{i2}\\). Then \\(\\tilde{y}_{(0)i} \\equiv \\tilde{y}_{i1}\\).\nConsider two scenarios:\n\n\\(Y_{i2}\\) is missing because \\(Y_{i1} &gt; \\phi\\). Given that \\(0^0 = 1\\), in this scenario the missingness mechanism can be translated as: \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{\\tilde{y}_{(0)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{\\tilde{y}_{(0)i} &gt; \\phi})^{1 - m_{i2}}\n\\]\n\\(Y_{i2}\\) is missing because \\(Y_{i2} &gt; \\phi\\). This mechanism is translated as \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{y_{(1)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{y_{(1)i} &gt; \\phi})^{1 - m_{i2}}\n\\] In scenario 1 the data are MAR because the mass function is a function of \\(\\tilde{y}_{(0)i}\\) only, (i.e it depends only on \\(Y_{i1}\\)), while in scenario 2 the data do not satisfy the definition of MAR.\n\n\nGeneralizing the example\nLet’s make this example more general. The following is from Little and Rubin (2019, 23). Again consider the bivariate case with \\(y_{i1}, y_{i2}\\). There are 4 possible missing data patterns: \\[\n(m_{i1}, m_{i2}) \\in \\{(0,0), (0,1), (1,0), (1,1)\\}\n\\] We’ll need to define \\(f_{M \\mid Y}(m_{i1} = r, m_{i2} = s \\mid y_{i1}, y_{i2}, \\phi)\\). To simplify the notation, let \\[\ng_{rs}(y_{i1}, y_{i2}, \\phi) = f_{M \\mid Y}(m_{i1} = r, m_{i2} = s \\mid y_{i1}, y_{i2}, \\phi)\n\\] The MAR assumption implies the following: \\[\n\\begin{aligned}\ng_{11}(y_{i1}, y_{i2}, \\phi) & = g_{11}(\\phi) \\\\\ng_{01}(y_{i1}, y_{i2}, \\phi) & = g_{01}(y_{i1}, \\phi) \\\\\ng_{10}(y_{i1}, y_{i2}, \\phi) & = g_{10}(y_{i2}, \\phi) \\\\\ng_{00}(y_{i1}, y_{i2}, \\phi) & = 1 - g_{10}(y_{i2}, \\phi)  - g_{01}(y_{i1}, \\phi) -  g_{11}(\\phi)\n\\end{aligned}\n\\] Thus the probability that \\(y_{ij}\\) is missing can depend only on \\(y_{i(-j)}\\), which is a bit odd.\nLittle and Rubin (2019) proposes the following modification:\n\\[\n\\begin{aligned}\ng_{11}(y_{i1}, y_{i2}, \\phi) & = g_{1+}(y_{i1}, \\phi)g_{+1}(y_{i2}, \\phi) \\\\\ng_{01}(y_{i1}, y_{i2}, \\phi) & = (1 - g_{1+}(y_{i1}, \\phi))g_{+1}(y_{i2}, \\phi) \\\\\ng_{10}(y_{i1}, y_{i2}, \\phi) & = g_{1+}(y_{i1}, \\phi)(1 - g_{+1}(y_{i2}, \\phi)) \\\\\ng_{00}(y_{i1}, y_{i2}, \\phi) & = (1 - g_{1+}(y_{i1}, \\phi))(1 - g_{+1}(y_{i2}, \\phi))\n\\end{aligned}\n\\] While this is maybe more realistic, though it does make an assumption that \\(m_{i1}\\) and \\(m_{i2}\\) are conditionally independent given \\(y_{i1}, y_{i2}\\), it is also hard to estimate, because we won’t observe missing values of \\(y_{i1}\\) and \\(y_{i2}\\).\nThis is a scenario called missing-not-at-random, or MNAR. This is defined in the next subsection."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#univariate-mar-data",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#univariate-mar-data",
    "title": "Missing data lecture 2",
    "section": "Univariate MAR data",
    "text": "Univariate MAR data\nUnivariate MAR data is useful to consider because it elucidates a key assumption that can be used in MAR analyses.\nSuppose we have a single measurement on \\(n\\) individuals, \\(Y_i\\) and \\(r\\) of the individuals in our sample have observations \\(\\tilde{y}_i\\) while \\(n-r\\) individuals have missing values for \\(Y_i\\) (i.e. \\(\\tilde{y}_{(1)i} = \\tilde{y}_{i}\\), \\(\\tilde{y}_{(0)i} = \\emptyset\\)). Suppose we also have iid measurements and an iid missingness process.\nWriting out the missingness mechanism for this scenario gives us: \\[\n\\begin{aligned}\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) & = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i}, \\phi) \\\\\n& = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid \\phi)  \n\\end{aligned}\n\\] This implies something about the relationship between the units the have observed data and those that do not: \\[\n\\begin{aligned}\nf_{Y_i \\mid M_i}(Y_{i} = y_{i} \\mid M_i = \\tilde{m}_i, \\theta) & = \\frac{f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{i} = y_i, \\phi) f_{Y_i}(Y_i = y_i \\mid \\theta)}{f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid \\phi)} \\\\\n& = f_{Y_i}(Y_i = y_i \\mid \\theta)\n\\end{aligned}\n\\] This implies that \\[\n\\begin{aligned}\nf_{Y_i \\mid M_i}(Y_{i} = y_{i} \\mid M_i = 1, \\theta) & = f_{Y_i \\mid M_i}(Y_{i} = y_{i} \\mid M_i = 0, \\theta)\n\\end{aligned}\n\\] Thus, we can use the distribution we learn from the complete cases as the distribution for the cases that are missing. This idea holds for more general missingness patterns with more measurements. We’ll see this later in the course."
  },
  {
    "objectID": "survival-material/lecture-3.html#type-i-censoring",
    "href": "survival-material/lecture-3.html#type-i-censoring",
    "title": "Lecture 3",
    "section": "2.1 Type I censoring",
    "text": "2.1 Type I censoring\nThe simplest censoring scenario is one in which all individuals have the same, nonrandom censoring time. Imagine a study is designed to follow \\(5\\) startups that are spun out of a tech incubator to study how long it takes a company to land its first contract. This information will be used for designing investments \\(2\\) years from the study date, so the study has a length of \\(1.75\\) years. We can say that all observations will have to have occurred, or not, by \\(1.75\\) years.\nFigure 1 shows a potential result of the study, where \\(2\\) out of the \\(5\\) companies have not landed a contract.\n\n\n\n\n\n\n\n\nFigure 1: Type I censoring\n\n\n\n\n\nIn this case, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3.html#generalized-type-i-censoring",
    "href": "survival-material/lecture-3.html#generalized-type-i-censoring",
    "title": "Lecture 3",
    "section": "2.2 Generalized type I censoring",
    "text": "2.2 Generalized type I censoring\nA more general scenario, which is closer to most examples in clinical trials, is when each individual has a different study entry time and the investigator has a preset study end time. This is called generalized Type I censoring. These study entry times are typically assumed to be independent of the survival time. This is shown in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Example of generalized Type I censoring, where each individual has a separate study entry time.\n\n\n\n\n\nWhen study entry is independent from survival time, the analysis proceeds as shown in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Example of generalized Type I censoring, viewed in patient time.\n\n\n\n\n\nFor generalized type I censoring, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\).\n\nThis is different from Type I censoring in that each individual has a different censoring time."
  },
  {
    "objectID": "survival-material/lecture-3.html#type-ii-censoring",
    "href": "survival-material/lecture-3.html#type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.3 Type II censoring",
    "text": "2.3 Type II censoring\nType II censoring occurs when all units have the same study entry time, but researchers design the study to end when \\(r &lt; n\\) units fail out of \\(n\\) total units under observation.\n\nFor the first \\(r\\), lucky or unlucky participants, \\(\\delta_i = 1 \\implies T_i = X_{(i)}\\) or the \\(i^\\mathrm{th}\\) order statistic.\nFor the remaining \\(n - r\\) individuals, \\(\\delta_i = 0 \\implies X_i &gt; X_{(r)}\\)."
  },
  {
    "objectID": "survival-material/lecture-3.html#generalized-type-ii-censoring",
    "href": "survival-material/lecture-3.html#generalized-type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.4 Generalized Type II censoring",
    "text": "2.4 Generalized Type II censoring\nYou may be wondering, what happens when units have differing start times but we want to end the trial after the \\(r\\)-th failure? It turns out that this was not a solved problem until , which was quite surprising to me."
  },
  {
    "objectID": "survival-material/lecture-3.html#independent-censoring",
    "href": "survival-material/lecture-3.html#independent-censoring",
    "title": "Lecture 3",
    "section": "2.5 Independent censoring",
    "text": "2.5 Independent censoring\nA third type of censoring, helpfully called independent censoring, takes \\(X_i \\indy C_i\\), and thus conclusions similar to those of generalized type I censoring can be drawn:\n\nFor all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3.html#reasons-for-informative-censoring",
    "href": "survival-material/lecture-3.html#reasons-for-informative-censoring",
    "title": "Lecture 3",
    "section": "3.1 Reasons for informative censoring",
    "text": "3.1 Reasons for informative censoring\nA simple hypothetical situation with informative censoring would be one in which sick patients are lost to follow-up."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#bias-of-cca",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#bias-of-cca",
    "title": "Missing data lecture 2",
    "section": "Bias of CCA",
    "text": "Bias of CCA\nWe showed earlier that the bias of the complete case analysis was dependent on the covariance between the missingness indicator and the response; another way to decompose the bias is via the following representation of the sample mean. Recall that \\(\\mathcal{R} = \\{i \\mid \\sum_{j=1}^K \\tilde{m}_{ij} = 0, i = 1, \\dots, n\\}\\); we can define the complement of this set \\(\\mathcal{R}^\\comp\\). Then we can represent the sample mean as: \\[\n\\bar{y} = \\frac{r}{n}\\bar{y}^\\mathrm{CC} + (1 - \\frac{r}{n})\\bar{y}^{\\mathcal{R}^\\comp}\n\\] Note that the sample mean may be unobservable because some units may be missing \\(y_i\\). Then, conditional on the missingness pattern that was observed, the bias of the\n\\[\n\\Exp{\\bar{y}^\\mathrm{CC} - \\bar{y} \\mid M = \\tilde{m}} = \\lp 1 - \\frac{r}{n}\\rp(\\Exp{\\bar{y}^\\mathrm{CC} \\mid M = \\tilde{m}} - \\Exp{\\bar{y}^{\\mathcal{R}^\\comp} \\mid M = \\tilde{m}})\n\\]"
  },
  {
    "objectID": "survival-material/lecture-3.html#likelihood-construction",
    "href": "survival-material/lecture-3.html#likelihood-construction",
    "title": "Lecture 3",
    "section": "4.1 Likelihood construction",
    "text": "4.1 Likelihood construction\nWe now turn to how to construct likelihoods in each of the prior scenarios, under censored or truncated data. As a reminder:\n\nLet \\(X_i\\) be the time to failure, or time to event for individual \\(i\\).\nLet \\(C_i\\) be the time to censoring. It may be helpful to think about \\(C_i\\) as the time to investigator measurement.\nLet \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\).\nLet \\(T_i = \\min(X_i, C_i)\\).\n\nWhen \\(\\delta_i = 1\\), we observe \\(T_i = X_i\\); this is the event that \\(\\{X_i = T_i, C_i \\geq X_i\\}\\). When \\(\\delta_i = 0\\), we observe \\(T_i = C_i\\); this is the event that \\(\\{C_i = T_i, C_i &lt; X_i\\}\\). Let the joint distribution of \\(X_i, C_i\\) be written as \\(P_\\theta(X &gt; x, C &gt; c)\\), and further let \\(\\theta = (\\eta, \\phi)\\) such that \\(P_\\theta(X &gt; x, C &gt; c) = P_\\eta(X &gt; x) P_\\phi(C &gt; c \\mid X &gt; x)\\). We showed that the likelihood corresponding to the random variables \\(T_i, \\delta_i\\), \\(f_{T_i, \\delta_i}(t, \\delta;\\,\\theta)\\), can be written in terms of partial derivatives of the joint density function when \\(X_i\\) and \\(C_i\\) are absolutely continuous random variables. \\[\\begin{align*}\n  f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq u, C \\geq t) \\mid_{u = t}  \\right)^\\delta \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq t, C \\geq u) \\mid_{u = t}  \\right)^{1 - \\delta}\n\\end{align*}\\] Let’s rewrite the partial derivatives in terms of their limits: \\[\\begin{align*}\n   f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t, C \\geq t) \\right)^\\delta\n    \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(X \\geq t, t \\leq C &lt; t + \\Delta t)   \\right)^{1 - \\delta}\n\\end{align*}\\] We can factorize the distribution function: \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^{1 - \\delta}\n\\end{align*}\\] . Let’s make the following substitutions for brevity’s sake: \\[\n\\begin{aligned}\n\\lambda_1(t; \\theta) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\\\\n\\lambda_2(t; \\theta) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\\\\n\\end{aligned}\n\\] Furthermore, let’s rewrite \\(P_{\\theta}(X \\geq t, C \\geq t)\\) in terms of \\(\\lambda_1(t)\\) and \\(\\lambda_2(t)\\). This will require a bit of work. First, we can write the \\(P_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\) as\n\\[\n\\begin{aligned}\nP_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) & = \\lambda_1(t;\\theta)\\Delta t  + o(\\Delta t)\\\\\nP_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) & = \\lambda_2(t;\\theta)\\Delta t  + o(\\Delta t)\n\\end{aligned}\n\\] We’ll also assume the following: \\[\nP_\\theta(t \\leq X &lt; t + \\Delta t, t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) = o(\\Delta t)\n\\]\nWe’ll work with discretized time steps. Let \\(t_0 = 0\\) and \\(t_n = t\\). Then \\([0, t) = \\bigcup_{j=0}^{n-1} [\\frac{j}{n}, \\frac{j}{n} + \\frac{1}{n})\\) and \\(\\Delta t = \\frac{1}{n}\\).\nThis construction means that we can write the survival function recursively for each \\(t\\): \\[\n\\begin{aligned}\nP_\\theta(X \\geq t + \\Delta t, C \\geq t + \\Delta t) & = P_\\theta(X \\geq t_{n} + \\Delta t, C \\geq t_{n}  + \\Delta t \\mid X \\geq t_{n}, C \\geq t_{n}) P_\\theta(X \\geq t_{n}, C \\geq t_{n}) \\\\\n& =  P_\\theta(X \\geq t_{n} + \\Delta t, C \\geq t_{n}  + \\Delta t \\mid X \\geq t_{n}, C \\geq t_{n}) \\\\\n& \\times P_\\theta(X \\geq t_{n-1} + \\Delta t, C \\geq t_{n-1} + \\Delta t \\mid X \\geq t_{n-1}, C \\geq t_{n-1}) P_\\theta(X \\geq t_{n-1}, C \\geq t_{n-1}) \\\\\n& \\vdots \\\\\n& = \\prod_{i=0}^{n-1} P_\\theta(X \\geq t_{i} + \\Delta t, C \\geq t_{i} + \\Delta t \\mid X \\geq t_{i}, C \\geq t_{i}) P_\\theta(X \\geq 0, C \\geq 0) \\\\\n& = \\prod_{i=0}^{n-1} P_\\theta(X \\geq t_{i} + \\Delta t, C \\geq t_{i} + \\Delta t \\mid X \\geq t_{i}, C \\geq t_{i})\n\\end{aligned}\n\\]\nLet’s write the conditional survival function, \\(P_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i)\\), or the probability that neither \\(A_i \\equiv \\{t_i \\leq X &lt; t_i \\Delta t\\}\\) nor \\(B_i \\equiv \\{t \\leq C &lt; t_i \\Delta t\\}\\) occur. In set notation, this is: \\[\nA_i\\comp \\cap B_i\\comp = (A_i \\cup B_i)\\comp\n\\]\n\\[\n\\begin{aligned}\nP_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i) & =  P_\\theta((A_i \\cup B_i)\\comp \\mid X \\geq t_i, C \\geq t_i) \\\\\n& = 1 - P_\\theta(A_i \\cup B_i \\mid X \\geq t_i, C \\geq t_i) \\\\\n& = 1 - (P_\\theta(A_i \\mid X \\geq t_i, C \\geq t_i) + P_\\theta(B_i \\mid X \\geq t_i, C \\geq t_i) \\\\\n& \\quad - P_\\theta(A_i \\cap B_i \\mid X \\geq t_i, C \\geq t_i))\n\\end{aligned}\n\\] Recall that \\[\n\\begin{aligned}\nP_\\theta(A_i \\cap B_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq X &lt; t_i + \\Delta t, t_i \\leq C &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = o(\\Delta t)\n\\end{aligned}\n\\] and \\[\n\\begin{aligned}\nP_\\theta(A_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq X &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = \\lambda_1(t_i; \\theta) \\Delta t + o(\\Delta t) \\\\\nP_\\theta(B_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq C &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = \\lambda_2(t_i; \\theta) \\Delta t + o(\\Delta t)\n\\end{aligned}\n\\]\nPutting this into the expression above, we get: \\[\nP_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i) = 1 - \\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t)\n\\] Finally: \\[\nP_\\theta(X \\geq t + \\Delta t, C \\geq t + \\Delta t) = \\prod_{i=0}^{n-1} \\lp 1 - \\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t) \\rp\n\\] We take logs of both sides to get: \\[\n\\log P_\\theta(X \\geq t_j + \\Delta t, C \\geq t_j + \\Delta t) = \\sum_{i=0}^n \\log \\lp 1 - \\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t) \\rp\n\\] Suppose that \\(\\lambda_1(t), \\lambda_2(t)\\) are bounded. Then we can use the Taylor series for \\(\\log 1 - x\\) as \\(x \\searrow 0\\).: \\[\n\\log(1 - x) = -x + o(x)\n\\]\nThen\n\\[\n\\begin{aligned}\n& \\log(1 - \\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t)) \\\\\n& = -(\\lambda_1(t_i; \\theta) + \\lambda_2(t_i; \\theta))\\Delta t + o(\\Delta t) + o(\\lambda_1(t_i; \\theta) \\Delta t - \\lambda_2(t_i; \\theta)\\Delta t + o(\\Delta t)) \\\\\n& = -(\\lambda_1(t_i; \\theta) + \\lambda_2(t_i; \\theta))\\Delta t + o(\\Delta t)\n\\end{aligned}\n\\] Putting this back into our expression gives us:\n\\[\n\\begin{aligned}\n\\log P_\\theta(X \\geq t + \\Delta t, C \\geq t + \\Delta t) = -\\sum_{i=1}^n (\\lambda_1(t_i; \\theta) + \\lambda_2(t_i; \\theta)) \\Delta t  + n o(\\Delta t)\n\\end{aligned}\n\\] Taking \\(n \\to \\infty\\) gives the integral: \\[\n\\begin{aligned}\n\\log P_\\theta(X &gt; t, C &gt; t) = -\\int_0^t (\\lambda_1(u; \\theta) + \\lambda_2(u; \\theta)) du \\\\\nP_\\theta(X &gt; t, C &gt; t) = \\exp \\lp -\\int_0^t (\\lambda_1(u;\\theta) + \\lambda_2(u;\\theta)) du \\rp\n\\end{aligned}\n\\] By noting that \\(\\Delta t = \\frac{1}{n}\\), so for \\(o(\\Delta t)\\), \\(n o(\\Delta t) \\overset{n\\to\\infty}{\\to} 0\\)\nThis is a long way to show that \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^{1 - \\delta} \\\\\n    & = \\lambda_1(t;\\theta)^\\delta \\lambda_2(t;\\theta)^{1-\\delta} e^{-\\int_0^t (\\lambda_1(u;\\theta) + \\lambda_2(u;\\theta)) du}\n\\end{align*} \\]\nAssuming noninformative censoring equates to:\n\\[\n\\begin{align*}\n    \\lambda_1(t;\\theta) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t) \\\\\n    & = \\lambda_X(t;\\eta)\n\\end{align*}.\n\\] and that \\(\\lambda_2(t;\\phi) = \\lambda_2(t;\\phi)\\).\nThis yields: \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\lambda_1(t;\\theta)^\\delta \\lambda_2(t;\\theta)^{1-\\delta} e^{-\\int_0^t (\\lambda_1(u;\\theta) + \\lambda_2(u;\\theta)) du} \\\\\n    & = \\lambda_X(t;\\eta)^\\delta e^{-\\int_0^t \\lambda_X(u;\\eta)du}  \\lambda_2(t;\\phi)^{1-\\delta}e^{-\\int_0^t\\lambda_2(u;\\phi)du}\n\\end{align*}\n\\]\nThis means that we can factorize the joint density into a piece that depends only on the distribution of \\(X\\) and \\(\\eta\\), and a piece that depends on \\(\\phi\\) and the rate of censoring given survival up to time \\(t\\). Thus, noninformative censoring and parameter separability yield a separable joint density. This means that when we want to do maximum likelihood for survival data, we can ignore the model for the censoring times, \\(f_{C_i \\mid X_i \\geq t, \\delta_i}(t, \\delta;\\,\\phi)\\), and focus on only the model for the failure times: \\[f_{X_i, \\delta_i}(t, \\delta;\\,\\eta) = \\lambda_{X_i}(t;\\eta)^\\delta P_\\eta(X \\geq t).\\]\n\\[\n\\begin{align*}\n    f_{X_i, \\delta_i}(t, \\delta;\\,\\eta) & \\propto \\lambda_X(t;\\eta)^\\delta e^{-\\int_0^t \\lambda_X(u;\\eta)du}\n\\end{align*}\n\\]\n\nMLE for exponential survival time Let \\(X_i \\overset{\\text{iid}}{\\sim} \\text{Exp}(\\alpha)\\) and assume we have independent censoring (\\(X_i \\perp\\!\\!\\!\\perp C_i\\)), the parameters for the censoring process are separable from \\(\\alpha\\), and that \\(C_i\\) are iid such that \\(\\Exp{C_i} &lt; \\infty\\). Then our observed data are \\(T_i = \\min(X_i, C_i)\\) and \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\). According to ?@eq-separable-like we can write the likelihood as \\[\\begin{align*}\nf_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\prod_{i=1}^n \\alpha^{\\delta_i} \\exp(-\\textstyle{\\sum_{i=1}^n \\int_0^{t_i} }\\alpha du)\\\\\n& = \\alpha^{\\sum_{i=1}^n \\delta_i} \\exp(-\\alpha \\textstyle\\sum_{i=1}^n t_i)\n\\end{align*}\\] The log-likelihood is \\[\\log(f_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) ) = \\log(\\alpha)\\sum_{i=1}^n \\delta_i -\\alpha \\sum_{i=1}^n t_i\\] which has the maximizer \\[\\hat{\\alpha} = \\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}.\\] Let’s show that this converges a.s. to \\(\\alpha\\) as \\(n\\to\\infty\\). We can rewrite \\(\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}\\) as \\[\\begin{align*}\n    \\frac{\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right)}{\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}\n\\end{align*}\\] The top and bottom expressions converge a.s. by Kolmogorov’s Strong Law of Large Numbers to \\[\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right) & \\overset{\\text{a.s.}}{\\to} \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} \\\\\n\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right) & \\overset{\\text{a.s.}}{\\to}  \\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)}\n\\end{align*}\\] We can evaluate the top expression using the tower property of expectation: \\[\\begin{align*}\n    \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} & =  \\ExpA{\\ExpA{\\mathbbm{1}\\left(X_i \\leq c\\right)\\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n    & = \\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] where the second line follows from the independent censoring condition. The bottom expression becomes: \\[\\begin{align*}\n\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)} & = \\ExpA{\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& + \\ExpA{\\ExpA{c \\mathbbm{1}\\left(X_i &gt; c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& = \\ExpA{\\frac{1}{\\alpha}(1 - (1 + \\alpha C_i) e^{-\\alpha C_i})}{C_i} + \\ExpA{C_i e^{-\\alpha C_i}}{C_i} \\\\\n& = \\frac{1}{\\alpha}\\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] Thus \\[\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i} \\overset{\\text{a.s.}}{\\to} \\alpha\\] To show that \\(\\int_0^c x \\alpha e^{-\\alpha x} dx = \\frac{1}{\\alpha}(1 - (1 + \\alpha c) e^{-\\alpha c})\\), we can use the trick of differentiating under the integral sign. \\[\\begin{align*}\n  \\alpha \\int_0^c xe^{-\\alpha x} dx & = \\alpha \\int_0^c -\\frac{d}{d \\alpha} e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\int_0^c e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\frac{1}{\\alpha} (1 - e^{-\\alpha c}) \\\\\n  & = \\alpha \\left(\\frac{1 - (1 + \\alpha c) e^{-\\alpha c}}{\\alpha^2}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#simple-estimator",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#simple-estimator",
    "title": "Missing data lecture 2",
    "section": "Simple estimator",
    "text": "Simple estimator\nThis same idea can be used for missing data and is called the inverse probability weighted complete case estimator, or IPWCC for short.\nSuppose that we have a sample of pairs \\((y_i, m_i), i = 1, \\dots, n\\).\nFor the next section, we’ll make the more stringent assumption that there are completely observed covariates, \\(X_i\\) such that the following holds \\(M_i \\indy Y_i \\mid X_i\\) and that we have specified the correct model for \\(M_i\\): \\[\nP(M_i = 0 \\mid X_i = x, \\beta) = \\pi(x,\\beta)\n\\]\n\\[\n\\bar{y}^\\mathrm{IPWCC} = \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i (1 - M_i)}{\\pi(x_i, \\beta)} \\overset{p}{\\to} \\ExpA{\\frac{Y_i (1 - M_i)}{\\pi(X_i, \\beta)}}{(Y_i, M_i, X_i)}\n\\] where the convergence in probability follows from the weak law of large numbers.\n\\[\n\\begin{aligned}\n\\ExpA{\\frac{Y_i (1 - M_i)}{\\pi(X_i, \\beta)}}{(Y_i, M_i, X_i)} & = \\ExpA{\\ExpA{\\frac{Y_i (1 - M_i)}{\\pi(X_i, \\beta)}}{M_i \\mid Y_i, X_i}}{(Y_i, X_i)} \\\\\n& = \\ExpA{\\frac{Y_i}{\\pi(X_i, \\beta)}\\ExpA{ (1 - M_i)}{M_i \\mid Y_i, X_i}}{(Y_i, X_i)} \\\\\n& = \\ExpA{\\frac{Y_i}{\\pi(X_i, \\beta)}\\pi(X_i, \\beta)}{(Y_i, X_i)} \\\\\n& = \\ExpA{Y_i}{(Y_i, X_i)}\n\\end{aligned}\n\\]\nThere are two drawbacks with the IPWCC estimator:\n\nIt requires that \\(\\pi(X_i, \\beta) &gt; 0\\) for all \\(X_i\\) in the population; this may not be a good assumption.\nEven if \\(\\pi(X_i, \\beta) &gt; 0\\) is strictly true, small values of \\(\\pi(X_i, \\beta)\\) may occur (the interpretation is that the \\(i^\\mathrm{th}\\) observation will represent an inordinate number of missing cases because its probability of being missing is so high). If this happens, these estimators can have very large variance\nIn practice, we don’t know \\(\\pi(X_i, \\beta)\\) and typically have to estimate \\(\\beta\\) with \\(\\hat{\\beta}\\). If the sample is small, even if the form \\(\\pi(X_i, \\beta)\\) is right, there may be high variance. If the model for \\(M_i \\mid X_i\\) is wrong, then we have no guarantees that our IPWCC estimator will be unbiased."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#correctly-specified-pix_ibeta",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#correctly-specified-pix_ibeta",
    "title": "Missing data lecture 2",
    "section": "Correctly specified \\(\\pi(x_i,\\beta)\\)",
    "text": "Correctly specified \\(\\pi(x_i,\\beta)\\)\nSuppose that \\(\\pi(X_i, \\beta)\\) is correctly specified, but \\(\\mu(x_i, \\gamma)\\) is misspecified. Then \\(\\hat{\\beta} \\overset{p}{\\to} \\beta^\\dagger\\) but \\(\\hat{\\gamma} \\overset{p}{\\to} \\gamma^\\prime\\) such that \\(\\gamma^\\prime \\neq \\gamma^\\dagger\\). By the WLLN, \\(\\bar{y}^\\mathrm{AIPWCC} \\overset{p}{\\to}\\) the expression: \\[\n\\ExpA{Y_i}{(Y_i, M_i, X_i)} - \\ExpA{\\lp\\frac{(1 - M_i) + \\pi(X_i, \\beta^\\dagger)}{\\pi(X_i, \\beta^\\dagger)}\\rp(Y_i - \\mu(x_i, \\gamma^\\prime))}{(Y_i, M_i, X_i)}\n\\] The second term simplifies: \\[\n\\begin{aligned}\n& \\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\dagger)}{\\pi(X_i, \\beta^\\dagger)}\\rp(Y_i - \\mu(x_i, \\gamma^\\prime))}{(Y_i, M_i, X_i)} \\\\\n& = \\ExpA{\\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\dagger)}{\\pi(X_i, \\beta^\\dagger)}\\rp(Y_i - \\mu(x_i, \\gamma^\\prime))}{M_i \\mid Y_i, X_i}}{(Y_i, X_i)} \\\\\n& = \\ExpA{(Y_i - \\mu(x_i, \\gamma^\\prime))\\frac{1}{\\pi(X_i, \\beta^\\dagger)}\\ExpA{(1 - M_i) - \\pi(X_i, \\beta^\\dagger)}{M_i \\mid Y_i, X_i}}{(Y_i, X_i)} \\\\\n& = 0\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#correctly-specified-mux_igamma",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#correctly-specified-mux_igamma",
    "title": "Missing data lecture 2",
    "section": "Correctly specified \\(\\mu(x_i,\\gamma)\\)",
    "text": "Correctly specified \\(\\mu(x_i,\\gamma)\\)\nSuppose that \\(\\mu(X_i, \\gamma)\\) is correctly specified, but \\(\\pi(x_i, \\beta)\\) is misspecified. Then \\(\\hat{\\gamma} \\overset{p}{\\to} \\gamma^\\dagger\\) but \\(\\hat{\\beta} \\overset{p}{\\to} \\beta^\\prime\\) such that \\(\\beta^\\prime \\neq \\beta^\\dagger\\). By the WLLN, \\(\\bar{y}^\\mathrm{AIPWCC} \\overset{p}{\\to}\\) the expression: \\[\n\\ExpA{Y_i}{(Y_i, M_i, X_i)} - \\ExpA{\\lp\\frac{(1 - M_i) + \\pi(X_i, \\beta^\\prime)}{\\pi(X_i, \\beta^\\prime)}\\rp(Y_i - \\mu(x_i, \\gamma^\\dagger))}{(Y_i, M_i, X_i)}\n\\] The second term simplifies: \\[\n\\begin{aligned}\n& \\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\prime)}{\\pi(X_i, \\beta^\\prime)}\\rp(Y_i - \\mu(x_i, \\gamma^\\dagger))}{(Y_i, M_i, X_i)} \\\\\n& = \\ExpA{\\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\prime)}{\\pi(X_i, \\beta^\\prime)}\\rp(Y_i - \\mu(x_i, \\gamma^\\dagger))}{Y_i \\mid M_i, X_i}}{(M_i, X_i)} \\\\\n& = \\ExpA{\\lp\\frac{(1 - M_i) - \\pi(X_i, \\beta^\\prime)}{\\pi(X_i, \\beta^\\prime)}\\rp\\ExpA{(Y_i - \\mu(x_i, \\gamma^\\dagger))}{Y_i \\mid M_i, X_i}}{(M_i, X_i)} \\\\\n& = 0\n\\end{aligned}\n\\]\nThis is great if you get one of the forms right. However, if both are wrong, there are no guarantees about the bias. Kang and Schafer (2007) investigates, via simulation study, how poorly AIPWCC estimators perform when both models are wrong. They find that regression imputation estimators, which are what we’ll study in the next section, perform better, both in terms of coverage and MSE, even when the regression equation is misspecified.\nThe key is that the AIPWCC estimators have high variance because of the denominator \\(\\pi(X_i, \\beta)\\)."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html",
    "href": "survival-material/lecture-3-alt.html",
    "title": "Lecture 3",
    "section": "",
    "text": "Now let’s delve into more detail about censoring, and how the likelihood can be built up from the hazard function and the survival function. define censoring as imprecise knowledge about an event time. If we observe a failure or an event exactly, the observation is not censored, but if we know only that an observation occurred within a range of values, we say the observation is censored. Let \\(X_i\\), as usual, be our failure time, which is not completely observed. Instead if:\n\n\\(X_i \\in [C, \\infty)\\), the observation is right censored\n\\(X_i \\in [0, U)\\), the observation is left censored\n\\(X_i \\in [C, U)\\), the observation is interval censored"
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#type-i-censoring",
    "href": "survival-material/lecture-3-alt.html#type-i-censoring",
    "title": "Lecture 3",
    "section": "2.1 Type I censoring",
    "text": "2.1 Type I censoring\nThe simplest censoring scenario is one in which all individuals have the same, nonrandom censoring time. Imagine a study is designed to follow \\(5\\) startups that are spun out of a tech incubator to study how long it takes a company to land its first contract. This information will be used for designing investments \\(2\\) years from the study date, so the study has a length of \\(1.75\\) years. We can say that all observations will have to have occurred, or not, by \\(1.75\\) years.\nFigure 1 shows a potential result of the study, where \\(2\\) out of the \\(5\\) companies have not landed a contract.\n\n\n\n\n\n\n\n\nFigure 1: Type I censoring\n\n\n\n\n\nIn this case, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#generalized-type-i-censoring",
    "href": "survival-material/lecture-3-alt.html#generalized-type-i-censoring",
    "title": "Lecture 3",
    "section": "2.2 Generalized type I censoring",
    "text": "2.2 Generalized type I censoring\nA more general scenario, which is closer to most examples in clinical trials, is when each individual has a different study entry time and the investigator has a preset study end time. This is called generalized Type I censoring. These study entry times are typically assumed to be independent of the survival time. This is shown in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Example of generalized Type I censoring, where each individual has a separate study entry time.\n\n\n\n\n\nWhen study entry is independent from survival time, the analysis proceeds as shown in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Example of generalized Type I censoring, viewed in patient time.\n\n\n\n\n\nFor generalized type I censoring, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\).\n\nThis is different from Type I censoring in that each individual has a different censoring time."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#type-ii-censoring",
    "href": "survival-material/lecture-3-alt.html#type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.3 Type II censoring",
    "text": "2.3 Type II censoring\nType II censoring occurs when all units have the same study entry time, but researchers design the study to end when \\(r &lt; n\\) units fail out of \\(n\\) total units under observation.\n\nFor the first \\(r\\), lucky or unlucky participants, \\(\\delta_i = 1 \\implies T_i = X_{(i)}\\) or the \\(i^\\mathrm{th}\\) order statistic.\nFor the remaining \\(n - r\\) individuals, \\(\\delta_i = 0 \\implies X_i &gt; X_{(r)}\\)."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#generalized-type-ii-censoring",
    "href": "survival-material/lecture-3-alt.html#generalized-type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.4 Generalized Type II censoring",
    "text": "2.4 Generalized Type II censoring\nYou may be wondering, what happens when units have differing start times but we want to end the trial after the \\(r\\)-th failure? It turns out that this was not a solved problem until , which was quite surprising to me."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#independent-censoring",
    "href": "survival-material/lecture-3-alt.html#independent-censoring",
    "title": "Lecture 3",
    "section": "2.5 Independent censoring",
    "text": "2.5 Independent censoring\nA third type of censoring, helpfully called independent censoring, takes \\(X_i \\indy C_i\\), and thus conclusions similar to those of generalized type I censoring can be drawn:\n\nFor all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#reasons-for-informative-censoring",
    "href": "survival-material/lecture-3-alt.html#reasons-for-informative-censoring",
    "title": "Lecture 3",
    "section": "3.1 Reasons for informative censoring",
    "text": "3.1 Reasons for informative censoring\nA simple hypothetical situation with informative censoring would be one in which sick patients are lost to follow-up."
  },
  {
    "objectID": "survival-material/lecture-3-alt.html#likelihood-construction",
    "href": "survival-material/lecture-3-alt.html#likelihood-construction",
    "title": "Lecture 3",
    "section": "4.1 Likelihood construction",
    "text": "4.1 Likelihood construction\nWe now turn to how to construct likelihoods in each of the prior scenarios, under censored or truncated data. As a reminder:\n\nLet \\(X_i\\) be the time to failure, or time to event for individual \\(i\\).\nLet \\(C_i\\) be the time to censoring. It may be helpful to think about \\(C_i\\) as the time to investigator measurement.\nLet \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\).\nLet \\(T_i = \\min(X_i, C_i)\\).\n\nWhen \\(\\delta_i = 1\\), we observe \\(T_i = X_i\\); this is the event that \\(\\{X_i = T_i, C_i \\geq X_i\\}\\). When \\(\\delta_i = 0\\), we observe \\(T_i = C_i\\); this is the event that \\(\\{C_i = T_i, C_i &lt; X_i\\}\\). Let the joint distribution of \\(X_i, C_i\\) be written as \\(P_\\theta(X &gt; x, C &gt; c)\\), and further let \\(\\theta = (\\eta, \\phi)\\) such that \\(P_\\theta(X &gt; x, C &gt; c) = P_\\eta(X &gt; x) P_\\phi(C &gt; c \\mid X &gt; x)\\). We showed that the likelihood corresponding to the random variables \\(T_i, \\delta_i\\), \\(f_{T_i, \\delta_i}(t, \\delta;\\,\\theta)\\), can be written in terms of partial derivatives of the joint density function when \\(X_i\\) and \\(C_i\\) are absolutely continuous random variables. \\[\\begin{align*}\n  f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq u, C \\geq t) \\mid_{u = t}  \\right)^\\delta \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq t, C \\geq u) \\mid_{u = t}  \\right)^{1 - \\delta}\n\\end{align*}\\] Let’s rewrite the partial derivatives in terms of their limits: \\[\\begin{align*}\n   f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t, C \\geq t) \\right)^\\delta\n    \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(X \\geq t, t \\leq C &lt; t + \\Delta t)   \\right)^{1 - \\delta}\n\\end{align*}\\] We can factorize the distribution function: \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^{1 - \\delta}\n\\end{align*}\\] . Let’s make the following substitutions for brevity’s sake: \\[\n\\begin{aligned}\n\\lambda_1(t) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\\\\n\\lambda_2(t) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\\\\n\\end{aligned}\n\\] Furthermore, let’s rewrite \\(P_{\\theta}(X \\geq t, C \\geq t)\\) in terms of \\(\\lambda_1(t)\\) and \\(\\lambda_2(t)\\). This will require a bit of work. First, we can write the \\(P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\) as \\[\nP_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) = \\lambda(t)\\Delta t  + o(\\Delta t)\n\\] We’ll also assume the following: \\[\nP_\\theta(t \\leq X &lt; t + \\Delta t, t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) = o(\\Delta t)\n\\]\nWe’ll work with discretized time steps. Let \\(t_0 = 0\\) and \\(t_n = t\\). Then \\([0, t) = \\bigcup_{j=0}^{n-1} [\\frac{j}{n}, \\frac{j}{n} + \\frac{1}{n})\\) and \\(\\Delta t = \\frac{1}{n}\\).\nThis construction means that we can write the survival function recursively for each \\(t_j\\): \\[\n\\begin{aligned}\nP(X \\geq t + \\Delta t, C \\geq t + \\Delta t) & = P(X \\geq t_{n} + \\Delta t, C \\geq t_{n}  + \\Delta t \\mid X \\geq t_{n}, C \\geq t_{n}) P(X \\geq t_{n}, C \\geq t_{n}) \\\\\n& =  P(X \\geq t_{n} + \\Delta t, C \\geq t_{n}  + \\Delta t \\mid X \\geq t_{n}, C \\geq t_{n}) \\\\\n& \\times P(X \\geq t_{n-1} + \\Delta t, C \\geq t_{n-1} + \\Delta t \\mid X \\geq t_{n-1}, C \\geq t_{n-1}) P(X \\geq t_{n-1}, C \\geq t_{n-1}) \\\\\n& \\vdots \\\\\n& = \\prod_{i=0}^{n-1} P(X \\geq t_{i} + \\Delta t, C \\geq t_{i} + \\Delta t \\mid X \\geq t_{i}, C \\geq t_{i}) P(X \\geq 0, C \\geq 0) \\\\\n& = \\prod_{i=0}^{n-1} P(X \\geq t_{i} + \\Delta t, C \\geq t_{i} + \\Delta t \\mid X \\geq t_{i}, C \\geq t_{i})\n\\end{aligned}\n\\]\nLet’s write the conditional survival function, \\(P(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i)\\), or the probability that neither \\(A_i \\equiv \\{t_i \\leq X &lt; t_i \\Delta t\\}\\) nor \\(B_i \\equiv \\{t \\leq C &lt; t_i \\Delta t\\}\\) occur. In set notation, this is: \\[\nA_i\\comp \\cap B_i\\comp = (A_i \\cup B_i)\\comp\n\\]\n\\[\n\\begin{aligned}\nP_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i) & =  P_\\theta((A_i \\cup B_i)\\comp \\mid X \\geq t_i, C \\geq t_i) \\\\\n& = 1 - P_\\theta(A_i \\cup B_i \\mid X \\geq t_i, C \\geq t_i) \\\\\n& = 1 - (P_\\theta(A_i \\mid X \\geq t_i, C \\geq t_i) + P_\\theta(B_i \\mid X \\geq t_i, C \\geq t_i) \\\\\n& \\quad - P_\\theta(A_i \\cap B_i \\mid X \\geq t_i, C \\geq t_i))\n\\end{aligned}\n\\] Recall that \\[\n\\begin{aligned}\nP_\\theta(A_i \\cap B_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq X &lt; t_i + \\Delta t, t_i \\leq C &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = o(\\Delta t)\n\\end{aligned}\n\\] and \\[\n\\begin{aligned}\nP_\\theta(A_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq X &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = \\lambda_1(t_i) \\Delta t + o(\\Delta t) \\\\\nP_\\theta(B_i \\mid X \\geq t_i, C \\geq t_i)) & = P_\\theta(t_i \\leq C &lt; t_i + \\Delta t \\mid X \\geq t, C \\geq t)  \\\\\n& = \\lambda_2(t_i) \\Delta t + o(\\Delta t)\n\\end{aligned}\n\\]\nPutting this into the expression above, we get: \\[\nP_\\theta(X \\geq t_i + \\Delta t, C \\geq t_i + \\Delta t \\mid X \\geq t_i, C \\geq t_i) = 1 - \\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t)\n\\] Finally: \\[\nP(X \\geq t + \\Delta t, C \\geq t + \\Delta t) = \\prod_{i=0}^{n-1} \\lp 1 - \\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t) \\rp\n\\] We take logs of both sides to get: \\[\n\\log P(X \\geq t_j + \\Delta t, C \\geq t_j + \\Delta t) = \\sum_{i \\mid t_i \\leq t_j} \\log \\lp 1 - \\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t) \\rp\n\\] Suppose that \\(\\lambda_1(t), \\lambda_2(t)\\) are bounded. Then we can use the Taylor series for \\(\\log 1 - x\\) as \\(x \\searrow 0\\).: \\[\n\\log(1 - x) = -x + o(x)\n\\]\nThen\n\\[\n\\begin{aligned}\n& \\log(1 - \\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t)) \\\\\n& = -(\\lambda_1(t_i) + \\lambda_2(t_i))\\Delta t + o(\\Delta t) + o(\\lambda_1(t_i) \\Delta t - \\lambda_2(t_i)\\Delta t + o(\\Delta t)) \\\\\n& = -(\\lambda_1(t_i) + \\lambda_2(t_i))\\Delta t + o(\\Delta t)\n\\end{aligned}\n\\] Putting this back into our expression gives us:\n\\[\n\\begin{aligned}\n\\log P(X \\geq t + \\Delta t, C \\geq t + \\Delta t) = -\\sum_{i=1}^n (\\lambda_1(t_i) + \\lambda_2(t_i)) \\Delta t  + n o(\\Delta t)\n\\end{aligned}\n\\] Taking \\(n \\to \\infty\\) gives the integral: \\[\n\\begin{aligned}\n\\log P(X &gt; t, C &gt; t) = -\\int_0^t (\\lambda_1(u) + \\lambda_2(u)) du \\\\\nP(X &gt; t, C &gt; t) = \\exp \\lp -\\int_0^t (\\lambda_1(u) + \\lambda_2(u)) du \\rp\n\\end{aligned}\n\\] By noting that \\(\\Delta t = \\frac{1}{n}\\), so for \\(o(\\Delta t)\\), \\(n o(\\Delta t) \\overset{n\\to\\infty}{\\to} 0\\)\nThis is a long way to show that \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(X \\geq t,C \\geq t) \\right)^{1 - \\delta} \\\\\n    & = \\lambda_1(t)^\\delta \\lambda_2(t)^{1-\\delta} e^{-\\int_0^t (\\lambda_1(u) + \\lambda_2(u)) du}\n\\end{align*} \\]\nAssuming noninformative censoring equates to:\n\\[\n\\begin{align*}\n    \\lambda_1(t) & = \\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t) \\\\\n    & = \\lambda_X(t)\n\\end{align*}.\n\\] This yields: \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\lambda_1(t)^\\delta \\lambda_2(t)^{1-\\delta} e^{-\\int_0^t (\\lambda_1(u) + \\lambda_2(u)) du} \\\\\n    & = \\lambda_X(t)^\\delta e^{-\\int_0^t \\lambda_X(u)du}  \\lambda_2(t)^{1-\\delta}e^{-\\int_0^t\\lambda_2(u)du}\n\\end{align*}\n\\]\nThis means that we can factorize the joint density into a piece that Thus, noninformative censoring and parameter separability yield a separable joint density. This means that when we want to do maximum likelihood for survival data, we can ignore the model for the censoring times, \\(f_{C_i, \\delta_i}(t, \\delta;\\,\\phi)\\), and focus on only the model for the failure times: \\[f_{X_i, \\delta_i}(t, \\delta;\\,\\eta) = \\lambda_{X_i}(t)^\\delta P_\\eta(X \\geq t).\\] We can write this expression fully in terms of the hazard function by recalling [eq:exp-hazard]: \\[\\begin{align}\nf_{X_i, \\delta_i}(t, \\delta;\\,\\eta) = \\lambda_{X_i}(t)^\\delta \\exp\\left(-\\int_0^t \\lambda_{X_i}(u) du \\right).\n\\end{align}\n\\tag{4}\\]\n\nMLE for exponential survival time Let \\(X_i \\overset{\\text{iid}}{\\sim} \\text{Exp}(\\alpha)\\) and assume we have independent censoring (\\(X_i \\perp\\!\\!\\!\\perp C_i\\)), the parameters for the censoring process are separable from \\(\\alpha\\), and that \\(C_i\\) are iid such that \\(\\Exp{C_i} &lt; \\infty\\). Then our observed data are \\(T_i = \\min(X_i, C_i)\\) and \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\). According to Equation 4 we can write the likelihood as \\[\\begin{align*}\nf_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\prod_{i=1}^n \\alpha^{\\delta_i} \\exp(-\\textstyle{\\sum_{i=1}^n \\int_0^{t_i} }\\alpha du)\\\\\n& = \\alpha^{\\sum_{i=1}^n \\delta_i} \\exp(-\\alpha \\textstyle\\sum_{i=1}^n t_i)\n\\end{align*}\\] The log-likelihood is \\[\\log(f_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) ) = \\log(\\alpha)\\sum_{i=1}^n \\delta_i -\\alpha \\sum_{i=1}^n t_i\\] which has the maximizer \\[\\hat{\\alpha} = \\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}.\\] Let’s show that this converges a.s. to \\(\\alpha\\) as \\(n\\to\\infty\\). We can rewrite \\(\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}\\) as \\[\\begin{align*}\n    \\frac{\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right)}{\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}\n\\end{align*}\\] The top and bottom expressions converge a.s. by Kolmogorov’s Strong Law of Large Numbers to \\[\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right) & \\overset{\\text{a.s.}}{\\to} \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} \\\\\n\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right) & \\overset{\\text{a.s.}}{\\to}  \\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)}\n\\end{align*}\\] We can evaluate the top expression using the tower property of expectation: \\[\\begin{align*}\n    \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} & =  \\ExpA{\\ExpA{\\mathbbm{1}\\left(X_i \\leq c\\right)\\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n    & = \\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] where the second line follows from the independent censoring condition. The bottom expression becomes: \\[\\begin{align*}\n\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)} & = \\ExpA{\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& + \\ExpA{\\ExpA{c \\mathbbm{1}\\left(X_i &gt; c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& = \\ExpA{\\frac{1}{\\alpha}(1 - (1 + \\alpha C_i) e^{-\\alpha C_i})}{C_i} + \\ExpA{C_i e^{-\\alpha C_i}}{C_i} \\\\\n& = \\frac{1}{\\alpha}\\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] Thus \\[\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i} \\overset{\\text{a.s.}}{\\to} \\alpha\\] To show that \\(\\int_0^c x \\alpha e^{-\\alpha x} dx = \\frac{1}{\\alpha}(1 - (1 + \\alpha c) e^{-\\alpha c})\\), we can use the trick of differentiating under the integral sign. \\[\\begin{align*}\n  \\alpha \\int_0^c xe^{-\\alpha x} dx & = \\alpha \\int_0^c -\\frac{d}{d \\alpha} e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\int_0^c e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\frac{1}{\\alpha} (1 - e^{-\\alpha c}) \\\\\n  & = \\alpha \\left(\\frac{1 - (1 + \\alpha c) e^{-\\alpha c}}{\\alpha^2}\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-4.html",
    "href": "survival-material/lecture-4.html",
    "title": "Lecture 4",
    "section": "",
    "text": "When we have \\((X_i, C_i) \\overset{\\text{iid}}{\\sim} F\\) such that noninformative censoring and parameter separability hold, we showed that we can write the likelihood for the survival process: \\[f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i} \\exp\\left(-\\int_0^{t_i} \\lambda_\\eta(u) du \\right).\\] This can again be simplified by collecting terms inside the exponential: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\sum_{i=1}^n \\int_0^{t_i} \\lambda_\\eta(u) du \\right).\n\\end{align}\\]\nLet’s make a slight change to how we write the survival function. Define the indicator function \\(Y(u)\\) to be \\[Y_i(u) = \\mathbbm{1}\\left(t_i \\geq u\\right).\\] This function is left-continuous, with right-hand limits, an example of which is shown in\n\n\n\n\n\n\n\n\nFigure 1: Example plot of an at-risk function\n\n\n\n\n\nThis allows us to rewrite our likelihood as follows: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\sum_{i=1}^n \\int_0^{\\infty} Y_i(u) \\lambda_\\eta(u) du \\right)\\\\\n& = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\int_0^{\\infty} \\lambda_\\eta(u) \\sum_{i=1}^n Y_i(u)  du \\right)\n\\end{align}\\] For notational convenience, we’ll define the function \\(\\widebar{Y}(u)\\) as: \\[\\widebar{Y}(u) = \\sum_{i=1}^n Y_i(u).\\] Then our likelihood is: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\int_0^{\\infty} \\lambda_\\eta(u) \\widebar{Y}(u)  du \\right)\n\\end{align}\\]\nWe can consider a nonparametric model for the hazard, estimating \\(\\lambda\\) at each \\(t_i\\) as a separate parameter. An example of this is shown in Figure 2, which corresponds to the discrete survival function in\n\n\n\n\n\n\n\n\nFigure 2: Example plot of a discrete hazard function\n\n\n\n\n\nIn order to evaluate the integral \\[\\int_0^{\\infty} \\lambda(u) \\widebar{Y}(u)  du,\\] note that we can rewrite \\(\\lambda(t_i)\\) as \\[\\lambda(t_i) = \\Lambda(t_i) - \\Lambda(t_i-),\\] where \\(\\Lambda(t)\\) is the cumulative hazard function. We’ll write as \\(\\lambda(u)\\) as \\(d\\Lambda(u)\\). Finally, recall that because \\(S(t)\\) is right-continuous, \\(\\Lambda(t)\\) is also right-continuous.\n\n\n\n\n\n\n\n\nFigure 3: Example plot of a discrete cumulative hazard function\n\n\n\n\n\nWe’ll also need a bit of integration theory from Lebesgue-Stieltjies integrals. Suppose that \\(G\\) is a right-continuous, monotone function on \\([0,\\infty)\\) with countably many discontinuities at \\(a_i\\), and let \\(dG(a_i) = G(a_i) - G(a_i-)\\). Then for a measurable function \\(F\\) on \\([0,\\infty)\\), the integral over a set \\(B\\) \\[\\int_B F(x) dG(x) = \\sum_{i \\mid a_i \\in B} F(a_i) dG(a_i).\\] Using these results, the integral can be evaluated to \\[\\int_0^{\\infty} \\left(\\widebar{Y}(u)\\right)d\\Lambda(u)  du = \\sum_{j = 1}^{n} \\lambda(t_j) \\widebar{Y}(t_j)\\] Let’s take the log of the expression to get a log-likelihood: \\[\\begin{align}\n\\log f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\sum_{i=1}^n \\delta_i \\log (\\lambda_\\eta(t_i)) -\\sum_{j=1}^{n} \\lambda_\\eta(t_j) \\widebar{Y}(t_j)\n\\end{align}\\] Taking the gradient with respect to \\(\\lambda_\\eta(t_i)\\) gives \\[\\begin{align}\n\\nabla \\log f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\frac{\\delta_i}{\\lambda_\\eta(t_i)} -\\widebar{Y}(t_i).\n\\end{align}\\] Note that the Hessian is also diagonal, which implies asymptotic independence of \\(\\lambda(t_i)\\). This is solved at \\[\\begin{align}\n  \\hat{\\lambda}_\\eta(t_i) & = \\frac{\\delta_i}{\\widebar{Y}(t_i)}\n\\end{align}\\] This gives an expression for \\(\\Lambda(t)\\): \\[\\begin{align}\n\\Lambda^{\\text{NA}}(t) =   \\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\widebar{Y}(t_i)}\n\\end{align} \\tag{1}\\] This also gives an expression for \\(S(t)\\): \\[\\begin{align}\n  S^{\\text{KM}}(t) & = \\prod_{i \\mid \\delta_i = 1, t_i \\leq t}(1 - \\frac{1}{\\widebar{Y}(t_i)}).\n\\end{align}\n\\tag{2}\\] This is also known as the Kaplan-Meier estimator. An alternative expression is: \\[\\begin{align}\n\\label{eq:surival-na}\n  S^{\\text{NA}}(t) & = \\exp\\left(-\\textstyle\\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\widebar{Y}(t_i)}\\right)\n\\end{align}\\] We can show that the cumulative hazard as implied by Equation 2 is asymptotically equivalent to Equation 1. Given \\[\nS_X(t) = \\exp\\lp-\\Lambda_X(t) \\rp\n\\] \\[\\begin{align}\n\\Lambda^{\\text{KM}} & = - \\log \\left(\\prod_{i \\mid \\delta_i = 1, t_i \\leq t}(1 - \\frac{1}{\\widebar{Y}(t_i)}) \\right)\\\\\n& = -\\sum_{i \\mid \\delta_i = 1, t_i \\leq t}\\log(1 - \\frac{1}{\\widebar{Y}(t_i)}) \\\\\n& \\approx \\sum_{i \\mid \\delta_i = 1, t_i \\leq t}\\frac{1}{\\widebar{Y}(t_i)}\n\\end{align}\\] where the last line follows from the Taylor approximation of \\(\\log(1 - x) \\approx -x\\) when \\(x \\approx 0\\).\n\n\nIn order to get the standard errors for the Kaplan-Meier estimator, we’ll use a Taylor expansion: \\[\\begin{align}\n  \\log(S^{\\text{KM}}(t)) \\approx \\log(S(t)) + \\frac{1}{S(t)}(S^{\\text{KM}}(t) - S(t))\n\\end{align}\\] which leads to \\[\\Var{\\log(S^{\\text{KM}}(t))} = \\frac{1}{S(t)^2}\\Var{S^{\\text{KM}}(t)}\\] or \\[\\Var{S^{\\text{KM}}(t)} = \\Var{\\log(S^{\\text{KM}}(t))}S(t)^2.\\] We use the plug-in estimator for \\(S(t)\\) here, so we get: \\[\\Var{S^{\\text{KM}}(t)} = \\Var{\\log(S^{\\text{KM}}(t))}(S^{\\text{KM}}(t))^2.\\] Now we need an expression for \\(\\Var{\\log(S^{\\text{KM}}(t))}\\). First we write the log of the KM estimator: \\[\\begin{align}\n  \\log \\hat{S}^{\\text{KM}}(t) = \\sum_{i \\mid t_i \\leq t} \\log(1 - \\hat{\\lambda}(t_i)).\n\\end{align}\\] First we find the Taylor expansion for each term, which is justified by the fact that \\((1 - \\hat{\\lambda}(t_i)) \\approx (1 - \\lambda(t_i))\\) for large samples: \\[\\begin{align}\n  \\log(1 - \\hat{\\lambda}(t_i)) \\approx  \\log(1 - \\lambda(t_i)) - \\frac{1}{1 - \\lambda(t_i)}(\\hat{\\lambda}(t_i) - \\lambda(t_i))\n\\end{align}\\] Then \\[\\Var{\\log(1 - \\hat{\\lambda}(t_i))} \\approx \\frac{1}{(1 - \\lambda(t_i))^2}\\Var{\\hat{\\lambda}(t_i)}\\] We can estimate the \\(\\Var{\\hat{\\lambda}(t_i)}\\) as: \\[\\Var{\\hat{\\lambda}(t_i)} = \\Var{\\delta_i} / \\bar{Y}^2(t_i)\\] Treating \\(\\delta_i\\) as a binomial random variable with \\(\\bar{Y}(t_i)\\) number of trials: \\[\\delta_i \\sim \\text{Binomial}(\\bar{Y}(t_i), p_i)\\] The variance of \\(\\delta_i\\) is \\(p_i (1 - p_i) \\bar{Y}(t_i)\\). Using \\(\\hat{\\lambda}(t_i)\\) as a plug-in estimator for \\(p_i\\) as \\(\\hat{\\lambda}(t_i)\\), this gives: \\[\\Var{\\delta_i} = \\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i)) \\bar{Y}(t_i)\\] Putting this together with the \\(\\bar{Y}^2(t_i)\\) in the denominator gives the following estimate for the variance of \\(\\hat{\\lambda}(t_i)\\): \\[\\frac{\\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i))}{\\bar{Y}(t_i)}.\\] Finally using the plug-in estimator for \\((1 - \\lambda(t_i))^2\\) in the denominator of the Taylor expansion formula gives: \\[\\begin{align}\n\\Var{\\log(1 - \\hat{\\lambda}(t_i))}  & \\approx \\frac{1}{(1 - \\hat{\\lambda}(t_i))^2}\\frac{\\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i))}{\\bar{Y}(t_i)}\\\\\n                                    & \\approx \\frac{\\hat{\\lambda}(t_i)}{\\bar{Y}(t_i)(1 - \\hat{\\lambda}(t_i))} \\\\\n              & \\approx \\frac{\\delta_i}{\\bar{Y}(t_i)(\\bar{Y}(t_i) - \\hat{\\lambda}(t_i))}\n\\end{align}\\] Putting this all together along with the fact that \\(\\lambda(t_i) \\overset{\\text{asymp}}{\\perp\\!\\!\\!\\perp} \\lambda(t_j)\\), yields what is known as Greenwood’s formula: \\[\\Var{S^{\\text{KM}}(t)} = (S^{\\text{KM}}(t))^2 \\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{\\delta_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - \\delta_i)}.\\]\n\n\n\n\nIf we wanted to construct asymptotic, point-wise confidence intervals for the KM estimator, we can go about it in several ways. The most straightforward way to compute confidence intervals is to directly use the estimated survival function at \\(t_0\\) and the standard error estimator from Greenwood’s formula. Let \\(\\hat{\\sigma}(t)\\) be \\[\\sqrt{\\sum_{i \\mid d_i = 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}}.\\] Then our confidence interval, \\(C^{\\text{KM}}\\), is \\[\\begin{align*}\n    C^{\\text{KM}} = \\left(\\hat{S}^{\\text{KM}}(t_0) - z_{1-\\alpha / 2} \\hat{\\sigma} \\hat{S}^{\\text{KM}}(t_0), \\hat{S}^{\\text{KM}}(t_0) + z_{1-\\alpha / 2} \\hat{\\sigma} \\hat{S}^{\\text{KM}}(t_0)\\right)\n\\end{align*}\\] The issue with this confidence interval is that it is not guaranteed to be greater than zero or less than 1, so we may have nonsensical results for upper and lower bounds. A solution is to build a confidence set for a suitably transformed Kaplan Meier estimator, and use the inverse transformation to enforce the natural \\([0,1]\\) bounds. One option is to use the logit transformation, another is to use the log-log transformation.\nWe’ll walk through the log-log transformation:\nNote that we have the following result: \\[\\begin{align}\n\\Var{\\log(\\hat{S}^{\\text{KM}}(t))} & = \\frac{1}{S(t)^2}\\Var{\\hat{S}^{\\text{KM}}(t)}\\\\\n& = \\sum_{i \\mid d_i = 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}.\n\\end{align}\\] Then \\[\\begin{align*}\n   \\log(-\\log(\\hat{S}^\\text{KM}(t))) \\approx  \\log(-\\log(S(t))) - \\frac{1}{\\log(S(t))}(\\log(S^{\\text{KM}}(t)) - \\log(S(t)))\n\\end{align*}\\] So \\[\\begin{align*}\n   \\Var{\\log(-\\log(\\hat{S}^\\text{KM}(t)))} \\approx  \\frac{1}{\\log(S(t))^2}\\Var{\\log(\\hat{S}^{\\text{KM}}(t)}\n\\end{align*}\\] or \\[\\begin{align*}\n   \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) \\approx  \\frac{1}{\\left| \\log(S(t)) \\right|}\\hat{\\sigma}(t)\n\\end{align*}\\] We don’t know \\(S(t)\\), so we’ll plug-in KM estimator for \\(S(t)\\): \\[\\begin{align*}\n   \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) \\approx  \\frac{1}{\\left| \\log(S^{\\text{KM}}(t)) \\right|}\\hat{\\sigma}(t)\n\\end{align*}\\]\nLet \\(u = \\log(-\\log S(t))\\), \\(\\hat{u} = \\log(-\\log(\\hat{S}^\\text{KM}(t)))\\), and \\(\\hat{\\sigma}_u = \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t))))\\). Then \\[\\hat{S}^\\text{KM}(t) = \\exp(-e^{\\hat{u}}).\\] Note that \\(\\exp(-e^u)\\) is a monotone decreasing function of its input, \\(u\\). This means that for a set \\([a,b]\\) \\[a \\leq u \\leq b \\implies \\exp(-e^a) \\geq \\exp(-e^u) \\geq \\exp(-e^b).\\] We’ll take it as a given that asymptotically, \\[\\frac{\\hat{u} - u}{\\hat{\\sigma}_u} \\overset{d}{\\to} \\mathcal{N}(0,1).\\] Then we can derive an alternative asymptotic confidence interval for the Kaplan-Meier estimator of survival at time \\(t_0\\) by transforming a confidence interval for \\(u\\). Let \\(z_{1 - \\alpha/2}\\) be the \\(1 - \\alpha/2\\) quantile of a standard normal distribution with CDF \\(\\Phi\\), or \\[z_{1 - \\alpha/2} = \\Phi^{-1}(1 - \\alpha/2).\\] \\[\\begin{align*}\nP(-z_{1 - \\alpha/2} \\leq \\frac{\\hat{u} - u}{\\hat{\\sigma}_u} \\leq z_{1 - \\alpha/2}) & =\nP(\\hat{u}-\\hat{\\sigma}_u z_{1 - \\alpha/2} \\leq u \\leq \\hat{u} + \\hat{\\sigma}_u z_{1 - \\alpha/2}) \\\\\n& = P(\\exp(-e^{\\hat{u}-\\hat{\\sigma}_u z_{1 - \\alpha/2}}) \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u} + \\hat{\\sigma}_u z_{1 - \\alpha/2}}))  \\\\\n& = P(\\exp(-e^{\\hat{u}}e^{-\\hat{\\sigma}_u z_{1 - \\alpha/2}}) \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u}}e^{\\hat{\\sigma}_u z_{1 - \\alpha/2}})) \\\\\n& = P(\\exp(-e^{\\hat{u}})^{e^{-\\hat{\\sigma}_u z_{1 - \\alpha/2}}} \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u}})^{e^{\\hat{\\sigma}_u z_{1 - \\alpha/2}}}) \\\\\n& = P((\\hat{S}^\\text{KM}(t))^{e^{-\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}} \\geq S(t) \\\\\n&\\qquad \\geq (\\hat{S}^\\text{KM}(t))^{e^{\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}}).\n\\end{align*}\\] So \\[\\begin{align}\n\\label{eq:log-log-CI}\nP\\left(S(t) \\in \\left((\\hat{S}^\\text{KM}(t))^{e^{\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}},(\\hat{S}^\\text{KM}(t))^{e^{-\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}}\\right)\\right)\\overset{\\text{asympt.}}{=} 1 - \\alpha\n\\end{align}\\]"
  },
  {
    "objectID": "survival-material/lecture-4.html#derivation-of-nelson-aalen-and-kaplan-meier-estimators",
    "href": "survival-material/lecture-4.html#derivation-of-nelson-aalen-and-kaplan-meier-estimators",
    "title": "Lecture 4",
    "section": "",
    "text": "When we have \\((X_i, C_i) \\overset{\\text{iid}}{\\sim} F\\) such that noninformative censoring and parameter separability hold, we showed that we can write the likelihood for the survival process: \\[f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i} \\exp\\left(-\\int_0^{t_i} \\lambda_\\eta(u) du \\right).\\] This can again be simplified by collecting terms inside the exponential: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\sum_{i=1}^n \\int_0^{t_i} \\lambda_\\eta(u) du \\right).\n\\end{align}\\]\nLet’s make a slight change to how we write the survival function. Define the indicator function \\(Y(u)\\) to be \\[Y_i(u) = \\mathbbm{1}\\left(t_i \\geq u\\right).\\] This function is left-continuous, with right-hand limits, an example of which is shown in\n\n\n\n\n\n\n\n\nFigure 1: Example plot of an at-risk function\n\n\n\n\n\nThis allows us to rewrite our likelihood as follows: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\sum_{i=1}^n \\int_0^{\\infty} Y_i(u) \\lambda_\\eta(u) du \\right)\\\\\n& = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\int_0^{\\infty} \\lambda_\\eta(u) \\sum_{i=1}^n Y_i(u)  du \\right)\n\\end{align}\\] For notational convenience, we’ll define the function \\(\\widebar{Y}(u)\\) as: \\[\\widebar{Y}(u) = \\sum_{i=1}^n Y_i(u).\\] Then our likelihood is: \\[\\begin{align}\nf_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) = \\left(\\prod_{i=1}^n \\lambda_\\eta(t_i)^{\\delta_i}\\right)\\exp\\left(-\\int_0^{\\infty} \\lambda_\\eta(u) \\widebar{Y}(u)  du \\right)\n\\end{align}\\]\nWe can consider a nonparametric model for the hazard, estimating \\(\\lambda\\) at each \\(t_i\\) as a separate parameter. An example of this is shown in Figure 2, which corresponds to the discrete survival function in\n\n\n\n\n\n\n\n\nFigure 2: Example plot of a discrete hazard function\n\n\n\n\n\nIn order to evaluate the integral \\[\\int_0^{\\infty} \\lambda(u) \\widebar{Y}(u)  du,\\] note that we can rewrite \\(\\lambda(t_i)\\) as \\[\\lambda(t_i) = \\Lambda(t_i) - \\Lambda(t_i-),\\] where \\(\\Lambda(t)\\) is the cumulative hazard function. We’ll write as \\(\\lambda(u)\\) as \\(d\\Lambda(u)\\). Finally, recall that because \\(S(t)\\) is right-continuous, \\(\\Lambda(t)\\) is also right-continuous.\n\n\n\n\n\n\n\n\nFigure 3: Example plot of a discrete cumulative hazard function\n\n\n\n\n\nWe’ll also need a bit of integration theory from Lebesgue-Stieltjies integrals. Suppose that \\(G\\) is a right-continuous, monotone function on \\([0,\\infty)\\) with countably many discontinuities at \\(a_i\\), and let \\(dG(a_i) = G(a_i) - G(a_i-)\\). Then for a measurable function \\(F\\) on \\([0,\\infty)\\), the integral over a set \\(B\\) \\[\\int_B F(x) dG(x) = \\sum_{i \\mid a_i \\in B} F(a_i) dG(a_i).\\] Using these results, the integral can be evaluated to \\[\\int_0^{\\infty} \\left(\\widebar{Y}(u)\\right)d\\Lambda(u)  du = \\sum_{j = 1}^{n} \\lambda(t_j) \\widebar{Y}(t_j)\\] Let’s take the log of the expression to get a log-likelihood: \\[\\begin{align}\n\\log f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\sum_{i=1}^n \\delta_i \\log (\\lambda_\\eta(t_i)) -\\sum_{j=1}^{n} \\lambda_\\eta(t_j) \\widebar{Y}(t_j)\n\\end{align}\\] Taking the gradient with respect to \\(\\lambda_\\eta(t_i)\\) gives \\[\\begin{align}\n\\nabla \\log f_\\eta(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\frac{\\delta_i}{\\lambda_\\eta(t_i)} -\\widebar{Y}(t_i).\n\\end{align}\\] Note that the Hessian is also diagonal, which implies asymptotic independence of \\(\\lambda(t_i)\\). This is solved at \\[\\begin{align}\n  \\hat{\\lambda}_\\eta(t_i) & = \\frac{\\delta_i}{\\widebar{Y}(t_i)}\n\\end{align}\\] This gives an expression for \\(\\Lambda(t)\\): \\[\\begin{align}\n\\Lambda^{\\text{NA}}(t) =   \\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\widebar{Y}(t_i)}\n\\end{align} \\tag{1}\\] This also gives an expression for \\(S(t)\\): \\[\\begin{align}\n  S^{\\text{KM}}(t) & = \\prod_{i \\mid \\delta_i = 1, t_i \\leq t}(1 - \\frac{1}{\\widebar{Y}(t_i)}).\n\\end{align}\n\\tag{2}\\] This is also known as the Kaplan-Meier estimator. An alternative expression is: \\[\\begin{align}\n\\label{eq:surival-na}\n  S^{\\text{NA}}(t) & = \\exp\\left(-\\textstyle\\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{1}{\\widebar{Y}(t_i)}\\right)\n\\end{align}\\] We can show that the cumulative hazard as implied by Equation 2 is asymptotically equivalent to Equation 1. Given \\[\nS_X(t) = \\exp\\lp-\\Lambda_X(t) \\rp\n\\] \\[\\begin{align}\n\\Lambda^{\\text{KM}} & = - \\log \\left(\\prod_{i \\mid \\delta_i = 1, t_i \\leq t}(1 - \\frac{1}{\\widebar{Y}(t_i)}) \\right)\\\\\n& = -\\sum_{i \\mid \\delta_i = 1, t_i \\leq t}\\log(1 - \\frac{1}{\\widebar{Y}(t_i)}) \\\\\n& \\approx \\sum_{i \\mid \\delta_i = 1, t_i \\leq t}\\frac{1}{\\widebar{Y}(t_i)}\n\\end{align}\\] where the last line follows from the Taylor approximation of \\(\\log(1 - x) \\approx -x\\) when \\(x \\approx 0\\).\n\n\nIn order to get the standard errors for the Kaplan-Meier estimator, we’ll use a Taylor expansion: \\[\\begin{align}\n  \\log(S^{\\text{KM}}(t)) \\approx \\log(S(t)) + \\frac{1}{S(t)}(S^{\\text{KM}}(t) - S(t))\n\\end{align}\\] which leads to \\[\\Var{\\log(S^{\\text{KM}}(t))} = \\frac{1}{S(t)^2}\\Var{S^{\\text{KM}}(t)}\\] or \\[\\Var{S^{\\text{KM}}(t)} = \\Var{\\log(S^{\\text{KM}}(t))}S(t)^2.\\] We use the plug-in estimator for \\(S(t)\\) here, so we get: \\[\\Var{S^{\\text{KM}}(t)} = \\Var{\\log(S^{\\text{KM}}(t))}(S^{\\text{KM}}(t))^2.\\] Now we need an expression for \\(\\Var{\\log(S^{\\text{KM}}(t))}\\). First we write the log of the KM estimator: \\[\\begin{align}\n  \\log \\hat{S}^{\\text{KM}}(t) = \\sum_{i \\mid t_i \\leq t} \\log(1 - \\hat{\\lambda}(t_i)).\n\\end{align}\\] First we find the Taylor expansion for each term, which is justified by the fact that \\((1 - \\hat{\\lambda}(t_i)) \\approx (1 - \\lambda(t_i))\\) for large samples: \\[\\begin{align}\n  \\log(1 - \\hat{\\lambda}(t_i)) \\approx  \\log(1 - \\lambda(t_i)) - \\frac{1}{1 - \\lambda(t_i)}(\\hat{\\lambda}(t_i) - \\lambda(t_i))\n\\end{align}\\] Then \\[\\Var{\\log(1 - \\hat{\\lambda}(t_i))} \\approx \\frac{1}{(1 - \\lambda(t_i))^2}\\Var{\\hat{\\lambda}(t_i)}\\] We can estimate the \\(\\Var{\\hat{\\lambda}(t_i)}\\) as: \\[\\Var{\\hat{\\lambda}(t_i)} = \\Var{\\delta_i} / \\bar{Y}^2(t_i)\\] Treating \\(\\delta_i\\) as a binomial random variable with \\(\\bar{Y}(t_i)\\) number of trials: \\[\\delta_i \\sim \\text{Binomial}(\\bar{Y}(t_i), p_i)\\] The variance of \\(\\delta_i\\) is \\(p_i (1 - p_i) \\bar{Y}(t_i)\\). Using \\(\\hat{\\lambda}(t_i)\\) as a plug-in estimator for \\(p_i\\) as \\(\\hat{\\lambda}(t_i)\\), this gives: \\[\\Var{\\delta_i} = \\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i)) \\bar{Y}(t_i)\\] Putting this together with the \\(\\bar{Y}^2(t_i)\\) in the denominator gives the following estimate for the variance of \\(\\hat{\\lambda}(t_i)\\): \\[\\frac{\\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i))}{\\bar{Y}(t_i)}.\\] Finally using the plug-in estimator for \\((1 - \\lambda(t_i))^2\\) in the denominator of the Taylor expansion formula gives: \\[\\begin{align}\n\\Var{\\log(1 - \\hat{\\lambda}(t_i))}  & \\approx \\frac{1}{(1 - \\hat{\\lambda}(t_i))^2}\\frac{\\hat{\\lambda}(t_i)(1 - \\hat{\\lambda}(t_i))}{\\bar{Y}(t_i)}\\\\\n                                    & \\approx \\frac{\\hat{\\lambda}(t_i)}{\\bar{Y}(t_i)(1 - \\hat{\\lambda}(t_i))} \\\\\n              & \\approx \\frac{\\delta_i}{\\bar{Y}(t_i)(\\bar{Y}(t_i) - \\hat{\\lambda}(t_i))}\n\\end{align}\\] Putting this all together along with the fact that \\(\\lambda(t_i) \\overset{\\text{asymp}}{\\perp\\!\\!\\!\\perp} \\lambda(t_j)\\), yields what is known as Greenwood’s formula: \\[\\Var{S^{\\text{KM}}(t)} = (S^{\\text{KM}}(t))^2 \\sum_{i \\mid \\delta_i = 1, t_i \\leq t} \\frac{\\delta_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - \\delta_i)}.\\]"
  },
  {
    "objectID": "survival-material/lecture-4.html#confidence-intervals",
    "href": "survival-material/lecture-4.html#confidence-intervals",
    "title": "Lecture 4",
    "section": "",
    "text": "If we wanted to construct asymptotic, point-wise confidence intervals for the KM estimator, we can go about it in several ways. The most straightforward way to compute confidence intervals is to directly use the estimated survival function at \\(t_0\\) and the standard error estimator from Greenwood’s formula. Let \\(\\hat{\\sigma}(t)\\) be \\[\\sqrt{\\sum_{i \\mid d_i = 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}}.\\] Then our confidence interval, \\(C^{\\text{KM}}\\), is \\[\\begin{align*}\n    C^{\\text{KM}} = \\left(\\hat{S}^{\\text{KM}}(t_0) - z_{1-\\alpha / 2} \\hat{\\sigma} \\hat{S}^{\\text{KM}}(t_0), \\hat{S}^{\\text{KM}}(t_0) + z_{1-\\alpha / 2} \\hat{\\sigma} \\hat{S}^{\\text{KM}}(t_0)\\right)\n\\end{align*}\\] The issue with this confidence interval is that it is not guaranteed to be greater than zero or less than 1, so we may have nonsensical results for upper and lower bounds. A solution is to build a confidence set for a suitably transformed Kaplan Meier estimator, and use the inverse transformation to enforce the natural \\([0,1]\\) bounds. One option is to use the logit transformation, another is to use the log-log transformation.\nWe’ll walk through the log-log transformation:\nNote that we have the following result: \\[\\begin{align}\n\\Var{\\log(\\hat{S}^{\\text{KM}}(t))} & = \\frac{1}{S(t)^2}\\Var{\\hat{S}^{\\text{KM}}(t)}\\\\\n& = \\sum_{i \\mid d_i = 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}.\n\\end{align}\\] Then \\[\\begin{align*}\n   \\log(-\\log(\\hat{S}^\\text{KM}(t))) \\approx  \\log(-\\log(S(t))) - \\frac{1}{\\log(S(t))}(\\log(S^{\\text{KM}}(t)) - \\log(S(t)))\n\\end{align*}\\] So \\[\\begin{align*}\n   \\Var{\\log(-\\log(\\hat{S}^\\text{KM}(t)))} \\approx  \\frac{1}{\\log(S(t))^2}\\Var{\\log(\\hat{S}^{\\text{KM}}(t)}\n\\end{align*}\\] or \\[\\begin{align*}\n   \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) \\approx  \\frac{1}{\\left| \\log(S(t)) \\right|}\\hat{\\sigma}(t)\n\\end{align*}\\] We don’t know \\(S(t)\\), so we’ll plug-in KM estimator for \\(S(t)\\): \\[\\begin{align*}\n   \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) \\approx  \\frac{1}{\\left| \\log(S^{\\text{KM}}(t)) \\right|}\\hat{\\sigma}(t)\n\\end{align*}\\]\nLet \\(u = \\log(-\\log S(t))\\), \\(\\hat{u} = \\log(-\\log(\\hat{S}^\\text{KM}(t)))\\), and \\(\\hat{\\sigma}_u = \\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t))))\\). Then \\[\\hat{S}^\\text{KM}(t) = \\exp(-e^{\\hat{u}}).\\] Note that \\(\\exp(-e^u)\\) is a monotone decreasing function of its input, \\(u\\). This means that for a set \\([a,b]\\) \\[a \\leq u \\leq b \\implies \\exp(-e^a) \\geq \\exp(-e^u) \\geq \\exp(-e^b).\\] We’ll take it as a given that asymptotically, \\[\\frac{\\hat{u} - u}{\\hat{\\sigma}_u} \\overset{d}{\\to} \\mathcal{N}(0,1).\\] Then we can derive an alternative asymptotic confidence interval for the Kaplan-Meier estimator of survival at time \\(t_0\\) by transforming a confidence interval for \\(u\\). Let \\(z_{1 - \\alpha/2}\\) be the \\(1 - \\alpha/2\\) quantile of a standard normal distribution with CDF \\(\\Phi\\), or \\[z_{1 - \\alpha/2} = \\Phi^{-1}(1 - \\alpha/2).\\] \\[\\begin{align*}\nP(-z_{1 - \\alpha/2} \\leq \\frac{\\hat{u} - u}{\\hat{\\sigma}_u} \\leq z_{1 - \\alpha/2}) & =\nP(\\hat{u}-\\hat{\\sigma}_u z_{1 - \\alpha/2} \\leq u \\leq \\hat{u} + \\hat{\\sigma}_u z_{1 - \\alpha/2}) \\\\\n& = P(\\exp(-e^{\\hat{u}-\\hat{\\sigma}_u z_{1 - \\alpha/2}}) \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u} + \\hat{\\sigma}_u z_{1 - \\alpha/2}}))  \\\\\n& = P(\\exp(-e^{\\hat{u}}e^{-\\hat{\\sigma}_u z_{1 - \\alpha/2}}) \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u}}e^{\\hat{\\sigma}_u z_{1 - \\alpha/2}})) \\\\\n& = P(\\exp(-e^{\\hat{u}})^{e^{-\\hat{\\sigma}_u z_{1 - \\alpha/2}}} \\geq \\exp(-e^{u}) \\geq \\exp(-e^{\\hat{u}})^{e^{\\hat{\\sigma}_u z_{1 - \\alpha/2}}}) \\\\\n& = P((\\hat{S}^\\text{KM}(t))^{e^{-\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}} \\geq S(t) \\\\\n&\\qquad \\geq (\\hat{S}^\\text{KM}(t))^{e^{\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}}).\n\\end{align*}\\] So \\[\\begin{align}\n\\label{eq:log-log-CI}\nP\\left(S(t) \\in \\left((\\hat{S}^\\text{KM}(t))^{e^{\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}},(\\hat{S}^\\text{KM}(t))^{e^{-\\text{SE}(\\log(-\\log(\\hat{S}^\\text{KM}(t)))) z_{1 - \\alpha/2}}}\\right)\\right)\\overset{\\text{asympt.}}{=} 1 - \\alpha\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#ipw-z-estimators",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#ipw-z-estimators",
    "title": "Missing data lecture 2",
    "section": "IPW-Z-estimators",
    "text": "IPW-Z-estimators\nMost estimators of interest can be formulated as solutions in \\(\\theta\\) to the following system of equations: \\[\n\\frac{1}{n} \\sum_{i=1}^n \\psi(\\theta, y_i, x_i) = 0\n\\] The key property of \\(\\psi(\\theta, Y_i, X_i)\\) is that the following identity holds: \\[\n\\ExpA{\\psi(\\theta^\\dagger, Y_i, X_i)}{(Y_i, X_i \\mid \\theta^\\dagger)} = 0\n\\] We can use the same idea as above: fit the complete cases with weights: \\[\n\\frac{1}{n} \\sum_{i=1}^n \\frac{(1 - m_i)}{\\pi(x_i, \\beta)} \\psi(\\theta, y_i, x_i) = 0\n\\] Given the inverse weighting, we’ll end up with an unbiased estimator of \\[\n\\ExpA{\\psi(\\theta, Y_i, X_i)}{(Y_i, X_i \\mid \\theta^\\dagger)}\n\\] which we can then use to find \\(\\hat{\\theta}\\)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "It makes a good bit of sense to fill in missing values with some sort of estimate of the missing values. There are myriad ways to fill in unknown values with imputations.\nThe first way to fill in missing values we’ll cover are single imputation methods, whereby we fill in each missing value with one value. The values of the missing observations can be generated from an explicit model or an implicit model. We’ll demonstrate our methods on the following example dataset.\nIn the example shown in Figure 1, \\(n=500\\) values of \\(Y_1, Y_2\\) are a sample from a bivariate normal distribution; each random variable is marginally distributed as standard normal (i.e. mean \\(0\\) and variance equal to \\(1\\)), with covariance equal to \\(\\sqrt{0.5}\\). The green values in Figure 1 are observed, while the red values are missing. Visually, we can see that the values are not missing completely at random because a preponderance of values are missing from the bottom left quadrant of the plot.\n\n\n\n\n\n\n\n\nFigure 1: How do you fill in the red points?\n\n\n\n\n\nSingle imputation methods are akin to poststratified estimators:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{ps} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\mu(x, \\hat{\\gamma})(1 - p_x)) \\frac{N_x}{N}\n\\end{aligned}\n\\] The estimator \\(\\bar{y}^{\\mathrm{ps}}\\) replaces all responses within a certain value of \\(x\\) that are unobserved with \\(\\mu(x,\\hat{\\gamma})\\).\nThis seems like a good approach at first because we’re taking into account the model for \\(Y_i \\mid X_i = x\\) when doing our imputation. The downside is that we’re understating uncertainty in our resulting estimator because all values lie exactly on the regression function.\nFigure 2 shows conditional mean imputation applied to the dataset above. The bright red dots are imputed data.\n\n\n\n\n\n\n\n\nFigure 2: Conditional mean imputation\n\n\n\n\n\nThe imputations don’t fit with the rest of the points because they lie directly on the regression line.\nThere are other even simpler approaches to imputation.\n\n\nThere is mean imputation, whereby we impute each missing observation with the overall mean of the dataset:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\bar{y}^\\mathrm{res} (1 - p_x)) \\frac{n_x}{n}\n\\end{aligned}\n\\]\nThis technique seems sketchy because if \\(Y_i\\) depends on \\(X_i\\) then we’ll incur some bias by ignoring this relationship. Furthermore, we’ll ignore the added uncertainty by the fact that we’ve used \\(\\bar{y}\\) for every missing response in the dataset.\n\n\n\n\n\n\n\n\nFigure 3: Conditional mean imputation\n\n\n\n\n\nFigure 3 shows this technique applied to our example. This is worse than the conditional mean imputation because we are ignoring the relationship between \\(Y_1\\) and \\(Y_2\\).\n\n\n\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\tilde{y}^\\mathrm{res} (1 - p_x)) \\frac{n_x}{n} \\\\\n\\tilde{y}^\\mathrm{res} & \\sim \\text{Normal}(\\bar{y}^\\mathrm{res}, \\frac{\\hat{\\sigma}^2_{\\mathrm{res}}}{r_x})\n\\end{aligned}\n\\]\nThis is a better approach than mean imputation, but we’ll still be ignoring the relationship between the covariates and our response.\n\n\n\n\n\n\n\n\nFigure 4: Conditional mean imputation\n\n\n\n\n\n\n\n\nThe most faithful single imputation method might be something like the following:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\tilde{y}_x (1 - p_x)) \\frac{n_x}{n} \\\\\n\\tilde{y}_x \\mid x & \\overset{\\text{indy}}{\\sim} \\text{Normal}\\lp\\mu(x, \\hat{\\gamma}), \\frac{\\hat{\\sigma}^2_{\\mathrm{res}}(x)}{r_x}\\rp\n\\end{aligned}\n\\]\nThis approach is shown in Figure 5. The purple short-dashed line is the regression obtained from the completed dataset. We can see that despite the added uncertainty in the imputation method, the resulting complete-data estimator is still biased compared to the true relationship. We’ll still be ignoring the uncertainty that arises from the fact that we could have generated many different values for \\(\\tilde{y}_x\\) versus the values that we used for the estimator.\n\n\n\n\n\n\n\n\nFigure 5: Conditional mean imputation\n\n\n\n\n\n\n\n\nThere are still more single imputation models.\n\n\nSuppose we consider \\(Y_1\\) the value of the first measurement in a longitudinal study with \\(2\\) measurement occasions. A popular method of single imputation in longitudinal analyses is last-observation-carried-forward, or LOCF for short. This is shown below:\n\n\n\n\n\n\n\n\nFigure 6: LOCF imputation\n\n\n\n\n\nWe have the same problem with conditional mean imputation, namely the lack of randomness in our imputations.\n\n\n\nHot deck imputation is something that is employed by the Census Bureau to handle item nonresponse on surveys. The simplest implementation is described as follows. Let there be \\(n\\) observations on \\(y_i\\), a univariate measurement, of which \\(n-r\\) are missing. The hot deck estimator is defined as follows: \\[\n\\begin{aligned}\n\\bar{y}^{\\mathrm{HD}} & = \\frac{1}{n} \\lp r \\bar{y}_{(0)} + (n-r) \\bar{y}_{(1)} \\rp \\\\\n\\bar{y}_{(1)} \\mid y_{(0)} & = \\frac{1}{n-r}\\sum_{i=1}^r h_i y_{(0)i} \\\\\n(H_1, \\dots, H_r) & \\sim \\text{Multinomial}((n-r) \\mid (1/r, \\dots, 1/r))\n\\end{aligned}\n\\] The units included in the fully observed data are called “donor” units because the set provides the values that can replace the missing values.\nWe can get the mean and variance of these estimators by using the law of total expectation and total variance:\n\\[\n\\begin{aligned}\n\\Exp{\\bar{y}^{\\mathrm{HD}}} & = \\ExpA{\\ExpA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}} \\\\\n\\Var{\\bar{y}^{\\mathrm{HD}}} & = \\VarA{\\ExpA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}} + \\ExpA{\\VarA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}}\n\\end{aligned}\n\\] Example 4.8 in Little and Rubin (2019) walks through the derivation of these expressions.\n\n\n\nGiven that the prior example is only useful when data are MCAR (as our book mentions), one can modify the donor units for each observation such that we only choose donors from a pool that is “close” to the unit with a missing value in terms of other completely observed values. For a unit for which \\(y_i\\) is missing but for which we have fully observed covariates \\(x_{i} = x_{i1}, \\dots x_{iK}\\), we would choose a donor unit with observed \\(y_j\\) and \\(x_j = x_{j1}, \\dots x_{jK}\\) such that some distance metric, \\(d(x_i, x_j)\\) is below a threshold value, \\(d_0\\).\n\n\n\nIn predictive mean matching, we fit a regression model on the completely observed units. Then we compute the predicted value for \\(y_i\\), or \\(\\hat{y}(x_i)\\) using \\(x_i\\) and pick donors for which \\((\\hat{y}(x_i) - \\hat{y}(x_j))^2\\) are below \\(d_0\\)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#mean-imputation",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#mean-imputation",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "There is mean imputation, whereby we impute each missing observation with the overall mean of the dataset:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\bar{y}^\\mathrm{res} (1 - p_x)) \\frac{n_x}{n}\n\\end{aligned}\n\\]\nThis technique seems sketchy because if \\(Y_i\\) depends on \\(X_i\\) then we’ll incur some bias by ignoring this relationship. Furthermore, we’ll ignore the added uncertainty by the fact that we’ve used \\(\\bar{y}\\) for every missing response in the dataset.\n\n\n\n\n\n\n\n\nFigure 3: Conditional mean imputation\n\n\n\n\n\nFigure 3 shows this technique applied to our example. This is worse than the conditional mean imputation because we are ignoring the relationship between \\(Y_1\\) and \\(Y_2\\)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#random-imputation",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#random-imputation",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\tilde{y}^\\mathrm{res} (1 - p_x)) \\frac{n_x}{n} \\\\\n\\tilde{y}^\\mathrm{res} & \\sim \\text{Normal}(\\bar{y}^\\mathrm{res}, \\frac{\\hat{\\sigma}^2_{\\mathrm{res}}}{r_x})\n\\end{aligned}\n\\]\nThis is a better approach than mean imputation, but we’ll still be ignoring the relationship between the covariates and our response.\n\n\n\n\n\n\n\n\nFigure 4: Conditional mean imputation"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#random-regression-imputation",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#random-regression-imputation",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "The most faithful single imputation method might be something like the following:\n\\[\n\\begin{aligned}\n\\bar{y}^\\mathrm{mean} & = \\sum_{x \\in \\mathcal{X}} (\\bar{y}_x^{\\mathrm{res}} p_x + \\tilde{y}_x (1 - p_x)) \\frac{n_x}{n} \\\\\n\\tilde{y}_x \\mid x & \\overset{\\text{indy}}{\\sim} \\text{Normal}\\lp\\mu(x, \\hat{\\gamma}), \\frac{\\hat{\\sigma}^2_{\\mathrm{res}}(x)}{r_x}\\rp\n\\end{aligned}\n\\]\nThis approach is shown in Figure 5. The purple short-dashed line is the regression obtained from the completed dataset. We can see that despite the added uncertainty in the imputation method, the resulting complete-data estimator is still biased compared to the true relationship. We’ll still be ignoring the uncertainty that arises from the fact that we could have generated many different values for \\(\\tilde{y}_x\\) versus the values that we used for the estimator.\n\n\n\n\n\n\n\n\nFigure 5: Conditional mean imputation"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#aipwcc-z-estimators",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#aipwcc-z-estimators",
    "title": "Missing data lecture 2",
    "section": "AIPWCC-Z-estimators",
    "text": "AIPWCC-Z-estimators\nThe same idea can be used for Z-estimators; \\(\\hat{\\theta}^\\mathrm{AIPWCC}\\) solves the following systems of equations: \\[\n\\frac{1}{n} \\sum_{i=1}^n \\frac{1 - m_i}{\\pi(x_i, \\hat{\\beta})}\\psi(\\theta, y_i, x_i) -\n\\lp\\frac{(1 - m_i) - \\pi(x_i, \\hat{\\beta})}{\\pi(x_i, \\hat{\\beta})}\\rp\\ExpA{\\psi(y_i,x_i, \\theta) \\mid X_i = x_i}{Y_i \\mid X_i}\n= 0\n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#last-observation-carried-forward-imputation-locf",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#last-observation-carried-forward-imputation-locf",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "Figure 6: Conditional mean imputation"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-3-notes.html#implicit-imputation-models",
    "href": "missing-data-material-W-26/notes/lecture-3-notes.html#implicit-imputation-models",
    "title": "Missing data lecture 3",
    "section": "",
    "text": "There are still more single imputation models.\n\n\nSuppose we consider \\(Y_1\\) the value of the first measurement in a longitudinal study with \\(2\\) measurement occasions. A popular method of single imputation in longitudinal analyses is last-observation-carried-forward, or LOCF for short. This is shown below:\n\n\n\n\n\n\n\n\nFigure 6: LOCF imputation\n\n\n\n\n\nWe have the same problem with conditional mean imputation, namely the lack of randomness in our imputations.\n\n\n\nHot deck imputation is something that is employed by the Census Bureau to handle item nonresponse on surveys. The simplest implementation is described as follows. Let there be \\(n\\) observations on \\(y_i\\), a univariate measurement, of which \\(n-r\\) are missing. The hot deck estimator is defined as follows: \\[\n\\begin{aligned}\n\\bar{y}^{\\mathrm{HD}} & = \\frac{1}{n} \\lp r \\bar{y}_{(0)} + (n-r) \\bar{y}_{(1)} \\rp \\\\\n\\bar{y}_{(1)} \\mid y_{(0)} & = \\frac{1}{n-r}\\sum_{i=1}^r h_i y_{(0)i} \\\\\n(H_1, \\dots, H_r) & \\sim \\text{Multinomial}((n-r) \\mid (1/r, \\dots, 1/r))\n\\end{aligned}\n\\] The units included in the fully observed data are called “donor” units because the set provides the values that can replace the missing values.\nWe can get the mean and variance of these estimators by using the law of total expectation and total variance:\n\\[\n\\begin{aligned}\n\\Exp{\\bar{y}^{\\mathrm{HD}}} & = \\ExpA{\\ExpA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}} \\\\\n\\Var{\\bar{y}^{\\mathrm{HD}}} & = \\VarA{\\ExpA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}} + \\ExpA{\\VarA{\\bar{y}^{\\mathrm{HD}}}{(H_1, \\dots, H_r) \\mid Y_{(0)}}}{Y_{(0)}}\n\\end{aligned}\n\\] Example 4.8 in Little and Rubin (2019) walks through the derivation of these expressions.\n\n\n\nGiven that the prior example is only useful when data are MCAR (as our book mentions), one can modify the donor units for each observation such that we only choose donors from a pool that is “close” to the unit with a missing value in terms of other completely observed values. For a unit for which \\(y_i\\) is missing but for which we have fully observed covariates \\(x_{i} = x_{i1}, \\dots x_{iK}\\), we would choose a donor unit with observed \\(y_j\\) and \\(x_j = x_{j1}, \\dots x_{jK}\\) such that some distance metric, \\(d(x_i, x_j)\\) is below a threshold value, \\(d_0\\).\n\n\n\nIn predictive mean matching, we fit a regression model on the completely observed units. Then we compute the predicted value for \\(y_i\\), or \\(\\hat{y}(x_i)\\) using \\(x_i\\) and pick donors for which \\((\\hat{y}(x_i) - \\hat{y}(x_j))^2\\) are below \\(d_0\\)."
  },
  {
    "objectID": "survival-material/lecture-5.html",
    "href": "survival-material/lecture-5.html",
    "title": "Lecture 5",
    "section": "",
    "text": "We had assumed that no two events could occur at the same time, but for most real datasets this isn’t realistic. A distinction must be made between a) assuming that ties are present in the data because, despite the true events happening in continuous time and thus no two events exactly coincide, the data have been rounded such that this exact ordering of events is lost, or b) that the true events happen in discrete time, and so there are truly events that co-occur.\nIn the continuous time scenario, (Aalen, Borgan, and Gjessing 2008) suggests using a modified estimator for hazard at time \\(t_i\\) when there are multiple \\(\\delta_i = 1\\). Let \\(d_i\\) be the number of events observed at time \\(t_i\\). Then the proposed estimator for \\(\\hat{\\lambda}(t_i)\\) is: \\[\\begin{align}\n    \\hat{\\lambda}(t_i) = \\sum_{j=0}^{d_i - 1} \\frac{1}{\\widebar{Y}(t_i) - j}\n\\end{align} \\tag{1}\\]\nIn discrete time the proposal is to use: \\[\\begin{align}\n    \\hat{\\lambda}(t_i) = \\frac{d_i}{\\widebar{Y}(t_i)}\n\\end{align} \\tag{2}\\]\n\n\nIt turns out, after some algebra, that using either Equation 1 or Equation 2 results in the following tie-corrected estimator for the KM estimator: \\[\\begin{align}\n    \\hat{S}^\\text{KM}(t) = \\prod_{i \\mid d_i \\geq 1, t_i \\leq t} (1 - \\frac{d_i}{\\widebar{Y}(t_i)})\n\\end{align}\\]\nGreenwood’s formula is then \\[\\Var{S^{\\text{KM}}(t)} = (S^{\\text{KM}}(t))^2 \\sum_{i \\mid d_i \\geq 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}.\\] This is the more commonly known form."
  },
  {
    "objectID": "survival-material/lecture-5.html#handling-ties-in-the-nelson-aalen-estimator",
    "href": "survival-material/lecture-5.html#handling-ties-in-the-nelson-aalen-estimator",
    "title": "Lecture 5",
    "section": "",
    "text": "We had assumed that no two events could occur at the same time, but for most real datasets this isn’t realistic. A distinction must be made between a) assuming that ties are present in the data because, despite the true events happening in continuous time and thus no two events exactly coincide, the data have been rounded such that this exact ordering of events is lost, or b) that the true events happen in discrete time, and so there are truly events that co-occur.\nIn the continuous time scenario, (Aalen, Borgan, and Gjessing 2008) suggests using a modified estimator for hazard at time \\(t_i\\) when there are multiple \\(\\delta_i = 1\\). Let \\(d_i\\) be the number of events observed at time \\(t_i\\). Then the proposed estimator for \\(\\hat{\\lambda}(t_i)\\) is: \\[\\begin{align}\n    \\hat{\\lambda}(t_i) = \\sum_{j=0}^{d_i - 1} \\frac{1}{\\widebar{Y}(t_i) - j}\n\\end{align} \\tag{1}\\]\nIn discrete time the proposal is to use: \\[\\begin{align}\n    \\hat{\\lambda}(t_i) = \\frac{d_i}{\\widebar{Y}(t_i)}\n\\end{align} \\tag{2}\\]"
  },
  {
    "objectID": "survival-material/lecture-5.html#handling-ties-in-the-kaplan-meier-estimator",
    "href": "survival-material/lecture-5.html#handling-ties-in-the-kaplan-meier-estimator",
    "title": "Lecture 5",
    "section": "",
    "text": "It turns out, after some algebra, that using either Equation 1 or Equation 2 results in the following tie-corrected estimator for the KM estimator: \\[\\begin{align}\n    \\hat{S}^\\text{KM}(t) = \\prod_{i \\mid d_i \\geq 1, t_i \\leq t} (1 - \\frac{d_i}{\\widebar{Y}(t_i)})\n\\end{align}\\]\nGreenwood’s formula is then \\[\\Var{S^{\\text{KM}}(t)} = (S^{\\text{KM}}(t))^2 \\sum_{i \\mid d_i \\geq 1, t_i \\leq t} \\frac{d_i}{\\widebar{Y}(t_i) (\\widebar{Y}(t_i) - d_i)}.\\] This is the more commonly known form."
  },
  {
    "objectID": "survival-material/weibull-mappings.html",
    "href": "survival-material/weibull-mappings.html",
    "title": "Weibull parameterizations",
    "section": "",
    "text": "Our course notes (and (Klein, Moeschberger, et al. 2003)) define the Weibull hazard as: \\[\\lambda(t) = \\gamma \\alpha t^{\\alpha - 1}\\] Base R defines the Weibull parameterization for rweibull(n, shape=\\(\\alpha\\), scale=\\(\\sigma\\)) as \\[\\lambda(t) = \\frac{\\alpha}{\\sigma} \\left(\\frac{t}{\\sigma}\\right)^{\\alpha - 1}\\] The survival package parameterizes the Weibull, with intercept=\\(\\mu\\), scale =\\(\\tau\\), as \\[\\lambda(t) = \\frac{1}{\\tau e^{\\mu/\\tau}} t^{1/\\tau - 1}\\] Thus, we can see that the following identities hold: \\[\\begin{align*}\n    \\gamma & = \\frac{1}{\\sigma^\\alpha} \\implies \\sigma = \\frac{1}{\\gamma^{1/\\alpha}} \\\\\n    \\gamma & = e^{-\\mu/\\tau} \\implies \\mu = -\\tau \\log(\\gamma)\n\\end{align*}\\] This also implies that regression coefficients from survreg are interpreted differently from the typical interpretation from a proportional hazards model. The proportional hazards Weibull model is typically written \\[\\gamma e^{\\boldsymbol{\\beta}^T\\mathbf{z}_i} \\alpha t^{\\alpha - 1}\\] But survreg parameterizes the model as \\[\\frac{1}{\\tau e^{(\\mu + \\boldsymbol{\\theta}^T \\mathbf{z}_i)/\\tau}} t^{1/\\tau - 1}\\] This means that: \\[\\begin{align*}\n\\boldsymbol{\\beta} & = -\\boldsymbol{\\theta} / \\tau \\\\\n\\gamma & = e^{-\\mu/\\tau}\n\\end{align*}\\] Thus, a positive coefficient in the parametric hazard which indicates that the variable increases hazard, all else being equal, will be negative in survreg’s coefficient results and vice versa.\n\n\n\n\nReferences\n\nKlein, John P, Melvin L Moeschberger, et al. 2003. Survival Analysis: Techniques for Censored and Truncated Data. Vol. 1230. Springer."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html",
    "href": "missing-data-material-W-26/homeworks/HW-1.html",
    "title": "HW 1",
    "section": "",
    "text": "In this question, you’ll be replicating some of the results of the Kang and Schafer (2007) paper and expanding on them. In the paper, the authors test the robustness of IPW, AIPW, and regression estimators via. a simulation study. The authors simulate a survey where an outcome \\(y_i\\) is measured for \\(n\\) individuals, but a substantial number of observations are missing (\\(\\sim 50\\%\\)). The target estimand is the population mean of \\(y_i\\), which is simulated to be \\(210\\). The \\(y_i\\) are simulated via a linear model conditional on covariates \\(Z_{i1}, Z_{i2}, Z_{i3}, Z_{i4}\\); the missingness indicators are simulated via a logistic regression model, again using a linear model related to the \\(Z\\) predictors.\nThe catch is that it is assumed that the analyst has access only to complex transformations of the predictors, in the form of \\(X_{i1}, X_{i2}, X_{i3}, X_{i4}\\). These transformed predictors can be used in a linear model to produce fits that are “good enough” meaning that they predict the outcomes pretty well and don’t indicate poor fits to the data.\nThe operating characteristics, namely bias, variance and MSE are measured for several different estimators that involve linear models and logistic regressions for \\(y_i \\mid x_i\\) and \\(m_i \\mid x_i\\). Let \\(e(x_i, \\hat{\\gamma})\\) be the fitted probabilities of not being missing, or \\(P(M_i = 0 \\mid X_i = x_i)\\) from the missingness model and let \\(\\mu(x_i, \\hat{\\beta})\\) be the fitted values from a linear regression of \\(y_i\\) on \\(x_i\\).\n\nAn IPW estimator: \\[\n\\bar{y}^{\\text{IPW}} = \\frac{\\sum_{i \\mid m_i = 0} y_i e(x_i, \\hat{\\gamma})^{-1}}{\\sum_{i \\mid m_i = 0} e(x_i, \\hat{\\gamma})^{-1}}\n\\]\nA regression estimator: \\(\\bar{y}^{\\text{reg}} = \\frac{1}{n} \\sum_{i=1}^n \\mu(x_i, \\hat{\\beta})\\)\nAn AIPW estimator: \\(\\bar{y}^{\\text{AIPW}} =  \\bar{y}^{\\text{reg}} +  \\frac{\\sum_{i \\mid m_i = 0}\\hat{\\epsilon}_i e(x_i, \\hat{\\gamma})^{-1}}{\\sum_{i \\mid m_i = 0} e(x_i, \\hat{\\gamma})^{-1}}\\)\n\nThe \\(\\hat{\\epsilon}_i\\) is the fitted residual from the regression on \\(i\\) such that \\(m_i = 0\\).\nI want you to roughly recreate the results of part of the paper by running the smallest simulation study scenario. I’ve written the simulation function for you, you have to write the code to run the regressions and the code to compute the estimators.\n\ngen_data &lt;- function(n=200) {\n  Z &lt;- matrix(rnorm(n * 4), n, 4)\n  y_hat &lt;- 210 + 27.4 * Z[,1] + 13.7 * rowSums(Z[,2:4])\n  y &lt;- y_hat + rnorm(n)\n  pi_y &lt;- plogis(-Z[,1] + 0.5 * Z[,2] - 0.25 * Z[,3] - 0.1 * Z[,4])\n  m &lt;- rbinom(n, 1, 1 - pi_y)\n  X &lt;- Z\n  X[,1] &lt;- exp(Z[,1] / 2)\n  X[,2] &lt;- Z[,2] / (1 + exp(Z[,1])) + 10\n  X[,3] &lt;- (Z[,1] * Z[,3] / 25 + 0.6)^3\n  X[,4] &lt;- (Z[,2] + Z[,4] + 20)^2\n  y_o &lt;- y\n  y_o[m == 1] &lt;- NA\n  df &lt;- data.frame(\n    y = y_o,\n    m = m,\n    x1 = X[,1],\n    x2 = X[,2],\n    x3 = X[,3],\n    x4 = X[,4]\n  )\n  return(df)\n}\n\n\n\nWrite functions that can take the data frame returned by gen_data and return a linear model for \\(\\Exp{Y_i \\mid X_i}\\) and a logistic regression for \\(P(M_i = 0 \\mid X_i)\\).\n\n\n\nWrite functions that compute \\(\\bar{y}^{\\text{IPW}},\\bar{y}^{\\text{AIPW}},\\bar{y}^{\\text{reg}}\\). These functions should take a model fit and a datatset as arguments to return the estimators.\n\n\n\nGenerate 1000 simulated datasets and use the functions you wrote above to compute the three estimators for each dataset\n\n\n\nCompute the bias, root mean square error, and % bias as a function of standard deviation of the estimator.\n\n\n\nPick your favorite machine learning method (random forests, support vector machines, gradient boosting machines, xgboost, etc.) and use your method to compute nonparametric versions of \\(\\mu(x_i, \\beta)\\) and \\(e(x_i, \\gamma)\\). Recompute the estimators using your machine learning methods and compute the same summary statistics as you did in question 1d\n\n\n\nWrite several sentences summarizing your results"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1a",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1a",
    "title": "HW 1",
    "section": "",
    "text": "Write functions that can take the data frame returned by gen_data and return a linear model for \\(\\Exp{Y_i \\mid X_i}\\) and a logistic regression for \\(P(M_i = 0 \\mid X_i)\\)."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1b",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1b",
    "title": "HW 1",
    "section": "",
    "text": "Write functions that compute \\(\\bar{y}^{\\text{IPW}},\\bar{y}^{\\text{AIPW}},\\bar{y}^{\\text{reg}}\\). These functions should take a model fit and a datatset as arguments to return the estimators."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1c",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1c",
    "title": "HW 1",
    "section": "",
    "text": "Generate 1000 simulated datasets and use the functions you wrote above to compute the three estimators for each dataset"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1d",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1d",
    "title": "HW 1",
    "section": "",
    "text": "Compute the bias, root mean square error, and % bias as a function of standard deviation of the estimator."
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1e",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1e",
    "title": "HW 1",
    "section": "",
    "text": "Pick your favorite machine learning method (random forests, support vector machines, gradient boosting machines, xgboost, etc.) and use your method to compute nonparametric versions of \\(\\mu(x_i, \\beta)\\) and \\(e(x_i, \\gamma)\\). Recompute the estimators using your machine learning methods and compute the same summary statistics as you did in question 1d"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-1f",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-1f",
    "title": "HW 1",
    "section": "",
    "text": "Write several sentences summarizing your results"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-3a",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-3a",
    "title": "HW 1",
    "section": "2.1 Question 3a",
    "text": "2.1 Question 3a\nSummarize the missingness patterns in the hw_data dataset. How many missingness patterns are there? What are they? What proportion of patients are associated with each missingness pattern?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-3b",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-3b",
    "title": "HW 1",
    "section": "2.2 Question 3b",
    "text": "2.2 Question 3b\nHow would you assess whether there is evidence that treatment affects missingness? Is there evidence that treatment affects the missingness pattern?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-3c",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-3c",
    "title": "HW 1",
    "section": "2.3 Question 3c",
    "text": "2.3 Question 3c\nHow many people dropped out of the study after Week 1, or Week 4 vs. had intermittent missingness? For patients who dropped out, is there evidence that PANSS at the prior measurement predicted dropout? What about for the patients with intermittent missingness?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-3d",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-3d",
    "title": "HW 1",
    "section": "2.4 Question 3d",
    "text": "2.4 Question 3d\nIs it reasonable to assume that missingness is MCAR for this dataset? Why or why not? What about MAR?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-2a",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-2a",
    "title": "HW 1",
    "section": "2.1 Question 2a",
    "text": "2.1 Question 2a\nSummarize the missingness patterns in the hw_data dataset. How many missingness patterns are there? What are they? What proportion of patients are associated with each missingness pattern?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-2b",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-2b",
    "title": "HW 1",
    "section": "2.2 Question 2b",
    "text": "2.2 Question 2b\nHow would you assess whether there is evidence that treatment affects missingness? Is there evidence that treatment affects the missingness pattern?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-2c",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-2c",
    "title": "HW 1",
    "section": "2.3 Question 2c",
    "text": "2.3 Question 2c\nHow many people dropped out of the study after Week 1, or Week 4 vs. had intermittent missingness? For patients who dropped out, is there evidence that PANSS at the prior measurement predicted dropout? What about for the patients with intermittent missingness?"
  },
  {
    "objectID": "missing-data-material-W-26/homeworks/HW-1.html#question-2d",
    "href": "missing-data-material-W-26/homeworks/HW-1.html#question-2d",
    "title": "HW 1",
    "section": "2.4 Question 2d",
    "text": "2.4 Question 2d\nIs it reasonable to assume that missingness is MCAR for this dataset? Why or why not? What about MAR?"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-4-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-4-notes.html",
    "title": "Missing data lecture 4",
    "section": "",
    "text": "This bit of the notes follows 6.1 pretty closely.\nFor the moment, let’s forget about missing data, and move to the simpler case where we have no missing observations and we’re just focused on learning the unknown parameters \\(\\theta\\) in the density \\(f_Y(y_i \\mid \\theta)\\). This unknown parameter is going to live in the space \\(\\Omega_\\theta\\). For example, if \\(\\theta = (\\mu, \\sigma^2)\\) and \\(f_Y(y_i \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2} (y_i - \\mu)^2}\\), then \\(\\Omega_\\theta\\) would be \\((\\R, \\R^+)\\).\nWe know densities are functions of \\(y_i\\) for fixed values of \\(\\theta\\). If we instead fix \\(y_i\\) and let \\(\\theta\\) vary, we get a likelihood function.\nLet the likelihood be defined for a fixed \\(y_i\\) as \\[\nL_Y(\\theta \\mid y_i) \\propto \\begin{cases}\nf(y_i \\mid \\theta) & \\theta \\in \\Omega_\\theta \\\\\n0 & \\theta \\not\\in \\Omega_\\theta\n\\end{cases}\n\\] This is a bit odd, because the expression means that anything proportional to the density of \\(Y\\) such that the factor by which \\(L_Y(\\theta \\mid y_i)\\) differs from \\(f(y_i \\mid \\theta)\\) is constant in \\(\\theta\\).\nTypically, we’ll have more than one observation, and under independence of \\(y_i\\) we get the likelihood for the full sample: \\[\nL_Y(\\theta \\mid y_1, \\dots, y_n) \\propto \\begin{cases}\n\\prod_{i=1}^n f(y_i \\mid \\theta) & \\theta \\in \\Omega_\\theta \\\\\n0 & \\theta \\not\\in \\Omega_\\theta\n\\end{cases}\n\\] We’ll often work with the log-likelihood, which is denoted in our book as: \\[\n\\ell_Y(\\theta \\mid y_1, \\dots, y_n) = \\log(L_Y(\\theta \\mid y_1, \\dots, y_n))\n\\] When we have independence, we get\n\\[\n\\ell_Y(\\theta \\mid y_1, \\dots, y_n) = \\sum_{i=1}^n \\log f(y_i \\mid \\theta) + C\n\\] where \\(C\\) is any term that doesn’t depend on \\(\\theta\\).\nMoving forward with the normal example from above, the likelihood of \\(n\\) observations from the normal distribution is: \\[\n\\begin{aligned}\nL_Y(\\mu, \\sigma^2 \\mid y_1, \\dots, y_n) & = \\prod_{i=1}^n  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2} (y_i - \\mu)^2} \\\\\n& = (2\\pi \\sigma^2)^{-\\frac{n}{2}} \\exp \\lp -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\\rp\n\\end{aligned}\n\\] Our definition of likelihood means that we can drop the factor of \\((2\\pi)^{-\\frac{n}{2}}\\) from the front of the expression. Taking logs and dropping that constant leads to the log-likelihood:\n\\[\n\\ell_Y(\\mu, \\sigma^2) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\n\\] which is a bivariate function of \\(\\mu\\) and \\(\\sigma^2\\).\nThe way that we’ll use the likelihood is to use the intuition that for two parameter values \\(\\theta^\\prime\\) and \\(\\theta^{\\prime\\prime}\\) if \\(L(\\theta^\\prime \\mid y) = 2L(\\theta^{\\prime\\prime} \\mid y)\\), then there is evidence against \\(\\theta^{\\prime\\prime}\\) being the parameter that generated the dataset.\nIf we were to find a \\(\\hat{\\theta}\\) such that \\(L_Y(\\hat{\\theta} \\mid y) \\geq L_Y(\\theta^* \\mid y)\\) for all \\(\\theta* \\neq \\hat{\\theta}\\), then this would be evidence against \\(\\theta\\) being anything other than \\(\\hat{theta}\\). This is the intuition behind maximum likelihood, or finding the value of the unknown parameters \\(\\theta\\) that maximizes the likelihood function (or, equivalently, the log-likelihood).\nWe’ll say that the maximum likelihood estimator (MLE) of \\(\\theta\\) is the value \\(\\hat{\\theta} \\in \\Omega_\\theta\\) that maximizes the log-likelihood.\nIn order to maximize the likelihood, we need to find the point at which the gradient of the log-likelihood, also known as the score function, is zero, or \\(\\frac{\\partial\\ell_Y(\\theta \\mid y)}{\\partial \\theta}\\mid_{\\theta = \\hat{\\theta}} = 0\\). If \\(\\theta\\) is \\(d\\)-dimensional, then there are \\(d\\) equations that need to be solved. In addition, the Hessian of the log-likelihood needs to be checked to see if it is negative semidefinite at the MLE: \\[\nz^T \\frac{\\partial^2 \\ell_Y(\\theta \\mid y)}{\\partial \\theta \\,\\partial \\theta^T}\\mid_{\\theta = \\hat{\\theta}} z \\leq 0 \\,\\forall\\, z \\in \\R^d\n\\] This ensures that we’ve found a maximum. Note that we can have several maxima, all of which lead to the same log-likelihood value.\n\n\nLet’s say that \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Exp}(\\lambda)\\). Then the likelihood will be \\[\nL(\\lambda \\mid y_1, \\dots, y_n) = \\lambda^{-n} e^{-\\frac{1}{\\lambda}\\sum_i y_i}\n\\] Then the score equation is:\n\\[\n-\\frac{n}{\\lambda} + \\sum_i \\frac{y_i}{\\lambda^2} = 0\n\\] This leads to \\(\\hat{\\lambda} = \\bar{y}\\) or the sample mean.\n\n\n\nSuppose \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\sigma^2)\\) for \\(i = 1, \\dots, n\\).\nWe wrote the log-likelihood above as: \\[\n\\ell_Y(\\mu, \\sigma^2) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\n\\]\nThis leads to two likelihood equations:\n\\[\n\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial \\mu } & = \\frac{1}{\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2 \\\\\n\\frac{\\partial \\ell}{\\partial \\sigma^2 } & =-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\textstyle\\sum_i(y_i - \\mu)^2\n\\end{aligned}\n\\] Setting these to zero and solving for \\((\\mu, \\sigma^2)\\) gives the MLEs \\(\\hat{\\mu} = \\bar{y}\\) and \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_i (y_i - \\bar{y})^2\\).\nIt’s straightforward to show that \\(\\Exp{\\hat{\\mu}}\\) under the data generating process above is equal to \\(\\mu\\). It is less straightforward and somewhat distressing that \\(\\Exp{\\hat{\\sigma^2}}\\) not equal to \\(\\sigma^2\\).\n\\[\n\\begin{aligned}\n\\Exp{\\hat{\\sigma^2}} & = \\frac{1}{n}\\Exp{\\sum_i (y_i^2 - 2 y_i \\bar{y} + \\bar{y}^2)} \\\\\n& = \\frac{1}{n}\\lp \\Exp{\\textstyle\\sum_i y_i^2 - 2 n \\bar{y}^2 + n\\bar{y}^2 \\rp}\\\\\n& = \\frac{1}{n}\\textstyle\\sum_i \\Exp{y_i^2} - \\Exp{\\bar{y}^2} \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2}\\Exp{\\textstyle \\sum_i y_i^2 + 2 \\sum_{i&lt;j} y_i y_j} \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2}\\lp \\textstyle \\sum_i \\Exp{y_i^2} + 2 \\sum_{i&lt;j} \\Exp{y_i y_j}\\rp \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2} \\lp n (\\mu^2 + \\sigma^2) + n(n-1) \\mu^2\\rp \\\\\n& = \\mu^2 + \\sigma^2 - \\mu^2 - \\frac{1}{n}\\sigma^2 \\\\\n& = \\frac{n-1}{n} \\sigma^2\n\\end{aligned}\n\\] Sorta odd that MLEs can give us biased estimates, but, you might say, as \\(n\\to\\infty\\) all is fine and we recover \\(\\sigma^2\\). Indeed, you can show that the MLE for \\(\\sigma^2\\) is , which means that as \\(n\\to\\infty\\) \\(\\hat{\\sigma}^2 \\to \\sigma\\) in probability.\n\n\n\nIs this always the case, that the MLE is consistent? The answer is no, and the MLE can fail pretty spectacularly. Consider the following problem:\n\\[\ny_{i1}, y_{i2} \\sim \\text{Normal}(\\mu_i, \\sigma^2)\n\\] Let’s say that we don’t care about inferring \\(\\mu_i\\) but we care only about \\(\\sigma^2\\). One way we could come up with an estimator is to difference the two observations for each group, so \\(z_i = y_{i1} - y_{i2}\\) and : \\[\nz_i \\sim \\text{Normal}(0, 2\\sigma^2)\n\\] Then we could reuse our work from above to solve for \\(\\sigma^2\\):\n\\[\n\\begin{aligned}\n2 \\sigma^2 & = \\frac{1}{n} \\textstyle \\sum_i z_i^2 \\\\\n\\end{aligned}\n\\] which leads to an estimator for \\(\\sigma^2\\) of: \\[\n\\hat{\\sigma}^2 = \\frac{1}{2n} \\textstyle \\sum_i z_i^2\n\\]\nBut this isn’t quite maximum likelihood because we’ve transformed the data, and then used the transformed data to derive an MLE for \\(\\sigma^2\\) using \\(z_i\\). What if we just use \\(y_{i1},y_{i2}\\)?\nThe likelihood is straightforward:\n\\[\nL_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y) = \\prod_{i=1}^n \\frac{1}{2 \\pi \\sigma^2} \\exp \\lp -\\frac{1}{2\\sigma^2} \\lp (y_{i1} - \\mu_i)^2 + (y_{i2} - \\mu_i)^2 \\rp \\rp\n\\]\nThe likelihood equations for \\(\\mu_i\\) are \\[\n\\frac{\\partial \\ell_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y)}{\\partial \\mu_i} = \\frac{1}{\\sigma^2}\\lp (y_{i1} - \\mu_i) + (y_{i2} - \\mu_i) \\rp\n\\] Which just leads to the estimator \\(\\hat{\\mu}_i = \\frac{y_{i1} + y_{i2}}{2} = \\bar{y}_i\\).\nLet’s write out the likelihood equations for \\(\\sigma^2\\) after plugging in our \\(\\hat{\\mu}_i = \\bar{y}_i\\): \\[\n\\frac{\\partial \\ell_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y)}{\\partial \\sigma^2} = -\\frac{n}{\\sigma^2} + \\frac{1}{2 \\sigma^2}\\textstyle\\sum_i \\sum_{j=1}^2 (y_{ij} - \\bar{y}_i)^2\n\\] This leads to an MLE of\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{2n}\\textstyle\\sum_i \\sum_{j=1}^2 (y_{ij} - \\bar{y}_i)^2\n\\] Which seems reasonable enough. But let’s write the inner sum in terms of \\(z_i\\): \\[\n\\begin{aligned}\n(y_{i1} - \\bar{y}_i)^2 + (y_{i2} - \\bar{y}_i)^2 & = y_{i1}^2 + y_{i2}^2 + 2\\bar{y}_i^2 - 2 \\bar{y_i}(y_{i1} + y_{i2}) \\\\\n& = y_{i1}^2 + y_{i2}^2 + \\frac{(y_{i1} + y_{i2})^2}{2} - (y_{i1} + y_{i2})^2 \\\\\n& = y_{i1}^2 + y_{i2}^2 - \\frac{(y_{i1} + y_{i2})^2}{2} \\\\\n& = \\frac{1}{2}(y_{i1}^2 + y_{i2}^2 - 2 y_{i1}y_{i2}) \\\\\n& = \\frac{1}{2} z_i^2\n\\end{aligned}\n\\]\nThis means that the MLE is equal to \\[\n\\hat{\\sigma}^2 = \\frac{1}{4n}\\textstyle\\sum_i z_i^2\n\\] This estimator will never approach \\(\\sigma^2\\), as \\(n\\to\\infty\\).\nThis is due to the fact that the dimension of the parameter space grows linearly with the sample size as \\(n\\to\\infty\\).\nThe point is that MLEs (and any other sort of likelihood-based inference) can lead you astray if you’re not careful, and they are not a panacea."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-4-notes.html#likelihood-based-inference",
    "href": "missing-data-material-W-26/notes/lecture-4-notes.html#likelihood-based-inference",
    "title": "Missing data lecture 4",
    "section": "",
    "text": "This bit of the notes follows 6.1 pretty closely.\nFor the moment, let’s forget about missing data, and move to the simpler case where we have no missing observations and we’re just focused on learning the unknown parameters \\(\\theta\\) in the density \\(f_Y(y_i \\mid \\theta)\\). This unknown parameter is going to live in the space \\(\\Omega_\\theta\\). For example, if \\(\\theta = (\\mu, \\sigma^2)\\) and \\(f_Y(y_i \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2} (y_i - \\mu)^2}\\), then \\(\\Omega_\\theta\\) would be \\((\\R, \\R^+)\\).\nWe know densities are functions of \\(y_i\\) for fixed values of \\(\\theta\\). If we instead fix \\(y_i\\) and let \\(\\theta\\) vary, we get a likelihood function.\nLet the likelihood be defined for a fixed \\(y_i\\) as \\[\nL_Y(\\theta \\mid y_i) \\propto \\begin{cases}\nf(y_i \\mid \\theta) & \\theta \\in \\Omega_\\theta \\\\\n0 & \\theta \\not\\in \\Omega_\\theta\n\\end{cases}\n\\] This is a bit odd, because the expression means that anything proportional to the density of \\(Y\\) such that the factor by which \\(L_Y(\\theta \\mid y_i)\\) differs from \\(f(y_i \\mid \\theta)\\) is constant in \\(\\theta\\).\nTypically, we’ll have more than one observation, and under independence of \\(y_i\\) we get the likelihood for the full sample: \\[\nL_Y(\\theta \\mid y_1, \\dots, y_n) \\propto \\begin{cases}\n\\prod_{i=1}^n f(y_i \\mid \\theta) & \\theta \\in \\Omega_\\theta \\\\\n0 & \\theta \\not\\in \\Omega_\\theta\n\\end{cases}\n\\] We’ll often work with the log-likelihood, which is denoted in our book as: \\[\n\\ell_Y(\\theta \\mid y_1, \\dots, y_n) = \\log(L_Y(\\theta \\mid y_1, \\dots, y_n))\n\\] When we have independence, we get\n\\[\n\\ell_Y(\\theta \\mid y_1, \\dots, y_n) = \\sum_{i=1}^n \\log f(y_i \\mid \\theta) + C\n\\] where \\(C\\) is any term that doesn’t depend on \\(\\theta\\).\nMoving forward with the normal example from above, the likelihood of \\(n\\) observations from the normal distribution is: \\[\n\\begin{aligned}\nL_Y(\\mu, \\sigma^2 \\mid y_1, \\dots, y_n) & = \\prod_{i=1}^n  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2} (y_i - \\mu)^2} \\\\\n& = (2\\pi \\sigma^2)^{-\\frac{n}{2}} \\exp \\lp -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\\rp\n\\end{aligned}\n\\] Our definition of likelihood means that we can drop the factor of \\((2\\pi)^{-\\frac{n}{2}}\\) from the front of the expression. Taking logs and dropping that constant leads to the log-likelihood:\n\\[\n\\ell_Y(\\mu, \\sigma^2) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\n\\] which is a bivariate function of \\(\\mu\\) and \\(\\sigma^2\\).\nThe way that we’ll use the likelihood is to use the intuition that for two parameter values \\(\\theta^\\prime\\) and \\(\\theta^{\\prime\\prime}\\) if \\(L(\\theta^\\prime \\mid y) = 2L(\\theta^{\\prime\\prime} \\mid y)\\), then there is evidence against \\(\\theta^{\\prime\\prime}\\) being the parameter that generated the dataset.\nIf we were to find a \\(\\hat{\\theta}\\) such that \\(L_Y(\\hat{\\theta} \\mid y) \\geq L_Y(\\theta^* \\mid y)\\) for all \\(\\theta* \\neq \\hat{\\theta}\\), then this would be evidence against \\(\\theta\\) being anything other than \\(\\hat{theta}\\). This is the intuition behind maximum likelihood, or finding the value of the unknown parameters \\(\\theta\\) that maximizes the likelihood function (or, equivalently, the log-likelihood).\nWe’ll say that the maximum likelihood estimator (MLE) of \\(\\theta\\) is the value \\(\\hat{\\theta} \\in \\Omega_\\theta\\) that maximizes the log-likelihood.\nIn order to maximize the likelihood, we need to find the point at which the gradient of the log-likelihood, also known as the score function, is zero, or \\(\\frac{\\partial\\ell_Y(\\theta \\mid y)}{\\partial \\theta}\\mid_{\\theta = \\hat{\\theta}} = 0\\). If \\(\\theta\\) is \\(d\\)-dimensional, then there are \\(d\\) equations that need to be solved. In addition, the Hessian of the log-likelihood needs to be checked to see if it is negative semidefinite at the MLE: \\[\nz^T \\frac{\\partial^2 \\ell_Y(\\theta \\mid y)}{\\partial \\theta \\,\\partial \\theta^T}\\mid_{\\theta = \\hat{\\theta}} z \\leq 0 \\,\\forall\\, z \\in \\R^d\n\\] This ensures that we’ve found a maximum. Note that we can have several maxima, all of which lead to the same log-likelihood value.\n\n\nLet’s say that \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Exp}(\\lambda)\\). Then the likelihood will be \\[\nL(\\lambda \\mid y_1, \\dots, y_n) = \\lambda^{-n} e^{-\\frac{1}{\\lambda}\\sum_i y_i}\n\\] Then the score equation is:\n\\[\n-\\frac{n}{\\lambda} + \\sum_i \\frac{y_i}{\\lambda^2} = 0\n\\] This leads to \\(\\hat{\\lambda} = \\bar{y}\\) or the sample mean.\n\n\n\nSuppose \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\sigma^2)\\) for \\(i = 1, \\dots, n\\).\nWe wrote the log-likelihood above as: \\[\n\\ell_Y(\\mu, \\sigma^2) = -\\frac{n}{2} \\log \\sigma^2 -\\frac{1}{2\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2\n\\]\nThis leads to two likelihood equations:\n\\[\n\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial \\mu } & = \\frac{1}{\\sigma^2} \\textstyle\\sum_i(y_i - \\mu)^2 \\\\\n\\frac{\\partial \\ell}{\\partial \\sigma^2 } & =-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\textstyle\\sum_i(y_i - \\mu)^2\n\\end{aligned}\n\\] Setting these to zero and solving for \\((\\mu, \\sigma^2)\\) gives the MLEs \\(\\hat{\\mu} = \\bar{y}\\) and \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_i (y_i - \\bar{y})^2\\).\nIt’s straightforward to show that \\(\\Exp{\\hat{\\mu}}\\) under the data generating process above is equal to \\(\\mu\\). It is less straightforward and somewhat distressing that \\(\\Exp{\\hat{\\sigma^2}}\\) not equal to \\(\\sigma^2\\).\n\\[\n\\begin{aligned}\n\\Exp{\\hat{\\sigma^2}} & = \\frac{1}{n}\\Exp{\\sum_i (y_i^2 - 2 y_i \\bar{y} + \\bar{y}^2)} \\\\\n& = \\frac{1}{n}\\lp \\Exp{\\textstyle\\sum_i y_i^2 - 2 n \\bar{y}^2 + n\\bar{y}^2 \\rp}\\\\\n& = \\frac{1}{n}\\textstyle\\sum_i \\Exp{y_i^2} - \\Exp{\\bar{y}^2} \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2}\\Exp{\\textstyle \\sum_i y_i^2 + 2 \\sum_{i&lt;j} y_i y_j} \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2}\\lp \\textstyle \\sum_i \\Exp{y_i^2} + 2 \\sum_{i&lt;j} \\Exp{y_i y_j}\\rp \\\\\n& = \\mu^2 + \\sigma^2 - \\frac{1}{n^2} \\lp n (\\mu^2 + \\sigma^2) + n(n-1) \\mu^2\\rp \\\\\n& = \\mu^2 + \\sigma^2 - \\mu^2 - \\frac{1}{n}\\sigma^2 \\\\\n& = \\frac{n-1}{n} \\sigma^2\n\\end{aligned}\n\\] Sorta odd that MLEs can give us biased estimates, but, you might say, as \\(n\\to\\infty\\) all is fine and we recover \\(\\sigma^2\\). Indeed, you can show that the MLE for \\(\\sigma^2\\) is , which means that as \\(n\\to\\infty\\) \\(\\hat{\\sigma}^2 \\to \\sigma\\) in probability.\n\n\n\nIs this always the case, that the MLE is consistent? The answer is no, and the MLE can fail pretty spectacularly. Consider the following problem:\n\\[\ny_{i1}, y_{i2} \\sim \\text{Normal}(\\mu_i, \\sigma^2)\n\\] Let’s say that we don’t care about inferring \\(\\mu_i\\) but we care only about \\(\\sigma^2\\). One way we could come up with an estimator is to difference the two observations for each group, so \\(z_i = y_{i1} - y_{i2}\\) and : \\[\nz_i \\sim \\text{Normal}(0, 2\\sigma^2)\n\\] Then we could reuse our work from above to solve for \\(\\sigma^2\\):\n\\[\n\\begin{aligned}\n2 \\sigma^2 & = \\frac{1}{n} \\textstyle \\sum_i z_i^2 \\\\\n\\end{aligned}\n\\] which leads to an estimator for \\(\\sigma^2\\) of: \\[\n\\hat{\\sigma}^2 = \\frac{1}{2n} \\textstyle \\sum_i z_i^2\n\\]\nBut this isn’t quite maximum likelihood because we’ve transformed the data, and then used the transformed data to derive an MLE for \\(\\sigma^2\\) using \\(z_i\\). What if we just use \\(y_{i1},y_{i2}\\)?\nThe likelihood is straightforward:\n\\[\nL_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y) = \\prod_{i=1}^n \\frac{1}{2 \\pi \\sigma^2} \\exp \\lp -\\frac{1}{2\\sigma^2} \\lp (y_{i1} - \\mu_i)^2 + (y_{i2} - \\mu_i)^2 \\rp \\rp\n\\]\nThe likelihood equations for \\(\\mu_i\\) are \\[\n\\frac{\\partial \\ell_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y)}{\\partial \\mu_i} = \\frac{1}{\\sigma^2}\\lp (y_{i1} - \\mu_i) + (y_{i2} - \\mu_i) \\rp\n\\] Which just leads to the estimator \\(\\hat{\\mu}_i = \\frac{y_{i1} + y_{i2}}{2} = \\bar{y}_i\\).\nLet’s write out the likelihood equations for \\(\\sigma^2\\) after plugging in our \\(\\hat{\\mu}_i = \\bar{y}_i\\): \\[\n\\frac{\\partial \\ell_Y(\\{\\mu_i\\}, \\sigma^2 \\mid y)}{\\partial \\sigma^2} = -\\frac{n}{\\sigma^2} + \\frac{1}{2 \\sigma^2}\\textstyle\\sum_i \\sum_{j=1}^2 (y_{ij} - \\bar{y}_i)^2\n\\] This leads to an MLE of\n\\[\n\\hat{\\sigma}^2 = \\frac{1}{2n}\\textstyle\\sum_i \\sum_{j=1}^2 (y_{ij} - \\bar{y}_i)^2\n\\] Which seems reasonable enough. But let’s write the inner sum in terms of \\(z_i\\): \\[\n\\begin{aligned}\n(y_{i1} - \\bar{y}_i)^2 + (y_{i2} - \\bar{y}_i)^2 & = y_{i1}^2 + y_{i2}^2 + 2\\bar{y}_i^2 - 2 \\bar{y_i}(y_{i1} + y_{i2}) \\\\\n& = y_{i1}^2 + y_{i2}^2 + \\frac{(y_{i1} + y_{i2})^2}{2} - (y_{i1} + y_{i2})^2 \\\\\n& = y_{i1}^2 + y_{i2}^2 - \\frac{(y_{i1} + y_{i2})^2}{2} \\\\\n& = \\frac{1}{2}(y_{i1}^2 + y_{i2}^2 - 2 y_{i1}y_{i2}) \\\\\n& = \\frac{1}{2} z_i^2\n\\end{aligned}\n\\]\nThis means that the MLE is equal to \\[\n\\hat{\\sigma}^2 = \\frac{1}{4n}\\textstyle\\sum_i z_i^2\n\\] This estimator will never approach \\(\\sigma^2\\), as \\(n\\to\\infty\\).\nThis is due to the fact that the dimension of the parameter space grows linearly with the sample size as \\(n\\to\\infty\\).\nThe point is that MLEs (and any other sort of likelihood-based inference) can lead you astray if you’re not careful, and they are not a panacea."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-5-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-5-notes.html",
    "title": "Missing data lecture 5",
    "section": "",
    "text": "Let \\(y_i \\in \\R^K\\), \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\Sigma)\\) for \\(n\\) samples so that the density for \\(y_i\\) is \\[\nf_{Y}(y_i \\mid \\mu, \\Sigma) = (2\\pi)^{-\\frac{K}{2}} (\\det\\Sigma)^{-\\frac{1}{2}} \\exp\\lp-\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu) \\rp\n\\]\nThe log-likelihood is: \\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = \\frac{1}{2} \\log \\det\\Sigma -\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu)\n\\]\nThe book gives the expressions for the MLEs of the mean and covariance matrix of the multivariate normal distribution without details. Going through the algebra can be useful for other more complicated problems. But in order to do so, we’ll need a slight change to how we’re used to thinking about partial differentiation. The following blurb on differentials is based on Magnus and Neudecker (2019)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-6-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-6-notes.html",
    "title": "Missing data lecture 6",
    "section": "",
    "text": "Last class we talked about MLEs for a simple repeated measure model:\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j\n\\end{aligned}\n\\] Let \\(y = (y_1^T, y_2^T, \\dots, y_n^T)^T\\) and let \\(X = (X_1^T, X_2^T, \\dots, X_n^T)^T\\), and let \\(\\epsilon = (\\epsilon_1^T, \\epsilon_2^T, \\dots, \\epsilon_n^T)^T\\). Then the model can be written: \\[\n\\begin{aligned}\ny \\mid X \\, & = X \\beta + \\epsilon \\\\\n\\epsilon & \\sim \\text{Normal}(0, I_n \\otimes \\Sigma)\n\\end{aligned}\n\\]\nWe showed that if \\(\\Sigma\\) were known, we could write the MLE for \\(\\beta\\) as: \\[\n\\hat{\\beta} = \\textstyle(\\sum_i X_i^T  \\Sigma^{-1} X)^{-1} (\\sum_i X_i \\Sigma^{-1} y_i)  \n\\] We can also show that the MLE for \\(\\Sigma\\) if \\(\\beta\\) were known is:\n\\[\n\\Sigma = \\frac{1}{n} \\sum_i (y_i - X_i \\beta) (y_i - X_i)^T\n\\] Combining these two fact together, we can iteratively maximize the MLE by doing:\n\\[\n\\begin{aligned}\n\\Sigma^{(t+1)} & = \\frac{1}{n} \\sum_i (y_i - X_i \\beta^{(t)}) (y_i - X_i \\beta)^T \\\\\n\\beta^{(t+1)} & = \\textstyle(\\sum_i X_i^T  (\\Sigma^{(t)})^{-1} X_i)^{-1} (\\sum_i X_i (\\Sigma^{(t)})^{-1} y_i)  \n\\end{aligned}\n\\] Which is similar to what the book has."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-6-notes.html#mles-in-repeated-measure-models",
    "href": "missing-data-material-W-26/notes/lecture-6-notes.html#mles-in-repeated-measure-models",
    "title": "Missing data lecture 6",
    "section": "",
    "text": "Last class we talked about MLEs for a simple repeated measure model:\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j\n\\end{aligned}\n\\] Let \\(y = (y_1^T, y_2^T, \\dots, y_n^T)^T\\) and let \\(X = (X_1^T, X_2^T, \\dots, X_n^T)^T\\), and let \\(\\epsilon = (\\epsilon_1^T, \\epsilon_2^T, \\dots, \\epsilon_n^T)^T\\). Then the model can be written: \\[\n\\begin{aligned}\ny \\mid X \\, & = X \\beta + \\epsilon \\\\\n\\epsilon & \\sim \\text{Normal}(0, I_n \\otimes \\Sigma)\n\\end{aligned}\n\\]\nWe showed that if \\(\\Sigma\\) were known, we could write the MLE for \\(\\beta\\) as: \\[\n\\hat{\\beta} = \\textstyle(\\sum_i X_i^T  \\Sigma^{-1} X)^{-1} (\\sum_i X_i \\Sigma^{-1} y_i)  \n\\] We can also show that the MLE for \\(\\Sigma\\) if \\(\\beta\\) were known is:\n\\[\n\\Sigma = \\frac{1}{n} \\sum_i (y_i - X_i \\beta) (y_i - X_i)^T\n\\] Combining these two fact together, we can iteratively maximize the MLE by doing:\n\\[\n\\begin{aligned}\n\\Sigma^{(t+1)} & = \\frac{1}{n} \\sum_i (y_i - X_i \\beta^{(t)}) (y_i - X_i \\beta)^T \\\\\n\\beta^{(t+1)} & = \\textstyle(\\sum_i X_i^T  (\\Sigma^{(t)})^{-1} X_i)^{-1} (\\sum_i X_i (\\Sigma^{(t)})^{-1} y_i)  \n\\end{aligned}\n\\] Which is similar to what the book has."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-6-notes.html#inference-for-mles",
    "href": "missing-data-material-W-26/notes/lecture-6-notes.html#inference-for-mles",
    "title": "Missing data lecture 6",
    "section": "Inference for MLEs",
    "text": "Inference for MLEs\nWe’ve shown that we can find a point in parameter space \\(\\hat{\\theta}\\) such that there is evidence against any other point \\(\\theta \\neq \\hat{\\theta}\\) being the parameter that generated the data under an assumed statistical model \\(f_{Y}(y_i \\mid \\theta)\\).\nHow do assess the uncertainty in this point estimate? One way to think about this is to consider the distribution of MLEs under different hypothetical datasets.\nIf we can characterize the distribution, we can build a confidence interval for \\(\\theta\\) that will contain the true value of \\(\\theta\\) for some prescribed proportion of our hypothetical datasets.\nConsider the model \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\sigma^2)\\), where \\(\\sigma^2\\) is known one-dimensional normal model. Then we know that \\(\\hat{\\mu} = \\bar{y}\\), and that its distribution is \\(\\bar{y} \\sim \\text{Normal}(\\mu, \\frac{\\sigma^2}{n})\\). Using this distribution, we can construct a statistic whose distribution doesn’t depend on any unknown parameters. This is also known as a pivotal quantity. \\[\n\\sqrt{n}(\\bar{y} - \\mu)/\\sigma \\sim \\text{Normal}(0, 1)\n\\] We know the cumulative distribution function for \\(N(0,1)\\), \\(\\Phi(x)\\) and we can use this distribution to build a confidence interval for \\(\\mu\\): \\[\nP(z_{\\alpha/2} &lt; \\sqrt{n}(\\bar{y} - \\mu)/\\sigma &lt; z_{1-\\alpha/2})\n\\] where \\(z_{p} = \\Phi^{-1}(p)\\) and \\(\\alpha\\) is usually \\(0.05\\). Now we solve the system of inequalities for \\(\\mu\\) to get our \\(1 - \\alpha\\) confidence interval:\n\\[\n\\begin{aligned}\nP(z_{\\alpha/2} &lt; \\sqrt{n}(\\bar{y} - \\mu)/\\sigma &lt; z_{1-\\alpha/2}) & = P(\\frac{\\sigma}{\\sqrt{n}} z_{\\alpha/2} &lt; \\bar{y} - \\mu  &lt; \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2}) \\\\\n& = P(\\frac{\\sigma}{\\sqrt{n}} z_{\\alpha/2} - \\bar{y} &lt; -\\mu  &lt; -\\bar{y} + \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2}) \\\\\n& = P(\\bar{y} - \\frac{\\sigma}{\\sqrt{n}} z_{\\alpha/2} &gt; \\mu  &gt; \\bar{y} - \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2}) \\\\\n\\end{aligned}\n\\] Because the distribution is symmetric, \\(z_{\\alpha/2} = -z_{1-\\alpha/2}\\) which gives us: \\[\nP(\\bar{y} - \\frac{\\sigma}{\\sqrt{n}} z_{1-\\alpha/2} &lt; \\mu  &lt; \\bar{y} + \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2})\n\\] Then the interval \\((\\bar{y} - \\frac{\\sigma}{\\sqrt{n}} z_{1-\\alpha/2},  \\bar{y} + \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2})\\) will contain \\(\\mu\\) in \\(0.95\\) of datasets generated under the assumed \\(N(\\mu, \\sigma^2)\\).\nFor all but the simplest models, the sampling distribution is intractable, but we can approximate the distribution using asymptotics.\nWe’ll need the multivariate central limit theorem, which we’ll just take as a given:\nWe can use this idea to get a pivotal quantity that involves the MLE for \\(\\theta\\) and the asymptotic variance covariance matrix of the MLE.\nLet the log-likelihood be denoted \\(\\ell(\\theta)\\), in which we suppress the dependence of the likelihood on the data. If we need to indicate the dependence on data, we’ll write it as \\(\\ell(\\theta; y)\\). Finally, denote the gradient of the log-likelihood with respect to \\(\\theta\\) evaluated at \\(\\theta^\\prime\\) as: \\[\n\\ell_{\\theta}(\\theta^\\prime; y) \\equiv \\nabla_\\theta \\log f_\\theta(y)\\mid_{\\theta = \\theta^\\prime}\n\\] and the Hessian of the log-likelihood (i.e. the matrix of second derivatives of the log-likelihood) be: \\[\n\\ell_{\\theta\\theta}(\\theta^\\prime; y) \\equiv \\nabla^2_\\theta \\log f_\\theta(y)\\mid_{\\theta = \\theta^\\prime}\n\\] Given that \\(\\theta \\in \\R^p\\), we’ll denote an element of the vector \\(\\ell_\\theta(\\theta^\\prime)\\) as \\((\\ell_\\theta(\\theta^\\prime))_j\\) and a row of \\(\\ell_{\\theta\\theta}(\\theta^\\prime; y)\\) as \\((\\ell_{\\theta\\theta}(\\theta^\\prime; y))_j\\).\nWe’ll start with some key assumptions:\n\nThe MLE is consistent for \\(\\theta\\), which means that as we collect more samples the MLE with converge in probability to \\(\\theta\\).\n\nThis will rule out the Neyman-Scott problem.\n\n\\(y_i\\) are iid with density \\(f_Y(y_i \\mid \\theta)\\), \\(\\theta \\subseteq \\R^d\\)\nThe support of the random variable \\(y_i\\) doesn’t depend on \\(\\theta\\).\n\nIn our earlier presentation of how things can go wrong with MLE, having support that depends on the value of the parameter can make things go awry, so we’ll assume that we’re not in that scenario (like the \\(y_i \\sim \\text{Uniform}(0, \\theta)\\)).\n\nThe true parameter \\(\\theta\\) is in the interior of the parameter space. This ensures that the gradient of the likelihood value at the maximizer \\(\\hat{\\theta}\\) will be zero.\nLocal boundedness of second derivatives for all \\(j\\): \\[\n\\sup_{\\norm{\\theta^\\prime - \\theta^\\dagger} \\leq r} \\norm{(\\ell_{\\theta\\theta}(\\theta^\\prime; x))_j - (\\ell_{\\theta\\theta}(\\theta^\\dagger; x))_j} \\leq H_r(x, \\theta^\\dagger),\\, \\lim_{r\\to 0}\\Exp{H_r(X, \\theta^\\dagger)} = 0\n\\]\n\nThe following proof is cobbled together from Schervish (2012) and Lehmann and Casella (1998).\nThe gradient of the log-likelihood evaluated at the MLE \\(\\hat{\\theta}\\) can be expanded around the true parameter value \\(\\theta^\\dagger\\): \\[\n(\\ell_\\theta(\\hat{\\theta}))_j = (\\ell_\\theta(\\theta^\\dagger))_j + (\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j(\\hat{\\theta} + \\theta)\n\\] where \\(\\tilde{\\theta}_j \\in \\R^p\\) and lies on the cord between \\(\\hat{\\theta}\\) and \\(\\theta^\\dagger\\) and may differ by the row \\(j\\) of \\(\\ell_\\theta(\\hat{\\theta})\\) of which we’re expanding.\nWe multiply each side by \\(\\frac{\\sqrt{n}}{n}\\):\n\\[\n\\frac{\\sqrt{n}}{n}(\\ell_\\theta(\\hat{\\theta}))_j = \\frac{1}{n}(\\ell_\\theta(\\theta^\\dagger))_j + (\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j\\sqrt{n}(\\hat{\\theta} + \\theta)\n\\]\nWe can show the following given assumption 5: \\[\n\\frac{1}{n}(\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j \\overset{p}{\\to}\\Exp{(\\ell_{\\theta\\theta}(\\theta^\\dagger; Y))_j} \\forall j\n\\] \\[\n\\begin{aligned}\n\\frac{1}{n}(\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j & = \\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j + \\frac{1}{n}\\lp(\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j - (\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\rp\n\\end{aligned}\n\\] In order to show that \\(\\Delta_{n,j} = \\frac{1}{n}(\\ell_{\\theta\\theta}(\\tilde{\\theta}_j))_j - (\\ell_{\\theta\\theta}(\\theta^\\dagger))_j \\overset{p}{\\to} 0\\), we need to show that \\(P(\\Delta_{n,j} &gt; \\epsilon) \\overset{n\\to\\infty}{\\to} 0\\).\n\\(\\Delta_{n,j} &gt; \\epsilon\\) when \\(\\hat{\\theta}_{n}\\) is not in \\(B_r(\\theta^\\dagger)\\), or the ball centered at \\(\\theta^\\dagger\\) of size \\(r\\) or when \\(\\frac{1}{n} \\sum_{i=1}^n H_r(x_i,\\theta^\\dagger) &gt; \\epsilon\\). If we pick \\(r\\) small enough, we can ensure \\(\\Exp{H_r(X, \\theta^\\dagger)} &lt; \\epsilon / 2\\).\nThen \\[\n\\begin{aligned}\n\\Prob{\\Delta_{n,j} &gt; \\epsilon}{\\theta^\\dagger} & \\leq \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r \\cup \\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) &gt; \\epsilon}{\\theta^\\dagger} \\\\\n& \\leq \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r}{\\theta^\\dagger} \\\\\n& + \\Prob{\\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) &gt; \\epsilon}{\\theta^\\dagger} \\\\\n& = \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r}{\\theta^\\dagger} \\\\\n& + \\Prob{\\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) - \\Exp{H_r(X_i, \\theta^\\dagger)} &gt; \\epsilon - \\Exp{H_r(X_i, \\theta^\\dagger)}}{\\theta^\\dagger} \\\\\n& \\leq \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r}{\\theta^\\dagger} \\\\\n& + \\Prob{\\abs{\\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) - \\Exp{H_r(X_i, \\theta^\\dagger)}} &gt; \\epsilon - \\Exp{H_r(X_i, \\theta^\\dagger)}}{\\theta^\\dagger} \\\\\n& \\leq \\Prob{\\norm{\\theta - \\hat{\\theta}_n} &gt; r}{\\theta^\\dagger} \\\\\n& + \\Prob{\\abs{\\frac{1}{n}\\sum_{i=1}^n H_r(X_i, \\theta^\\dagger) - \\Exp{H_r(X_i, \\theta^\\dagger)}} &gt; \\epsilon / 2}{\\theta^\\dagger} \\\\\n\\end{aligned}\n\\] By the weak law of large numbers, the second term goes to zero, and the first term goes to zero by consistency. This leaves \\(\\frac{1}{n}(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j\\) which \\(\\overset{p}{\\to} \\Exp{(\\ell_{\\theta\\theta}(\\theta^\\dagger))_j}\\) by the WLLN.\nCollecting our \\(p\\) equations into one set of equations yields: \\[\n\\sqrt{n}\\lp\\frac{1}{n} \\ell_\\theta(\\theta^\\dagger)\\rp = (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))} + o_p(1)) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\tag{1}\\]\nWriting out the expressions as explicit sums: \\[\\begin{align*}\n\\frac{\\sqrt{n}}{n}  \\sum_{i=1}^n \\ell_\\theta(\\theta^\\dagger; x_i) = (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))} + o_p(1)) \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\n\\end{align*} \\tag{2}\\]\nThe left-hand side of Equation 2 will be amenable to a multivariate version of the CLT. We’ll take the following multivariate CLT as given:\n\nTheorem 1. Multivariate CLT, (Keener 2010) Let \\(X_1, X_2, \\dots\\) be i.i.d random vectors in \\(\\R^k\\) with a common mean \\(\\Exp{X_i} = \\mu\\) and common covariance matrix \\(\\Sigma = \\Exp{(X_i - \\mu)(X_i - \\mu)^T}\\). If \\(\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\\), then \\[\\sqrt{n}(\\bar{X} - \\mu) \\overset{d}{\\to} \\text{Normal}(0, \\Sigma)\\]\n\nRecall that \\[\n\\Exp{\\ell_\\theta(\\theta; X_i)} = 0\n\\] By the multivariate central limit (MCLT) theorem, Equation 2 converges in distribution to a multivariate normal distribution with mean zero and covariance matrix \\(\\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}\\).\nWe’ll also need a lemma about the solutions to random linear equations:\n\nLemma 1 (Lemma 5.2 in (Lehmann and Casella 1998)) Suppose there are a set of \\(p\\) equations, \\(j = 1, \\dots, p\\): \\[\\sum_{k=1}^p A_{jkn} Y_{kn} = T_{jn}.\\] Let \\(T_{1n}, \\dots, T_{pn}\\) converge in distribution to \\(T_1, \\dots, T_p\\). Furthermore, suppose that for each \\(j,k\\), \\(A_{jkn} \\overset{p}{\\to} a_{jk}\\) such that the matrix \\(A\\) with \\((j,k)^{\\mathrm{th}}\\) element \\(a_{jk}\\) is nonsingular. Then if the distribution of \\(T_1, \\dots, T_p\\) has a disitribution with repsect to the Lebesgue measure over \\(\\R^p\\), \\(Y_{1n}, \\dots, Y_{pn}\\) tend in probability to \\(A^{-1} T\\).\nWritten in matrix form and using the fact that convergence in probability implies convergence in distribution: \\[\nT_n = A_n Y_n  \\implies Y_n \\overset{d}{\\to} A^{-1}T  \n\\]\n\nWe have that the left-hand side of Equation 1 converges in distribtuion to a multivariate normal distribution, and we have that the matrix on the RHS of Equation 1 convegens in probability to \\(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger))}\\), which by assumption is invertble. Thus by Lemma 1 \\(\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)\\) converges in probability to \\[\\begin{align*}\n\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\overset{p}{\\to} (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}^{1/2}\\mathcal{Z}\n\\end{align*}\\] where \\(\\mathcal{Z} \\sim \\text{Normal}(0, I_p)\\), or \\[\\begin{align*}\n&\\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger)  \\overset{d}{\\to} \\mathcal{N}\\left(0, (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1} \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}(-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\right)\n\\end{align*}\\]\nAssuming further that \\[\n\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)} + \\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T}=0 \\implies (-\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)})^{-1}\\Exp{\\ell_\\theta(\\theta;X_i)\\ell_\\theta(\\theta;X_i)^T} = I_p\n\\] Putting this all together shows that \\[\\begin{align*}\n     \\sqrt{n}(\\hat{\\theta}_n - \\theta^\\dagger) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger)^{-1})\n\\end{align*}\\] where \\(\\mathcal{I}(\\theta^\\dagger) = -\\Exp{\\ell_{\\theta\\theta}(\\theta^\\dagger; X_i)}\\)\nThen we can build an asymptotic confidence interval using the pivotal quantity strategy we had above:\n\\[\nP(\\bar{y} - \\frac{\\sigma}{\\sqrt{n}} z_{1-\\alpha/2} &lt; \\mu  &lt; \\bar{y} + \\frac{\\sigma}{\\sqrt{n}}z_{1-\\alpha/2})\n\\] But with \\(\\bar{y}\\) equaling \\(\\hat{\\theta}_1\\) and \\(\\sigma = \\sqrt{\\mathcal{I}(\\hat{\\theta})^{-1}_{1,1}}\\). It is sometimes hard to calculate the Fisher information because it involves taking expectations of the negative Hessian of the log-likelihood function. If we’d prefer, we can instead use an estimator for \\(\\mathcal{I}(\\hat{\\theta})\\) as\n\\[\n\\mathcal{I}(\\hat{\\theta}) = -\\frac{1}{n} \\sum_i \\ell_{\\theta\\theta}(\\hat{\\theta}; y_i)\n\\] The \\(\\mathcal{I}\\) should have a hat over it, but I can’t get the math to compile correctly when I add the \\hat over it.\nThere are ways to build multivariate confidence intervals, but I won’t go over those right now, though they are covered in the book in Chapter 6."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-6-notes.html#bayes",
    "href": "missing-data-material-W-26/notes/lecture-6-notes.html#bayes",
    "title": "Missing data lecture 6",
    "section": "Bayes",
    "text": "Bayes\nThe machinery for Frequentist inference often relies on asymptotic arguments for complex models. Bayesian inference, on the other hand, does not, and gives exact finite sample inference. There are caveats though, which we’ll cover.\nMLEs are concerned with finding a single point that, in some sense agrees with the dataset at hand, though our inference depends on hypothetical replications of the experiment that would generate alternative datasets. Bayesian inference requires that we characterize the distribution of parameters that agree with the dataset at hand.\nThe idea of Bayesian inference starts with Bayes rule. Given a prior distribution \\(p(\\theta)\\) we can combine that with the observational density of data \\(f_Y(y \\mid \\theta)\\) to give us an updated distribution \\(p(\\theta \\mid y)\\) given the dataset at hand: \\[\np(\\theta \\mid y) = \\frac{f_Y(y \\mid \\theta) p(\\theta)}{\\int_{\\Omega_\\theta}f_Y(y \\mid \\theta) p(\\theta) d \\theta}\n\\] The nice thing about Bayesian inference is that we get a distribution over \\(\\theta\\) given the dataset that we can now use to make probability statements about \\(\\theta\\). The statement With 0.95 probability \\(\\theta\\) is in the interval \\(C\\) is just a manipulation of the posterior density: \\(P(\\theta \\in C \\mid y) = \\int_C p(\\theta \\mid y) d\\theta\\).\nLet’s look at a specific example: the standard \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Bernuolli}(\\theta)\\) with \\(\\theta \\sim \\text{Beta}(\\alpha, \\beta)\\).\nThe likelihood is:\n\\[\n\\prod_i \\theta^{y_i}(1 - \\theta)^{1 - y_i} = \\theta^{\\sum_i y_i}(1 - \\theta)^{n - \\sum_i y_i}\n\\] The prior for \\(\\theta\\) will be:\n\\[\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}\n\\] The numerator the posterior is the product of these two expressions, where we let \\(s=\\sum_i y_i\\) for convenience:\n\\[\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{s + \\alpha - 1}(1 - \\theta)^{n - s + \\beta - 1}\n\\] Integrating over \\(\\theta\\) will give us the denominator of our expression:\n\\[\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + s)\\Gamma(\\beta + n - s)}{\\Gamma(\\alpha + \\beta + n)}\n\\] The ratio of the numerator and the denominator gives us: \\[\n\\frac{\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{s + \\alpha - 1}(1 - \\theta)^{n - s + \\beta - 1}}{\n\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\frac{\\Gamma(\\alpha + s)\\Gamma(\\beta + n - s)}{\\Gamma(\\alpha + \\beta + n)}\n}\n\\] which simplifies to: \\[\n\\frac{\\Gamma(\\alpha + \\beta + n)}{\\Gamma(\\alpha + s)\\Gamma(\\beta + n - s)}\\theta^{s + \\alpha - 1}(1 - \\theta)^{n - s + \\beta - 1}\n\\] This is just the Beta distribution with updated coefficients.\nThe prior mean is \\(\\frac{\\alpha}{\\alpha + \\beta}\\), while the posterior mean is \\(\\frac{s + \\alpha}{n + \\alpha + \\beta}\\). We can rewrite this to get a better understanding of the posterior mean represents in this circumstance: \\[\n\\begin{aligned}\n\\frac{s + \\alpha}{n + \\alpha + \\beta} = \\frac{\\alpha}{\\alpha + \\beta}\\frac{\\alpha + \\beta}{n + \\alpha + \\beta} + \\lp 1 - \\frac{\\alpha + \\beta}{n + \\alpha + \\beta}\\rp \\frac{s}{n}\n\\end{aligned}\n\\] This shows that the posterior mean is a weighted average of the prior mean and the data mean. This sort of exemplifies what we would hope for from Bayesian inference, some adjudication between the prior and the data. The posterior distribution is a Beta distribution, so we can get probability statements easily by using qbeta in R.\nWe didn’t have to go through all of the marginalization above. We could have noticed that the kernel of the posterior, namely the expression that depends on the unknown parmaeters, had a familiar form: \\[\np(\\theta \\mid s) \\propto \\theta^{s + \\alpha - 1}(1 - \\theta)^{n - s + \\beta - 1}\n\\] This is the kernel of the beta distribution, so we could have just stopped here and said that \\[\np(\\theta \\mid s) \\equiv \\text{Beta}(\\alpha + s, \\beta + n - s)\n\\] This procedure is aided by conjugate priors, which match the likelihood in a way; the functional form of the prior slots into the way the parameters are expressed in the likelihood to yield a family of posteriors that are in the same family as the prior.\nThese probabilities are strictly “right” under our prior assumption because of the math of Bayes’ theorem. Whether or not these are useful is another question.\nThere are some downsides to using a prior. MLEs have a nice property called invariance, namely that if \\(\\hat{\\theta}\\) is the MLE then the MLE for a function of \\(\\theta\\), say \\(g(\\theta)\\), is just \\(g(\\hat{\\theta})\\). This isn’t true in Bayesian inference generally, because we’re now dealing with distributions rather than points. So usually the posterior for \\(\\theta\\) won’t be the same as the posterior for \\(g(\\theta)\\): Let \\(\\eta = g(\\theta)\\), and assume for simplicity’s sake that \\(g\\) is one-to-one. Then \\(\\theta = g^{-1}(\\eta)\\). If \\(\\theta\\) has posterior \\(p(\\theta \\mid y)\\), the posterior for \\(g(\\theta)\\) is:\n\\[\np(g^{-1}(\\eta) \\mid y) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\] In fact, this is true of priors too! Given \\(p(\\theta)\\) and a transformation \\(\\eta = g(\\theta)\\) the prior for \\(\\eta\\) is: \\[\np(\\eta) = p(g^{-1}(\\eta)) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\] This is somewhat problematic if you think about how to represent ignorance. Let’s say you want to learn about a parameter \\(\\theta \\in [0,1]\\). The simplest prior for this is the flat prior \\(p(\\theta) \\propto 1\\). That implies that \\(\\theta\\) is equally likely to be anywhere in the interval of \\([0,1]\\). But what does that imply about \\(\\eta = \\theta^2\\)? Well on \\([0,1]\\), \\(g(x) =x^2\\) is one-to-one, and the inverse is \\(\\sqrt{g(\\eta)} = \\theta\\). The derivative of \\(\\sqrt{\\eta}\\) is proportional to \\(\\eta^{-1/2}\\), which implies a downward sloping distribution on \\([0,1]\\). But why would you have knowledge of \\(\\theta^2\\) without knowledge of \\(\\theta\\)? It seems counterintuitive.\nThe dyed-in-the-wool Bayesian would argue that there is no such thing as true ignorance, and that the problem that you face will have consequences for where you expect your parameter to lie. Suppose you’re modeling the proportion of Corvallis residents with synovial sarcoma, which is a very rare cancer. You’re probably going to use a prior that favors small values of \\(\\theta\\).\nWhat if instead you’re looking at the proportion of rainy days in Corvallis from November to April? You’d probably use a prior that at the very least avoided \\(\\theta\\) near 0."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-7-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-7-notes.html",
    "title": "Missing data lecture 7",
    "section": "",
    "text": "If \\(\\hat{\\theta}\\) is the MLE then the MLE for a function of \\(\\theta\\), say \\(g(\\theta)\\), is just \\(g(\\hat{\\theta})\\).\nBayesian (in)variance:\nLet \\(\\eta = g(\\theta)\\), and assume for simplicity’s sake that \\(g\\) is one-to-one. Then \\(\\theta = g^{-1}(\\eta)\\). If \\(\\theta\\) has posterior \\(p(\\theta \\mid y)\\), the posterior for \\(g(\\theta)\\) is:\n\\[\np(g^{-1}(\\eta) \\mid y) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\]\nThis can lead to contradictions under ``ignorance”.\nThis presentation follows Gelman et al. (2013) somewhat.\nThere are priors called Jeffreys’ priors (for Harold Jeffreys) that are invariant to reparameterizations. Remember that the Fisher information, or: \\[\n\\mathcal{I}(\\theta) = \\Exp{\\ell_\\theta(\\theta; Y)\\ell_\\theta(\\theta; Y)^T}\n\\] under a reparameterization \\(\\eta = g(\\theta)\\) with Jacobian \\((J_{\\eta,\\theta})_{ij} = \\frac{\\partial \\eta_i}{\\partial \\theta_j}\\) is: \\[\n\\mathcal{I}(\\theta(\\eta)) = J_{\\eta,\\theta}^T\\Exp{\\ell_\\theta(g^{-1}(\\eta); Y)\\ell_\\theta(g^{-1}(\\eta); Y)^T}J_{\\eta,\\theta}\n\\] For \\(\\eta = g(\\theta)\\), assume for simplicity that \\(g\\) is one-to-one, then a prior for \\(\\theta\\) that is proportional to the square root of the determinant of the Fisher information will be invariant to reparameterization:\n\\[\np(\\theta) \\propto \\det(\\mathcal{I}(\\theta))^{1/2}\n\\] Why is this the case? Because under the change of measure formula above the prior for \\(\\eta\\) is: \\[\np(\\eta) \\propto p(g^{-1}(\\eta)) \\det J_{\\eta,\\theta}\n\\] which is \\[\n\\begin{aligned}\np(\\eta) & \\propto \\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det J_{\\eta,\\theta} \\\\\n& \\propto \\det (J_{\\eta,\\theta})^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T)^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T\\mathcal{I}(g^{-1}(\\eta))^{1/2}J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det(\\mathcal{I}(\\eta))^{1/2}\n\\end{aligned}\n\\] Thus giving some sense of invariance under a coordinate change. As stated in Gelman et al. (2013), more or less:\n\nAny rule for determining the prior density \\(p(\\theta)\\) should yield an equivalent result if aplied to the transformed parameter; that is, \\(p(\\eta)\\) generated using \\(p(\\theta)\\) using the change of measure formula should yield the same prior as would have been obtained directly from the model \\(p(\\eta) p(y \\mid \\eta)\\)\n\nOne issue with Jeffreys’ prior is that it is dependent on a likelihood, which can be controversial.\nFor the Bernoulli trial example from last class, the Jeffreys prior is \\(\\text{Beta}(1/2, 1/2)\\).\n\n\n\nPosterior probabilities are strictly “right” under our prior assumption because of the math of Bayes’ theorem. However, if we take a Frequentist view of probability, namely that probabilities are defined as limiting proportions of events, we’ll need to think about alternative draws of our prior and of our data.\nThe coverage of our posterior credible intervals will only match the nominal probabilities if the prior we use for our analysis matches that which generated the data. We can show this as computing the marginal posterior \\(p(\\theta \\mid y)\\) under repeated draws from the prior and data distribution \\(p(y \\mid \\theta)\\), which is the distribution associated with the density \\(f_Y(y \\mid \\theta)\\) we’ll use in our posterior:\n\\[\n\\begin{aligned}\n\\theta^\\prime & \\sim p(\\theta) \\\\\ny & \\sim p(y \\mid \\theta^\\prime) \\\\\n\\theta & \\sim p(\\theta \\mid y)\n\\end{aligned}\n\\] Another way to represent this sampling diagram is through integrals:\n\\[\n\\begin{aligned}\n\\int_{\\Omega_\\theta}\\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime) dy \\, d\\theta^\\prime & = \\int_{\\mathcal{Y}} \\int_{\\Omega_\\theta}\\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} \\int_{\\Omega_\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} p(\\theta) f_Y(y \\mid \\theta) \\\\\n& = p(\\theta)\n\\end{aligned}\n\\]\nSee Talts et al. (2018) for more info about how we can use this identity to test whether our algorithms are working correctly.\n\n\n\nLike Frequentist confidence intervals, we can only compute \\(p(\\theta \\mid y)\\) exactly under special circumstances, like conjugate priors. The reason for this is that the integral in the denominator is usually intractable.\nWe will usually have to do approximate inference on Bayesian models by using Markov Chain Monte Carlo samplers, which iteratively generate samples that converge in distribution to the true posterior distribution. Bayesian approximate methods instead operate on an expression that is proportional to the posterior:\n\\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta) p(\\theta)\n\\]\nOne way to think about the MLE is that it is the posterior mode under a prior of \\(p(\\theta) \\propto 1\\): \\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta)\n\\] The difference between the likelihood \\(L_Y(\\theta \\mid y)\\) and the posterior \\(p(\\theta \\mid y)\\) lies in how we treat the expression. In MLE we’re going to maximize the likelihood. In Bayesian inference we care about the full distribution of \\(\\theta\\).\nThis gives some intuition about Bayesian inference. We can think of doing MLE and penalizing certain values of \\(\\theta\\):\n\\[\n\\ell_Y(\\theta \\mid y) + \\text{penalty}(\\theta)\n\\] that will allow the maximizer to favor certain values of \\(\\theta\\) over others.\nIf we look at the implied log-posterior ignoring the constant that doesn’t depend on \\(\\theta\\):\n\\[\n\\log p(\\theta \\mid y) = \\log f_Y(y \\mid \\theta) + \\log p(\\theta)\n\\]\nIf we maximize this expression we can rewrite this as \\[\n\\log p(\\theta \\mid y) = \\ell_Y(\\theta \\mid y) + \\log p(\\theta)\n\\] and we get the penalized likelihood expression where the penalty is a probability density.\nOne question might be: ok, we have a full distribution for \\(\\theta\\). What do we do with it? While the MLE is a single choice, we now have myriad choices for point estimates derived from Bayesian models. We could use the posterior mean: \\[\n\\Exp{\\theta \\mid y}\n\\] We could use the posterior median, \\(\\theta_m\\): \\[\nP(\\theta &gt; \\theta_m \\mid y) = P(\\theta \\leq \\theta_m \\mid y) = 1/2.\n\\]\nWe could use another posterior quantile. We could use the mode of the posterior as well.\nAsymptotically, one might hope that the Bayesian estimates converge to the Frequentist estimates, and this is true, though one needs to be careful in scenarios where the dimensionality of the parameter space increases with sample size and about how one uses priors.\nIn Frequentist inference, the only limits on the parameter space come from the likelihood; the normal density requires that \\(\\mu \\in \\R\\) and \\(\\sigma^2 \\in (0, \\infty)\\). In Bayesian inference, the prior can also restrict the parameter space. For example, in the normal example, one could use a prior for \\(\\mu\\) that enforced \\(\\mu &gt; 0\\). The posterior would then only be able to represent \\(\\mu &gt; 0\\). If the true \\(\\mu\\) were negative, a Bayesian point-estimator wouldn’t converge to the true \\(\\mu\\).\nWhile the prior adds an extra degree of freedom which seems dangerous, it can yield better estimates when there are small datasets, because there isn’t as much information in the data. An example of this would be a simple regression model: \\[\ny_i \\sim \\text{Normal}(X_i^T \\beta, \\sigma^2)\n\\] We might have some good information that we don’t expect \\(\\beta\\) to be nearly infinite, and in fact we expect it to be pretty well concentrated to \\([-10, 10]\\). Then we could use independent \\(\\text{Normal}(0,5^2)\\) priors for the regression coefficients."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-7-notes.html#bayes-recap",
    "href": "missing-data-material-W-26/notes/lecture-7-notes.html#bayes-recap",
    "title": "Missing data lecture 7",
    "section": "",
    "text": "If \\(\\hat{\\theta}\\) is the MLE then the MLE for a function of \\(\\theta\\), say \\(g(\\theta)\\), is just \\(g(\\hat{\\theta})\\).\nBayesian (in)variance:\nLet \\(\\eta = g(\\theta)\\), and assume for simplicity’s sake that \\(g\\) is one-to-one. Then \\(\\theta = g^{-1}(\\eta)\\). If \\(\\theta\\) has posterior \\(p(\\theta \\mid y)\\), the posterior for \\(g(\\theta)\\) is:\n\\[\np(g^{-1}(\\eta) \\mid y) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\]\nThis can lead to contradictions under ``ignorance”.\nThis presentation follows Gelman et al. (2013) somewhat.\nThere are priors called Jeffreys’ priors (for Harold Jeffreys) that are invariant to reparameterizations. Remember that the Fisher information, or: \\[\n\\mathcal{I}(\\theta) = \\Exp{\\ell_\\theta(\\theta; Y)\\ell_\\theta(\\theta; Y)^T}\n\\] under a reparameterization \\(\\eta = g(\\theta)\\) with Jacobian \\((J_{\\eta,\\theta})_{ij} = \\frac{\\partial \\eta_i}{\\partial \\theta_j}\\) is: \\[\n\\mathcal{I}(\\theta(\\eta)) = J_{\\eta,\\theta}^T\\Exp{\\ell_\\theta(g^{-1}(\\eta); Y)\\ell_\\theta(g^{-1}(\\eta); Y)^T}J_{\\eta,\\theta}\n\\] For \\(\\eta = g(\\theta)\\), assume for simplicity that \\(g\\) is one-to-one, then a prior for \\(\\theta\\) that is proportional to the square root of the determinant of the Fisher information will be invariant to reparameterization:\n\\[\np(\\theta) \\propto \\det(\\mathcal{I}(\\theta))^{1/2}\n\\] Why is this the case? Because under the change of measure formula above the prior for \\(\\eta\\) is: \\[\np(\\eta) \\propto p(g^{-1}(\\eta)) \\det J_{\\eta,\\theta}\n\\] which is \\[\n\\begin{aligned}\np(\\eta) & \\propto \\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det J_{\\eta,\\theta} \\\\\n& \\propto \\det (J_{\\eta,\\theta})^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T)^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T\\mathcal{I}(g^{-1}(\\eta))^{1/2}J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det(\\mathcal{I}(\\eta))^{1/2}\n\\end{aligned}\n\\] Thus giving some sense of invariance under a coordinate change. As stated in Gelman et al. (2013), more or less:\n\nAny rule for determining the prior density \\(p(\\theta)\\) should yield an equivalent result if aplied to the transformed parameter; that is, \\(p(\\eta)\\) generated using \\(p(\\theta)\\) using the change of measure formula should yield the same prior as would have been obtained directly from the model \\(p(\\eta) p(y \\mid \\eta)\\)\n\nOne issue with Jeffreys’ prior is that it is dependent on a likelihood, which can be controversial.\nFor the Bernoulli trial example from last class, the Jeffreys prior is \\(\\text{Beta}(1/2, 1/2)\\).\n\n\n\nPosterior probabilities are strictly “right” under our prior assumption because of the math of Bayes’ theorem. However, if we take a Frequentist view of probability, namely that probabilities are defined as limiting proportions of events, we’ll need to think about alternative draws of our prior and of our data.\nThe coverage of our posterior credible intervals will only match the nominal probabilities if the prior we use for our analysis matches that which generated the data. We can show this as computing the marginal posterior \\(p(\\theta \\mid y)\\) under repeated draws from the prior and data distribution \\(p(y \\mid \\theta)\\), which is the distribution associated with the density \\(f_Y(y \\mid \\theta)\\) we’ll use in our posterior:\n\\[\n\\begin{aligned}\n\\theta^\\prime & \\sim p(\\theta) \\\\\ny & \\sim p(y \\mid \\theta^\\prime) \\\\\n\\theta & \\sim p(\\theta \\mid y)\n\\end{aligned}\n\\] Another way to represent this sampling diagram is through integrals:\n\\[\n\\begin{aligned}\n\\int_{\\Omega_\\theta}\\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime) dy \\, d\\theta^\\prime & = \\int_{\\mathcal{Y}} \\int_{\\Omega_\\theta}\\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} \\int_{\\Omega_\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} p(\\theta) f_Y(y \\mid \\theta) \\\\\n& = p(\\theta)\n\\end{aligned}\n\\]\nSee Talts et al. (2018) for more info about how we can use this identity to test whether our algorithms are working correctly.\n\n\n\nLike Frequentist confidence intervals, we can only compute \\(p(\\theta \\mid y)\\) exactly under special circumstances, like conjugate priors. The reason for this is that the integral in the denominator is usually intractable.\nWe will usually have to do approximate inference on Bayesian models by using Markov Chain Monte Carlo samplers, which iteratively generate samples that converge in distribution to the true posterior distribution. Bayesian approximate methods instead operate on an expression that is proportional to the posterior:\n\\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta) p(\\theta)\n\\]\nOne way to think about the MLE is that it is the posterior mode under a prior of \\(p(\\theta) \\propto 1\\): \\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta)\n\\] The difference between the likelihood \\(L_Y(\\theta \\mid y)\\) and the posterior \\(p(\\theta \\mid y)\\) lies in how we treat the expression. In MLE we’re going to maximize the likelihood. In Bayesian inference we care about the full distribution of \\(\\theta\\).\nThis gives some intuition about Bayesian inference. We can think of doing MLE and penalizing certain values of \\(\\theta\\):\n\\[\n\\ell_Y(\\theta \\mid y) + \\text{penalty}(\\theta)\n\\] that will allow the maximizer to favor certain values of \\(\\theta\\) over others.\nIf we look at the implied log-posterior ignoring the constant that doesn’t depend on \\(\\theta\\):\n\\[\n\\log p(\\theta \\mid y) = \\log f_Y(y \\mid \\theta) + \\log p(\\theta)\n\\]\nIf we maximize this expression we can rewrite this as \\[\n\\log p(\\theta \\mid y) = \\ell_Y(\\theta \\mid y) + \\log p(\\theta)\n\\] and we get the penalized likelihood expression where the penalty is a probability density.\nOne question might be: ok, we have a full distribution for \\(\\theta\\). What do we do with it? While the MLE is a single choice, we now have myriad choices for point estimates derived from Bayesian models. We could use the posterior mean: \\[\n\\Exp{\\theta \\mid y}\n\\] We could use the posterior median, \\(\\theta_m\\): \\[\nP(\\theta &gt; \\theta_m \\mid y) = P(\\theta \\leq \\theta_m \\mid y) = 1/2.\n\\]\nWe could use another posterior quantile. We could use the mode of the posterior as well.\nAsymptotically, one might hope that the Bayesian estimates converge to the Frequentist estimates, and this is true, though one needs to be careful in scenarios where the dimensionality of the parameter space increases with sample size and about how one uses priors.\nIn Frequentist inference, the only limits on the parameter space come from the likelihood; the normal density requires that \\(\\mu \\in \\R\\) and \\(\\sigma^2 \\in (0, \\infty)\\). In Bayesian inference, the prior can also restrict the parameter space. For example, in the normal example, one could use a prior for \\(\\mu\\) that enforced \\(\\mu &gt; 0\\). The posterior would then only be able to represent \\(\\mu &gt; 0\\). If the true \\(\\mu\\) were negative, a Bayesian point-estimator wouldn’t converge to the true \\(\\mu\\).\nWhile the prior adds an extra degree of freedom which seems dangerous, it can yield better estimates when there are small datasets, because there isn’t as much information in the data. An example of this would be a simple regression model: \\[\ny_i \\sim \\text{Normal}(X_i^T \\beta, \\sigma^2)\n\\] We might have some good information that we don’t expect \\(\\beta\\) to be nearly infinite, and in fact we expect it to be pretty well concentrated to \\([-10, 10]\\). Then we could use independent \\(\\text{Normal}(0,5^2)\\) priors for the regression coefficients."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-7-notes.html#linear-regression-with-conjugate-priors",
    "href": "missing-data-material-W-26/notes/lecture-7-notes.html#linear-regression-with-conjugate-priors",
    "title": "Missing data lecture 7",
    "section": "Linear regression with conjugate priors",
    "text": "Linear regression with conjugate priors\nThis and the following section follow Chapter 2 in Rossi, Allenby, and Misra (2024) quite closely.\nLet’s look at the linear regression model with conjugate priors. \\[\ny_i = x_i^T \\beta + \\epsilon_i, \\quad \\epsilon_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(0, \\sigma^2)\n\\] where \\(x_i \\in \\R^p\\). A full model would imply a model for \\(x_i\\) as well: \\[\nf_{X,Y}((x_1, y_1), \\dots, (x_n, y_n) \\mid \\beta, \\psi) = \\prod_i f_{X}(x_i \\mid \\psi) f_Y(y_i \\mid x_i,\\beta, \\sigma^2)\n\\] If we have a prior for \\(\\psi,\\beta, \\sigma^2\\) that is independent, \\(p(\\psi, \\beta, \\sigma^2) = p(\\psi) p(\\beta, \\sigma^2)\\), then the posterior will factorize into independent distributions as well: \\[\n\\begin{aligned}\np_(\\beta, \\psi,\\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) & \\propto \\prod_i f_{X}(x_i \\mid \\psi) p(\\psi) f_Y(y_i \\mid x_i,\\beta, \\sigma^2) p(\\beta, \\sigma^2) \\\\\n& \\propto (\\prod_i f_{X}(x_i \\mid \\psi) p(\\psi)) \\prod_i f_Y(y_i \\mid x_i,\\beta, \\sigma^2) p(\\beta)  \\\\\n& \\propto p(\\psi \\mid x_1, \\dots, x_n) p(\\beta, \\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) \\\\\n\\end{aligned}\n\\] This means we can do inference on \\((\\beta,\\sigma)\\) without worrying about a model for \\(x_i\\).\nRemember from last class how we could intuit the form of the joint prior if we examined the likelihood for \\(\\theta\\) and chose a prior with the same functional form as that of the likelihood.\nIn the Bernoulli example, we had a likelihood of the form: \\(L_Y(\\theta \\mid y) = \\theta^{k}(1 - \\theta)^{n - k}\\), where \\(k = \\sum_i = y\\), which suggested a prior of the form \\(\\theta^{a}(1-\\theta)^b\\), which we could recognize as a Beta distribution.\nWe’ll do the same for the regression example. The likelihood for the linear model is: \\[\n(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2}\\sum_i (y_i - x_i^T \\beta)^2 \\rp\n\\] which can be simplified somewhat by writing the sum as a dot product between the vector of errors, \\(e = y - X \\beta\\) where \\(y = (y_1, \\dots, y_n)\\) and \\(X^T = (x_1, \\dots, x_n)\\). \\[\n(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2} (y - X \\beta)^T(y - X \\beta) \\rp\n\\]\nWe can rewrite the term \\((y - X \\beta)^T (y - X \\beta)\\) in terms of the least-squares estimator for \\(\\beta\\), \\(\\hat{\\beta} = (X^T X)^{-1}X^T y\\) by decomposing \\(y\\) as \\(y = X \\hat{\\beta} + y - X \\hat{\\beta}\\):\n\\[\n\\begin{aligned}\n(y - X \\beta)^T (y - X \\beta) & = (X \\hat{\\beta} + y - X \\hat{\\beta} - X \\beta)^T(X \\hat{\\beta} + y - X \\hat{\\beta} - X \\beta) \\\\\n& = (y - X \\hat{\\beta})^T(y - X \\hat{\\beta}) + (X \\beta - X\\hat{\\beta})^T(X \\beta - X\\hat{\\beta}) - 2(X \\beta - X\\hat{\\beta})^T (y - X \\hat{\\beta}) \\\\\n& = (y - X \\hat{\\beta})^T(y - X \\hat{\\beta}) + (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\n\\end{aligned}\n\\]\nLet \\(s^2 = \\frac{1}{n-p}(y-X\\hat{\\beta})^T(y-X\\hat{\\beta})\\), and \\(\\nu = n - p\\), so we can rewrite the sum more compactly as: \\[\n(y - X \\beta)^T (y - X \\beta) = \\nu s^2 + (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\n\\] This leads to a likelihood:\n\\[\nL_Y(\\beta, \\sigma^2 \\mid y, X) \\propto (\\sigma^2)^{-\\nu/2} \\exp\\lp\\frac{\\nu s^2}{2 \\sigma^2}\\rp (\\sigma^2)^{-(n - \\nu) / 2} \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\\rp\n\\] Before we derive conjugate priors from this likelihood, we can see that the posterior under flat priors for \\(\\beta\\) and a prior for \\(\\sigma^2\\), \\(\\sigma^{-2}\\), leads to a posterior: \\[\np(\\beta, \\sigma^2 \\mid y, X) \\propto (\\sigma^2)^{-(\\nu/2+1)} \\exp\\lp\\frac{\\nu s^2}{2 \\sigma^2}\\rp (\\sigma^2)^{-(n - \\nu) / 2} \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\\rp\n\\] which is a conditional normal posterior for \\(\\beta\\) with a scaled inverse chi-squared posterior for \\(\\sigma^2\\).\nThis suggests a conjugate prior of the form \\(p(\\beta,\\sigma^2) = p(\\sigma^2)p(\\beta \\mid \\sigma^2)\\): \\[\np(\\sigma^2) \\propto (\\sigma^2)^{-(\\nu_0/2 + 1)} \\exp\\lp \\frac{\\nu_0 s_0}{2 \\sigma^2} \\rp\n\\]\nand a conditional normal prior for \\(\\beta\\):\n\\[\np(\\beta \\mid \\sigma^2) \\propto (\\sigma^2)^{-p / 2} \\exp\\lp-\\frac{1}{2\\sigma^2}(\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta - \\mu_0) \\rp\n\\]\nThis can be seen as the posterior from a regression run with a prior of \\(p(\\sigma^2) \\propto \\sigma^{-2}\\) and a flat prior on \\(\\beta\\).\nThen the posterior for \\(\\sigma^2, \\beta\\) is simply the product of the priors and the likelihood, which we write as above:\n\\[\n\\begin{aligned}\np(\\beta, \\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) \\propto & (\\sigma^2)^{-(\\nu_0/2 + 1)} \\exp\\lp \\frac{\\nu_0 s_0}{2 \\sigma^2} \\rp(\\sigma^2)^{-p / 2} \\exp\\lp-\\frac{1}{2\\sigma^2}(\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta - \\mu_0) \\rp \\\\\n&(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2} (y - X \\beta)^T(y - X \\beta) \\rp\n\\end{aligned}\n\\]\nThis is definitley formidable, but we can simplify things a bit by collecting the terms with \\(\\beta\\):\n\\[\n(y - X\\beta)^T (y - X\\beta) + (\\mu_0 - \\beta)^T \\Sigma_0^{-1}(\\mu_0 - \\beta)\n\\] and decomposing \\(\\Sigma_0^{-1} = L^T L\\), and noting that we can write the sum as the following inner product: \\[\n\\begin{aligned}\n\\begin{bmatrix}\n(y - X\\beta)^T & (L\\mu_0 - L\\beta)^T\n\\end{bmatrix}\n\\begin{bmatrix}\n(y - X\\beta) \\\\\nL \\mu_0 - L\\beta)\n\\end{bmatrix}\n\\end{aligned}\n\\] This can be further simplified by constructing a vector \\[\nu = \\begin{bmatrix} y \\\\ L\\mu_0 \\end{bmatrix}\n\\] and a matrix \\(W\\) \\[\nW = \\begin{bmatrix} X \\\\ L \\end{bmatrix}\n\\] and writing the expresssion as \\((u - W\\beta)^T(u - W\\beta)\\). We can then use the same trick as above, by representing \\(u\\) as the projection into the column space of \\(W\\) and the residual:\n\\[\n(W \\bar{\\beta} + u - W \\bar{\\beta} - W \\beta)^T(W \\bar{\\beta} + u - W \\bar{\\beta} - W \\beta)\n\\] The expression for \\(\\bar{\\beta}\\) is:\n\\[\n\\begin{aligned}\n\\bar{\\beta} & = (X^T X + L^T L)^{-1}(X^T y + L^T L \\mu_0) \\\\\n& = (X^T X + \\Sigma_0^{-1})^{-1}(X^T y + \\Sigma_0^{-1} \\mu_0)\n\\end{aligned}\n\\]\nWhich simplifies as\n\\[\n(u - W \\bar{\\beta})^T(u - W \\bar{\\beta}) +(\\beta - \\bar{\\beta})^T W^T W (\\beta - \\bar{\\beta})\n\\] and after some algebra comes to \\[\n(y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) + (\\mu_0 - \\bar{\\beta})^T\\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta}) + (\\beta - \\bar{\\beta})^T (X^T X + \\Sigma_0^{-1}) (\\beta - \\bar{\\beta})\n\\] In the following, let \\(n s^2 = (y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) + (\\mu_0 - \\bar{\\beta})^T\\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta})\\). The posterior is:\n\\[\n\\begin{aligned}\np(\\beta, \\sigma^2 \\mid y, X) \\propto & (\\sigma^2)^{-(n + \\nu_0)/2 + 1} \\exp\\lp\\frac{(n + \\nu_0)(n s^2 + \\nu_0 s_0^2)/(n + \\nu_0)}{2 \\sigma^2}\\rp \\times (\\sigma^2)^{-p / 2} \\\\\n& \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\bar{\\beta})^T (X^T X + \\Sigma_\\beta^{-1})(\\beta - \\bar{\\beta})\\rp\n\\end{aligned}\n\\] \\[\n\\bar{\\beta} = (X^T X + \\Sigma_\\beta^{-1})^{-1}(\\Sigma_\\beta^{-1} \\mu_\\beta + X^T X \\hat{\\beta})\n\\] Like the Bernoulli problem, the posterior mean for \\(\\beta\\) is a weighted average between the prior mean and the information from the likelihood, which in this case is the least-squared estimator for \\(\\beta\\). This is often a consequence of using conjugate priors, that the posterior is a compromise between the prior and the likelihood.\nThis implies the following distributions for \\(\\sigma^2\\) and \\(\\beta \\mid \\sigma^2\\): \\[\n\\begin{aligned}\n\\sigma^2 & \\sim \\text{Inv-}\\chi^2\\lp n + \\nu_0, \\frac{n s^2 + \\nu_0 s^2_0}{n + \\nu_0}\\rp \\\\\n\\beta \\mid \\sigma^2 & \\sim \\text{Normal}(\\bar{\\beta}, \\sigma^2 \\lp X^T X + \\Sigma_0^{-1} \\rp^{-1})\n\\end{aligned}\n\\]\nThe posterior mean for \\(\\sigma^2\\) is: \\[\n\\Exp{\\sigma^2 \\mid y, X} = \\frac{n + \\nu_0}{n + \\nu_0 - 2}\\frac{n s^2 + \\nu_0 s^2_0}{n + \\nu_0}\n\\] The expression for \\(n s^2\\) is interesting because it involves the squared error of the posterior linear predictor for \\(y\\): \\[\n\\begin{aligned}\n(y - \\Exp{X \\beta \\mid y, X})^T(y - \\Exp{X \\beta \\mid y, X})^T & = (y - X \\Exp{\\beta \\mid y, X})^T(y - X \\Exp{\\beta \\mid y, X}) \\\\\n& = (y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) \\\\\n\\end{aligned}\n\\]\nbut it also involves the error in the prior mean with respect to the prior covariance matrix: \\[\n(\\mu_0 - \\bar{\\beta})^T \\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta})\n\\] The effect of this term will decrease as the number of observations increases, but it elucidates how the posterior mean of the error variance is decomposed into several pieces depending on different aspects of the prior and the data.\n\nBayesian inference in repeated measure models\nTwo lectures ago we went through how to compute the MLE from this regression model:\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j.\n\\end{aligned}\n\\]\nThis required sequentially computing the MLE for \\(\\beta\\) given an estimate for \\(\\Sigma\\) and computing \\(\\hat{\\Sigma}\\) given the last estimate for \\(\\hat{\\beta}\\).\nLet’s write down the likelihood for this model to see if we can come up with a conjugate prior for the problem. \\[\nL_{Y}(\\beta, \\Sigma \\mid y, X) \\propto \\det(I_n \\otimes \\Sigma)^{-1/2} \\exp\\lp-\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\\rp\n\\] If we start with the prior for \\(\\beta \\mid \\Sigma\\) we can ignore the determinant and focus on the term in the exponential:\n\\[\n-\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\n\\]\nLet’s try a multivariate normal prior:\n\\[\n\\beta \\sim \\text{Normal}(\\mu_0, \\Sigma_0)\n\\]\nso we can multiply the likelihood by the prior to get\n\\[\n-\\frac{1}{2}\\lp (y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta) + (\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta-\\mu_0) \\rp\n\\] which we’ll rewrite for convenience as \\[\n-\\frac{1}{2}\\lp (A(y - X \\beta))^T A(y - X \\beta) + (L(\\beta - \\mu_0))^T L(\\beta-\\mu_0) \\rp\n\\] where \\(A^T A = (I_n \\otimes \\Sigma)^{-1}\\) and \\(L^T L = \\Sigma_0^{-1}\\).\nThis looks familiar! We can use the same trick as we did above: create a new vector \\(u\\) and matrix \\(W\\): \\[\nu =\n\\begin{bmatrix}\nA y \\\\\nL \\mu_0\n\\end{bmatrix},\\quad\nW =\n\\begin{bmatrix}\nA X \\\\\nL\n\\end{bmatrix}\n\\] and can write \\[\n(u - W\\beta)^T(u - W \\beta) = \\lp (A(y - X \\beta))^T A(y - X \\beta) + (L(\\beta - \\mu_0))^T L(\\beta-\\mu_0) \\rp.\n\\] Furthermore, write \\(u = W\\bar{\\beta} + u - W\\bar{\\beta}\\), where \\(\\bar{\\beta}\\) is the least-squares coeffients of the regression of \\(u\\) on \\(W\\): \\[\n\\bar{\\beta} = (W^T W + L^T L)^{-1}W^T u = (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma_0^{-1})^{-1}(X^T (I_n \\otimes \\Sigma)^{-1}y + \\Sigma_0^{-1} \\mu_0)\n\\] This leads to \\((u - W\\bar{\\beta})^T W = 0\\), which allows us to cleanly partition \\((u-W\\beta)^T (u-W\\beta)\\) into two pieces: \\(u - W\\bar{\\beta}\\) and \\(W\\beta\\):\n\\[\n\\begin{aligned}\n(W\\bar{\\beta} + u - W\\bar{\\beta} - &W \\beta)^T(W\\bar{\\beta} + u - W\\bar{\\beta} - W \\beta) \\\\\n& = (u - W\\bar{\\beta} + W\\bar{\\beta}  - W \\beta)^T(u - W\\bar{\\beta}+ W\\bar{\\beta}  - W \\beta) \\\\\n& = (u - W\\bar{\\beta})^T(u - W\\bar{\\beta}) + (W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta) + 2(u - W\\bar{\\beta})^T(W\\bar{\\beta}  - W \\beta) \\\\\n& = (u - W\\bar{\\beta})^T(u - W\\bar{\\beta}) + (W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta)\n\\end{aligned}\n\\] where the last line follows because \\((u - W\\bar{\\beta})^T W = 0\\). Because we’re focusing only on the posterior, which is a function of \\(\\beta\\) and not data, we can ignore the \\((u - W\\bar{\\beta})^T(u - W\\bar{\\beta})\\) term because it does not involve \\(\\beta\\) and involves only functions of \\(X,y,A,L\\), which are fixed with respect to \\(\\beta\\).\nWe rewrite \\[\n(W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta)\n\\] as \\[\n(\\beta - \\bar{\\beta})^T W^T W (\\beta - \\bar{\\beta}) = (\\beta - \\bar{\\beta})^T(X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1}) (\\beta - \\bar{\\beta})\n\\] This shows that \\(\\beta \\mid \\Sigma\\) is multivariate normal with: \\[\n\\beta \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1})^{-1})\n\\] Now let’s focus on the conditional distribution of \\(\\Sigma \\mid \\beta\\). We’ll start with the likelihood written in simpler terms: \\[\nL_Y(\\beta, \\Sigma \\mid y, X) \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i (y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta)\\rp\n\\]\nWe can use the trace trick to rearrange things:\n\\[\n\\begin{aligned}\nL_Y(\\beta, \\Sigma \\mid y, X) & \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i (y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta)\\rp \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i \\text{tr}((y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta))\\rp \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i \\text{tr}((y_i - X_i \\beta) (y_i - X_i \\beta)^T\\Sigma^{-1})\\rp  \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\text{tr}((\\sum_i (y_i - X_i \\beta) (y_i - X_i \\beta)^T)\\Sigma^{-1})\\rp  \\\\\n\\end{aligned}\n\\]\nThis suggests that a conjugate prior for \\(\\Sigma\\) has the form:\n\\[\np(\\Sigma) \\propto \\det(\\Sigma)^{-a/2} \\exp\\lp-\\frac{1}{2}\\text{tr}(V_0 \\Sigma^{-1})\\rp\n\\]\nFortunately, we’re in luck! The Inverse Wishart distribution has the density:\n\\[\np(\\Sigma) \\propto \\det(\\Sigma)^{-(\\nu_0 + p + 1)/2} \\exp\\lp-\\frac{1}{2}\\text{tr}(V_0 \\Sigma^{-1})\\rp\n\\]\nCombining the likelihood with the prior we get something proportional to the conditional posterior for \\(\\Sigma\\):\n\\[\np(\\Sigma \\mid y, X, \\beta) \\propto \\det(\\Sigma)^{-(n + \\nu_0 + p + 1)/2} \\exp\\lp-\\frac{1}{2}\\text{tr}((V_0 + \\textstyle\\sum_i (y_i - X_i \\beta)(y_i - X_i \\beta)^T) \\Sigma^{-1})\\rp\n\\]\nPutting this together we get the following two conditional posteriors:\n\\[\n\\begin{aligned}\n\\beta \\mid \\Sigma, y, X & \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1})^{-1}) \\\\\n\\Sigma \\mid \\beta, y, X & \\sim \\text{Inverse-Wishart}(n + \\nu_0, V_0 + \\textstyle\\sum_i (y_i - X_i \\beta)(y_i - X_i \\beta)^T)\n\\end{aligned}\n\\]\nWe can use the theory of integral operators to show that given intial conditions \\(\\Sigma^0\\) and \\(\\beta^0\\) the following algorithm for \\(t = 1, \\dots, S\\):\n\\[\n\\begin{aligned}\n\\beta^{t+1} \\mid \\Sigma^{t}, y, X & \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma^t)^{-1} X + (\\Sigma^t)^{-1})^{-1}) \\\\\n\\Sigma^{t+1} \\mid \\beta^{t}, y, X & \\sim \\text{Inverse-Wishart}(n + \\nu_0, V_0 + \\textstyle\\sum_i (y_i - X_i \\beta^t)(y_i - X_i \\beta^t)^T)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-5-notes.html#maximum-likelihood-for-multivariate-normal-distribution",
    "href": "missing-data-material-W-26/notes/lecture-5-notes.html#maximum-likelihood-for-multivariate-normal-distribution",
    "title": "Missing data lecture 5",
    "section": "",
    "text": "Let \\(y_i \\in \\R^K\\), \\(y_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(\\mu, \\Sigma)\\) for \\(n\\) samples so that the density for \\(y_i\\) is \\[\nf_{Y}(y_i \\mid \\mu, \\Sigma) = (2\\pi)^{-\\frac{K}{2}} (\\det\\Sigma)^{-\\frac{1}{2}} \\exp\\lp-\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu) \\rp\n\\]\nThe log-likelihood is: \\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = \\frac{1}{2} \\log \\det\\Sigma -\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu)\n\\]\nThe book gives the expressions for the MLEs of the mean and covariance matrix of the multivariate normal distribution without details. Going through the algebra can be useful for other more complicated problems. But in order to do so, we’ll need a slight change to how we’re used to thinking about partial differentiation. The following blurb on differentials is based on Magnus and Neudecker (2019)."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-5-notes.html#differentials-and-matrix-differentiation",
    "href": "missing-data-material-W-26/notes/lecture-5-notes.html#differentials-and-matrix-differentiation",
    "title": "Missing data lecture 5",
    "section": "Differentials and matrix differentiation",
    "text": "Differentials and matrix differentiation\nIt all starts with rearranging the derivative:\n\\[\nf^\\prime(c)  = \\lim_{u \\to 0} \\frac{f(c + u) - f(c)}{u},\n\\] to get a linear approximation to \\(f\\) at the point \\(c\\):\n\\[\nf(c + u)  = f(c) + f^\\prime(c) u + r_c(u)\n\\] where \\(r_c(u) = o(u)\\) or \\(\\lim_{u\\to0} \\frac{r_c(u)}{u} = 0\\). This is the one term Taylor expansion of the function \\(f\\) at \\(c + u\\) about \\(c\\).\nBringing \\(f(u)\\) to the left-hand side gives: \\(f(c + u) - f(u) = f^\\prime(c) u + r_c(u)\\). We can define the change in the linear approximation of \\(f\\) from \\(c\\) to \\(c+u\\) as \\(\\mathrm{d}f(c;\\,u)\\), or the first differential of \\(f\\) at \\(c\\) with increment \\(u\\): \\[\n\\mathrm{d} f(c ; u) = u f^\\prime(c)\n\\]\nSubbing this back into the linear approximation for \\(f\\) gives: \\[\nf(c + u)  = f(c) + \\mathrm{d} f(c ; u) + r_c(u)\n\\tag{1}\\]\nWe can identify the differential by finding the linear approximation to a function at \\(c\\): \\[\nf(c + u)  = f(c) + \\alpha u + r_c(u).\n\\] If we can find an \\(\\alpha\\) that depends on \\(c\\) but not on \\(u\\) such that \\(r_c(u) = o(u)\\) we say that \\(\\alpha = f^\\prime(c)\\).\nLet \\(f\\) now be a function \\(\\R^m \\to \\R\\) and let the differential be constructed via the same argument as above, but now let \\(c, u \\in \\R^m\\), and define \\(r_c(u)\\) such that \\(\\lim_{u\\to 0}\\frac{r_c(u)}{\\lVert u \\rVert} = 0\\): \\[\nf(c + u)  = f(c) + A(c) u + r_c(u).\n\\] If we equate the row vector \\(A(c)\\) with the partial derivative of \\(f\\) with respect to \\(u\\), we can recognize this as the multivariate Taylor expansion of \\(f(c + u)\\) around \\(f(c)\\).\nIn fact, this is exactly what \\(A(c)\\) is, and \\(A(c) \\equiv \\nabla_x f(x) \\mid_{x = c}\\)\n\nExample 1 (Product example) Let’s determine the differential of \\(f(x, y) = x^T y\\) for \\(x,y \\in \\R^m\\), denoted as: \\[\n\\mathrm{d}(x^T y).\n\\] We’re looking to use the left-hand side of in Equation 1 to yield something that looks like \\(x^T y + \\diff{x^Ty} u + r_c(u)\\).\nLet \\(c_x\\) and \\(c_y\\) be the values of \\(x\\) and \\(y\\) about which we’ll define our linear approximation. Specifically, we’ll define the linear approximation at the coordinates \\(c_x + u_x, c_y + u_y\\) In other words, we’d like to approximate the function: \\((c_x + u_x)^T (c_y + u_y)\\) with the value at \\(c_x^T c_y\\) plus the differential and a small remainder term. We’ll accomplish this by expanding the product \\((c_x + u_x)^T (c_y + u_y)\\) into four parts: \\[\n\\begin{align}\n(u_x + c_x)^T (u_y + c_y) & = c_x^T c_y + c_x^T u_y + u_x^T c_y + u_x^T u_y \\\\\n& = c_x^T c_y + c_x^T u_y + c_y^T u_x  + u_x^T u_y \\\\\n& = c_x^T c_y + \\begin{bmatrix}c_x^T & c_y^T \\end{bmatrix}  \\begin{bmatrix}u_y \\\\ u_x \\end{bmatrix} + u_x^T u_y \\tag{a}\\label{eq:1}\n\\end{align}\n\\] In line \\(\\eqref{eq:1}\\) we can see that \\(f(c) \\equiv c_x^T c_y\\), and \\(\\lim_{u_x, u_y \\to 0} \\frac{u_x^T u_y}{\\sqrt{\\lVert u_x \\rVert^2 + \\lVert u_y \\rVert^2}} = 0\\), so \\(u_x^T u_y\\) is our \\(r_c(u)\\). That means that the vector \\(A(c)\\) here is \\((c_x^T, c_y^T)\\) as we’ve ordered our variables as \\((u_y,u_x)\\).\n\n\nRules for the differential operatior\n\nThe differential operator is denote \\(\\diff{\\cdot}\\). It is linear: \\[\n\\diff{a x + b y} = a\\diff{x} + b\\diff{y}\n\\]\nThe differential of a constant is zero: \\[\n\\diff{a} = 0\n\\]\nThe differential of a transposed variable is the transpose of the differential \\[\n\\diff{x^T} = \\diff{x}^T\n\\]\nThe differential of a product is the sum of the differential applied to each variable (as shown in Example 1) \\[\n\\diff{XY} = \\diff{X}Y + X\\diff{Y}\n\\]\n\nExample 1 demonstrates another useful property of differentials. We recognize the fact that our variables partition naturally into two vectors, \\(x\\) and \\(y\\). When we have a natural partition of variables \\(u\\) into \\(u_1\\) and \\(u_2\\) we can write the differential for \\(f(u)\\) more easily in terms of two differentials: \\[\n\\begin{aligned}\n\\mathrm{d}f(c;\\,u) & = A(c) u \\\\\n& = A(c_1) u_1 + A(c_2)u_2\n\\end{aligned}\n\\] which just differentiates between the two sets of variables, so that \\(A(c_1)\\) is the partial derivative of \\(f\\) with respect to \\(u_1\\) and \\(A(c_2)\\) is the partial derivative with respect to \\(u_2\\). In Example 1, \\(u_1\\) is \\(u_y\\) and \\(u_2\\) is \\(u_x\\), so we can write the differnetial above in the equivalent form:\n\\[\n\\begin{aligned}\n\\mathrm{d}(x^T y) & = x^T u_y + u_x^T y \\\\\n& = x^T u_y + y^T u_x\n\\end{aligned}\n\\] In order to simplify the notation, we’ll write \\(\\diff{x}\\) instead of \\(u_x\\), so the above would be: \\[\n\\begin{aligned}\n\\mathrm{d}(x^T y) & = x^T \\diff{y} + y^T \\diff{x}\n\\end{aligned}\n\\]\nThis expression shows that we can read off \\(\\nabla_y x^T y\\) as \\(x^T\\) and \\(\\nabla_x x^T y\\) as \\(y^T\\). In fact, if we cared only about \\(\\diff{y}\\) then we could ignore the differential \\(\\diff{x}\\), essentially treating \\(x\\) as a constant so \\(\\diff{x} = 0\\).\nThis is important for thinking about differentials of log-likelihoods like the multivariate normal where we’ll have two sets of parameters that we’d like to find the partial derivatives with respect to, \\(\\Sigma\\) and \\(\\mu\\): \\[\n\\ell_Y(\\mu, \\Sigma \\mid y) = \\frac{1}{2} \\log \\det\\Sigma -\\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu)\n\\] \\[\n\\mathrm{d}\\ell_Y(\\mu, \\Sigma \\mid y) = \\frac{1}{2} \\mathrm{d}(\\log \\det\\Sigma) -\\frac{1}{2}\\mathrm{d}((y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu))\n\\tag{2}\\]\nTo the extent we’d prefer to focus only on \\(\\diff{\\mu}\\) for example, we would ignore the first term on the RHS of Equation 2, and focus only on the second term.\n\n\nGeneralizing to vector valued functions\nWe can generalize to vector functions: Let \\(f(x): \\R^m \\to \\R^n\\): \\[\nf(c + u)  = f(c) + A(c) u + r_c(u).\n\\] for \\(\\lim_{u\\to 0} r_c(u) / \\norm{u} = 0\\). Then \\(\\mathrm{d}f(c;u) = A(c)u\\) is the differential of \\(f\\) evaluated at \\(c\\) of increment \\(u\\).\nIn fact, this is the multivariate Taylor expansion. Each row of \\(A(c)\\) is the term \\(\\nabla_x f_i(x) \\mid_{x = c}\\) where \\(f_i\\) is the \\(i^\\mathrm{th}\\) entry of the length-\\(n\\) vector \\(f(x)\\).\n\n\nGeneralizing to matrix valued functions\nThe same idea applies to matrices, when combined with the \\(\\text{vec}\\) function, which concatenates an \\(n \\times p\\) matrix column by column into an \\(n \\times p\\)-length vector. Let \\(F\\) be a matrix function \\(\\R^{n \\times q} \\to \\R^{m \\times p}\\). Let \\(C\\) and \\(U\\) be in \\(\\R^{m\\times q}\\). If \\(A(C) \\in \\R^{mp \\times nq}\\) such that: \\[\n\\text{vec}(F(C + U))  = \\text{vec}(F(C)) + A(C) \\text{vec}(U) + \\text{vec}(R_c(U)).\n\\] Then the \\(m\\times p\\) matrix \\(\\mathrm{d} F(C;\\,U)\\) is defined by \\(\\text{vec}(\\mathrm{d} F(C;\\,U)) = A(C) \\text{vec}(U)\\).\nThe reason to do this is because the differential generalizes to matrices a bit easier than do partial derivatives. This is because it isn’t clear along which dimensions the partial derivatives should lie: Should the partials of a matrix function become a third dimension, like a 3-d array?\nUnder this framework, the rows of the matrices \\(A(c)\\) and \\(A(C)\\) correspond to a dimension of the range of the function \\(f(c)\\) or \\(F(C)\\), while the columns correspond to a dimension of the domain.\n\n\nThe chain rule\nThe power of the differentials is laid bare when working through the chain rule, which is called Cauchy’s rule of invariance in differential-land. Let \\(f: \\R^m \\to \\R^p\\) and \\(g: \\R^p \\to \\R^n\\), and let \\(h = g \\circ f\\). Then \\(h: \\R^m \\to \\R^n\\). If \\(b = f(c)\\) and \\(h = g(b)\\) the differential of \\(h\\) is: \\[\n\\begin{aligned}\n\\mathrm{d}(h;\\,u) & = \\mathrm{d}(h;\\,\\mathrm{d}(f;\\,c)) \\\\\n& = A_{g}(b) A_{f}(c) u\n\\end{aligned}\n\\] where \\(A_g(b) \\in R^{n \\times p}\\) and \\(A_f(c) \\in R^{p \\times m}\\) and \\(u \\in \\R^m\\).\nWe can show this rigorously with our previous definitions. \\[\n\\begin{align}\nh(c + u) & = g(f(c + u)) \\\\\n& = g(f(c) + A_f(c)u + r_c(u))  \\\\\n& = g(b + v) \\tag{$v = A_f(c)u + r_c(u)$}\\\\\n& = g(b) + A_g(b) v + r_b(v) \\\\\n& = g(b) + A_g(b) \\lp A_f(c) u  + r_c(u) \\rp + r_b(A_f(c) u + r_c(u)) \\\\\n& = h(c) + A_g(b) A_f(c) u  + A_g(b) r_c(u) + r_c(u) \\\\\n& = h(c) + A_g(b) A_f(c) u  + r_c(u)\n\\end{align}\n\\]\n\n\nDifferential with respect to \\(\\mu\\)\nFirst we’ll ignore the differential with respect to \\(\\Sigma\\). We’ll expand out that quadratic form into the parts that depend only on \\(\\mu\\): \\[\n\\mathrm{d} \\ell_{Y}(\\mu, \\Sigma \\mid y_i) = y_i^T\\Sigma^{-1}\\mathrm{d}\\mu - \\frac{1}{2}\\mathrm{d}(\\mu^T\\Sigma^{-1}\\mu)\n\\]\nTaking the gradient with respect to \\(\\mu\\) we get: \\[\n\\begin{aligned}\n\\mathrm{d} \\ell_{Y}(\\mu, \\Sigma \\mid y_i) & = y_i^T\\Sigma^{-1}\\mathrm{d} \\mu - \\frac{1}{2}\\mathrm{d}(\\mu^T)\\Sigma^{-1}\\mu - \\frac{1}{2}\\mu^T\\mathrm{d}(\\Sigma^{-1}\\mu) \\\\\n& = y_i^T\\Sigma^{-1}\\mathrm{d} \\mu - \\frac{1}{2}\\mathrm{d}(\\mu)^T\\Sigma^{-1}\\mu - \\frac{1}{2}\\mu^T\\Sigma^{-1}\\mathrm{d}\\mu \\\\\n& = y_i^T\\Sigma^{-1}\\mathrm{d} \\mu - \\frac{1}{2}\\mu^T\\Sigma^{-1}\\mathrm{d}\\mu - \\frac{1}{2}\\mu^T\\Sigma^{-1}\\mathrm{d}\\mu \\\\\n& = y_i^T\\Sigma^{-1}\\mathrm{d} \\mu - \\mu^T\\Sigma^{-1}\\mathrm{d}\\mu \\\\\n& = (y_i - \\mu)^T\\Sigma^{-1}\\mathrm{d} \\mu  \\\\\n\\end{aligned}\n\\] If we sum over the \\(n\\) terms of the log-likelihood we get: \\[\n\\begin{aligned}\n\\frac{\\partial \\ell_{Y}(\\mu, \\Sigma \\mid y_i)}{\\partial \\mu} & = (\\sum_i y_i - n \\mu)^T\\Sigma^{-1}\n\\end{aligned}\n\\] leading to the MLE for \\(\\mu\\): \\[\n\\hat{\\mu} = \\frac{1}{n}\\sum_i y_i\n\\]\nIt’ll be useful to write the log-likelihood a bit differently to find the MLE for \\(\\Sigma\\). Remember that \\(\\det A^{-1} = (\\det A)^{-1}\\). This will enable us to write everything in terms of \\(\\Sigma^{-1}\\) instead of \\(\\Sigma\\): \\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = \\frac{1}{2} \\log (\\det\\Sigma^{-1}) - \\frac{1}{2}(y_i - \\mu)^T \\Sigma^{-1}(y_i - \\mu)\n\\] Also remember that \\(\\text{tr}(A) = \\sum_{i} A_{ii}\\), \\(\\text{tr}(A + B) = \\text{tr}(A) + \\text{tr}(B)\\), and that \\(f(x) = \\text{tr}(f(x))\\) for a univariate function \\(f(x)\\). Finally, recall that \\(\\text{tr}(ABC) = \\text{tr}(CAB) = \\text{tr}(BCA)\\). This will let us rewrite the\nPutting all this together allows us to write the log-likelihood for the multivariate normal as such: \\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = \\frac{1}{2} \\log (\\det\\Sigma^{-1}) -\\frac{1}{2}\\text{tr}\\lp(y_i - \\mu) (y_i - \\mu)^T \\Sigma^{-1}\\rp\n\\]\nFor the partial derivative of \\(\\det \\Sigma^{-1}\\) with respect to \\(\\Sigma^{-1}\\), we get \\[\n\\frac{\\partial \\det \\Sigma^{-1}}{\\partial \\Sigma^{-1}} = \\det \\Sigma^{-1} ((\\Sigma^{-1})^{-1})^{T}\n\\] and for the partial derivative of \\(\\text{tr}(AB)\\) with respect to \\(B\\) we get \\(A^T\\), so the partial derivative with respect to \\(\\Sigma^{-1}\\) of the log-likelihood gives us: \\[\n\\frac{\\partial \\ell_{Y}(\\mu, \\Sigma \\mid y_i)}{\\partial \\Sigma^{-1}} = \\frac{1}{2} \\Sigma -\\frac{1}{2}(y_i - \\mu) (y_i - \\mu)^T\n\\] Summing over the \\(n\\) terms gives: \\[\n\\frac{\\partial \\ell_{Y}(\\mu, \\Sigma \\mid Y)}{\\partial \\Sigma^{-1}} = \\frac{n}{2} \\Sigma -\\frac{1}{2}\\sum_i (y_i - \\mu) (y_i - \\mu)^T\n\\] \\[\n\\hat{\\Sigma} = \\frac{1}{n}\\textstyle\\sum_i (y_i - \\hat{\\mu}) (y_i - \\hat{\\mu})^T\n\\]\n\n\nNormal repeated measures models\nIn many longitudinal studies where some outcome of interest is measured for participants \\(K\\) times, the following model may describe the data generating process well, where \\(y_i \\in \\R^K\\) and \\(X_i\\) is a \\(K \\times m\\) design matrix: \\[\n\\begin{aligned}\ny_i \\mid X_i \\sim \\text{MultiNormal}(X_i \\beta, \\Sigma(\\psi))\n\\end{aligned}\n\\] The textbook lists several models that could describe different scenarios.\n\nIndependent-but-not-identically-distributed observations within groups: \\[\n\\begin{aligned}\ny_{ik} \\mid (X_{i})_k \\, & = (X_{i})_k \\beta + \\epsilon_{ik} \\\\\n\\epsilon_{ik} & \\sim \\text{Normal}(0, \\sigma^2_k) \\\\\n\\epsilon_{ik} & \\indy \\epsilon_{jl} \\forall i\\neq j \\cup k \\neq l\n\\end{aligned}\n\\] This implies the following simple structure for \\(\\Sigma(\\psi)\\) above:\n\n\\(\\Sigma(\\psi) = \\text{diag}(\\sigma^2_1, \\dots, \\sigma^2_K)\\).\n\nCompound symmetry (I’ll call this a random intercept model): \\[\n\\begin{aligned}\ny_{ik} \\mid (X_{i})_k \\, & = (X_{i})_k \\beta + \\gamma_i + \\epsilon_{ik} \\\\\n\\epsilon_{ik} & \\sim \\text{Normal}(0, \\sigma^2) \\\\\n\\epsilon_{ik} & \\indy \\epsilon_{jl} \\forall i\\neq j \\cup k \\neq l \\\\\n\\gamma_i & \\sim \\text{Normal}(0, \\tau^2) \\\\\n\\gamma_i & \\indy \\gamma_j \\forall i \\neq j \\\\\n\\gamma_i & \\indy \\epsilon_{ij} \\forall j\n\\end{aligned}\n\\] Conditional on \\(X_i\\), the covariance between \\(y_{ik}\\) and \\(y_{ij}\\) is: \\[\n\\begin{aligned}\n\\text{Cov}(y_{ik}, y_{ij} \\mid X_i) & = \\text{Cov}((X_{i})_k \\beta + \\gamma_i + \\epsilon_{ik}, (X_{i})_j \\beta + \\gamma_i + \\epsilon_{ij} \\mid X_i) \\\\\n& = \\tau^2\n\\end{aligned}\n\\] While the variance is \\(\\tau^2 + \\sigma^2\\). This implies that we can write the variance-covariance matrix as \\[\n\\Sigma(\\psi) = \\tau^2 1_{K} 1_K^T + \\sigma^2 I_K.\n\\]\nAutoregressive (I’ll call this a random intercept model):\n\n\\[\n\\begin{aligned}\ny_{ik} \\mid (X_{i})_k \\, & = (X_{i})_k \\beta + \\epsilon_{ik} \\\\\n\\epsilon_{ik} \\mid \\epsilon_{i,k-1} & \\sim \\text{Normal}(\\rho \\epsilon_{i,k-1}, \\sigma^2) \\\\\n\\epsilon_{i1} & \\sim \\text{Normal}(0, \\frac{\\sigma^2}{1 - \\rho^2}) \\\\\n\\epsilon_{ik} & \\indy \\epsilon_{jl} \\forall i\\neq j \\cap \\forall k, l\n\\end{aligned}\n\\]\nThis implies the following simple structure for \\(\\Sigma(\\psi)\\):\n\\[\\Sigma(\\psi)_{ij} = \\frac{\\sigma^2}{1-\\rho^2} \\rho^{\\abs{i-j}}\\]\n\nRandom effects model\n\nThis is a more general version of the random intercept model. Let \\(z_k \\in \\R^q\\).\n\\[\n\\begin{aligned}\ny_{ik} \\mid (X_{i})_k \\, & = (X_{i})_k \\beta + z_k^T \\gamma_i + \\epsilon_{ik} \\\\\n\\epsilon_{ik} & \\sim \\text{Normal}(0, \\sigma^2) \\\\\n\\epsilon_{ik} & \\indy \\epsilon_{jl} \\forall i\\neq j \\cup k \\neq l \\\\\n\\gamma_i & \\sim \\text{Normal}(0, \\Omega) \\\\\n\\gamma_i & \\indy \\gamma_j \\forall i \\neq j \\\\\n\\gamma_i & \\indy \\epsilon_{ij} \\forall j\n\\end{aligned}\n\\] We can write this in matrix form as:\n\\[\ny_i \\mid X_i = X_i \\beta + Z \\gamma + \\epsilon_i\n\\] The conditional covariance is \\[\n\\begin{aligned}\n\\text{Cov}(y_i \\mid X_i) & = \\text{Cov}(X_i \\beta + Z \\gamma + \\epsilon_i \\mid X_i) \\\\\n& = Z \\Omega Z^T + \\sigma^2 I_K\n\\end{aligned}\n\\]\n\nHierarchical Gaussian process model\n\nSuppose we have time points \\(t_{i1}, \\dots, t_{iK}\\) associated with each measurement \\(y_{i1}, \\dots, y_{iK}\\).\nLet’s define the function \\(\\Omega\\), which is from \\(\\R^K \\to \\Sigma\\) where \\(\\Sigma\\) is the space of positive definite \\(m\\times m\\) matrices. Let \\(t_i \\in \\R^K\\) and let \\(\\Omega(\\mathbf{t} \\mid \\ell, \\sigma^2)\\) be defined \\[\n\\Omega(t_i \\mid \\ell, \\sigma^2)_{jk} = \\sigma^2 \\exp(-\\lp t_{ij} - t_{ik} \\rp^2/(2\\ell^2))\n\\] Then the following model is a hierarchical Gaussian process model\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & \\sim \\text{MultiNormal} \\lp X_{i} \\beta, \\Omega(t_i \\mid \\ell, \\sigma^2)\\rp\n\\end{aligned}\n\\]\n\n\nMLEs in repeated measure models\nThe book suggests the following strategy to find the MLEs in the unstructured case, which is \\(1\\) above:\nTake \\(\\beta^{(0)}\\) and \\(\\Sigma^{(0)}\\) as initial guesses. Then for \\(t = 1\\) until some termination criterion iterate:\n\\[\n\\beta^{(t+1)} = \\left(\\sum_i X_i^T (\\Sigma^{(t)})^{-1}X_i\\right)^{-1} \\sum_i X_i^T (\\Sigma^{(t)})^{-1}y_i\n\\] and \\[\n\\Sigma^{(t+1)} = \\frac{1}{n}\\sum_i (y_i - X_i \\beta^{(t+1)}) (y_i - X_i \\beta^{(t+1)})^T\n\\] We can derive these update rules from the log-likelihood, but we’ll need to rewrite the model so that it looks a little more familiar.\nThe model as written in matrix form by unit \\(i\\) is: \\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j\n\\end{aligned}\n\\] Let \\(y = (y_1^T, y_2^T, \\dots, y_n^T)^T\\) and let \\(X = (X_1^T, X_2^T, \\dots, X_n^T)^T\\), and let \\(\\epsilon = (\\epsilon_1^T, \\epsilon_2^T, \\dots, \\epsilon_n^T)^T\\). Then the model can be written: \\[\n\\begin{aligned}\ny \\mid X \\, & = X \\beta + \\epsilon \\\\\n\\epsilon & \\sim \\text{Normal}(0, I_n \\otimes \\Sigma)\n\\end{aligned}\n\\] so \\(\\text{Cov}(\\epsilon)\\) is block-diagonal: \\[\n\\begin{bmatrix}\n\\Sigma & 0 & \\dots & 0 \\\\\n0 & \\Sigma & \\dots & 0 \\\\\n0 & 0 & \\ddots & 0 \\\\\n0 & 0 & \\dots & \\Sigma\n\\end{bmatrix}\n\\] The log-likelihood is:\n\\[\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) = -\\frac{1}{2} \\log \\det(I_n \\otimes \\Sigma) -\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\n\\] The determinant of \\(I_n \\otimes \\Sigma\\) is \\(\\det(\\Sigma)^n\\) because it’s just block-diagonal, and the inverse of \\(I_n \\otimes \\Sigma\\) is similarly \\(I_n \\otimes \\Sigma^{-1}\\).\nLet’s focus on the \\(\\beta\\) terms. Expanding the quadratic form gives: \\[\n-\\frac{1}{2}(y^T(I_n \\otimes \\Sigma)^{-1}y + y^T (I_n \\otimes \\Sigma)^{-1}X \\beta\n-\\frac{1}{2} \\beta^T X^T (I_n \\otimes \\Sigma)^{-1}X \\beta\n\\]\nTaking the derivative with respect to \\(\\beta\\) gives: \\[\ny^T (I_n \\otimes \\Sigma)^{-1}X \\mathrm{d}\\beta\n-\\frac{1}{2} \\mathrm{d} \\beta^T X^T (I_n \\otimes \\Sigma)^{-1}X \\beta -\\frac{1}{2}  \\beta^T X^T (I_n \\otimes \\Sigma)^{-1}X \\mathrm{d}\\beta\n\\] Collecting terms gives: \\[\n(y^T (I_n \\otimes \\Sigma^{-1})X\n- \\beta^T X^T (I_n \\otimes \\Sigma)^{-1}X) \\mathrm{d}\\beta\n\\] This looks more daunting than it is, we can use block matrix multiplication to get: \\[\n(\\sum_i y_i^T \\Sigma^{-1} X_i - \\beta^T \\sum_i X_i^T  \\Sigma^{-1} X ) \\mathrm{d}\\beta\n\\] If \\(\\Sigma\\) were known, we could solve this equation simply:\n\\[\n\\hat{\\beta} = \\textstyle(\\sum_i X_i^T  \\Sigma^{-1} X)^{-1} (\\sum_i X_i \\Sigma^{-1} y_i)  \n\\]\nLike we did above, we can rewrite the likelihood in terms of \\(\\Sigma^{-1}\\) to give:\n\\[\n\\begin{aligned}\n\\ell_{Y}(\\mu, \\Sigma \\mid y_i) & = \\frac{n}{2} \\log \\det(\\Sigma^{-1}) - \\frac{1}{2}\\sum_i(y_i - X_i \\beta)^T\\Sigma^{-1}(y_i - X_i \\beta)  \\\\\n& = \\frac{n}{2} \\log \\det(\\Sigma^{-1}) -\\frac{1}{2}\\sum_i \\text{tr}(y_i - X_i \\beta)^T\\Sigma^{-1}(y_i - X_i \\beta)  \\\\\n& = \\frac{n}{2} \\log \\det(\\Sigma^{-1}) -\\frac{1}{2}\\sum_i \\text{tr}(y_i - X_i \\beta)(y_i - X_i \\beta)^T\\Sigma^{-1}  \\\\\n& = \\frac{n}{2} \\log \\det(\\Sigma^{-1}) -\\frac{1}{2}\\lp \\sum_i \\text{tr}(y_i - X_i \\beta)(y_i - X_i \\beta)^T\\rp\\Sigma^{-1}  \\\\\n\\end{aligned}\n\\] Taking derivatives with respect to \\(\\Sigma^{-1}\\) gives:\n\\[\n\\begin{aligned}\n\\mathrm{d} \\ell_{Y}(\\mu, \\Sigma \\mid y_i) & = \\frac{n}{2} \\Sigma\\,\\mathrm{d} \\Sigma^{-1} -\\frac{1}{2}\\sum_i (y_i - X_i \\beta) (y_i - X_i \\beta)^T \\mathrm{d} \\Sigma^{-1} \\\\\n& = \\lp \\frac{n}{2} \\Sigma -\\frac{1}{2}\\sum_i (y_i - X_i \\beta) (y_i - X_i \\beta)^T\\rp \\mathrm{d} \\Sigma^{-1}\n\\end{aligned}\n\\] Both of these derivatives have to be zero at the maximum likeihood estimate (assuming we’re not on a boundary of the parameter space), so we’ll get two sets of equations:\n\\[\n\\Sigma^{(t+1)} = \\frac{1}{n} \\sum_i (y_i - X_i \\beta^{(t)}) (y_i - X_i \\beta)^T\n\\]\n\\[\n\\beta^{(t+1)} = \\textstyle(\\sum_i X_i^T  (\\Sigma^{(t)})^{-1} X_i)^{-1} (\\sum_i X_i (\\Sigma^{(t)})^{-1} y_i)  \n\\]"
  },
  {
    "objectID": "survival-material/lecture-6.html",
    "href": "survival-material/lecture-6.html",
    "title": "Lecture 6",
    "section": "",
    "text": "1 More on log-rank tests\nI motivated the log-rank test by stating that we wanted to compare estimates of the hazard function. Let’s do a quick derivation to show why this is the case: We start with the weighted log-rank test as we have derived it: \\[\\begin{align}\n    Z_j(\\tau) & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(d_{ij} - d_i\\frac{\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\n\\end{align}\\] We can express this in terms of hazard estimators \\(\\hat{\\lambda}_j(t_i) = \\frac{d_{ij}}{\\widebar{Y}_j(t_i)}\\): Let’s let \\(j \\in \\{1,2\\}\\). Then \\[\\begin{align*}\n    \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(d_{ij} - d_i\\frac{\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)& = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(\\frac{d_{ij}\\widebar{Y}(t_i)-d_i\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\\\\\n    & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(\\frac{d_{ij}\\widebar{Y}(t_i)-(d_{ij} + d_{ij^\\prime})\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\\\\\n    & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\left(\\frac{d_{ij}\\widebar{Y}_{j^\\prime}(t_i)-d_{ij^\\prime}\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\\\\\n    & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\frac{\\widebar{Y}_{j^\\prime}(t_i)\\widebar{Y}_{j}(t_i)}{\\widebar{Y}(t_i)}\\left(\\frac{d_{ij}}{\\widebar{Y}_j(t_i)}-\\frac{d_{ij^\\prime}}{\\widebar{Y}_{j^\\prime}(t_i)}\\right)\n\\end{align*}\\] Thus we can see that \\(Z_1(\\tau) = -Z_2(\\tau)\\). Let’s rewrite this in terms of integrals over the positive reals \\[\\begin{align*}\n    \\sum_{i=1 \\mid t_i \\leq \\tau}^{n_1 + n_2} W(t_i) \\frac{\\widebar{Y}_{j^\\prime}(t_i)\\widebar{Y}_{j}(t_i)}{\\widebar{Y}(t_i)}\\left(\\frac{d_{ij}}{\\widebar{Y}_j(t_i)}-\\frac{d_{ij^\\prime}}{\\widebar{Y}_{j^\\prime}(t_i)}\\right)& = \\int_0^\\infty W(u) \\frac{\\widebar{Y}_{j^\\prime}(u)\\widebar{Y}_{j}(u)}{\\widebar{Y}(u)} \\left(d\\hat{\\Lambda}_1(u) - d\\hat{\\Lambda}_2(u)\\right)\\\\\n    & = \\int_0^\\infty W(u) \\frac{\\widebar{Y}_{j^\\prime}(u)\\widebar{Y}_{j}(u)}{\\widebar{Y}(u)}  d\\left(\\hat{\\Lambda}_1(u) - \\hat{\\Lambda}_2(u) \\right)\n\\end{align*}\\] A more general Lebesgue-Stieltjies theory will show that the integral above is well-defined. More on this later…\nLet’s say we’re going to test multiple groups for equality of hazard rates. Then we will write the log-rank statistic like so, with \\(n = \\sum_{j=1}^J n_j\\): \\[\\begin{align}\n    Z_j(\\tau) & = \\sum_{i=1 \\mid t_i \\leq \\tau}^{n} W(t_i) \\left(d_{ij} - d_i\\frac{\\widebar{Y}_j(t_i)}{\\widebar{Y}(t_i)}\\right)\n\\end{align}\\] The variance of \\(Z_j(\\tau)\\) is as was derived. We can show that \\(d_{i1}, \\dots, d_{iJ} \\mid d_i, \\widebar{Y}_1(t_i), \\dots, \\widebar{Y}_J(t_i)\\) is multivariate hypergeometric distributed. That means we can derive the variance and the covariance for these random variables. I’ll spare the details here. Given the result that in the two-group test, \\(Z_1(\\tau) = -Z_2(\\tau)\\), we might expect the \\(Z_j(\\tau)\\) to be linearly dependent. This is indeed the case, which we can see from the fact that the sum of all \\(Z_j(\\tau)\\) is zero. Then we might ask how do we construct a test statistic from a degenerate random variable. The answer is that we choose \\(J-1\\) of the statistics, and it doesn’t matter which statistics we choose. Given the covariance matrix \\(\\Sigma\\), we can construct a quadratic form: \\[\\begin{align}\n    \\chi^2 & = (Z_1(\\tau), Z_2(\\tau), \\dots, Z_{J-1}(\\tau)) \\Sigma^{-1} (Z_1(\\tau), Z_2(\\tau), \\dots, Z_{J-1}(\\tau))^T\n\\end{align}\\] which, under \\(H_0\\), is asymptotically distributed \\(\\chi^2\\) with \\(J-1\\) degrees of freedom.\nLet \\(\\mathbf{Z}(\\tau) = (Z_1(\\tau), Z_2(\\tau), \\dots, Z_{J}(\\tau))^T\\) and let \\(\\Sigma = \\text{Cov}(\\mathbf{Z}(\\tau))\\). To show why it doesn’t matter which groups we choose, imagine we have two matrices \\(A\\in\\R^{J-1 \\times J}\\) and \\(B\\in\\R^{J-1 \\times J}\\) which, when left multiplying the vector\\(\\mathbf{Z}(\\tau)\\) select subsets of the \\(J-1\\) groups. An example of \\(A\\) for \\(J = 3\\) might be: \\[\\begin{align}\n    \\begin{bmatrix}\n        1 & 0 & 0 \\\\\n        0 & 1 & 0\n    \\end{bmatrix}\n\\end{align}\\] Let both \\(A\\) and \\(B\\) be rank \\(J - 1\\). We define \\(\\chi^2_A\\) to be \\[\\begin{align}\n    \\chi^2_A & = (A \\mathbf{Z}(\\tau))^T (A \\Sigma A^T)^{-1} A\\mathbf{Z}(\\tau) \\\\\n    \\chi^2_B & = (B \\mathbf{Z}(\\tau))^T (B \\Sigma B^T)^{-1} B\\mathbf{Z}(\\tau)\n\\end{align}\\] As \\(A\\) and \\(B\\) are full-row-rank there exists an invertible matrix \\(C\\) such that \\(B = C A\\). Then \\[\\begin{align}\n    \\chi^2_B & = (C A \\mathbf{Z}(\\tau))^T (C A \\Sigma A^T C^T)^{-1} C A \\mathbf{Z}(\\tau) \\\\\n    & = \\mathbf{Z}(\\tau))^T A^T C^T (C^T)^{-1}(A \\Sigma A^T)^{-1} C^{-1} C A \\mathbf{Z}(\\tau)  \\\\\n    & = \\mathbf{Z}(\\tau))^T A^T (A \\Sigma A^T)^{-1} A \\mathbf{Z}(\\tau)   \\\\\n    & = (A \\mathbf{Z}(\\tau))^T (A \\Sigma A^T)^{-1} A \\mathbf{Z}(\\tau)    \\\\\n    & = \\chi^2_A\n\\end{align}\\]"
  },
  {
    "objectID": "survival-material/lecture-7.html",
    "href": "survival-material/lecture-7.html",
    "title": "Lecture 7",
    "section": "",
    "text": "1 Parametric and nonparametric regression models\nThis chapter combines content from (Aalen, Borgan, and Gjessing 2008), (Klein, Moeschberger, et al. 2003), (Harrell et al. 2001), (Collett 1994), and (Keener 2010).\nThus far we have dealt exclusively with simple univariate estimation. More often than not, we will also have covariates associated with our failure time observations. Let the observed failure data, be, as usual \\(X_i\\) is time to failure, \\(C_i\\) is time to censoring, \\(T_i = \\min(X_i, C_i)\\), is the observed event time, and \\(\\delta = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\) is the censoring indicator. Suppose we also have covariates for each individual \\(i\\) \\(\\mathbf{z}_i \\in \\R^k\\). These could be age, sex at birth, comorbidities. Over a short enough timespan, these covariates can be considered fixed over time. Other covariates, like blood pressure, or time since last colonoscopy, would be time varying covariates, which we’ll denote as \\(\\mathbf{z}(x)_i\\).\nMuch of our study has been on the hazard function \\(\\lambda(t)\\). We’ll consider this parameterized by a vector of parameters \\(\\boldsymbol{\\theta}\\), so we’ll write \\(\\lambda(t \\mid \\boldsymbol{\\theta})\\) for the hazard function. In order to incorporate covariates into the hazard rate, we’ll work with relative risk regression, or \\[\\lambda_i(t) = \\lambda_0(t \\mid \\boldsymbol{\\theta}) r(\\boldsymbol{\\beta}, \\mathbf{z}_i)\\] where \\(r\\) is a function \\(\\R \\to \\R^+\\). Note that this assumes that all individuals share a common baseline hazard, \\(\\lambda_0(t \\mid \\boldsymbol{\\theta})\\), and have time-invariant, individual relative risk contributions \\(r(\\boldsymbol{\\beta},\\boldsymbol{z}_i)\\). A common choice is that \\(r(\\boldsymbol{\\beta},\\boldsymbol{z}_i) \\equiv \\exp(\\boldsymbol{z}_i^T \\boldsymbol{\\beta})\\).\nThe function is called the relative risk function because when we compare the hazard rates for two individuals \\(i\\) and \\(j\\), the common baseline hazard drops out of the comparison: \\[\\frac{\\lambda_i(t)}{\\lambda_j(t)} = \\exp(\\boldsymbol{z}_i)^T \\boldsymbol{\\beta}) / \\exp(\\boldsymbol{z}_j^T \\boldsymbol{\\beta}).\\] Of course, the above holds with general \\(r(\\boldsymbol{\\beta},\\boldsymbol{z}_i)\\). Let’s see what this implies for the survival function for \\(i\\) vs. \\(j\\): \\[\\begin{align*}\n    S_i(t) & = \\exp\\left(-\\int_0^t  e^{\\mathbf{z}_i^T \\boldsymbol{\\beta}} \\lambda_0(u \\mid \\boldsymbol{\\theta}) du\\right)\\\\\n    & = \\exp\\left(-\\int_0^t \\lambda_0(u \\mid \\boldsymbol{\\theta}) du \\right)^{e^{\\mathbf{z}_i^T \\boldsymbol{\\beta}}} \\\\\n    & = \\left(\\exp\\left(-\\int_0^t \\lambda_0(u \\mid \\boldsymbol{\\theta}) du\\right)^{e^{\\mathbf{z}_j^T \\boldsymbol{\\beta}}}\\right)^{\\frac{e^{\\mathbf{z}_i^T \\boldsymbol{\\beta}}}{e^{\\mathbf{z}_j^T \\boldsymbol{\\beta}}}}  \\\\\n    & = \\left(\\exp\\left(-\\int_0^t \\lambda_0(u \\mid \\boldsymbol{\\theta}) du\\right)^{e^{\\mathbf{z}_j^T \\boldsymbol{\\beta}}}\\right)^{e^{(\\mathbf{z}_i^T - \\mathbf{z}_j^T) \\boldsymbol{\\beta}}}  \\\\\n    & = S_j(t)^{e^{(\\mathbf{z}_i^T - \\mathbf{z}_j^T) \\boldsymbol{\\beta}}}\n\\end{align*}\\] What this means is that the survival curves never cross. To see why, note that \\(S_i(0) = S_j(0) = 1\\), and WLOG, suppose \\((\\mathbf{z}_i^T - \\mathbf{z}_j^T) \\boldsymbol{\\beta} \\leq 0\\). Then \\(S_i(t) \\geq S_j(t)\\) for all \\(t\\). See (Figure 1) for a demonstration of proportional hazards.\n\n\n\n\n\n\n\n\nFigure 1: Example of survival functions with proportional\n\n\n\n\n\nSee (Figure 1) for a demonstration of proportional hazards and (Figure 2) for a demonstration of survival functions which do not exhibit proportional hazards.\n\n\n\n\n\n\n\n\nFigure 2: Example of survival functions that do not adhere to proportional hazards\n\n\n\n\n\nProportional hazards (or relative risk) models assume that the survival functions never cross, which is a strong assumption.\nLet’s do a simple example.\n\nExample 1.1. Simple exponential regressionThe following example is adapted from (Collett 1994). Suppose we have individuals grouped into two groups, groups 1 and 2, and let \\(\\mathbf{z}_i\\) equal \\(1\\) for those in group 2 and \\(0\\) for those in group 1. Suppose further we have noninformative censoring, parameter separability, and exponentially distributed survival times with common baseline hazard of \\(\\lambda\\), so we have observed the following dataset: \\[\\{(t_i, \\delta_i, z_i), i = 1, \\dots, n\\}\\] Then the hazard rate for group \\(1\\) is \\(\\lambda\\), while the hazard in group \\(2\\) is \\(\\lambda e^\\beta\\). Let \\(n_1 = \\sum_i (1 - z_i)\\) and \\(n_2 = \\sum_i z_i\\). Then the likelihood contribution for the individuals for whom \\(z_i = 0\\) is \\[\\prod_{i \\mid z_i = 0} \\lambda^{\\delta_i} e^{-\\lambda t_i}\\] and the likelihood contribution for individuals in group 2 is \\[\\prod_{i \\mid z_i = 1} (\\lambda e^\\beta)^{\\delta_i} e^{-\\lambda e^{\\beta} t_i}\\] We can simplify this. Let \\(r_1 = \\sum_i (1 - z_i) \\delta_i\\), and let \\(r_2 = \\sum_i z_i \\delta_i\\). Let \\(T_1 = \\sum_i (1 - z_i) t_i\\), and \\(T_2 = \\sum_i z_i t_i\\). Then the joint likelihood may be written: \\[\\lambda^{r_1} e^{-\\lambda T_1} (\\lambda e^{\\beta})^{r_2} e^{-\\lambda e^\\beta T_2} = \\lambda^{r_1 + r_2} e^{-\\lambda T_1} e^{r_2 \\beta} e^{-\\lambda e^\\beta T_2}.\\] Let \\(\\ell(\\lambda, \\beta)\\) be the log-likelihood function. Then the score equations are \\[\\begin{align*}\n\\frac{\\partial}{\\partial \\lambda} \\ell(\\lambda, \\beta) &: \\frac{r_1 + r_2}{\\lambda} - T_1 - e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta} \\ell(\\lambda, \\beta) &: r_2 - \\lambda e^\\beta T_2\n\\end{align*}\\] solving these for the unknowns is \\[\\begin{align*}\n\\frac{r_1 + r_2}{T_1 + e^\\beta T_2} = \\lambda \\\\\n\\frac{r_2}{\\lambda T_2} = e^\\beta\n\\end{align*}\\] which simplifies to \\[\\begin{align*}\n\\hat{\\lambda} & = \\frac{r_1}{T_1} \\\\\n\\hat{e^\\beta} & = \\frac{T_1/r_1}{T_2/r_2} \\\\\n& = \\frac{r_2}{T_2}\\frac{T_1}{r_1}\n\\end{align*}\\] These estimates make sense: The first is the reciprocal of the average survival time for those in Group 1, and the second is the ratio of the average survival times in each group.\nWe can show using [exmp:mle-exp] that both of these estimators converge a.s. to the true values. \\(\\frac{r_2}{T_2} \\overset{\\text{a.s.}}{\\to} \\lambda e^{\\beta}\\), \\(\\frac{T_1}{r_1} \\overset{\\text{a.s.}}{\\to} \\frac{1}{\\lambda}\\)\nLet’s find the asymptotic variance of the estimand \\(\\beta\\)\n\\[\\begin{align}\n\\frac{\\partial}{\\partial \\lambda}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -\\frac{r_1 + r_2}{\\lambda^2} \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\lambda}\\ell(\\lambda, \\psi)\\right)& = -e^\\beta T_2 \\\\\n\\frac{\\partial}{\\partial \\beta}\\left(\\frac{\\partial}{\\partial \\beta}\\ell(\\lambda, \\psi)\\right)& = -\\lambda e^\\beta T_2\n\\end{align}\\] Then the observed information matrix is \\[\\begin{align}\n    \\begin{bmatrix}\n        \\frac{r_1 + r_2}{\\lambda^2} & e^\\beta T_2 \\\\\n        e^\\beta T_2 & \\lambda e^\\beta T_2\n    \\end{bmatrix}\n\\end{align}\\] which has the inverse: \\[\\begin{align}\n    \\frac{1}{\\frac{(r_1 + r_2)e^\\beta T_2}{\\lambda} - e^{2 \\beta} T_2^2}\\begin{bmatrix}\n      \\lambda e^\\beta T_2  & -e^\\beta T_2 \\\\\n        -e^\\beta T_2 & \\frac{r_1 + r_2}{\\lambda^2}\n    \\end{bmatrix}\n\\end{align}\\] So the plug-in standard error for \\(\\beta\\) is \\[\\sqrt{\\frac{\\frac{r_1 + r_2}{\\lambda^2}}{\\frac{(r_1 + r_2)e^\\beta T_2}{\\lambda} - e^{2 \\beta} T_2^2}}\\] Plugging in the MLEs gives \\[\\sqrt{\\frac{\\frac{r_1 + r_2}{(r_1 / T_1)^2}}{\\frac{(r_1 + r_2)\\frac{T_1r_2}{r_1}}{r_1 / T_1} - (\\frac{T_1r_2}{r_1})^2 }} = \\sqrt{\\frac{r_1 + r_2}{r_1 r_2}}\\] We can use this expression to generate an asymptotic confidence interval for \\(\\beta\\): \\[\\begin{align*}\n    P(\\beta \\in C^\\beta) = P\\left(\\beta \\in \\left(e^{\\hat{\\beta}} - z_{1-\\alpha/2} \\sqrt{\\frac{r_1 + r_2}{r_1 r_2}}, e^{\\hat{\\beta}} + z_{1-\\alpha/2} \\sqrt{\\frac{r_1 + r_2}{r_1 r_2}}\\right)\\right)\n\\end{align*}\\]\n\n\n\n\n\n\nReferences\n\nAalen, Odd, Ornulf Borgan, and Hakon Gjessing. 2008. Survival and Event History Analysis: A Process Point of View. Springer Science & Business Media.\n\n\nCollett, David. 1994. Modelling Survival Data in Medical Research. Chapman & Hall.\n\n\nHarrell, Frank E et al. 2001. Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis. Vol. 608. Springer.\n\n\nKeener, Robert W. 2010. Theoretical Statistics. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-93839-4.\n\n\nKlein, John P, Melvin L Moeschberger, et al. 2003. Survival Analysis: Techniques for Censored and Truncated Data. Vol. 1230. Springer."
  },
  {
    "objectID": "survival-material/lecture-8.html",
    "href": "survival-material/lecture-8.html",
    "title": "Lecture 8",
    "section": "",
    "text": "In the preceding example, we shied away from using the Fisher information because \\(T_2\\) was not easily accessible. But we can use the results from [[exmp:mle-exp]] to derive an exact expression for the asymptotic sampling variance for the MLE."
  },
  {
    "objectID": "survival-material/lecture-8.html#asymptotic-confidence-intervals",
    "href": "survival-material/lecture-8.html#asymptotic-confidence-intervals",
    "title": "Lecture 8",
    "section": "1.1 Asymptotic confidence intervals",
    "text": "1.1 Asymptotic confidence intervals\nFor the most part, we’ll be concerned with univariate confidence intervals, but in multivariate models like the Weibull distribution we’ll need to compute the full inverse of the Fisher information. WLOG, let the index of the parameter of interest be \\(1\\), so the asymptotic variance of our MLE for the parameter of interest is \\(\\sigma_1^2(\\theta^\\dagger) = \\mathcal{I}(\\theta^\\dagger)^{-1}_{1,1}\\). We can also define \\[\\sigma_1^2(\\hat{\\theta}) = \\mathcal{I}(\\hat{\\theta})^{-1}_{1,1}.\\] I’ll also ditch the \\(n\\) subscript and just let \\(\\hat{\\theta}\\) be our MLE based on \\(n\\) observations. By [eq:cont-map], \\[\\frac{\\sigma_1^2(\\hat{\\theta})}{\\sigma_1^2(\\theta^\\dagger)} \\overset{p}{\\to} 1.\\] This allows us to use a plug-in estimator for \\(\\mathcal{I}(\\theta^\\dagger)^{-1}\\), \\(\\mathcal{I}(\\hat{\\theta})^{-1}\\). \\[\\begin{align*}\n     \\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta_1^\\dagger)}{\\sigma_1(\\hat{\\theta})} & = \\frac{\\sigma_1(\\theta^\\dagger)}{\\sigma_1(\\hat{\\theta})}\\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta_1^\\dagger)}{\\sigma_1(\\theta^\\dagger)} \\\\\n     & \\overset{d}{\\to} \\mathcal{N}(0, 1)\n\\end{align*}\\] Using [eq:prod-slutsky], we can create an asymptotic confidence interval by noting that: \\[P\\left(\\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta^\\dagger_1)}{\\sigma_1(\\hat{\\theta})} \\leq x\\right) = \\Phi(x),\\] where \\(\\Phi(x)\\) is the CDF a normal distribution with zero mean and unit variance.\nThen \\[\\begin{align*}\nP\\left(\\frac{\\sqrt{n}(\\hat{\\theta}_1 - \\theta^\\dagger_1)}{\\sigma_1(\\hat{\\theta}_1)} \\in (-z_{1-\\alpha/2},z_{1-\\alpha/2})\\right) & = P\\left(\\theta_1^\\dagger \\in \\left(\\hat{\\theta}_1 - z_{1-\\alpha/2} \\frac{\\sigma_1(\\hat{\\theta}_1)}{\\sqrt{n}}, \\hat{\\theta}_1 + z_{1-\\alpha/2} \\frac{\\sigma_1(\\hat{\\theta}_1)}{\\sqrt{n}}\\right)\\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-8.html#asymptotic-tests",
    "href": "survival-material/lecture-8.html#asymptotic-tests",
    "title": "Lecture 8",
    "section": "1.2 Asymptotic tests",
    "text": "1.2 Asymptotic tests\n\n1.2.1 Wald test\nThe Wald test is derived directly from the asymptotic distribution of the MLE. Under the null hypothesis \\(\\theta^\\dagger = \\theta_0\\), the test statistic: \\[\\begin{align*}\n     \\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta_0)^{-1})\n\\end{align*}\\] so \\[n (\\hat{\\theta}_n - \\theta_0)^T \\mathcal{I}(\\theta_0) (\\hat{\\theta}_n - \\theta_0) \\sim \\chi^2(p)\\] This follows from the simple fact that if a random vector in \\(\\R^n\\), \\(Z\\), is distributed multivariate normal, or \\(Z \\sim \\mathcal{N}(0, \\Sigma)\\), then \\(\\Sigma^{-1/2} Z \\sim \\mathcal{N}(0, I)\\), so \\(Z^T\\Sigma^{-1/2}\\Sigma^{-1/2}Z = \\sum_{i=1}^n X_i^2\\) where \\(X_i \\sim \\mathcal{N}(0,1)\\).\n\n\n1.2.2 Rao’s score test\nIn our proof of the asymptotic distribution of the MLE, we used the fact that \\[\\sqrt{n}\\frac{1}{n} \\sum_{i=1}^n (\\nabla_\\theta \\log f_\\theta(X_i))\\mid_{\\theta = \\theta^\\dagger} \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger)).\\] This idea can be used to derive the Rao’s Score test, which uses the fact that under \\(H_0: \\theta \\in \\Theta_0\\), the gradient evaluated at the restricted MLE (i.e. the MLE restricted to the parameter space \\(\\Theta_0\\)) is nearly zero, and we can recover a similar limiting distribution. As above let \\[\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\theta^\\dagger} = \\sqrt{n}\\frac{1}{n} \\sum_{i=1}^n (\\nabla_\\theta \\log f_\\theta(X_i))\\mid_{\\theta = \\theta^\\dagger}\\] Assuming that under the null distribution the restrited MLE \\(\\hat{\\theta}_0\\) is consistent for \\(\\theta^\\dagger \\in \\Theta_0\\), then \\[\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta^\\dagger))\\] The Score test statistic is: \\[T_S = \\left(\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0} \\right)^T \\mathcal{I}(\\hat{\\theta}_0)^{-1}\\frac{1}{\\sqrt{n}} \\nabla_\\theta \\ell(\\theta) \\mid_{\\theta = \\hat{\\theta}_0}\\] This test statistic is distribution \\(\\chi^2(p)\\) under \\(H_0\\).\n\n\n1.2.3 Likelihood ratio test\nThe LRT comes from a two-term asymptotic expansion of the log-likelihood, as opposed to the one term expansion: \\[\\begin{align*}\n    -\\ell(\\theta_0) & = -\\ell(\\hat{\\theta}) - \\nabla_\\theta \\ell(\\theta)\\mid_{\\theta=\\hat{\\theta}}(\\hat{\\theta}-\\theta_0) - \\frac{1}{2}(\\hat{\\theta}-\\theta_0)^T\\nabla^2_\\theta \\ell(\\theta)\\mid_{\\theta=\\tilde{\\theta}}(\\hat{\\theta}-\\theta_0) \\\\\n    \\ell(\\hat{\\theta}) - \\ell(\\theta_0)  & = -\\frac{1}{2}(\\hat{\\theta}-\\theta_0)^T\\nabla^2_\\theta \\ell(\\theta)\\mid_{\\theta=\\tilde{\\theta}}(\\hat{\\theta}-\\theta_0) \\\\\n    & = \\frac{1}{2}(\\sqrt{n}(\\hat{\\theta}-\\theta_0))^T\\frac{-\\nabla^2_\\theta \\ell(\\theta)\\mid_{\\theta=\\tilde{\\theta}}}{n}(\\sqrt{n}(\\hat{\\theta}-\\theta_0)) \\\\\n\\end{align*}\\] As before, \\[\\sqrt{n}(\\hat{\\theta}-\\theta_0) \\overset{d}{\\to} \\mathcal{N}(0, \\mathcal{I}(\\theta_0)^{-1})\\] and \\[-\\frac{\\nabla^2_\\theta \\ell(\\theta)\\mid_{\\theta=\\tilde{\\theta}}}{n} \\overset{p}{\\to} \\mathcal{I}(\\theta_0)\\] so \\[2 (\\ell(\\hat{\\theta}) - \\ell(\\theta_0)) \\overset{d}{\\to} \\chi^2(p)\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-8-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-8-notes.html",
    "title": "Missing data lecture 5: Bayes",
    "section": "",
    "text": "If \\(\\hat{\\theta}\\) is the MLE then the MLE for a function of \\(\\theta\\), say \\(g(\\theta)\\), is just \\(g(\\hat{\\theta})\\).\nBayesian (in)variance:\nLet \\(\\eta = g(\\theta)\\), and assume for simplicity’s sake that \\(g\\) is one-to-one. Then \\(\\theta = g^{-1}(\\eta)\\). If \\(\\theta\\) has posterior \\(p(\\theta \\mid y)\\), the posterior for \\(g(\\theta)\\) is:\n\\[\np(g^{-1}(\\eta) \\mid y) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\]\nThis can lead to contradictions under ``ignorance”.\nThis presentation follows Gelman et al. (2013) somewhat.\nThere are priors called Jeffreys’ priors (for Harold Jeffreys) that are invariant to reparameterizations. Remember that the Fisher information, or: \\[\n\\mathcal{I}(\\theta) = \\Exp{\\nabla_\\theta \\ell_Y(\\theta \\mid y)\\nabla_\\theta \\ell_Y(\\theta \\mid y)^T}\n\\] under a reparameterization \\(\\eta = g(\\theta)\\) with Jacobian \\((J_{\\eta,\\theta})_{ij} = \\frac{\\partial \\eta_i}{\\partial \\theta_j}\\) is: \\[\n\\mathcal{I}(\\theta(\\eta)) = J_{\\eta,\\theta}^T\\Exp{(\\nabla_\\theta \\ell_Y(\\theta \\mid y))\\mid_{\\theta = g^{-1}(\\eta)} \\nabla_\\theta (\\ell_Y(\\theta \\mid y)\\mid_{\\theta = g^{-1}(\\eta))}^T}J_{\\eta,\\theta}\n\\] For \\(\\eta = g(\\theta)\\), assume for simplicity that \\(g\\) is one-to-one, then a prior for \\(\\theta\\) that is proportional to the square root of the determinant of the Fisher information will be invariant to reparameterization:\n\\[\np(\\theta) \\propto \\det(\\mathcal{I}(\\theta))^{1/2}\n\\] Why is this the case? Because under the change of measure formula above the prior for \\(\\eta\\) is: \\[\np(\\eta) \\propto p(g^{-1}(\\eta)) \\det J_{\\eta,\\theta}\n\\] which is \\[\n\\begin{aligned}\np(\\eta) & \\propto \\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det J_{\\eta,\\theta} \\\\\n& \\propto \\det (J_{\\eta,\\theta})^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T)^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T\\mathcal{I}(g^{-1}(\\eta))^{1/2}J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det(\\mathcal{I}(\\eta))^{1/2}\n\\end{aligned}\n\\] Thus giving some sense of invariance under a coordinate change. As stated in Gelman et al. (2013), more or less:\n\nAny rule for determining the prior density \\(p(\\theta)\\) should yield an equivalent result if aplied to the transformed parameter; that is, \\(p(\\eta)\\) generated using \\(p(\\theta)\\) using the change of measure formula should yield the same prior as would have been obtained directly from the model \\(p(\\eta) p(y \\mid \\eta)\\)\n\nOne issue with Jeffreys’ prior is that it is dependent on a likelihood, which can be controversial.\nFor the Bernoulli trial example from last class, the Jeffreys prior is \\(\\text{Beta}(1/2, 1/2)\\).\n\n\n\nPosterior probabilities are strictly ``right” under our prior assumption because of the math of Bayes’ theorem. However, if we take a Frequentist view of probability, namely that probabilities are defined as limiting proportions of events, we’ll need to think about alternative draws of our prior and of our data.\nThe coverage of our posterior credible intervals will only match the nominal probabilities if the prior we use for our analysis matches that which generated the data. We can show this as computing the marginal posterior \\(p(\\theta \\mid y)\\) under repeated draws from the prior and data distribution \\(p(y \\mid \\theta)\\), which is the distribution associated with the density \\(f_Y(y \\mid \\theta)\\) we’ll use in our posterior:\n\\[\n\\begin{aligned}\n\\theta^\\prime & \\sim p(\\theta) \\\\\ny & \\sim p(y \\mid \\theta^\\prime) \\\\\n\\theta & \\sim p(\\theta \\mid y)\n\\end{aligned}\n\\] Another way to represent this sampling diagram is through integrals:\n\\[\n\\begin{aligned}\n\\int_{\\Omega_\\theta}\\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime) dy \\, d\\theta^\\prime & = \\int_{\\mathcal{Y}} \\int_{\\Omega_\\theta}\\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} \\int_{\\Omega_\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} p(\\theta) f_Y(y \\mid \\theta) \\\\\n& = p(\\theta)\n\\end{aligned}\n\\]\nSee Talts et al. (2018) for more info about how we can use this identity to test whether our algorithms are working correctly.\n\n\n\nLike Frequentist confidence intervals, we can only compute \\(p(\\theta \\mid y)\\) exactly under special circumstances, like conjugate priors. The reason for this is that the integral in the denominator is usually intractable.\nWe will usually have to do approximate inference on Bayesian models by using Markov Chain Monte Carlo samplers, which iteratively generate samples that converge in distribution to the true posterior distribution. Bayesian approximate methods instead operate on an expression that is proportional to the posterior:\n\\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta) p(\\theta)\n\\]\nOne way to think about the MLE is that it is the posterior mode under a prior of \\(p(\\theta) \\propto 1\\): \\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta)\n\\] The difference between the likelihood \\(L_Y(\\theta \\mid y)\\) and the posterior \\(p(\\theta \\mid y)\\) lies in how we treat the expression. In MLE we’re going to maximize the likelihood. In Bayesian inference we care about the full distribution of \\(\\theta\\).\nThis gives some intuition about Bayesian inference. We can think of doing MLE and penalizing certain values of \\(\\theta\\):\n\\[\n\\ell_Y(\\theta \\mid y) + \\text{penalty}(\\theta)\n\\] that will allow the maximizer to favor certain values of \\(\\theta\\) over others.\nIf we look at the implied log-posterior ignoring the constant that doesn’t depend on \\(\\theta\\):\n\\[\n\\log p(\\theta \\mid y) = \\log f_Y(y \\mid \\theta) + \\log p(\\theta)\n\\]\nIf we maximize this expression we can rewrite this as \\[\n\\log p(\\theta \\mid y) = \\ell_Y(\\theta \\mid y) + \\log p(\\theta)\n\\] and we get the penalized likelihood expression where the penalty is a probability density.\nOne question might be: ok, we have a full distribution for \\(\\theta\\). What do we do with it? While the MLE is a single choice, we now have myriad choices for point estimates derived from Bayesian models. We could use the posterior mean: \\[\n\\Exp{\\theta \\mid y}\n\\] We could use the posterior median, \\(\\theta_m\\): \\[\nP(\\theta &gt; \\theta_m \\mid y) = P(\\theta \\leq \\theta_m \\mid y) = 1/2.\n\\]\nWe could use another posterior quantile. We could use the mode of the posterior as well.\nAsymptotically, one might hope that the Bayesian estimates converge to the Frequentist estimates, and this is true, though one needs to be careful in scenarios where the dimensionality of the parameter space increases with sample size and about how one uses priors.\nIn Frequentist inference, the only limits on the parameter space come from the likelihood; the normal density requires that \\(\\mu \\in \\R\\) and \\(\\sigma^2 \\in (0, \\infty)\\). In Bayesian inference, the prior can also restrict the parameter space. For example, in the normal example, one could use a prior for \\(\\mu\\) that enforced \\(\\mu &gt; 0\\). The posterior would then only be able to represent \\(\\mu &gt; 0\\). If the true \\(\\mu\\) were negative, a Bayesian point-estimator wouldn’t converge to the true \\(\\mu\\).\nWhile the prior adds an extra degree of freedom which seems dangerous, it can yield better estimates when there are small datasets, because there isn’t as much information in the data. An example of this would be a simple regression model: \\[\ny_i \\sim \\text{Normal}(X_i^T \\beta, \\sigma^2)\n\\] We might have some good information that we don’t expect \\(\\beta\\) to be nearly infinite, and in fact we expect it to be pretty well concentrated to \\([-10, 10]\\). Then we could use independent \\(\\text{Normal}(0,5^2)\\) priors for the regression coefficients."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-8-notes.html#bayes-recap",
    "href": "missing-data-material-W-26/notes/lecture-8-notes.html#bayes-recap",
    "title": "Missing data lecture 5: Bayes",
    "section": "",
    "text": "If \\(\\hat{\\theta}\\) is the MLE then the MLE for a function of \\(\\theta\\), say \\(g(\\theta)\\), is just \\(g(\\hat{\\theta})\\).\nBayesian (in)variance:\nLet \\(\\eta = g(\\theta)\\), and assume for simplicity’s sake that \\(g\\) is one-to-one. Then \\(\\theta = g^{-1}(\\eta)\\). If \\(\\theta\\) has posterior \\(p(\\theta \\mid y)\\), the posterior for \\(g(\\theta)\\) is:\n\\[\np(g^{-1}(\\eta) \\mid y) \\det \\nabla_{\\eta} g^{-1}(\\eta)\n\\]\nThis can lead to contradictions under ``ignorance”.\nThis presentation follows Gelman et al. (2013) somewhat.\nThere are priors called Jeffreys’ priors (for Harold Jeffreys) that are invariant to reparameterizations. Remember that the Fisher information, or: \\[\n\\mathcal{I}(\\theta) = \\Exp{\\nabla_\\theta \\ell_Y(\\theta \\mid y)\\nabla_\\theta \\ell_Y(\\theta \\mid y)^T}\n\\] under a reparameterization \\(\\eta = g(\\theta)\\) with Jacobian \\((J_{\\eta,\\theta})_{ij} = \\frac{\\partial \\eta_i}{\\partial \\theta_j}\\) is: \\[\n\\mathcal{I}(\\theta(\\eta)) = J_{\\eta,\\theta}^T\\Exp{(\\nabla_\\theta \\ell_Y(\\theta \\mid y))\\mid_{\\theta = g^{-1}(\\eta)} \\nabla_\\theta (\\ell_Y(\\theta \\mid y)\\mid_{\\theta = g^{-1}(\\eta))}^T}J_{\\eta,\\theta}\n\\] For \\(\\eta = g(\\theta)\\), assume for simplicity that \\(g\\) is one-to-one, then a prior for \\(\\theta\\) that is proportional to the square root of the determinant of the Fisher information will be invariant to reparameterization:\n\\[\np(\\theta) \\propto \\det(\\mathcal{I}(\\theta))^{1/2}\n\\] Why is this the case? Because under the change of measure formula above the prior for \\(\\eta\\) is: \\[\np(\\eta) \\propto p(g^{-1}(\\eta)) \\det J_{\\eta,\\theta}\n\\] which is \\[\n\\begin{aligned}\np(\\eta) & \\propto \\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det J_{\\eta,\\theta} \\\\\n& \\propto \\det (J_{\\eta,\\theta})^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T)^{1/2}\\det \\mathcal{I}(g^{-1}(\\eta))^{1/2} \\det (J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det (J_{\\eta,\\theta}^T\\mathcal{I}(g^{-1}(\\eta))^{1/2}J_{\\eta,\\theta})^{1/2} \\\\\n& \\propto \\det(\\mathcal{I}(\\eta))^{1/2}\n\\end{aligned}\n\\] Thus giving some sense of invariance under a coordinate change. As stated in Gelman et al. (2013), more or less:\n\nAny rule for determining the prior density \\(p(\\theta)\\) should yield an equivalent result if aplied to the transformed parameter; that is, \\(p(\\eta)\\) generated using \\(p(\\theta)\\) using the change of measure formula should yield the same prior as would have been obtained directly from the model \\(p(\\eta) p(y \\mid \\eta)\\)\n\nOne issue with Jeffreys’ prior is that it is dependent on a likelihood, which can be controversial.\nFor the Bernoulli trial example from last class, the Jeffreys prior is \\(\\text{Beta}(1/2, 1/2)\\).\n\n\n\nPosterior probabilities are strictly ``right” under our prior assumption because of the math of Bayes’ theorem. However, if we take a Frequentist view of probability, namely that probabilities are defined as limiting proportions of events, we’ll need to think about alternative draws of our prior and of our data.\nThe coverage of our posterior credible intervals will only match the nominal probabilities if the prior we use for our analysis matches that which generated the data. We can show this as computing the marginal posterior \\(p(\\theta \\mid y)\\) under repeated draws from the prior and data distribution \\(p(y \\mid \\theta)\\), which is the distribution associated with the density \\(f_Y(y \\mid \\theta)\\) we’ll use in our posterior:\n\\[\n\\begin{aligned}\n\\theta^\\prime & \\sim p(\\theta) \\\\\ny & \\sim p(y \\mid \\theta^\\prime) \\\\\n\\theta & \\sim p(\\theta \\mid y)\n\\end{aligned}\n\\] Another way to represent this sampling diagram is through integrals:\n\\[\n\\begin{aligned}\n\\int_{\\Omega_\\theta}\\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime) dy \\, d\\theta^\\prime & = \\int_{\\mathcal{Y}} \\int_{\\Omega_\\theta}\\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} \\frac{p(\\theta) f_Y(y \\mid \\theta)}{\\int_{\\Omega_\\theta} p(\\theta) f_Y(y \\mid \\theta) d\\theta} \\int_{\\Omega_\\theta} f_Y(y \\mid \\theta^\\prime) p(\\theta^\\prime)  d\\theta^\\prime \\, dy \\\\\n& = \\int_{\\mathcal{Y}} p(\\theta) f_Y(y \\mid \\theta) \\\\\n& = p(\\theta)\n\\end{aligned}\n\\]\nSee Talts et al. (2018) for more info about how we can use this identity to test whether our algorithms are working correctly.\n\n\n\nLike Frequentist confidence intervals, we can only compute \\(p(\\theta \\mid y)\\) exactly under special circumstances, like conjugate priors. The reason for this is that the integral in the denominator is usually intractable.\nWe will usually have to do approximate inference on Bayesian models by using Markov Chain Monte Carlo samplers, which iteratively generate samples that converge in distribution to the true posterior distribution. Bayesian approximate methods instead operate on an expression that is proportional to the posterior:\n\\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta) p(\\theta)\n\\]\nOne way to think about the MLE is that it is the posterior mode under a prior of \\(p(\\theta) \\propto 1\\): \\[\np(\\theta \\mid y) \\propto f_Y(y \\mid \\theta)\n\\] The difference between the likelihood \\(L_Y(\\theta \\mid y)\\) and the posterior \\(p(\\theta \\mid y)\\) lies in how we treat the expression. In MLE we’re going to maximize the likelihood. In Bayesian inference we care about the full distribution of \\(\\theta\\).\nThis gives some intuition about Bayesian inference. We can think of doing MLE and penalizing certain values of \\(\\theta\\):\n\\[\n\\ell_Y(\\theta \\mid y) + \\text{penalty}(\\theta)\n\\] that will allow the maximizer to favor certain values of \\(\\theta\\) over others.\nIf we look at the implied log-posterior ignoring the constant that doesn’t depend on \\(\\theta\\):\n\\[\n\\log p(\\theta \\mid y) = \\log f_Y(y \\mid \\theta) + \\log p(\\theta)\n\\]\nIf we maximize this expression we can rewrite this as \\[\n\\log p(\\theta \\mid y) = \\ell_Y(\\theta \\mid y) + \\log p(\\theta)\n\\] and we get the penalized likelihood expression where the penalty is a probability density.\nOne question might be: ok, we have a full distribution for \\(\\theta\\). What do we do with it? While the MLE is a single choice, we now have myriad choices for point estimates derived from Bayesian models. We could use the posterior mean: \\[\n\\Exp{\\theta \\mid y}\n\\] We could use the posterior median, \\(\\theta_m\\): \\[\nP(\\theta &gt; \\theta_m \\mid y) = P(\\theta \\leq \\theta_m \\mid y) = 1/2.\n\\]\nWe could use another posterior quantile. We could use the mode of the posterior as well.\nAsymptotically, one might hope that the Bayesian estimates converge to the Frequentist estimates, and this is true, though one needs to be careful in scenarios where the dimensionality of the parameter space increases with sample size and about how one uses priors.\nIn Frequentist inference, the only limits on the parameter space come from the likelihood; the normal density requires that \\(\\mu \\in \\R\\) and \\(\\sigma^2 \\in (0, \\infty)\\). In Bayesian inference, the prior can also restrict the parameter space. For example, in the normal example, one could use a prior for \\(\\mu\\) that enforced \\(\\mu &gt; 0\\). The posterior would then only be able to represent \\(\\mu &gt; 0\\). If the true \\(\\mu\\) were negative, a Bayesian point-estimator wouldn’t converge to the true \\(\\mu\\).\nWhile the prior adds an extra degree of freedom which seems dangerous, it can yield better estimates when there are small datasets, because there isn’t as much information in the data. An example of this would be a simple regression model: \\[\ny_i \\sim \\text{Normal}(X_i^T \\beta, \\sigma^2)\n\\] We might have some good information that we don’t expect \\(\\beta\\) to be nearly infinite, and in fact we expect it to be pretty well concentrated to \\([-10, 10]\\). Then we could use independent \\(\\text{Normal}(0,5^2)\\) priors for the regression coefficients."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-8-notes.html#linear-regression-with-conjugate-priors",
    "href": "missing-data-material-W-26/notes/lecture-8-notes.html#linear-regression-with-conjugate-priors",
    "title": "Missing data lecture 5: Bayes",
    "section": "Linear regression with conjugate priors",
    "text": "Linear regression with conjugate priors\nThis and the following section follow Chapter 2 in Rossi, Allenby, and Misra (2024) quite closely.\nLet’s look at the linear regression model with conjugate priors. \\[\ny_i = x_i^T \\beta + \\epsilon_i, \\quad \\epsilon_i \\overset{\\text{iid}}{\\sim} \\text{Normal}(0, \\sigma^2)\n\\] where \\(x_i \\in \\R^p\\). A full model would imply a model for \\(x_i\\) as well: \\[\nf_{X,Y}((x_1, y_1), \\dots, (x_n, y_n) \\mid \\beta, \\psi) = \\prod_i f_{X}(x_i \\mid \\psi) f_Y(y_i \\mid x_i,\\beta, \\sigma^2)\n\\] If we have a prior for \\(\\psi,\\beta, \\sigma^2\\) that is independent, \\(p(\\psi, \\beta, \\sigma^2) = p(\\psi) p(\\beta, \\sigma^2)\\), then the posterior will factorize into independent distributions as well: \\[\n\\begin{aligned}\np_(\\beta, \\psi,\\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) & \\propto \\prod_i f_{X}(x_i \\mid \\psi) p(\\psi) f_Y(y_i \\mid x_i,\\beta, \\sigma^2) p(\\beta, \\sigma^2) \\\\\n& \\propto (\\prod_i f_{X}(x_i \\mid \\psi) p(\\psi)) \\prod_i f_Y(y_i \\mid x_i,\\beta, \\sigma^2) p(\\beta)  \\\\\n& \\propto p(\\psi \\mid x_1, \\dots, x_n) p(\\beta, \\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) \\\\\n\\end{aligned}\n\\] Remember from last class how we could intuit the form of the joint prior if we examined the likelihood for \\(\\theta\\) and chose a prior with the same functional form as that of the likelihood.\nIn the Bernoulli example, we had a likelihood of the form: \\(L_Y(\\theta \\mid y) = \\theta^{k}(1 - \\theta)^{n - k}\\), where \\(k = \\sum_i = y\\), which suggested a prior of the form \\(\\theta^{a}(1-\\theta)^b\\), which we could regonize as a Beta distribution.\nWe’ll do the same for the regression example. The likelihood for the linear model is: \\[\n(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2}\\sum_i (y_i - x_i^T \\beta)^2 \\rp\n\\] which can be simplified somewhat by writing the sum as a dot product between the vector of errors, \\(e = y - X \\beta\\) where \\(y = (y_1, \\dots, y_n)\\) and \\(X^T = (x_1, \\dots, x_n)\\). \\[\n(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2} (y - X \\beta)^T(y - X \\beta) \\rp\n\\]\nWe can rewrite the term \\((y - X \\beta)^T (y - X \\beta)\\) in terms of the least-squares estimator for \\(\\beta\\), \\(\\hat{\\beta} = (X^T X)^{-1}X^T y\\) by decomposing \\(y\\) as \\(y = X \\hat{\\beta} + y - X \\hat{\\beta}\\):\n\\[\n\\begin{aligned}\n(y - X \\beta)^T (y - X \\beta) & = (X \\hat{\\beta} + y - X \\hat{\\beta} - X \\beta)^T(X \\hat{\\beta} + y - X \\hat{\\beta} - X \\beta) \\\\\n& = (y - X \\hat{\\beta})^T(y - X \\hat{\\beta}) + (X \\beta - X\\hat{\\beta})^T(X \\beta - X\\hat{\\beta}) - 2(X \\beta - X\\hat{\\beta})^T (y - X \\hat{\\beta}) \\\\\n& = (y - X \\hat{\\beta})^T(y - X \\hat{\\beta}) + (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\n\\end{aligned}\n\\]\nLet \\(s^2 = \\frac{1}{n-p}(y-X\\hat{\\beta})^T(y-X\\hat{\\beta})\\), and \\(\\nu = n - p\\), so we can rewrite the sum more compactly as: \\[\n(y - X \\beta)^T (y - X \\beta) = \\nu s^2 + (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\n\\] This leads to a likelihood:\n\\[\nL_Y(\\beta, \\sigma^2 \\mid y, X) \\propto (\\sigma^2)^{-\\nu/2} \\exp\\lp\\frac{\\nu s^2}{2 \\sigma^2}\\rp (\\sigma^2)^{-(n - \\nu) / 2} \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\\rp\n\\] Before we derive conjugate priors from this likelihood, we can see that the posterior under flat priors for \\(\\beta\\) and a prior for \\(\\sigma^2\\), \\(\\sigma^{-2}\\), leads to a posterior: \\[\np(\\beta, \\sigma^2 \\mid y, X) \\propto (\\sigma^2)^{-(\\nu/2+1)} \\exp\\lp\\frac{\\nu s^2}{2 \\sigma^2}\\rp (\\sigma^2)^{-(n - \\nu) / 2} \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\hat{\\beta})^T X^T X (\\beta - \\hat{\\beta})\\rp\n\\] which is a conditional normal posterior for \\(\\beta\\) with a scaled inverse chi-squared posterior for \\(\\sigma^2\\).\nThis suggests a conjugate prior of the form \\(p(\\beta,\\sigma^2) = p(\\sigma^2)p(\\beta \\mid \\sigma^2)\\): \\[\np(\\sigma^2) \\propto (\\sigma^2)^{-(\\nu_0/2 + 1)} \\exp\\lp \\frac{\\nu_0 s_0}{2 \\sigma^2} \\rp\n\\]\nand a conditional normal prior for \\(\\beta\\):\n\\[\np(\\beta \\mid \\sigma^2) \\propto (\\sigma^2)^{-p / 2} \\exp\\lp-\\frac{1}{2\\sigma^2}(\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta - \\mu_0) \\rp\n\\]\nThis can be seen as the posterior from a regression run with a prior of \\(p(\\sigma^2) \\propto \\sigma^{-2}\\) and a flat prior on \\(\\beta\\).\nThen the posterior for \\(\\sigma^2, \\beta\\) is simply the product of the priors and the likelihood, which we write as above:\n\\[\n\\begin{aligned}\np(\\beta, \\sigma^2 \\mid (x_1, y_1),\\dots,(x_n, y_n)) \\propto & (\\sigma^2)^{-(\\nu_0/2 + 1)} \\exp\\lp \\frac{\\nu_0 s_0}{2 \\sigma^2} \\rp(\\sigma^2)^{-p / 2} \\exp\\lp-\\frac{1}{2\\sigma^2}(\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta - \\mu_0) \\rp \\\\\n&(2\\pi \\sigma^2)^{-n/2} \\exp \\lp\\frac{1}{2 \\sigma^2} (y - X \\beta)^T(y - X \\beta) \\rp\n\\end{aligned}\n\\]\nThis is definitley formidable, but we can simplify things a bit by collecting the terms with \\(\\beta\\):\n\\[\n(y - X\\beta)^T (y - X\\beta) + (\\mu_0 - \\beta)^T \\Sigma_0^{-1}(\\mu_0 - \\beta)\n\\] and decomposing \\(\\Sigma_0^{-1} = L^T L\\), and noting that we can write the sum as the following inner product: \\[\n\\begin{aligned}\n\\begin{bmatrix}\n(y - X\\beta)^T & (L\\mu_0 - L\\beta)^T\n\\end{bmatrix}\n\\begin{bmatrix}\n(y - X\\beta) \\\\\nL \\mu_0 - L\\beta)\n\\end{bmatrix}\n\\end{aligned}\n\\] This can be further simplified by constructing a vector \\[\nu = \\begin{bmatrix} y \\\\ L\\mu_0 \\end{bmatrix}\n\\] and a matrix \\(W\\) \\[\nW = \\begin{bmatrix} X \\\\ L \\end{bmatrix}\n\\] and writing the expresssion as \\((u - W\\beta)^T(u - W\\beta)\\). We can then use the same trick as above, by representing \\(u\\) as the projection into the column space of \\(W\\) and the residual:\n\\[\n(W \\bar{\\beta} + u - W \\bar{\\beta} - W \\beta)^T(W \\bar{\\beta} + u - W \\bar{\\beta} - W \\beta)\n\\] The expression for \\(\\bar{\\beta}\\) is:\n\\[\n\\begin{aligned}\n\\bar{\\beta} & = (X^T X + L^T L)^{-1}(X^T y + L^T L \\mu_0) \\\\\n& = (X^T X + \\Sigma_0^{-1})^{-1}(X^T y + \\Sigma_0^{-1} \\mu_0)\n\\end{aligned}\n\\]\nWhich simplifies as\n\\[\n(u - W \\bar{\\beta})^T(u - W \\bar{\\beta}) +(\\beta - \\bar{\\beta})^T W^T W (\\beta - \\bar{\\beta})\n\\] and after some algebra comes to \\[\n(y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) + (\\mu_0 - \\bar{\\beta})^T\\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta}) + (\\beta - \\bar{\\beta})^T (X^T X + \\Sigma_0^{-1}) (\\beta - \\bar{\\beta})\n\\] In the following, let \\(n s^2 = (y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) + (\\mu_0 - \\bar{\\beta})^T\\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta})\\). The posterior is:\n\\[\n\\begin{aligned}\np(\\beta, \\sigma^2 \\mid y, X) \\propto & (\\sigma^2)^{-(n + \\nu_0)/2 + 1} \\exp\\lp\\frac{(n + \\nu_0)(n s^2 + \\nu_0 s_0^2)/(n + \\nu_0)}{2 \\sigma^2}\\rp \\times (\\sigma^2)^{-p / 2} \\\\\n& \\exp\\lp-\\frac{1}{2 \\sigma^2} (\\beta - \\bar{\\beta})^T (X^T X + \\Sigma_\\beta^{-1})(\\beta - \\bar{\\beta})\\rp\n\\end{aligned}\n\\] \\[\n\\bar{\\beta} = (X^T X + \\Sigma_\\beta^{-1})^{-1}(\\Sigma_\\beta^{-1} \\mu_\\beta + X^T X \\hat{\\beta})\n\\] Like the Bernoulli problem, the posterior mean for \\(\\beta\\) is a weighted average between the prior mean and the information from the likelihood, which in this case is the least-squared estimator for \\(\\beta\\). This is often a consequence of using conjugate priors, that the posterior is a compromise between the prior and the likelihood.\nThis implies the following distributions for \\(\\sigma^2\\) and \\(\\beta \\mid \\sigma^2\\): \\[\n\\begin{aligned}\n\\sigma^2 & \\sim \\text{Inv-}\\chi^2\\lp n + \\nu_0, \\frac{n s^2 + \\nu_0 s^2_0}{n + \\nu_0}\\rp \\\\\n\\beta \\mid \\sigma^2 & \\sim \\text{Normal}(\\bar{\\beta}, \\sigma^2 \\lp X^T X + \\Sigma_0^{-1} \\rp^{-1})\n\\end{aligned}\n\\]\nThe posterior mean for \\(\\sigma^2\\) is: \\[\n\\Exp{\\sigma^2 \\mid y, X} = \\frac{n + \\nu_0}{n + \\nu_0 - 2}\\frac{n s^2 + \\nu_0 s^2_0}{n + \\nu_0}\n\\] The expression for \\(n s^2\\) is interesting because it involves the squared error of the posterior linear predictor for \\(y\\): \\[\n\\begin{aligned}\n(y - \\Exp{X \\beta \\mid y, X})^T(y - \\Exp{X \\beta \\mid y, X})^T & = (y - X \\Exp{\\beta \\mid y, X})^T(y - X \\Exp{\\beta \\mid y, X}) \\\\\n& = (y - X \\bar{\\beta})^T(y - X \\bar{\\beta}) \\\\\n\\end{aligned}\n\\]\nbut it also involves the error in the prior mean with respect to the prior covariance matrix: \\[\n(\\mu_0 - \\bar{\\beta})^T \\Sigma_0^{-1}(\\mu_0 - \\bar{\\beta})\n\\] The effect of this term will decrease as the number of observations increases, but it elucidates how the posterior mean of the error variance is decomposed into several pieces depending on different aspects of the prior and the data.\n\nBayesian inference in repeated measure models\nTwo lectures ago we went through how to compute the MLE from this regression model:\n\\[\n\\begin{aligned}\ny_{i} \\mid X_{i} \\, & = X_{i} \\beta + \\epsilon_{i} \\\\\n\\epsilon_{i} & \\sim \\text{Normal}(0, \\Sigma) \\\\\n\\epsilon_{i} & \\indy \\epsilon_{j} \\forall i\\neq j.\n\\end{aligned}\n\\]\nThis required sequentially computing the MLE for \\(\\beta\\) given an estimate for \\(\\Sigma\\) and computing \\(\\hat{\\Sigma}\\) given the last estimate for \\(\\hat{\\beta}\\).\nLet’s write down the likelihood for this model to see if we can come up with a conjugate prior for the problem. \\[\nL_{Y}(\\beta, \\Sigma \\mid y, X) \\propto \\det(I_n \\otimes \\Sigma)^{-1/2} \\exp\\lp-\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\\rp\n\\] If we start with the prior for \\(\\beta \\mid \\Sigma\\) we can ignore the determinant and focus on the term in the exponential:\n\\[\n-\\frac{1}{2}(y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta)\n\\]\nLet’s try a multivariate normal prior:\n\\[\n\\beta \\sim \\text{Normal}(\\mu_0, \\Sigma_0)\n\\]\nso we can multiply the likelihood by the prior to get\n\\[\n-\\frac{1}{2}\\lp (y - X \\beta)^T (I_n \\otimes \\Sigma)^{-1}(y - X \\beta) + (\\beta - \\mu_0)^T \\Sigma_0^{-1}(\\beta-\\mu_0) \\rp\n\\] which we’ll rewrite for convenience as \\[\n-\\frac{1}{2}\\lp (A(y - X \\beta))^T A(y - X \\beta) + (L(\\beta - \\mu_0))^T L(\\beta-\\mu_0) \\rp\n\\] where \\(A^T A = (I_n \\otimes \\Sigma)^{-1}\\) and \\(L^T L = \\Sigma_0^{-1}\\).\nThis looks familiar! We can use the same trick as we did above: create a new vector \\(u\\) and matrix \\(W\\): \\[\nu =\n\\begin{bmatrix}\nA y \\\\\nL \\mu_0\n\\end{bmatrix},\\quad\nW =\n\\begin{bmatrix}\nA X \\\\\nL\n\\end{bmatrix}\n\\] and can write \\[\n(u - W\\beta)^T(u - W \\beta) = \\lp (A(y - X \\beta))^T A(y - X \\beta) + (L(\\beta - \\mu_0))^T L(\\beta-\\mu_0) \\rp.\n\\] Furthermore, write \\(u = W\\bar{\\beta} + u - W\\bar{\\beta}\\), where \\(\\bar{\\beta}\\) is the least-squares coeffients of the regression of \\(u\\) on \\(W\\): \\[\n\\bar{\\beta} = (W^T W + L^T L)^{-1}W^T u = (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma_0^{-1})^{-1}(X^T (I_n \\otimes \\Sigma)^{-1}y + \\Sigma_0^{-1} \\mu_0)\n\\] This leads to \\((u - W\\bar{\\beta})^T W = 0\\), which allows us to cleanly partition \\((u-W\\beta)^T (u-W\\beta)\\) into two pieces: \\(u - W\\bar{\\beta}\\) and \\(W\\beta\\):\n\\[\n\\begin{aligned}\n(W\\bar{\\beta} + u - W\\bar{\\beta} - &W \\beta)^T(W\\bar{\\beta} + u - W\\bar{\\beta} - W \\beta) \\\\\n& = (u - W\\bar{\\beta} + W\\bar{\\beta}  - W \\beta)^T(u - W\\bar{\\beta}+ W\\bar{\\beta}  - W \\beta) \\\\\n& = (u - W\\bar{\\beta})^T(u - W\\bar{\\beta}) + (W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta) + 2(u - W\\bar{\\beta})^T(W\\bar{\\beta}  - W \\beta) \\\\\n& = (u - W\\bar{\\beta})^T(u - W\\bar{\\beta}) + (W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta)\n\\end{aligned}\n\\] where the last line follows because \\((u - W\\bar{\\beta})^T W = 0\\). Because we’re focusing only on the posterior, which is a function of \\(\\beta\\) and not data, we can ignore the \\((u - W\\bar{\\beta})^T(u - W\\bar{\\beta})\\) term because it does not involve \\(\\beta\\) and involves only functions of \\(X,y,A,L\\), which are fixed with respect to \\(\\beta\\).\nWe rewrite \\[\n(W\\bar{\\beta}  - W \\beta)^T(W\\bar{\\beta}  - W \\beta)\n\\] as \\[\n(\\beta - \\bar{\\beta})^T W^T W (\\beta - \\bar{\\beta}) = (\\beta - \\bar{\\beta})^T(X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1}) (\\beta - \\bar{\\beta})\n\\] This shows that \\(\\beta \\mid \\Sigma\\) is multivariate normal with: \\[\n\\beta \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1})^{-1})\n\\] Now let’s focus on the conditional distribution of \\(\\Sigma \\mid \\beta\\). We’ll start with the likelihood written in simpler terms: \\[\nL_Y(\\beta, \\Sigma \\mid y, X) \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i (y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta)\\rp\n\\]\nWe can use the trace trick to rearrange things:\n\\[\n\\begin{aligned}\nL_Y(\\beta, \\Sigma \\mid y, X) & \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i (y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta)\\rp \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i \\text{tr}((y_i - X_i \\beta)^T \\Sigma^{-1}(y_i - X_i \\beta))\\rp \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\sum_i \\text{tr}((y_i - X_i \\beta) (y_i - X_i \\beta)^T\\Sigma^{-1})\\rp  \\\\\n& \\propto \\det(\\Sigma)^{-n/2} \\exp\\lp-\\frac{1}{2}\\textstyle \\text{tr}((\\sum_i (y_i - X_i \\beta) (y_i - X_i \\beta)^T)\\Sigma^{-1})\\rp  \\\\\n\\end{aligned}\n\\]\nThis suggests that a conjugate prior for \\(\\Sigma\\) has the form:\n\\[\np(\\Sigma) \\propto \\det(\\Sigma)^{-a/2} \\exp\\lp-\\frac{1}{2}\\text{tr}(V_0 \\Sigma^{-1})\\rp\n\\]\nFortunately, we’re in luck! The Inverse Wishart distribution has the density:\n\\[\np(\\Sigma) \\propto \\det(\\Sigma)^{-(\\nu_0 + p + 1)/2} \\exp\\lp-\\frac{1}{2}\\text{tr}(V_0 \\Sigma^{-1})\\rp\n\\]\nCombining the likelihood with the prior we get something proportional to the conditional posterior for \\(\\Sigma\\):\n\\[\np(\\Sigma \\mid y, X, \\beta) \\propto \\det(\\Sigma)^{-(n + \\nu_0 + p + 1)/2} \\exp\\lp-\\frac{1}{2}\\text{tr}((V_0 + \\textstyle\\sum_i (y_i - X_i \\beta)(y_i - X_i \\beta)^T) \\Sigma^{-1})\\rp\n\\]\nPutting this together we get the following two conditional posteriors:\n\\[\n\\begin{aligned}\n\\beta \\mid \\Sigma, y, X & \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma)^{-1} X + \\Sigma^{-1})^{-1}) \\\\\n\\Sigma \\mid \\beta, y, X & \\sim \\text{Inverse-Wishart}(n + \\nu_0, V_0 + \\textstyle\\sum_i (y_i - X_i \\beta)(y_i - X_i \\beta)^T)\n\\end{aligned}\n\\]\nWe can use the theory of integral operators to show that given intial conditions \\(\\Sigma^0\\) and \\(\\beta^0\\) the following algorithm for \\(t = 1, \\dots, S\\):\n\\[\n\\begin{aligned}\n\\beta^{t+1} \\mid \\Sigma^{t}, y, X & \\sim \\text{Normal}(\\bar{\\beta}, (X^T (I_n \\otimes \\Sigma^t)^{-1} X + (\\Sigma^t)^{-1})^{-1}) \\\\\n\\Sigma^{t+1} \\mid \\beta^{t}, y, X & \\sim \\text{Inverse-Wishart}(n + \\nu_0, V_0 + \\textstyle\\sum_i (y_i - X_i \\beta^t)(y_i - X_i \\beta^t)^T)\n\\end{aligned}\n\\]"
  }
]