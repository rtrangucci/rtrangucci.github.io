[
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "My papers",
    "section": "",
    "text": "Here’s a selection of my published papers:\n\nModeling Racial/ethnic Differences in COVID-19 Incidence with Covariates Subject to Non-random Missingness (Trangucci, Chen, and Zelner 2023)\nRacial Disparities in Coronavirus Disease 2019 (COVID-19) Mortality Are Driven by Unequal Infection Risks (Zelner et al. 2021)\nQuantifying Observed Prior Impact (Jones, Trangucci, and Chen 2021)\nModeling Spatial Risk of Diarrheal Disease Associated with Household Proximity to Untreated Wastewater Used for Irrigation in the Mezquital Valley, Mexico (Contreras Jesse D. et al. 2020)\nBayesian Hierarchical Weighting Adjustment and Survey Inference. (Si 2020)\nEffects of Sequential Influenza A(H1N1)Pdm09 Vaccination on Antibody Waning (Zelner et al. 2019)"
  },
  {
    "objectID": "papers.html#preprints",
    "href": "papers.html#preprints",
    "title": "My papers",
    "section": "Preprints",
    "text": "Preprints\n\nIdentified vaccine efficacy for binary post-infection outcomes under misclassification without monotonicity (Trangucci, Chen, and Zelner 2022)\nBayesian Methods for Modeling Cumulative Exposure to Extensive Environmental Health Hazards (Submitted) (Trangucci et al. 2024)"
  },
  {
    "objectID": "papers.html#conference-papers",
    "href": "papers.html#conference-papers",
    "title": "My papers",
    "section": "Conference papers",
    "text": "Conference papers\n\nHierarchical Gaussian Processes in Stan (Trangucci 2017)\nPrior formulation for Gaussian process hyperparameters. (Trangucci, Betancourt, and Vehtari 2016)"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "STAT 625 - Winter 2024\n\n\n\n\n\nSTAT 625 - Winter 2025\nSTAT 599 - Winter 2025\n\n\n\n\n\nSTAT 625 - Winter 2026\nSTAT 599 - Winter 2026"
  },
  {
    "objectID": "posts/heckman-in-stan.html",
    "href": "posts/heckman-in-stan.html",
    "title": "Heckman models in Stan",
    "section": "",
    "text": "This is cmdstanr version 0.8.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/robertntrangucci/.cmdstan/cmdstan-2.36.0\n\n\n- CmdStan version: 2.36.0\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\nI took a class in the first year of my Master’s program about missing data from Ben Goodrich. The class was all about what to do when you encountered some sort of missing observations when analyzing a dataset. Missingness can arise for many reasons when analyzing data: it can arise from people declining to participate in a survey, survey respondents refusing to answer a question, from patients who drop out of a randomized study, or from the fact that an outcome of interest is only observable for a subset of patients who experience (or don’t experience) an intermediate event.\nWhen dealing with missing data, it is crucial that we understand why an observation, be it an outcome, a covariate, or a unit, is missing. If the\nThe Heckman selection model was one of the first examples we were shown in class of a missing data problem where the probability that an individual had a missing observation depended on the value of the missing observation (Heckman 1979). These are the hardest problems to address in missing data because\nA standard survey example would be that people with very high and those with very low incomes may decline to report income on a survey more than those with moderate incomes. In the vaccine example, the selection effect we are concerned with is that people with weaker immune systems may become infected even with a vaccine and thus have higher viral loads. A naive comparison of viral loads between vaccinated and unvaccinated people might show that the vaccine increases viral loads in those who are vaccinated (Hudgens, Hoering, and Self 2003).\nTo make things a little more concrete, suppose we’re interested in learning about the how vaccines moderate post-infection viral loads. The precise quantity we’re interested in is the change in post-infection viral load for people who would be infected with the pathogen no matter their vaccination status. These unlucky people are referred to as the ``Always-Infected’’ group. We might be tempted to get a rough estimate of this quantity by comparing viral loads in infected people in the vaccinated group vs. those in the unvaccinated group. The problem with this analysis is that if the vaccine has any causal effect on infection the individual characteristics of the two groups, infected-vaccinated people and infected-unvaccinated people, may differ. In this problem missingness arises in two forms. The first is that we observe only one outcome for each participant in our trial: the outcome corresponding to the treatment arm assignment. This is what Holland called the fundamental problem of causal inference. The second is more unique to this setting: we observe viral loads only in those who are infected.\nLet \\(Z_i\\) be the vaccine treatment, with \\(Z_i = 1\\) indicating treatment and \\(Z_i = 0\\) indicating placebo. For the ease of exposition, let’s suppose that treatment is randomized. We don’t need more selection bias (though selection into treatment could very well be a source of additional selection bias). Let \\(S_i(z)\\) be the infection status of individual \\(i\\) with vaccination status \\(z\\), and let \\(Y_i(z)\\) be the viral load of individual \\(i\\). If \\(S_i(z) = 0\\), we set \\(Y_i(z) = *\\).\nSuppose we also observe the study site, denoted as \\(R_i\\). This is the hospital or health clinic that enrolled patient \\(i\\) into the study. This study site is assumed to impact that probability of infection, but it is assumed to be independent of post-infection viral load. The standard term for a variable like \\(R_i\\) is an instrumental variable.\nThe model we’ll simulate data from is the following: \\[\n\\begin{aligned}\n\\log Y_i(z) & = \\mu_Y(z) + \\sigma_z\\, U_i(z) \\\\\n\\tilde{S}_i(z) \\mid R_i & = \\mu_S(R_i) + z \\,\\gamma_S + W_i \\\\\nS_i(z) & = 1(\\tilde{S}_i(z) > 0) \\\\\n(U_i(0),U_i(1),W_i) & \\sim \\text{Normal}(\\mathbf{0}, \\boldsymbol{\\Omega})\n\\end{aligned}\n\\] The individuals who are always infected are identified as those with \\(\\{S_i(1) = 1,S_i(0) = 1\\}\\). One key assumption of this model is that these individuals can be identified with certainty. This is also known as a assumption. To see why, let’s see what our model implies about the always-infected event: \\[\n\\begin{aligned}\n\\{S_i(1) = 1, S_i(0) = 1\\} & = \\{\\tilde{S}_i(1) > 0, \\tilde{S}_i(0) > 0\\} \\\\\n& = \\{\\mu_S(R_i) + \\gamma_S > -W_i, \\mu_S(R_i) > -W_i\\} \\\\\n& = \\{\\min(\\mu_S(R_i) + \\gamma_S,\\mu_S(R_i))  > -W_i\\}\n\\end{aligned}\n\\] The event that \\(S_i(z) = 1\\) is equivalent to \\[\n\\{\\mu_S(R_i) + \\gamma_S > -W_i\\}\n\\] Thus, if \\(\\gamma_S\\) is less than zero, as we would hope in a vaccine efficacy trial, then those who are infected in the vaccine arm are also those who would be infected in the placebo arm. The monotonicity assumption is a consequence of two assumptions of the model: - there is one error term for the probit regression - there is no individual heterogeneity in the coefficient \\(\\gamma_S\\).\nThe likelihood for the model is fairly straightforward: For individuals who remain uninfected, the likelihood term corresponds to \\(P(\\tilde{S}_i(z) \\leq 0)\\), which corresponds to the univariate standard normal CDF evaluated at \\(-\\mu_S(z,r)\\), or: \\[\nP(\\mu_S(z,r) + W_i \\leq 0) = \\Phi(-\\mu_S(z,r))\n\\]\nThe likelihood for an individual \\(i\\) who is infected corresponds to observing \\(\\log Y_i(z) = y_i\\) and \\(\\tilde{S}_i(z) > 0 \\mid \\log Y_i(z) = y_i, R_i = r_i\\): \\[\n\\begin{aligned}\nL_i(\\theta) & = \\frac{1}{\\sqrt{2\\pi} \\sigma_z}\\exp\\left(-\\frac{1}{2\\sigma_z^2}(y_i - \\mu_Y(z_i))^2\\right) \\\\\n& \\Phi\\left(\\frac{\\mu_S(z_i) + \\frac{\\rho_{z_i}}{\\sigma_z}(y_i - \\mu_Y(z_i))}{\\sqrt{1 - \\rho_{z_i}^2}}\\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(\\rho_z = \\text{Cor}(W_i,U_i(z))\\).\nTo get a sense for what this model implies about the treatment effect, let’s examine the conditional expectation of \\(Y_i(z)\\) given \\(\\tilde{S}_i(z) > 0\\) when a patient is vaccinated vs. when the patient is unvaccinated:\n\\[\n\\tau^\\text{naive} = \\mathbb{E} \\left[Y_i(1) \\mid \\tilde{S}_i(1) > 0 \\right] - \\mathbb{E} \\left[Y_i(0) \\mid \\tilde{S}_i(0) > 0 \\right]\n\\] This is what we’ll call the naive estimand, because it conditions on a post-treatment outcome, namely infection, which can induce selection bias, as we’ll see.\nThe expectation \\(\\mathbb{E} \\left[Y_i(z) \\mid \\tilde{S}_i(z) > 0 \\right]\\) is the following:\n\\[\n\\begin{aligned}\n\\frac{(2 \\pi\\sigma_z^2)^{-1/2}}{\\Phi(\\mu_S(z_i))}\\int_{-\\infty}^{\\infty}  \\exp\\left(y-\\frac{1}{2\\sigma_z^2}(y - \\mu_Y(z_i))^2\\right)\n\\Phi\\left(\\frac{\\mu_S(z_i) + \\frac{\\rho_{z_i}}{\\sigma_z}(y - \\mu_Y(z_i))}{\\sqrt{1 - \\rho_{z_i}^2}}\\right) dy\n\\end{aligned}\n\\] To my knowledge, this doesn’t have a closed form expression because of the exponentiation of \\(y\\), though there might be a clever way to evaluate this integral. We can, however, use R’s 1-d numerical integration routine to approximate this expectation.\nThe code below evaluates \\(\\tau^\\text{naive}\\), as well as\n\ncond_exp <- function(y, mu_y, mu_s, sd_y, rho) {\n  exp(y + dnorm(y, mu_y, sd_y, log = TRUE) + pnorm((mu_s + rho / sd_y * (y - mu_y))/sqrt(1 - rho^2),log.p = TRUE))\n}\ntau_naive <- function(rho_0, rho_1, sd_y, mu_y, mu_s, s_eff, y_eff) {\n  I1 <- integrate(cond_exp, -Inf, Inf, \n                  mu_y = mu_y + y_eff, \n                  mu_s = mu_s + s_eff, \n                  sd_y = sd_y, \n                  rho = rho_1, \n                  abs.tol = 0L, \n                  rel.tol = .Machine$double.eps^0.85\n  )\n  stopifnot(I1$subdivisions > 3)\n  D1 <- pnorm(mu_s + s_eff)\n  I2 <- integrate(cond_exp, -Inf, Inf, \n                  mu_y = mu_y, \n                  mu_s = mu_s, \n                  sd_y = sd_y, \n                  rho = rho_0, \n                  abs.tol = 0L, \n                  rel.tol = .Machine$double.eps^0.85)\n  stopifnot(I2$subdivisions > 3)\n  D2 <- pnorm(mu_s)\n  return(I1$value / D1 - I2$value / D2)\n}\ntrue_eff <- function(sd_y, mu_y, y_eff) {\n  mu_0 <- exp(mu_y + sd_y^2 / 2)\n  mu_1 <- exp(mu_y + y_eff + sd_y^2 / 2)\n  return(mu_1 - mu_0)\n}\ndelta_s <- function(mu_z, s_eff) {\n  p_0 <- pnorm(mu_z)\n  p_1 <- pnorm(mu_z + s_eff)\n  return(p_1 - p_0)\n}\n\nFirst we’ll explore a scenario where there is high positive correlation between the errors in the viral load equation and the latent variable related to infection. We set \\(\\rho_1 = \\rho_0 = 0.9\\), \\(\\sigma_0 = \\sigma_1 = 0.5\\), \\(\\mu_Y(0) = 10\\) and \\(\\mu_S(0) = -1.5\\). Let the true effect of the vaccine on mean log-viral load be \\(\\delta_y = -0.25\\).\nWe will plot how the naive estimand of viral load effect changes with the change in infection probability.\n\neffs <- seq(-3, 3, by = 0.01)\np_s <- sapply(effs, \\(x) delta_s(-0.5, x)) \nestimand <- sapply(effs, \\(x) f_naive_est(rho = 0.9, sd_y = 0.5, mu_y = 10, mu_s = -1.5, s_eff = x, y_eff = -0.25))\np_change_in_sign <- p_s[which.min(abs(estimand))]\nplot(p_s, \n     estimand,\n     main = \"Naive effect estimand vs. infection effect\",\n     ylab = bquote(E(y ~ \"|\" ~ S == 1, Z == 1) - E(y ~ \"|\" ~ S == 1, Z == 0)),\n     xlab = bquote(P(S == 1 ~ \"|\" ~ Z == 1) - P(S == 1 ~ \"|\" ~ Z == 0)),\n     type=\"l\")\nabline(v = p_change_in_sign, col = \"red\")\nabline(h = 0, col = \"red\")\n\n\n\n\nThis graph shows that as the vaccine becomes more effective at preventing infection, the apparent benefit of the vaccine on viral load decreases. In fact, at around a -0.18 decrease in the probability of infection, the naive estimand shows that the vaccine increases viral load, which is not the case.\nIf we fix a scenario and\n\neffs <- seq(-3, 3, by = 0.01)\np_s <- sapply(effs, \\(x) delta_s(-0.5, x)) \nestimand <- sapply(effs, \\(x) f_naive_est(rho = 0.9, sd_y = 0.5, mu_y = 10, mu_s = -1.5, s_eff = x, y_eff = -0.25))\np_change_in_sign <- p_s[which.min(abs(estimand))]\nplot(p_s, \n     estimand,\n     main = \"Naive effect estimand vs. infection effect\",\n     ylab = bquote(E(y ~ \"|\" ~ S == 1, Z == 1) - E(y ~ \"|\" ~ S == 1, Z == 0)),\n     xlab = bquote(P(S == 1 ~ \"|\" ~ Z == 1) - P(S == 1 ~ \"|\" ~ Z == 0)),\n     type=\"l\")\nabline(v = p_change_in_sign, col = \"red\")\nabline(h = 0, col = \"red\")\n\n\n\n\nNote that \\(\\rho_{01} = \\text{Cor}(U_i(0),U_i(1))\\) does not enter into the likelihood, which makes sense because we’ll never observe both \\(S_i(0)\\) and \\(S_i(1)\\) for the same individual. This does not mean that we won’t have any information about \\(\\rho_{01}\\); because \\(\\boldsymbol{\\Omega}\\) must be positive definite, the \\(\\det \\boldsymbol{\\Omega} > 0\\). This means that what we do learn about \\(\\rho_0\\) and \\(\\rho_1\\) will constrain the domain of \\(\\rho_{01}\\) so that \\(\\det \\boldsymbol{\\Omega} > 0\\). This translates to the following bounds on \\(\\rho_{01}\\) \\[\n\\rho_{01} \\in \\left(\\rho_0\\rho_1 - \\sqrt{1 - \\rho_0^2 - \\rho_1^2 + \\rho_0^2\\rho_1^2},\\, \\rho_0\\rho_1 + \\sqrt{1 - \\rho_0^2 - \\rho_1^2 + \\rho_0^2\\rho_1^2}\\right)\n\\] If we incorporate prior information, the asymptotic posterior for \\(\\rho_{01}\\) will take on the shape of the conditional prior for \\(\\rho_{01}\\) given the values for \\(\\rho_0\\) and \\(\\rho_1\\).\nGiven our focus on comparing the impact of vaccination on viral load, the target causal estimand is a measure of the average decrease (hopefully) in viral load caused by vaccination in individuals who are always infected. Mathematically, this is: \\[\\mathbb{E}[Y_i(1) - Y_i(0) \\mid S_i(1) = S_i(0) = 1].\\] This estimand is somewhat odd in that we can never observe both infection outcomes, so we can never calculate this estimand exactly from observed data without extra assumptions. I explained above why we can’t equate this target estimand with the ``naive’’ version, the difference in viral load between vaccinated and unvaccinated people: \\[\n\\mathbb{E}[Y_i(1) \\mid S_i(1) = 1] - \\mathbb{E}[Y_i(0) \\mid S_i(0) = 1].\n\\] Logically, it makes sense, because the naive estimand is a mixture over several groups, instead of just the always-infected group, but we’ll simulate some data to get a better sense of how different the two estimands can be.\nIn our simulated example, we’ll suppose we have a randomized vaccine trial with 20,000 participants spread evenly across 10 study sites. We’ll set \\(\\rho_0\\) and \\(\\rho_1\\) to \\(0.75\\), which equates to strong residual correlation between infection risk and viral load; that might be reasonable if there is an unobserved factor impacting both infection and viral load. Maybe this is prior exposure to the virus plus genetic factors related to immune system health.\nThe covariates we’re including in the infection risk equation, \\(\\mu_S(z)\\) are age and study site, while the only covariates we’re including in the viral load model are categorical age predictors.\n\nset.seed(12222)\nn <- 20000\n\nsigma_0 <- 0.7\nsigma_1 <- 0.7\nsigma_W <- 1\nrho_0 <- 0.9\nrho_1 <- 0.9\nlb <- rho_0*rho_1 - sqrt(1 - rho_0^2 - rho_1^2 + rho_0^2*rho_1^2)\nub <- rho_0*rho_1 + sqrt(1 - rho_0^2 - rho_1^2 + rho_0^2*rho_1^2)\nrho_01 <- 0.7\ndiag_sigs <- diag(c(sigma_0,sigma_1,sigma_W))\nOmega <- matrix(c(1,rho_01,rho_0,rho_01,1,rho_1,rho_0,rho_1,1),3,3)\nSigma <- diag_sigs %*% Omega  %*% diag_sigs\nU0_U1_W <- mvrnorm(n = n, mu = c(0, 0, 0), Sigma = Sigma)\n\nThe plot of \\(U_0\\) vs. \\(W\\) gives a sense of how related the two error terms are:\n\n\n\n\n\nThe covariates we will include in the model are age, which we treat as a categorical variable, the study site, which is also treated as a categorical variable, and the binary treatment variable, which is assumed to be balanced within study sites.\n\nN_age <- 4\nN_sites <- 10\nage_vec <- sample(N_age, n, TRUE) |> factor()\nsite_vec <- rep(1:N_sites,each=n / N_sites) |> factor()\ntreat <- rep(c(rep(0,n/20),rep(1,n/20)),10)\n\nWe’ll create the model matrices from these variables, along with the coefficients for each model equation:\n\nX_S <- model.matrix(~ age_vec + site_vec)\nX_Y <- model.matrix(~ age_vec)\nd_S <- ncol(X_S)\nd_Y <- ncol(X_Y)\ngamma_Y <- -0.25\ngamma_S <- -0.85\n\nbeta_S <- c(-1.5, 0.1, 0.2, 0.3, rnorm(N_sites - 1) * 0.1)\nbeta_Y <- c(10.3, 0.5, 0.5, 0.5)\n\nS_0  <- (X_S %*% beta_S + U0_U1_W[,3]) >= 0\nS_1  <- (X_S %*% beta_S + gamma_S + U0_U1_W[,3]) >= 0\nS <- cbind(as.integer(S_0), as.integer(S_1))\nY_0  <- X_Y %*% beta_Y + U0_U1_W[,1]\nY_1  <- X_Y %*% beta_Y + gamma_Y + U0_U1_W[,2]\nY <- cbind(ifelse(S_0 == 1, Y_0, NA),\n           ifelse(S_1 == 1, Y_1, NA))\n\nsel <- as.vector(rbind(1-treat, treat))\nS_o <- as.vector(t(S))[sel == 1]\nY_o <- as.vector(t(Y))[sel == 1]\n\nidx_S_o <- which(S_o == 1)\nidx_AI <- which(S_0 == 1 & S_1 == 1)\n\nY_o_sel <- Y_o[idx_S_o]\nX_Y_sel <- X_Y[idx_S_o, ]\n\n\nnaive_estimand <- mean(exp(Y_o_sel[treat[idx_S_o] == 1])) -\n                  mean(exp(Y_o_sel[treat[idx_S_o] == 0]))\n\nY_estimand <- mean(exp(Y_1[idx_AI])) -\n               mean(exp(Y_0[idx_AI]))\n\nn_s_0 <- sum(treat[idx_S_o] == 1)\nn_s_1 <- sum(treat[idx_S_o] == 0)\n\nn_AI <- length(idx_AI)\n\nAfter all is said and done, the finite-sample estimator for the population estimand is -63,883, with a standard error of 6888.1207534 while the naive estimator for the population estimand is 41,600 with a standard error of 8335.141496. Note that these standard errors are understated a bit because the selection indicators are treated as fixed. In reality, both the selection and the outcome are random, so a more complete standard error calculation would account for the randomness in both sets of outcomesboth sets of outcomes.\nThis means that the naive estimator understates the benefit of the vaccine for people who are always infected by about 165%.\nThe approximate standard error for the naive estimator is 6888.1207534, while the approximate standard error .\nLet’s build the Stan model block by block. First, the data block needs to take in the sizes and types of all the matrices and vectors that we’ll need to fit the model.\n\nData block\n\n\ndata {\n  int<lower=1> N;     \n  int<lower=1> N_neg; \n  int<lower=1> N_pos; \n  int<lower=1> D_S;   \n  int<lower=1> D_Y;   \n  vector[N_pos] Y;    \n  array[N] int<lower=0,upper=1> S;  \n  array[N] int<lower=0,upper=1> Z;  \n  matrix[N_pos,D_Y] X_Y; \n  matrix[N,D_S] X_S;  \n}\n\nWe need the number of observations, N, the number of negative and positive cases (N_neg and N_pos). We also need the dimensions of the predictors for the \\(S\\) equation and the \\(Y\\) equation, D_S and D_Y. The observed viral loads are collected into a length-N_pos vector Y, while the observed infection status and treatment assignment are collected into length-N binary integer arrays S and Z. Finally, we have the predictors for the \\(Y\\) equation, X_Y, which is an N_pos by D_Y matrix, and the predictors for the \\(S\\) equation, which is an N by D_S matrix.\n\nTransformed data block\n\nThe transformed data block is needed to collect the indices, measured from \\(1,\\dots,N\\), of the negative cases (\\(S_i = 0\\)) and positive cases (\\(S_i = 1\\)). The reason we need this information is because the likelihood contribution to the infection status probit model, shown in Equation 1, requires the value of the log-viral load, so we’ll need to match the infection status outcomes to the viral load outcomes. We could do this matching in the R code outside the Stan model, but I prefer to do it within the Stan code so everything is in one place. We’ll collect the indices of the positive cases in n_pos, which is an array of length N_pos and holds integers between 1 and N, while negative case indices are collected in n_neg, defined similarly.\nThe other piece of information the infection status likelihood will need for positive cases is the value \\(\\rho_z\\), which depends on the individual treatment assignment. This information is collected in the vector Z_idx.\n\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\n\n\nParameters block\n\nThe parameter block is where we define which unknown parameters we need to estimate with our model. Our unknown parameters is the set: - \\(\\boldsymbol{\\Omega}\\): correlation matrix between the error terms for the regressions - \\(\\sigma_0\\): error standard deviation for log-viral load regression in placebo group - \\(\\sigma_1\\): error standard deviation for log-viral load regression in treatment group - \\(\\beta_y\\): regression coefficients for the log-viral load regression - \\(\\beta_s\\) - \\(\\gamma_s\\) _y)$, the correlation matrix for the errors. In our case, we have a\n\nparameters {\n  cholesky_factor_corr[3] L_Omega;\n  vector<lower=0>[2] sd_Y;\n  vector[D_Y] b_Y;\n  vector[D_S] b_S;\n  real gamma_S;\n  real gamma_Y;\n}\n\nThe Stan model is written like so:\n\ndata {\n  int<lower=1> N;     \n  int<lower=1> N_neg; \n  int<lower=1> N_pos; \n  int<lower=1> D_S;   \n  int<lower=1> D_Y;   \n  vector[N_pos] Y;    \n  array[N] int<lower=0,upper=1> S;  \n  array[N] int<lower=0,upper=1> Z;  \n  matrix[N_pos,D_Y] X_Y; \n  matrix[N,D_S] X_S;  \n}\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\nparameters {\n  cholesky_factor_corr[3] L_Omega;\n  vector<lower=0>[2] sd_Y;\n  vector[D_Y] b_Y;\n  vector[D_S] b_S;\n  real gamma_S;\n  real gamma_Y;\n}\ntransformed parameters {\n  matrix[3,3] Omega = L_Omega * L_Omega';\n}\nmodel {\n  vector[N] mu_S = X_S * b_S + to_vector(Z) * gamma_S;\n  vector[N_pos] mu_Y = X_Y * b_Y + to_vector(Z[n_pos]) * gamma_Y;\n  array[2] real rho = {Omega[1,3], Omega[2,3]};\n\n  b_Y ~ normal(0, 1);\n  b_S ~ normal(0, 1);\n  sd_Y ~ normal(0, 2);\n  gamma_S ~ normal(0, 5);\n  gamma_Y ~ normal(0, 5);\n\n  for (n in 1:N_neg) {\n    target += log(Phi(-mu_S[n_neg[n]]));\n  }\n  for (n in 1:N_pos) {\n    int treat_idx = Z_idx[n_pos[n]];\n    target += log(Phi(sqrt((1 - rho[treat_idx]) * (1 + rho[treat_idx]))^(-1)*(mu_S[n_pos[n]]\n                               + rho[treat_idx] / sd_Y[treat_idx]\n                               * (Y[n] - mu_Y[n]))));\n    Y[n] ~ normal(mu_Y[n], sd_Y[treat_idx]);\n  }\n}\ngenerated quantities {\n  array[2] real rho = {Omega[1,3], Omega[2,3]};\n  real Y_estimand = 0;\n  {\n    vector[N] mu_S = X_S * b_S;\n    matrix[3,3] Sigma = quad_form_diag(Omega, append_row(sd_Y, [1]'));\n    int tot_11 = 0;\n    for (n in 1:N) {\n      vector[3] errors = multi_normal_rng(rep_vector(0,3), Sigma);\n      if (errors[3] > -mu_S[n] - gamma_S && errors[3] > -mu_S[n]) {\n        real mu_Y_cf = X_S[n,1:D_Y] * b_Y;\n        real Y_0 = errors[1] + mu_Y_cf;\n        real Y_1 = errors[2] + mu_Y_cf + gamma_Y;\n        Y_estimand += Y_1 -  Y_0;\n        tot_11 += 1;\n      }\n    }\n    Y_estimand /= tot_11;\n  }\n}\n\n\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\n\nSome comments on the Stan code are in order. The data block is pretty straightforward, though it’s worth keeping in mind that we’ll need to keep track of three dimensions: the total number of data points, the number of infected and uninfected patients. This is because we’ll only have access to viral loads in the infected patients.\nLet’s generate some data to see how the errors change how the .\nFirst, let’s see whether the\n\n\n\n\nReferences\n\nHeckman, James J. 1979. “Sample Selection Bias as a Specification Error.” Econometrica 47 (1): 153. https://doi.org/10.2307/1912352.\n\n\nHudgens, Michael G., Antje Hoering, and Steven G. Self. 2003. “On the Analysis of Viral Load Endpoints in HIV Vaccine Trials.” Statistics in Medicine 22 (14): 2281–98. https://doi.org/10.1002/sim.1394."
  },
  {
    "objectID": "posts/beta-processes-in-stan.html",
    "href": "posts/beta-processes-in-stan.html",
    "title": "Gamma and Beta Processes in Stan",
    "section": "",
    "text": "rdirichlet <- function(n, alpha) {\n  d <- length(alpha)\n  rates <- rep(1, d)\n  gammas <- replicate(n = n,\n                      rgamma(d, shape = alpha,\n                             rate = rates)\n                      )\n  draws <- sweep(gammas, MARGIN = 2, STATS = colSums(gammas), FUN = \"/\")\n  return(t(draws))\n}\n\nI taught the PhD-level survival analysis course in the 2024 Winter quarter at Oregon State (website). The prior years’ courses in survival analysis were exclusively focused on Frequentist nonparametric and semiparametrci methods for inference: Kaplan-Meier, Cox proportional hazards model, etc. I wanted to see if I could bring a bit more Bayesian inference into the course. Ultimately, I didn’t include any Bayesian nonparametrics, but it gave me the opportunity to try some simple nonparametric methods in Stan (Stan is always my first choice to develop bespoke models, it’s second nature at this point).\nMy starting point to learn more about Bayesian nonparametric survival analysis modeling was our textbook for the course Klein and Moeschberger (Klein and Moeschberger 2003). They review several nonparameteric Bayesian models for survival analysis in 6.4: Dirichlet processes, and beta processes. Both methods put a nonparametric prior over the survival function, or \\(S(t)\\), but they do so in different ways.\nThese methods are used to model time-to-event data. Let’s call the time-to-event random variable \\(T_i\\) for an individual \\(i\\). The survival function for \\(T_i\\) is the complement of the cumulative distribution function, \\(P(T_i \\leq t)\\):\n\\[\nS(t) = P(T_i > t) = 1 - P(T_i \\leq t)\n\\]\nTime-to-event data is complicated by the fact that we often can’t observe all times to failure, but instead we observe only a subset of these failures corresponding to those times that occur prior to a censoring time. In the simplest case, called Type I censoring, we observe only \\(T_i\\) that are less than a constant \\(C\\).\nThe standard set-up for survival analysis data is to define two new random variables for each observation \\(i\\):\n\\[\nX_i = 1(T_i \\leq C) T_i + C 1(T_i > C), \\Delta_i = 1(T_i \\leq C).\n\\]\nNow we have a single time-to-event random variable \\(X_i\\) which is equal to \\(T_i\\) if \\(T_i\\) occurs prior to the censoring time \\(C\\). In this case, \\(\\Delta_i = 1\\). If \\(T_i\\) occurs after the censoring time, we don’t observe \\(T_i\\) but rather we observe only the censoring time. This sort of censoring set up occurs if we enroll patients in a randomized controlled trial, administer a treatment, and follow each patient for, say, 10 months. If the event of interest occurs within the 10-month window, we will have observed \\(T_i\\), otherwise, our only knowledge of the event time is that it is greater than 10-months.\nBack to our modeling strategy to learn about \\(S(t)\\) from a set of data, \\(\\{(X_i, \\Delta_i), i.= 1, \\dots, n\\}\\). One strategy to model the survival function nonparametrically is to define a partition of the positive real line, \\(\\{t_j, j = 0, \\dots, J\\}\\) with \\(t_0 = 0, t_J = \\infty\\)to model increments of the survival function \\(S(t_j) - S(t_{j-1})\\) over this partition. These increments will be between \\((0,1)\\) and they will sum to \\(1\\). A natural distribution for these random variables is a Dirichlet distribution, which respects the natural constraints of the increments process:\n\\[\n(S(t_0) - S(t_1), \\dots, S(t_j) - S(t_{j-1}), S(t_{J-1}) - S(t_{J})) \\sim \\text{Dirichlet}(c\\,\\boldsymbol{\\alpha}).\n\\]\nTypically, \\(\\boldsymbol{\\alpha}\\) is chosen so that it too is a function of time \\(t\\) and that it corresponds to a known parametric survival function, like say the exponential function: \\(S(t) = \\exp(-\\lambda t)\\).\nWhen we combine the prior over the survival function with counts of failures within an interval\n, but the function that we’re modeling is different between the models.\nThe Dirichlet process directly models the unknown survival function, while the beta process models the unknown cumulative hazard function.\nAnother popular choice for Bayesian nonparametric survival analysis, not covered by KM for some reason, is a gamma process. My first choice was a gamma process.\nOne of the proposed methods is the Beta process, which is rigorously developed in (Hjort 1990).\n\n\n\n\nReferences\n\nHjort, Nils Lid. 1990. “Nonparametric Bayes Estimators Based on Beta Processes in Models for Life History Data.” The Annals of Statistics 18 (3). https://doi.org/10.1214/aos/1176347749.\n\n\nKlein, John P., and Melvin L. Moeschberger. 2003. Survival Analysis: Techniques for Censored and Truncated Data. 2nd ed. Statistics for Biology and Health. New York: Springer."
  },
  {
    "objectID": "st_625_w_24.html",
    "href": "st_625_w_24.html",
    "title": "ST 625 - W 24",
    "section": "",
    "text": "Required: Survival Analysis: Techniques for censored and truncated data, Klein and Moeschberger\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nNotes"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rob Trangucci",
    "section": "",
    "text": "I am an assistant professor of Statistics at Oregon State University. My research focuses on novel statistical methodology in missing data analysis and causal inference for problems in epidemiology, designing Bayesian methods for survey inference, and creating tools to quantify how priors impact posterior inferences. Before I returned to academia to pursue a doctorate in statistics, I worked as a data scientist for a fintech startup, a statistical consultant for a Big Five publisher, and a core developer for the Stan statistical modeling and inference platform (https://mc-stan.org/). I earned a PhD in Statistics from the University of Michigan, an M.A. in Quantitative Methods in the Social Sciences from Columbia University, and a B.A. in Physics from Bucknell University."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Rob Trangucci",
    "section": "Interests",
    "text": "Interests\n\nCausal inference for vaccine efficacy\nMissing data\nPrincipal stratification\nPrior influence\nMultilevel regression and poststratification (MRP)\nBayesian inference"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rob Trangucci",
    "section": "Education",
    "text": "Education\n\nPhD in Statistics, 2023\n\nUniversity of Michigan\n\nMA in Quantitative Methods in the Social Science, 2014\n\nColumbia University\n\nBA in Physics, 2009\n\nBucknell University"
  },
  {
    "objectID": "st_625_w_25.html",
    "href": "st_625_w_25.html",
    "title": "ST 625 - W 25",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nNotes\n\n\n\n\n\nHW 1\nHW 2\nHW 3\nHW 4\nHW 5\nHW 6"
  },
  {
    "objectID": "st_599_w_25.html",
    "href": "st_599_w_25.html",
    "title": "ST 599 - W 25: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are identifiable for a given data generating process.\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes\nLecture 3 notes\nLecture 4 notes\nLecture 5 notes\nLecture 6 notes\nLecture 7 notes\nLecture 8 notes\nLecture 9 notes\nLecture 10 notes\nLecture 11 notes\nLecture 12 notes\nLecture 13 notes\nLecture 14 notes\nLecture 15 notes\nLecture 16 notes\n\n\n\n\n\nHW 1\nHW 2\nHW 3"
  },
  {
    "objectID": "courses.html#academic-courses",
    "href": "courses.html#academic-courses",
    "title": "Courses",
    "section": "",
    "text": "STAT 625 - Winter 2024\n\n\n\n\n\nSTAT 625 - Winter 2025\nSTAT 599 - Winter 2025\n\n\n\n\n\nSTAT 625 - Winter 2026\nSTAT 599 - Winter 2026"
  },
  {
    "objectID": "st_599_w_26.html",
    "href": "st_599_w_26.html",
    "title": "ST 599 - W 26: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are identifiable for a given data generating process.\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReading\nLO\nAssignments\n\n\n\n\n1/6/2026\nMissingness mechanisms and patterns\nLR 1.1-1.4\n1,4\nNA\n\n\n1/8/2026\nMissing data techniques Complete case analysis  Weighting\nLR Ch. 3\n1\nNA\n\n\n1/13/2026\nSingle imputation techniques\nLR Ch. 4\n1\nNA\n\n\n1/15/2026\nMultiple imputation techniques\nLR Ch. 5\n1\nNA\n\n\n1/20/2026\nLikelihood theory\nLR Ch. 6\n1\nNA\n\n\n\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes\n\n\n\n\n\nHW 1"
  },
  {
    "objectID": "st_599_w_26.html#syllabus",
    "href": "st_599_w_26.html#syllabus",
    "title": "ST 599 - W 26: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are identifiable for a given data generating process.\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReading\nLO\nAssignments\n\n\n\n\n1/6/2026\nMissingness mechanisms and patterns\nLR 1.1-1.4\n1,4\nNA\n\n\n1/8/2026\nMissing data techniques Complete case analysis  Weighting\nLR Ch. 3\n1\nNA\n\n\n1/13/2026\nSingle imputation techniques\nLR Ch. 4\n1\nNA\n\n\n1/15/2026\nMultiple imputation techniques\nLR Ch. 5\n1\nNA\n\n\n1/20/2026\nLikelihood theory\nLR Ch. 6\n1\nNA\n\n\n\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes\n\n\n\n\n\nHW 1"
  },
  {
    "objectID": "st_625_w_26.html",
    "href": "st_625_w_26.html",
    "title": "ST 625 - W 26",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1\nLecture 2\nLecture 3\n\n\n\n\n\nHW 1"
  },
  {
    "objectID": "st_625_w_26.html#syllabus",
    "href": "st_625_w_26.html#syllabus",
    "title": "ST 625 - W 26",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nProject proposal\n\n\n\n\n\nLecture 1\nLecture 2\nLecture 3\n\n\n\n\n\nHW 1"
  },
  {
    "objectID": "papers.html#published-papers",
    "href": "papers.html#published-papers",
    "title": "My papers",
    "section": "",
    "text": "Here’s a selection of my published papers:\n\nModeling Racial/ethnic Differences in COVID-19 Incidence with Covariates Subject to Non-random Missingness (Trangucci, Chen, and Zelner 2023)\nRacial Disparities in Coronavirus Disease 2019 (COVID-19) Mortality Are Driven by Unequal Infection Risks (Zelner et al. 2021)\nQuantifying Observed Prior Impact (Jones, Trangucci, and Chen 2021)\nModeling Spatial Risk of Diarrheal Disease Associated with Household Proximity to Untreated Wastewater Used for Irrigation in the Mezquital Valley, Mexico (Contreras Jesse D. et al. 2020)\nBayesian Hierarchical Weighting Adjustment and Survey Inference. (Si 2020)\nEffects of Sequential Influenza A(H1N1)Pdm09 Vaccination on Antibody Waning (Zelner et al. 2019)"
  },
  {
    "objectID": "survival-material/lecture-1.html",
    "href": "survival-material/lecture-1.html",
    "title": "Lecture 1",
    "section": "",
    "text": "This introduction is based in part on Klein, Moeschberger, et al. (2003), and in part on Aalen, Borgan, and Gjessing (2008) plus Fleming and Harrington (2005).\nSurvival analysis is the modeling and analysis of time-to-event data; this means we will be studying how to model nonnegative random variables (time will always be measured in such a way so that the observations are nonnegative). Think about a clinical trial for a new COVID vaccine and how you might model the length of time between study entry and infection in each arm of the trial. Let \\(X_i\\) be the time from trial entry to infection for the \\(i\\)-th participant. These sorts of trials are typically run until a prespecified number of people have become infected. Let \\(n\\) be the total number of participants in the trial and let \\(r\\) be the prespecified number of infections. Let \\(T_i\\) be the observed infection time for the \\(i\\)-th participant. This means that for \\(r\\) participants, \\(T_i = X_i\\), but for \\(n-r\\) participants we know only that the time-to-infection is larger than the observed time. Let \\(C_i\\) denote the time from study entry for participant \\(i\\) to study end. Then \\(T_i = \\min (X_i, C_i)\\), and let \\(\\delta_i = \\ind{T_i = X_i}\\). The density of \\(T_i\\) is related to the joint probability for \\(X_i\\) and \\(C_i\\), which is indexed by a possibly infinite dimensional parameter \\(\\theta\\): \\(P_{\\theta}(X_i &gt; t, C_i &gt; c)\\). When \\(\\delta_i = 1\\), and \\(T_i = X_i\\), the likelihood of the observation is \\[\n\\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_i &gt; u, C_i &gt; t)\\rp\\right\\rvert_{u = t},\n\\] while the likelihood for \\(\\delta_i = 0\\) is \\[\n\\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_i &gt; t, C_i &gt; u)\\rp\\right\\rvert_{u = t},\n\\] Then \\(T_i = C_i\\) for the other \\(n-r\\) participants. Under the null hypothesis that the vaccine has no effect, the population distribution function for all \\(n\\) participants for \\(X_i, C_i\\) is \\(P_{\\theta}(X_1 &gt; x, C_1 &gt; c)\\) (i.e. the distribution for survival times in the treatment group and the placebo group is the same). Then the joint density for the observed infection times is as follows: \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\theta) & = n! \\prod_{i=1}^r \\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_1 &gt; u, C_1 &gt; t_{(i)})\\rp\\right\\rvert_{u = t_{(i)}} \\\\\n& \\times \\prod_{i=r+1}^n \\left.\\lp-\\frac{\\partial}{\\partial u}P_{\\theta}(X_1 &gt; t_{(i)}, C_1 &gt; u)\\rp\\right\\rvert_{u = t_{(i)}},\n\\end{align*}\\] where \\(t_{(i)}\\) is the \\(i\\)-th order statistic of the set \\(\\{t_1, \\dots, t_n\\}\\). Note that this is different from most other data analysis where missing observations are not expected to occur with much frequency. On the contrary, in survival analysis, missingness, both truncation and censoring are expected to occur with nearly every dataset, so much of our time will be spent ensuring our methods work when data arise with these peculiarities.\n\n\nNow suppose that \\(X_1 \\indy C_1\\), and that \\(\\theta\\) partitions into \\(\\eta\\) and \\(\\phi\\), such that \\[\nP_{\\theta}(X_1 &gt; x, C_1 &gt; c) = P_{\\eta}(X_1 &gt; x)P_{\\phi}(C_1 &gt; c).\n\\] Then we can rewrite the joint observational density for \\(T_i\\) as: \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\theta) & = n! \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)}) \\\\\n& \\times \\lp \\prod_{i=1}^r P_{\\phi}(C_1 &gt; t_{(i)}) \\rp \\prod_{i=r+1}^n f_C(t_{(i)} ;\\, \\phi).\n\\end{align*}\\] If we are only interested about inference about \\(\\eta\\), the parameters that govern the distribution of the true time-to-infection random variables, we can ignore the the distribution for the censoring random variables \\(C_1\\), and maximize the likelihood because, in \\(\\eta\\): \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\eta) \\propto \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)})  \n\\end{align*}\\] We will talk in more detail about censoring in the coming lectures.\n\n\n\nAalen, Borgan, and Gjessing (2008) notes that we cannot even compute a simple mean in this situation, so something like a t-test will be useless. As an aside, let’s try to compute a mean from the data above. Let \\(\\bar{T} = \\frac{1}{n} \\sum_{i=1}^n T_i\\). We can show that \\(\\lim_{n \\to \\infty} \\bar{T} \\leq \\Exp{X_i}\\) with probability \\(1\\).\n\nProof. Let \\(T_i = X_i \\ind{X_i \\leq C_i} + C_i \\ind{X_i &gt; C_i}\\). Then by the SLLN \\(\\bar{T} \\overset{\\text{a.s.}}{\\to} \\Exp{T_i}\\). \\[\\begin{align*}\n\\Exp{T_i} & = \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{C_i \\ind{X_i &gt; C_i}} \\\\\n& \\leq \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{X_i \\ind{X_i &gt; C_i}} = \\Exp{X_i}\n\\end{align*}\\]\n\n\n\n\nHow can we compute the mean time to infection then? One way to estimate the mean time to infection is to first estimate the function \\(S_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i &gt; t)\\), which is also known as the survival function. Recall this fact about non-negative random variables \\(X_i \\geq 0\\) w.p. 1: \\[\\begin{align*}\n\\Exp{X_i} = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt    \n\\end{align*}\\] This follows from an application of Fubini’s theorem applied to the integral: \\[\\begin{align*}\n\\Exp{X_i} & = \\int_0^\\infty u dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dt \\, dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dP_{X_i}(u ;\\, \\theta) dt \\\\\n& = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt\n\\end{align*}\\]\n\n\nLet \\(F_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i \\leq t)\\). Then because the survival function is defined as \\(S_{X_i}(t ;\\, \\theta) = 1 - F_{X_i}(t ;\\, \\theta)\\) (also known as the complementary CDF) the survival function inherits its properties from the CDF. The survival function:\n\n\\(S_{X_i}(t ;\\, \\theta)\\) is a nonincreasing function\n\\(S_{X_i}(0 ;\\, \\theta) = 1\\)\n\\(\\lim_{t\\to\\infty} S_{X_i}(t ;\\, \\theta) = 0\\)\nHas lefthand limits: \\(\\lim_{s \\nearrow t} S_{X_i}(s ;\\, \\theta) = S_{X_i}(t-;\\, \\theta).\\)\nIs right continuous: \\(\\lim_{s \\searrow t} S_{X_i}(s;\\, \\theta) = S_{X_i}(t;\\, \\theta).\\)\n\nAn example of a discrete survival function is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Example plot of a survival function for a discrete survival time, bounded between \\([0,10]\\)"
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-indycensor",
    "href": "survival-material/lecture-1.html#sec-indycensor",
    "title": "Lecture 1",
    "section": "",
    "text": "Now suppose that \\(X_1 \\indy C_1\\), and that \\(\\theta\\) partitions into \\(\\eta\\) and \\(\\phi\\), such that \\[\nP_{\\theta}(X_1 &gt; x, C_1 &gt; c) = P_{\\eta}(X_1 &gt; x)P_{\\phi}(C_1 &gt; c).\n\\] Then we can rewrite the joint observational density for \\(T_i\\) as: \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\theta) & = n! \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)}) \\\\\n& \\times \\lp \\prod_{i=1}^r P_{\\phi}(C_1 &gt; t_{(i)}) \\rp \\prod_{i=r+1}^n f_C(t_{(i)} ;\\, \\phi).\n\\end{align*}\\] If we are only interested about inference about \\(\\eta\\), the parameters that govern the distribution of the true time-to-infection random variables, we can ignore the the distribution for the censoring random variables \\(C_1\\), and maximize the likelihood because, in \\(\\eta\\): \\[\\begin{align*}\nf_{T_1, \\dots, T_n}(t_1, \\dots, t_n ;\\, \\eta) \\propto \\lp \\prod_{i=1}^r f_{X_1}(t_{(i)} ;\\, \\eta) \\rp \\prod_{i=r+1}^n P_{\\eta}(X_1 &gt; t_{(i)})  \n\\end{align*}\\] We will talk in more detail about censoring in the coming lectures."
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-prop-surv-fun",
    "href": "survival-material/lecture-1.html#sec-prop-surv-fun",
    "title": "Lecture 1",
    "section": "3.1 Properties of the survival function",
    "text": "3.1 Properties of the survival function\nLet \\(F_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i \\leq t)\\). Then because the survival function is defined as \\(S_{X_i}(t ;\\, \\theta) = 1 - F_{X_i}(t ;\\, \\theta)\\) (also known as the complementary CDF) the survival function inherits its properties from the CDF. The survival function:\n\n\\(S_{X_i}(t ;\\, \\theta)\\) is a nonincreasing function\n\\(S_{X_i}(0 ;\\, \\theta) = 1\\)\n\\(\\lim_{t\\to\\infty} S_{X_i}(t ;\\, \\theta) = 0\\)\nHas lefthand limits: \\(\\lim_{s \\nearrow t} S_{X_i}(s ;\\, \\theta) = S_{X_i}(t-;\\, \\theta).\\)\nIs right continuous: \\(\\lim_{s \\searrow t} S_{X_i}(s;\\, \\theta) = S_{X_i}(t;\\, \\theta).\\)\n\nAn example of a discrete survival function is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Example plot of a survival function for a discrete survival time, bounded between \\([0,10]\\)"
  },
  {
    "objectID": "survival-material/lecture-2.html",
    "href": "survival-material/lecture-2.html",
    "title": "Lecture 2",
    "section": "",
    "text": "Another way to characterize the random variable \\(X_i\\) is the hazard function, which is typically denoted as \\(\\lambda(t)\\) or \\(h(t)\\) and is defined as \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X_i &lt; t + \\Delta t \\mid X_i \\geq t}{\\theta} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t }{\\theta}}\n\\end{align*}\\] First, note that we can define \\(\\Prob{X_i \\geq t}{\\theta}\\) in terms of the survival function as: \\[\n\\Prob{X_i \\geq t}{\\theta} = \\lim_{s\\nearrow t} S_{X_i}(s ;\\, \\theta).\n\\] Using the notation introduced in the last lecture, we can write this as \\[\n\\Prob{X_i \\geq t}{\\theta} = S_{X_i}(t- ;\\, \\theta).\n\\] Of course, when \\(X_i\\) is absolutely continuous,\\(S_{X_i}(t- ;\\, \\theta) = S_{X_i}(t ;\\, \\theta)\\), but when \\(X_i\\) is discrete, or mixed discrete and continuous, as noted above, it is not true in general that the survival function is left-continuous.\nA few things to note about \\(\\lambda_{X_i}(t ;\\, \\theta)\\): when \\(X_i\\) is an absolutely continuous random variable, which occurs when we’re considering survival in continuous time, we can write this in terms of the probability density function \\(f_{X_i}(t ;\\, \\theta)\\) and the cumulative distribution function \\(F_{X_i}(t ;\\, \\theta)\\): \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{F_{X_i}(t + \\Delta t;\\, \\theta) - F_{X_i}(t;\\, \\theta)}{\\Delta t} \\times \\frac{1}{1 - F_{X_i}(t;\\, \\theta)} \\\\\n& = \\frac{f_{X_i}(t;\\, \\theta)}{1 - F_{X_i}(t;\\, \\theta)}.\n\\end{align*}\\] Let’s examine how the survival function and the hazard function fit together. \\[\n\\lambda_{X_i}(t) = \\frac{f_{X_i}(t ;\\, \\theta)}{S_{X_i}(t- ;\\, \\theta)}.\n\\] Note that we can write the hazard function in terms of the survival function instead of the density, when \\(X_i\\) is absolutely continuous: \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{S_{X_i}(t ;\\, \\theta)-S_{X_i}(t + \\Delta t ;\\, \\theta) }{\\Delta t} \\times \\frac{1}{S_{X_i}(t ;\\, \\theta)} \\\\\n& = -\\frac{d}{dt} S_{X_i}(t ;\\, \\theta)/S_{X_i}(t ;\\, \\theta).\n\\end{align*}\\] This implies that \\[\n\\lambda_{X_i}(t) = -\\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta).\n\\] If we integrate both sides, we get another important identity in survival analysis: \\[\\begin{align}\n   \\int_{0}^u \\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta) dt & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\\\\n   \\log S_{X_i}(u ;\\, \\theta) - \\log S_{X_i}(0 ;\\, \\theta) & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\quad \\text{note}\\,\\,\\, S_{X_i}(0 ;\\, \\theta) = 1\\\\\n   S_{X_i}(u ;\\, \\theta) & = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp \\label{eq:exp-hazard}\n\\end{align}\\]\n\n\nThe relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-2.html#properties-of-the-hazard-function",
    "href": "survival-material/lecture-2.html#properties-of-the-hazard-function",
    "title": "Lecture 2",
    "section": "",
    "text": "The relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-1.html#properties-of-the-hazard-function",
    "href": "survival-material/lecture-1.html#properties-of-the-hazard-function",
    "title": "Lecture 1",
    "section": "4.1 Properties of the hazard function",
    "text": "4.1 Properties of the hazard function\nThe relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-mttf",
    "href": "survival-material/lecture-1.html#sec-mttf",
    "title": "Lecture 1",
    "section": "",
    "text": "Aalen, Borgan, and Gjessing (2008) notes that we cannot even compute a simple mean in this situation, so something like a t-test will be useless. As an aside, let’s try to compute a mean from the data above. Let \\(\\bar{T} = \\frac{1}{n} \\sum_{i=1}^n T_i\\). We can show that \\(\\lim_{n \\to \\infty} \\bar{T} \\leq \\Exp{X_i}\\) with probability \\(1\\).\n\nProof. Let \\(T_i = X_i \\ind{X_i \\leq C_i} + C_i \\ind{X_i &gt; C_i}\\). Then by the SLLN \\(\\bar{T} \\overset{\\text{a.s.}}{\\to} \\Exp{T_i}\\). \\[\\begin{align*}\n\\Exp{T_i} & = \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{C_i \\ind{X_i &gt; C_i}} \\\\\n& \\leq \\Exp{X_i \\ind{X_i \\leq C_i}} + \\Exp{X_i \\ind{X_i &gt; C_i}} = \\Exp{X_i}\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-survfun",
    "href": "survival-material/lecture-1.html#sec-survfun",
    "title": "Lecture 1",
    "section": "",
    "text": "How can we compute the mean time to infection then? One way to estimate the mean time to infection is to first estimate the function \\(S_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i &gt; t)\\), which is also known as the survival function. Recall this fact about non-negative random variables \\(X_i \\geq 0\\) w.p. 1: \\[\\begin{align*}\n\\Exp{X_i} = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt    \n\\end{align*}\\] This follows from an application of Fubini’s theorem applied to the integral: \\[\\begin{align*}\n\\Exp{X_i} & = \\int_0^\\infty u dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dt \\, dP_{X_i}(u ;\\, \\theta) \\\\\n& = \\int_0^\\infty \\int_{0}^\\infty \\ind{0 \\leq t \\leq u} dP_{X_i}(u ;\\, \\theta) dt \\\\\n& = \\int_0^\\infty P_{\\theta}(X_i &gt; t) dt\n\\end{align*}\\]\n\n\nLet \\(F_{X_i}(t ;\\, \\theta) = P_{\\theta}(X_i \\leq t)\\). Then because the survival function is defined as \\(S_{X_i}(t ;\\, \\theta) = 1 - F_{X_i}(t ;\\, \\theta)\\) (also known as the complementary CDF) the survival function inherits its properties from the CDF. The survival function:\n\n\\(S_{X_i}(t ;\\, \\theta)\\) is a nonincreasing function\n\\(S_{X_i}(0 ;\\, \\theta) = 1\\)\n\\(\\lim_{t\\to\\infty} S_{X_i}(t ;\\, \\theta) = 0\\)\nHas lefthand limits: \\(\\lim_{s \\nearrow t} S_{X_i}(s ;\\, \\theta) = S_{X_i}(t-;\\, \\theta).\\)\nIs right continuous: \\(\\lim_{s \\searrow t} S_{X_i}(s;\\, \\theta) = S_{X_i}(t;\\, \\theta).\\)\n\nAn example of a discrete survival function is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Example plot of a survival function for a discrete survival time, bounded between \\([0,10]\\)"
  },
  {
    "objectID": "survival-material/lecture-1.html#hazard-function",
    "href": "survival-material/lecture-1.html#hazard-function",
    "title": "Lecture 1",
    "section": "",
    "text": "Another way to characterize the random variable \\(X_i\\) is the hazard function, which is typically denoted as \\(\\lambda(t)\\) or \\(h(t)\\) and is defined as \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X_i &lt; t + \\Delta t \\mid X_i \\geq t}{\\theta} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t }{\\theta}}\n\\end{align*}\\] First, note that we can define \\(\\Prob{X_i \\geq t}{\\theta}\\) in terms of the survival function as: \\[\n\\Prob{X_i \\geq t}{\\theta} = \\lim_{s\\nearrow t} S_{X_i}(s ;\\, \\theta).\n\\] Using the notation introduced in Section 1.3.1, we can write this as \\[\n\\Prob{X_i \\geq t}{\\theta} = S_{X_i}(t- ;\\, \\theta).\n\\] Of course, when \\(X_i\\) is absolutely continuous,\\(S_{X_i}(t- ;\\, \\theta) = S_{X_i}(t ;\\, \\theta)\\), but when \\(X_i\\) is discrete, or mixed discrete and continuous, as noted above, it is not true in general that the survival function is left-continuous.\nA few things to note about \\(\\lambda_{X_i}(t ;\\, \\theta)\\): when \\(X_i\\) is an absolutely continuous random variable, which occurs when we’re considering survival in continuous time, we can write this in terms of the probability density function \\(f_{X_i}(t ;\\, \\theta)\\) and the cumulative distribution function \\(F_{X_i}(t ;\\, \\theta)\\): \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{F_{X_i}(t + \\Delta t;\\, \\theta) - F_{X_i}(t;\\, \\theta)}{\\Delta t} \\times \\frac{1}{1 - F_{X_i}(t;\\, \\theta)} \\\\\n& = \\frac{f_{X_i}(t;\\, \\theta)}{1 - F_{X_i}(t;\\, \\theta)}.\n\\end{align*}\\] Let’s examine how the survival function and the hazard function fit together. \\[\n\\lambda_{X_i}(t) = \\frac{f_{X_i}(t ;\\, \\theta)}{S_{X_i}(t- ;\\, \\theta)}.\n\\] Note that we can write the hazard function in terms of the survival function instead of the density, when \\(X_i\\) is absolutely continuous: \\[\\begin{align*}\n\\lambda_{X_i}(t) & = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\frac{\\Prob{t \\leq X_i &lt; t + \\Delta t}{\\theta}}{\\Prob{X_i \\geq t}{\\theta}} \\\\\n& = \\lim_{\\Delta t \\searrow 0}\\frac{S_{X_i}(t ;\\, \\theta)-S_{X_i}(t + \\Delta t ;\\, \\theta) }{\\Delta t} \\times \\frac{1}{S_{X_i}(t ;\\, \\theta)} \\\\\n& = -\\frac{d}{dt} S_{X_i}(t ;\\, \\theta)/S_{X_i}(t ;\\, \\theta).\n\\end{align*}\\] This implies that \\[\n\\lambda_{X_i}(t) = -\\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta).\n\\] If we integrate both sides, we get another important identity in survival analysis: \\[\\begin{align}\n   \\int_{0}^u \\frac{d}{dt} \\log S_{X_i}(t ;\\, \\theta) dt & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\\\\n   \\log S_{X_i}(u ;\\, \\theta) - \\log S_{X_i}(0 ;\\, \\theta) & = -\\int_{0}^u\\lambda_{X_i}(t) dt \\quad \\text{note}\\,\\,\\, S_{X_i}(0 ;\\, \\theta) = 1\\\\\n   S_{X_i}(u ;\\, \\theta) & = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp \\label{eq:exp-hazard}\n\\end{align}\\]\n\n\nThe relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-1.html#density-function-for-survival-time",
    "href": "survival-material/lecture-1.html#density-function-for-survival-time",
    "title": "Lecture 1",
    "section": "",
    "text": "Given that we have \\(S_{X_i}(t;\\,\\theta)\\) and \\(\\lambda(t) = \\frac{f_{X_i}(t;\\,\\theta)}{S_{X_i}(t-;\\,\\theta)}\\), we can recover the density, \\(f_{X_i}(t;\\,\\theta)\\) easily: \\[\nf_{X_i}(t;\\,\\theta) = \\lambda_{X_i}(t) S_{X_i}(t-;\\,\\theta)\n\\]"
  },
  {
    "objectID": "survival-material/lecture-1.html#sec-cumu-haz",
    "href": "survival-material/lecture-1.html#sec-cumu-haz",
    "title": "Lecture 1",
    "section": "",
    "text": "One final important quantity that describes a survival distribution is that of , which we’ll denote as \\(\\Lambda_{X_i}(t)\\), though it is also denoted as \\(H(t)\\) in . This is defined as you might expect: \\[\n\\Lambda_{X_i}(t) = \\int_{0}^t \\lambda_{X_i}(u) du.\n\\] It has the important property that for any absolutely continuous failure time \\(X_i\\) with a given cumulative hazard function, the random variable \\(Y_i = \\Lambda_{X_i}(X_i)\\) is exponentially distributed with rate \\(1\\). The derivation is straightforward. Remember that \\(P(X_i &gt; t) = \\exp\\lp-\\Lambda_{X_i}(t)\\rp\\) \\[\\begin{align*}\nP(\\Lambda_{X_i}(X_i) &gt; t) & = P(X_i &gt; \\Lambda_{X_i}^{-1}(t)) \\\\\n& = \\exp\\lp-\\Lambda_{X_i}(\\Lambda_{X_i}^{-1}(t))\\rp \\\\\n& = \\exp\\lp-t\\rp\n\\end{align*}\\]"
  },
  {
    "objectID": "survival-material/lecture-1.html#discrete-survival-time",
    "href": "survival-material/lecture-1.html#discrete-survival-time",
    "title": "Lecture 1",
    "section": "",
    "text": "We’ve been working with continuous survival times until now. If \\(X_i\\) is a discrete random variable with support on \\(\\{t_1, t_2, \\dots\\}\\), we lose some of the tidyness of the previous derivations. We can define the distribution of \\(X_i\\) in terms of the survival function, \\(P_{\\theta}(X_i &gt; t)\\). First let \\(p_j = P_{\\theta}(X_i = t_j)\\), so \\[\nS_{X_i}(t;\\,\\theta) = P_{\\theta}(X_i &gt; t) = \\sum_{j \\mid t_j &gt; t} p_j\n\\] We can also define the hazard function for a discrete random variable: \\[\n\\lambda_{X_i}(t_j) = \\frac{p_j}{S_{X_i}(t_{j-1};\\,\\theta)} = \\frac{p_j}{p_j + p_{j+1} + \\dots}\n\\] Note that \\(p_j = S_{X_i}(t_{j-1};\\,\\theta) - S_{X_i}(t_{j};\\,\\theta)\\), then \\[\n\\lambda_{X_i}(t_j) = 1 - \\frac{S_{X_i}(t_j;\\,\\theta)}{S_{X_i}(t_{j-1};\\,\\theta)}.\n\\] If we let \\(t_0 = 0\\) then \\(S_{X_i}(t_0;\\,\\theta) = 1\\). This allows us to write the survival function in a sort of telescoping product: \\[\\begin{align*}\nP_{\\theta}(X_i &gt; t_j) & = P_{\\theta}(X_i &gt; t_0) \\frac{P_{\\theta}(X_i &gt; t_1)}{P_{\\theta}(X_i &gt; t_0)} \\frac{P_{\\theta}(X_i &gt; t_2)}{P_{\\theta}(X_i &gt; t_1)} \\dots \\frac{P_{\\theta}(X_i &gt; t_j)}{P_{\\theta}(X_i &gt; t_{j-1})} \\\\\n   & =  1 \\frac{S_{X_i}(t_1;\\,\\theta)}{S_{X_i}(t_0;\\,\\theta)}\\frac{S_{X_i}(t_2;\\,\\theta)}{S_{X_i}(t_1;\\,\\theta)}  \\dots \\frac{S_{X_i}(t_j;\\,\\theta)}{S_{X_i}(t_{j-1};\\,\\theta)}\n\\end{align*}\\] This yields another way to write \\(S_{X_i}(t;\\,\\theta)\\): \\[\\begin{align}\\label{eq:discrete-survival}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j \\leq t} (1 - \\lambda_{X_i}(t_j)).\n\\end{align}\\] It turns out that we can write the survival function for continuous random variables in the same way. ## Connection between discrete and continuous survival functions {#sec:disc-continue} Recall the definition of the hazard function: \\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html",
    "title": "Lecture-1",
    "section": "",
    "text": "Defined in the book roughly as missing values that would be meaningful for you analysis if it had been observed\nMissing data is ubiquitous\nI’d bet everyone has analyzed data with missing values\nWhat did you do with the units that had missing data?"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#introduction",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#introduction",
    "title": "Lecture-1",
    "section": "",
    "text": "Defined in the book roughly as missing values that would be meaningful for you analysis if it had been observed\nMissing data is ubiquitous\nI’d bet everyone has analyzed data with missing values\nWhat did you do with the units that had missing data?"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#missing-data-patterns",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#missing-data-patterns",
    "title": "Lecture-1",
    "section": "Missing data patterns",
    "text": "Missing data patterns\n\n\nCourse will focus on rectangular or tabular datasets\nAssume we have \\(p\\) measurements on \\(n\\) units, arranged into matrix \\(\\mathbf{Y}\\)\nWe can create an \\(n \\times p\\) matrix \\(\\mathbf{M}\\), where \\(\\mathbf{M}_{ij} = 1\\) if \\(\\mathbf{Y}_{ij}\\) is missing and \\(\\mathbf{M}_{ij} = 0\\) if \\(\\mathbf{Y}_{ij}\\) is observed\n\nFor the next few slides, we’ll use \\(n=10\\) and \\(p=3\\)"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#univariate-missingness",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#univariate-missingness",
    "title": "Lecture-1",
    "section": "Univariate missingness",
    "text": "Univariate missingness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Graph with M\nplot(1, type = \"n\", xlim = c(0.5, 5.5), ylim = c(0, 1), axes = FALSE, xlab = \"\", ylab = \"\")\n\n# Define the bar positions and heights\nblue_heights &lt;- c(1, 1, 0.4)\nred_heights &lt;- c(0, 0, 0.6)\n\n# Adjusted bar positions for no spacing\nx_positions &lt;- c(1, 1.5, 2)\nbar_width &lt;- 0.5\n\n# Draw the blue bars\nrect(x_positions[1] - bar_width / 2, 0, x_positions[1] + bar_width / 2, blue_heights[1], col = \"#ADD8E6\", border = \"blue\", lwd = 2)\nrect(x_positions[2] - bar_width / 2, red_heights[2], x_positions[2] + bar_width / 2, red_heights[2] + blue_heights[2], col = \"#ADD8E6\", border = \"blue\", lwd = 2)\nrect(x_positions[3] - bar_width / 2, red_heights[3], x_positions[3] + bar_width / 2, red_heights[3] + blue_heights[3], col = \"#ADD8E6\", border = \"blue\", lwd = 2)\n\n# Draw the red bars\nrect(x_positions[3] - bar_width / 2, 0, x_positions[3] + bar_width / 2, red_heights[3], col = \"#FFCCCB\", border = \"red\", lwd = 2)\n\n# Add annotations\ntext(x_positions[2], red_heights[2] - 0.2, \"?\", col = \"red\", cex = 1.5)\ntext(x_positions[3], red_heights[3] - 0.2, \"?\", col = \"red\", cex = 1.5)\n\n# Add category labels\ntext(x_positions[1], par(\"usr\")[4] + 0.05, labels = bquote(Y[1]), xpd = TRUE)\ntext(x_positions[2], par(\"usr\")[4] + 0.05, labels = bquote(Y[2]), xpd = TRUE)\ntext(x_positions[3], par(\"usr\")[4] + 0.05, labels = bquote(Y[3]), xpd = TRUE)\n\n\nx_positions &lt;- c(2.5, 3, 3.5)\nbar_width &lt;- 0.5\n\n# Draw the M columns\nrect(x_positions[1] - bar_width / 2, 0, x_positions[1] + bar_width / 2, 1, col = \"white\", border = \"blue\", lwd = 2)\nrect(x_positions[2] - bar_width / 2, 0, x_positions[2] + bar_width / 2, 1, col = \"white\", border = \"blue\", lwd = 2)\nrect(x_positions[3] - bar_width / 2, 0, x_positions[3] + bar_width / 2, 1, col = \"white\", border = \"blue\", lwd = 2)\ndf &lt;- data.frame(y = seq(0.05,0.95, length.out = 10),\n                 m1 = rep(0,10),\n                 m2 = rep(0,10),\n                 m3 = c(rep(1,6),rep(0,4)))\ntext(rep(x_positions[1],10), df$y, label = df$m1)\ntext(rep(x_positions[2],10), df$y, label = df$m2)\ntext(rep(x_positions[3],6), df$y[1:6], label = df$m3[1:6], col = \"red\")\ntext(rep(x_positions[3],4), df$y[7:10], label = df$m3[7:10])\n\n# Add category labels\ntext(x_positions[1], par(\"usr\")[4] + 0.05, labels = bquote(M[1]), xpd = TRUE)\ntext(x_positions[2], par(\"usr\")[4] + 0.05, labels = bquote(M[2]), xpd = TRUE)\ntext(x_positions[3], par(\"usr\")[4] + 0.05, labels = bquote(M[3]), xpd = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nExample for this sort of missingness would be unit nonresponse on a survey; \\(Y_1, Y_2\\) would be design variables that are known for all potential survey respondenents, while \\(Y_3\\) would be measurement of interest"
  },
  {
    "objectID": "missing-data-material-W-26/lecture-slides/lecture-1.html#does-missingness-matter",
    "href": "missing-data-material-W-26/lecture-slides/lecture-1.html#does-missingness-matter",
    "title": "Lecture-1",
    "section": "Does missingness matter?",
    "text": "Does missingness matter?\n\n\nThe book’s definition of missing data implies that knowing the missing value would meaningfully change our inferences\nWe can bound this"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html",
    "title": "Missing data lecture 1",
    "section": "",
    "text": "At first glance the title of our textbook, Statistical Analysis with Missing Data, seems redundant. What is statistics but the study of drawing conclusions from limited data? One of the most basic applications of statistics is about how to make inferences about a population quantity from a simple random sample of that population. The measurements from those who were not sampled are, by definition, missing. Because we have a simple random sample from our population, however, we can be sure that the sample mean of the measurements from the sampled units will be an unbiased estimator of the population-level mean. What if some of the sampled units refuse to participate in the survey? Suppose the survey asks about income, and some respondents refuse to report income?\nOur definition of missing data for this course will condition on the sample drawn; in other words, we will focus on values that haven’t been recorded for the sample of data we observe. Suppose we have a sample of \\(n\\) units, on which we have \\(K\\) measurements, collected into a \\(n \\times K\\) matrix \\(Y\\) with elements \\(y_{ij}\\).\nPaired with this matrix of measurements is another \\(n \\times K\\) matrix \\(M\\) with elements \\(m_{ij}\\), called the missingness indicator matrix. This matrix encodes the information about whether the \\((i,j)^\\mathrm{th}\\) element of \\(Y\\) is missing or not. Let \\(m_{ij} = 1\\) if \\(y_{ij}\\) is missing, and \\(0\\) if \\(y_{ij}\\) is observed.\nThroughout the course we’ll keep in mind that we’re never looking to explicitly fill in the missing values with a single ``best” value. Instead, we’re going to consider the distribution of possible values that could be filled in and look at how our estimates change for each filled-in dataset.\n\n\nThe textbook defines missing data roughly as missing values that would be meaningful for your analysis if it had been observed (Roderick JA Little and Rubin 2019). The word ``meaningful” is doing a lot of work here; we’ll need to define meaningful for ourselves. Conceptually, we need to define why data is missing in the first place. For example, let’s say we’re analyzing the results from a longitudinal trial comparing Infliximab for severe Crohn’s disease, which is an autoinflammatory disease and a category of inflammatory bowel disease (IBD) (note this is not IBS, or irritable bowel syndrome, which is fairly common), to a new treatment. The investigators are interested comparing Crohn’s Disease Activity Index (CDAI) between the two arms, which is a measure of the severity of symptoms in Crohn’s patients. Imagine two scenarios: one in which an enrolled patient subsequently drops out of the study after several infusions of the new treatment, and one in which an enrolled patient dies prior to the end of the study. In the first scenario, it makes sense to consider that patient’s measure of CDAI to be missing, whereas in the second scenario, it doesn’t make much sense to think about imputing a CDAI for someone who has died.\nLet’s say we’re in the first scenario, and we’re confronted with some proportion of patients that have dropped out of the study. Dropout is common in longitudinal studies; in one dataset we’ll encounter later investigating the efficacy of a treatment for schizophrenia, about 37% of patients dropped out by the end of the study (Van Der Elst et al. 2024). The default way to deal with missing data in R is to use na.omit. This is also known as complete case analysis (CC analysis). How much would just using the complete cases impact our inferences?\nBeing statisticians, we’ll focus on the bias and variance of our estimates. Let’s make things more concrete. Suppose in our Crohn’s trial the outcome \\(Y_{i}\\) is the change from baseline CDAI to CDAI at the final visit. Assume that all participants have an initial CDAI, so \\(M_i\\) is \\(1\\) if the individual dropped out prior the final visit. As for bias, one can show (read: you’ll show on HW 1) that the following relationship holds: \\[\n\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y} = \\frac{\\text{Cov}(Y, \\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] We can bound the magnitude of this expression by using Cauchy-Schwarz:\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\frac{\\text{SD}(Y) \\text{SD}(\\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] which simplifies to \\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}}\\text{SD}(Y)\n\\] This makes sense; for a given proportion of missing values, the larger the variance of \\(Y\\) the larger the potential bias will be by excluding some of them.\nThis standard deviation is of course not estimable because we don’t have the missing values of \\(Y\\), but for variables with bounded support \\(Y \\in [a, b]\\), we can get a further upper bound on the standard deviation using Popoviciu’s inequality: \\[\n\\text{Var}(Y) \\leq \\frac{(b - a)^2}{4}\n\\] In the next code cell, I’ve written an expression for the upper bound of CDAI.\n\nupper_bound_CDAI &lt;- \n  7 * 20 * 2 +  \n  1 * 30 +\n  7 * 3 * 5 +\n  7 * 4 * 7 +\n  6 * 20 +\n  5 * 10 +\n  (42 - 3) * 6 +\n  50\n\nThe upper bound is approximately 1100 (see (Best 2006) for more details, the only value that may not have a hard upper bound is the number of stools per day, which I’ve set to \\(20\\) above, but may be higher)\nThis is useful in our hypothetical example because the CDAI scale runs from \\([0,1100]\\)\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}} \\times 550\n\\]\n\n\n\n\n\n\n\n\n\nWhile these are worst-case bounds, this shows that even small proportions of missing values can impact inferences if the missingness is correlated with the outcome value.\nTo give a sense of how large a proportion of patient might have missing values, a recent clinical trial evaluating Skyrizi, a popular treatment for Crohn’s, had three treatment groups (1 placebo and two active treatment arms), of which 12%, 8% and 15% had missing values for CDAI at week 52.\nThere is also the variance to consider. Even if the covariance between the missing values is zero, we will lose efficiency by dropping observations that have missing values. In the case where our estimator is a sample mean, and there are \\(n\\) units with \\(m\\) missing values, the variance of the CC estimator will be larger by \\(1 + \\frac{m}{n - m}\\).\nThus, in many cases, even if there are only small proportions of missing values, it can make sense to use partial information from incomplete cases to improve our estimators."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#does-missingness-matter",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#does-missingness-matter",
    "title": "Missing data lecture 1",
    "section": "",
    "text": "The textbook defines missing data roughly as missing values that would be meaningful for your analysis if it had been observed (Roderick JA Little and Rubin 2019). The word ``meaningful” is doing a lot of work here; we’ll need to define meaningful for ourselves. Conceptually, we need to define why data is missing in the first place. For example, let’s say we’re analyzing the results from a longitudinal trial comparing Infliximab for severe Crohn’s disease, which is an autoinflammatory disease and a category of inflammatory bowel disease (IBD) (note this is not IBS, or irritable bowel syndrome, which is fairly common), to a new treatment. The investigators are interested comparing Crohn’s Disease Activity Index (CDAI) between the two arms, which is a measure of the severity of symptoms in Crohn’s patients. Imagine two scenarios: one in which an enrolled patient subsequently drops out of the study after several infusions of the new treatment, and one in which an enrolled patient dies prior to the end of the study. In the first scenario, it makes sense to consider that patient’s measure of CDAI to be missing, whereas in the second scenario, it doesn’t make much sense to think about imputing a CDAI for someone who has died.\nLet’s say we’re in the first scenario, and we’re confronted with some proportion of patients that have dropped out of the study. Dropout is common in longitudinal studies; in one dataset we’ll encounter later investigating the efficacy of a treatment for schizophrenia, about 37% of patients dropped out by the end of the study (Van Der Elst et al. 2024). The default way to deal with missing data in R is to use na.omit. This is also known as complete case analysis (CC analysis). How much would just using the complete cases impact our inferences?\nBeing statisticians, we’ll focus on the bias and variance of our estimates. Let’s make things more concrete. Suppose in our Crohn’s trial the outcome \\(Y_{i}\\) is the change from baseline CDAI to CDAI at the final visit. Assume that all participants have an initial CDAI, so \\(M_i\\) is \\(1\\) if the individual dropped out prior the final visit. As for bias, one can show (read: you’ll show on HW 1) that the following relationship holds: \\[\n\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y} = \\frac{\\text{Cov}(Y, \\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] We can bound the magnitude of this expression by using Cauchy-Schwarz:\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\frac{\\text{SD}(Y) \\text{SD}(\\ind{M=0})}{\\Exp{\\ind{M=0}}}\n\\] which simplifies to \\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}}\\text{SD}(Y)\n\\] This makes sense; for a given proportion of missing values, the larger the variance of \\(Y\\) the larger the potential bias will be by excluding some of them.\nThis standard deviation is of course not estimable because we don’t have the missing values of \\(Y\\), but for variables with bounded support \\(Y \\in [a, b]\\), we can get a further upper bound on the standard deviation using Popoviciu’s inequality: \\[\n\\text{Var}(Y) \\leq \\frac{(b - a)^2}{4}\n\\] In the next code cell, I’ve written an expression for the upper bound of CDAI.\n\nupper_bound_CDAI &lt;- \n  7 * 20 * 2 +  \n  1 * 30 +\n  7 * 3 * 5 +\n  7 * 4 * 7 +\n  6 * 20 +\n  5 * 10 +\n  (42 - 3) * 6 +\n  50\n\nThe upper bound is approximately 1100 (see (Best 2006) for more details, the only value that may not have a hard upper bound is the number of stools per day, which I’ve set to \\(20\\) above, but may be higher)\nThis is useful in our hypothetical example because the CDAI scale runs from \\([0,1100]\\)\n\\[\n\\abs{\\Exp{Y \\mid \\ind{M = 0}} - \\Exp{Y}} \\leq \\sqrt{\\frac{\\Exp{\\ind{M=1}}}{\\Exp{\\ind{M=0}}}} \\times 550\n\\]\n\n\n\n\n\n\n\n\n\nWhile these are worst-case bounds, this shows that even small proportions of missing values can impact inferences if the missingness is correlated with the outcome value.\nTo give a sense of how large a proportion of patient might have missing values, a recent clinical trial evaluating Skyrizi, a popular treatment for Crohn’s, had three treatment groups (1 placebo and two active treatment arms), of which 12%, 8% and 15% had missing values for CDAI at week 52.\nThere is also the variance to consider. Even if the covariance between the missing values is zero, we will lose efficiency by dropping observations that have missing values. In the case where our estimator is a sample mean, and there are \\(n\\) units with \\(m\\) missing values, the variance of the CC estimator will be larger by \\(1 + \\frac{m}{n - m}\\).\nThus, in many cases, even if there are only small proportions of missing values, it can make sense to use partial information from incomplete cases to improve our estimators."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-patterns",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-patterns",
    "title": "Missing data lecture 1",
    "section": "Missingness patterns",
    "text": "Missingness patterns\nConsider three variables, \\(Y_1, Y_2, Y_3\\) that we’ve measured on a sample of \\(n\\) participants. Each variable has an associated binary vector: \\(M_1, M_2, M_3\\). Missingness patterns refer to the observed sample space for the vectors \\([m_{i1}, m_{i2}, m_{i3}]\\). The simplest missingness pattern is where only one of the variables is subject to missingness: \\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,0,1]\\}.\n\\] This is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Univariate missingness pattern (Credit goes in part to ChatGPT who wrote the initial version of this plot)\n\n\n\n\n\nWe could also have multivariate missingness with only two missingness patterns \\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,1,1]\\}\n\\] which is shown in Figure 2:\n\n\n\n\n\n\n\n\nFigure 2: Multivariate missingness pattern\n\n\n\n\n\nWe could have something called monotone missingness, where we can order the missingness matrix such that if \\(M_{i2} = 1\\) then so is \\(M_{i3} = 1\\): \\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,1,1], [0,0,1]\\}\n\\]\n\n\n\n\n\n\n\n\nFigure 3: Monotone missingness pattern\n\n\n\n\n\nThe least restricted missingness pattern is called a general pattern. This would be a case where there is no special structure. Of course, for \\(p\\) variables each subject to missingness, the general missingness has a sample space of size \\(2^p\\)\nIf we consider that only \\(Y_2, Y_3\\) are subject to missingness, then we have the following, depicted in :\n\\[\n[m_{i1}, m_{i2}, m_{i3}] \\in \\{[0,0,0], [0,1,0], [0,0,1], [0,1,1]\\}\n\\]\n\n\n\n\n\n\n\n\nFigure 4: General missingness pattern for two variables\n\n\n\n\n\nNot surprisingly, the general missingness pattern is the most realistic. You might imagine these patterns occurring during a survey. The pattern \\([0,1,1]\\) represents unit nonresponse (where a person who is contacted declines to participate in the survey), while \\([0,1,0], [0,0,1]\\) would be item nonresponse.\nThe reason that categorizing patterns of missingness is useful is because it can suggest different methods for dealing with missing data. It’s also something that is observable; missing values are not observable, but the patterns are. Thus, in the survey unit and item nonresponse, we might consider different strategies for dealing with unit nonresponse and item nonresponse."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-mechanisms",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#missingness-mechanisms",
    "title": "Missing data lecture 1",
    "section": "Missingness mechanisms",
    "text": "Missingness mechanisms\nThe most important paper in missing data was published by Don Rubin in 1976 Rubin (1976). Somewhat surprisingly to me, this paper was rejected by many stats journals. Rod Little says that he was assigned to review the paper as a graduate student when it was submitted to Biometrika, and he was convinced the paper was wrong after writing a long review. Luckily Little was overridden by his advisor, David Cox, who thought the paper was right, and decided to accept the paper.\nThe paper was important because it formalized methods of modeling missingness indicators, or the \\(M\\) matrix from above. Prior to this paper, the \\(M\\) matrix was not considered an outcome that could be modeled. Rubin’s paper instead categorized \\(M\\) as a random variable, and determined how the conditional distribution \\(M \\mid Y\\) impacted inferences using only the observed values of \\(Y\\).\nThe various ways in which \\(M\\) can depend on \\(Y\\) is really an investigation of why a value is missing. Is a survey respondent unwilling to report their income because is high? Did the patient drop out of the study because of side-effects of a drug, or because the drug exacerbated their condition? Did a database error lead to the random dropping of records?\nThe crux of missing data analysis hinges in what we’re willing to believe about why data are missing. These beliefs aren’t typically testable, unless we have designed our study to have missingness1.\nAgain, let \\(M\\) be the matrix with \\((i,j)^\\text{th}\\) entry \\(M_{ij}\\). Further, let \\(M_i\\) be the \\(i^\\text{th}\\) row of \\(M\\), let \\(\\tilde{m}_i\\) be a particular realization of \\(M_i\\) and let \\(m_i\\) be a dummy value. Let \\(Y\\) be the \\(n \\times K\\) matrix of measurements of interest (possibly including covariates), with elements \\(Y_{ij}\\). Let \\(Y_i\\) be the \\(i^\\mathrm{th}\\) row of matrix \\(Y\\), while \\(\\tilde{y}_i\\) is a particular realization of \\(Y_i\\) and \\(y_i\\) is a dummy value. We assume for simplicity (and for much of the book) that \\((M_i, Y_i)\\) are independent between rows.\nTo put a finer point on it, there are generally three categories of missingness mechanisms. They each relate to the distribution: \\[\nf_{M_i \\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi),\n\\] where \\(\\phi\\) are the parameters that govern the missingness mechanism.\nIt will be useful in the next subsections to define the following partitions of \\(Y_i\\): Let \\[Y_{(0)i} = (Y_{ij} : M_{ij} = 0)\\] be the vector of components of \\(Y_i\\) that are observed for unit \\(i\\) and let \\[Y_{(1)i} = (Y_{ij} : M_{ij} = 1)\\] denote the vector of \\(Y_i\\) components that are missing for \\(Y_i\\). The corresponding values, \\(\\tilde{y}_{(0)i}, \\tilde{y}_{(1)i}\\). For example: \\[\\tilde{y}_{(0)i} = (\\tilde{y}_{ij} : \\tilde{m}_{ij} = 0)\\] are particular realizations of these variables, while \\(y_{(0)i}, y_{(1)i}\\) will be dummy values. For example: \\[y_{(0)i} = (y_{ij} : m_{ij} = 0)\\]\nWe can see that \\(Y_{(0)i}, Y_{(1)i}, \\tilde{y}_{(0)i}, y_{(1)i}, y_{(0)i}, y_{(1)i}\\) depend on the missingness indicator variables, \\(M_i, \\tilde{m}_i, m_i\\), respectively.\n\nMissing completely at random (MCAR)\nThe simplest mechanism is called missing-completely-at-random (MCAR). This is where the missingness is unrelated to the outcome. Data are said to MCAR if the following holds for all \\(i\\), \\(y_i\\), \\(y^\\star_i\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi) = f_{M\\mid Y}(M_i = \\tilde{m}_i \\mid Y_i = y^*_i, \\phi).\n\\] An imporant clarification is that this NOT a conditional independence assumption. It is an assumption about the evaluation of the conditional mass function \\(f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi)\\) at a specific \\(m_i\\) (Mealli and Rubin 2015). This point is often misunderstood (including by me).\nConditional independence, \\(M_i \\indy Y_i\\) would be characterized as missing-always-completely-at-random (MACAR): Data are said to MACAR if the following holds for all \\(i\\), \\(m_i\\), \\(y_i\\), \\(y^\\star_i\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = m_i \\mid Y_i = y_i, \\phi) = f_{M\\mid Y}(M_i = m_i \\mid Y_i = y^*_i, \\phi).\n\\] The next mechanism is less restrictive than MCAR.\n\n\nMissing at random (MAR)\nMissing at random data are characterized by the following equality for all \\(i\\), \\(y_{(1)i}\\), \\(y^*_{(1)i}\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i} \\phi) = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i} \\phi)  \n\\] Again, as with MCAR, this is a statement about the evaluation of the function \\(f_{M\\mid Y}(m_i \\mid y_i, \\phi)\\). We can define a missing-at-random variant, missing-always-at-random (MAAR) that is equivalent to \\(M \\indy Y_{(1)i} \\mid Y_{(0)i}\\)2.\nThe following example is adapted from Mealli and Rubin (2015): Suppose we’re analyzing data from that Crohn’s disease trial and \\(Y_i\\) has two components: \\(Y_{i1}\\) is CDAI at visit 1 and \\(Y_{i2}\\) is CDAI at visit 2. For patient \\(i\\) suppose that \\(m_i = (0, 1)\\), or that \\(Y_{i2}\\) is missing but \\(Y_{i1}\\) is observed.\nBringing this example into line with our previous notation, \\(Y_{(0)i} \\equiv Y_{i1}\\) and \\(Y_{(1)i} \\equiv Y_{i2}\\). Then \\(\\tilde{y}_{(0)i} \\equiv \\tilde{y}_{i1}\\).\nConsider two scenarios:\n\n\\(Y_{i2}\\) is missing because \\(Y_{i1} &gt; \\phi\\). Given that \\(0^0 = 1\\), in this scenario the missingness mechanism can be translated as: \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{\\tilde{y}_{(0)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{\\tilde{y}_{(0)i} &gt; \\phi})^{1 - m_{i2}}\n\\]\n\\(Y_{i2}\\) is missing because \\(Y_{i2} &gt; \\phi\\). This mechanism is translated as \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{y_{(1)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{y_{(1)i} &gt; \\phi})^{1 - m_{i2}}\n\\] In scenario 1 the data are MAR because the mass function is a function of \\(\\tilde{y}_{(0)i}\\) only, (i.e it depends only on \\(Y_{i1}\\)), while in scenario 2 the data do not satisfy the definition of MAR."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-1-notes.html#footnotes",
    "href": "missing-data-material-W-26/notes/lecture-1-notes.html#footnotes",
    "title": "Missing data lecture 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne way that can happen is in a univariate missingness setting where \\(Y_3\\) respresents a hard-to-measure quantity (say number of REM cycles per night) and \\(Y_1, Y_2\\) are proxies for this quantity. If we randomly select a subset of our participants in which to measure \\(Y_3\\) then we know that missingness \\(M\\) is not related to \\(Y\\) (assuming that none of our selected participants refuse to participate!).↩︎\nMissing always at random (MAAR) Missing always at random data are characterized by the following equality for all \\(i\\), \\(m_i\\) \\(y_{(1)i}\\), \\(y^*_{(1)i}\\): \\[\nf_{M_i\\mid Y_i}(M_i = m_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = f_{M_i\\mid Y_i}(M_i = m_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i}, \\phi)  \n\\] This is a more restrictive assumption than MAR alone, though one could see why MAAR might be invoked for asymptotic arguments (Roderick J. Little 2021).↩︎"
  },
  {
    "objectID": "survival-material/lecture-2.html#connection-between-discrete-and-continuous-survival-functions-secdisc-continue-recall-the-definition-of-the-hazard-function",
    "href": "survival-material/lecture-2.html#connection-between-discrete-and-continuous-survival-functions-secdisc-continue-recall-the-definition-of-the-hazard-function",
    "title": "Lecture 2",
    "section": "4.1 Connection between discrete and continuous survival functions {#sec:disc-continue} Recall the definition of the hazard function:",
    "text": "4.1 Connection between discrete and continuous survival functions {#sec:disc-continue} Recall the definition of the hazard function:\n\\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use Equation 1 to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "survival-material/lecture-2.html#sec-disc-continue",
    "href": "survival-material/lecture-2.html#sec-disc-continue",
    "title": "Lecture 2",
    "section": "4.1 Connection between discrete and continuous survival functions",
    "text": "4.1 Connection between discrete and continuous survival functions\nRecall the definition of the hazard function:\n\\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use Equation 1 to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html",
    "title": "Missing data lecture 2",
    "section": "",
    "text": "Notation\nMeaning\nDomain\n\n\n\n\n\\(Y\\)\n\\(n\\times K\\) matrix, collection of measurements of interest\n\\(\\R^{n \\times K}\\)\n\n\n\\(\\tilde{y}\\)\nRealization of \\(Y\\)\n\\(\\R^{n \\times K}\\)\n\n\n\\(Y_{ij}\\)\nElement of \\(Y\\), random variables\n\\(\\R\\)\n\n\n\\(\\tilde{y}_{ij}\\)\nParticular realization of \\(Y_{ij}\\)\n\\(\\R\\)\n\n\n\\(Y_{i}\\)\n\\(i^\\mathrm{th}\\) row of \\(Y\\), random variables\n\\(\\R^K\\)\n\n\n\\(\\tilde{y}_{i}\\)\nParticular realization of \\(Y_{i}\\)\n\\(\\R^K\\)\n\n\n\\(y_{i}\\)\nArbitrary element of \\(\\R^K\\), for use in density functions related to \\(Y_i\\)\n\\(\\R^K\\)\n\n\n\\(M\\)\n\\(n\\times K\\) binary matrix of missingness indicators\n\\(\\{0,1\\}^{n \\times K}\\)\n\n\n\\(\\tilde{m}\\)\nRealization of \\(M\\)\n\\(\\{0,1\\}^{n \\times K}\\)\n\n\n\\(M_{ij}\\)\nElement of \\(M\\), random variable\n\\(\\{0,1\\}\\)\n\n\n\\(M_{i}\\)\n\\(i^\\mathrm{th}\\) row of \\(M\\), random variables\n\\(\\{0,1\\}^{K}\\)\n\n\n\\(\\tilde{m}_{ij}\\)\nRealization of \\(M_{ij}\\)\n\\(\\{0,1\\}\\)\n\n\n\\(\\tilde{m}_{i}\\)\nRealization of \\(M_{i}\\)\n\\(\\{0,1\\}^K\\)\n\n\n\\(m_{i}\\)\nArbitrary element of \\(\\{0,1\\}^K\\) for use in PMFs for \\(M_i\\)\n\\(\\{0,1\\}^K\\)\n\n\n\\(R_i\\)\nNumber of observed values for row \\(i\\) in \\(Y\\), \\(\\sum_{j=1}^K (1 - M_{ij})\\)\n\\(\\mathbb{N}\\)\n\n\n\\(Y_{(0)i}\\)\nObserved elements of \\(Y_i\\)\n\\(\\R^{R_i}\\)\n\n\n\\(\\tilde{y}_{(0)i}\\)\nRealization of \\(Y_{(0)i}\\)\n\\(\\R^{R_i}\\)\n\n\n\\(y_{(0)i}\\)\nArbitrary element of \\(\\R^{R_i}\\)\n\\(\\R^{R_i}\\)\n\n\n\\(Y_{(1)i}\\)\nUnobserved or missing elements of \\(Y_i\\)\n\\(\\R^{K - R_i}\\)\n\n\n\\(\\tilde{y}_{(1)i}\\)\nRealization of \\(Y_{(1)i}\\)\n\\(\\R^{K - R_i}\\)\n\n\n\\(y_{(1)i}, y^\\star_{(1)i}\\)\nArbitrary elements of \\(\\R^{K - R_i}\\)\n\\(\\R^{K - R_i}\\)"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#footnotes",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#footnotes",
    "title": "Missing data lecture 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne exception is regression with missing predictors; scenarios in which the predictors have MNAR missingness that doesn’t depend on the outcome can be analyzed with CCA.↩︎"
  },
  {
    "objectID": "survival-material/lecture-3.html",
    "href": "survival-material/lecture-3.html",
    "title": "Lecture 3",
    "section": "",
    "text": "Now let’s delve into more detail about censoring, and how the likelihood can be built up from the hazard function and the survival function. define censoring as imprecise knowledge about an event time. If we observe a failure or an event exactly, the observation is not censored, but if we know only that an observation occurred within a range of values, we say the observation is censored. Let \\(X_i\\), as usual, be our failure time, which is not completely observed. Instead if:\n\n\\(X_i \\in [C, \\infty)\\), the observation is right censored\n\\(X_i \\in [0, U)\\), the observation is left censored\n\\(X_i \\in [C, U)\\), the observation is interval censored"
  },
  {
    "objectID": "survival-material/lecture-3.html#properties-of-the-hazard-function",
    "href": "survival-material/lecture-3.html#properties-of-the-hazard-function",
    "title": "Lecture 2",
    "section": "",
    "text": "The relationship \\(S_{X_i}(u;\\,\\theta) = \\exp \\lp -\\int_{0}^u\\lambda_{X_i}(t) dt\\rp\\) and the properties of the survival function reveal the following facts about the hazard function and highlight its differences with a probability density.\n\n\\(\\lim_{t\\to\\infty} S_{X_i}(t;\\,\\theta) = 0\\) implies that \\(\\lim_{t\\to\\infty} \\int_0^t \\lambda_X(u) du = \\infty\\)\nGiven that \\(S_{X_i}(t;\\,\\theta)\\) is a nonincreasing function, \\(\\lambda_X(t) \\geq 0\\) for all \\(t\\).\n\nSo unlike a probability density function, \\(\\lambda_X(t)\\) isn’t integrable over the support of the random variable."
  },
  {
    "objectID": "survival-material/lecture-3.html#sec-disc-continue",
    "href": "survival-material/lecture-3.html#sec-disc-continue",
    "title": "Lecture 2",
    "section": "4.1 Connection between discrete and continuous survival functions",
    "text": "4.1 Connection between discrete and continuous survival functions\nRecall the definition of the hazard function:\n\\[\n\\lambda_{X_i}(t) = \\lim_{\\Delta t \\searrow 0}\\frac{1}{\\Delta t}\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\n\\] Note that $_{X_i}(t) ,t $ is approximately \\(\\Prob{t \\leq X &lt; t + \\Delta t \\mid X \\geq t}{\\theta}\\). Let \\(\\mathcal{T}\\) be a partition of \\((0,\\infty)\\) with partition size \\(\\Delta t\\), \\(t_0 = 0\\): \\[\n\\mathcal{T} = \\bigcup_{j=0}^\\infty [t_j, t_j + \\Delta t).\n\\] Then we can use Equation 1 to represent the survival function: \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) = \\prod_{j \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t).\n\\end{align}\\] We can show that as the partition of the time domain gets finer and finer, we will recover \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\) \\[\\begin{align}\nS_{X_i}(t;\\,\\theta) & = \\prod_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} (1 - \\lambda_{X_i}(t_j)\\Delta t) \\\\\n\\log S_{X_i}(t;\\,\\theta)& = \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} \\log(1 - \\lambda_{X_i}(t_j)\\Delta t)\n\\end{align}\\] We use the Taylor expansion of \\(\\log(1 - \\lambda_{X_i}(t_j) \\Delta t)\\) for small \\(\\lambda_{X_i}(t_j) \\Delta t\\), assuming that \\(\\lambda_{X_i}(t)\\) is sufficiently well-behaved for all \\(t\\). \\[\n\\log(1 - \\lambda_{X_i}(t_j) \\Delta t) \\approxeq -\\lambda_{X_i}(t_j) \\Delta t.\n\\] Then \\[\\begin{align}\n\\log S_{X_i}(t;\\,\\theta)& \\approxeq \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t\n\\end{align}\\] As \\[\n\\lim_{\\Delta t \\searrow 0} \\sum_{j \\in \\mathcal{T} \\mid t_j + \\Delta t \\leq t} -\\lambda_{X_i}(t_j)\\Delta t = -\\int_0^t \\lambda_{X_i}(u) du.\n\\] So, \\(S_{X_i}(t;\\,\\theta) = \\exp(-\\int_0^t\\lambda_{X_i}(u)du)\\), or \\[\\begin{align}\\label{eq:suvival-exp-cumulative-hazard}\nS_{X_i}(t;\\,\\theta) = \\exp(-\\lambda_{X_i}(t))\n\\end{align}\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#missingness-mechanisms",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#missingness-mechanisms",
    "title": "Missing data lecture 2",
    "section": "Missingness mechanisms",
    "text": "Missingness mechanisms\nMechanisms relate to the distribution: \\[\nf_{M_i \\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi),\n\\] where \\(\\phi\\) are the parameters that govern the missingness mechanism.\n\nMissing completely at random (MCAR)\nData are said to be MCAR if the following holds for all \\(i\\), \\(y_i\\), \\(y^\\star_i\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_i = y_i, \\phi) = f_{M\\mid Y}(M_i = \\tilde{m}_i \\mid Y_i = y^\\star_i, \\phi).\n\\]\n\n\nMissing at random (MAR)\nMissing at random data are characterized by the following equality for all \\(i\\), \\(y_{(1)i}\\), \\(y^\\star_{(1)i}\\), and \\(\\phi\\): \\[\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i}, \\phi)  \n\\]"
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#continuing-with-our-example-from-last-lecture",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#continuing-with-our-example-from-last-lecture",
    "title": "Missing data lecture 2",
    "section": "Continuing with our example from last lecture",
    "text": "Continuing with our example from last lecture\nThe following example is adapted from Mealli and Rubin (2015): Suppose we’re analyzing data from that Crohn’s disease trial and \\(Y_i\\) has two components: \\(Y_{i1}\\) is CDAI at visit 1 and \\(Y_{i2}\\) is CDAI at visit 2. For patient \\(i\\) suppose that \\(m_i = (0, 1)\\), or that \\(Y_{i2}\\) is missing but \\(Y_{i1}\\) is observed.\nBringing this example into line with our previous notation, \\(Y_{(0)i} \\equiv Y_{i1}\\) and \\(Y_{(1)i} \\equiv Y_{i2}\\). Then \\(\\tilde{y}_{(0)i} \\equiv \\tilde{y}_{i1}\\).\nConsider two scenarios:\n\n\\(Y_{i2}\\) is missing because \\(Y_{i1} &gt; \\phi\\). Given that \\(0^0 = 1\\), in this scenario the missingness mechanism can be translated as: \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{\\tilde{y}_{(0)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{\\tilde{y}_{(0)i} &gt; \\phi})^{1 - m_{i2}}\n\\]\n\\(Y_{i2}\\) is missing because \\(Y_{i2} &gt; \\phi\\). This mechanism is translated as \\[\nf_{M_{i2}\\mid Y_i}(M_{i2} = m_{i2} \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) = \\ind{y_{(1)i} &gt; \\phi}^{m_{i2}}(1 - \\ind{y_{(1)i} &gt; \\phi})^{1 - m_{i2}}\n\\] In scenario 1 the data are MAR because the mass function is a function of \\(\\tilde{y}_{(0)i}\\) only, (i.e it depends only on \\(Y_{i1}\\)), while in scenario 2 the data do not satisfy the definition of MAR.\n\n\nGeneralizing the example\nLet’s make this example more general. The following is from Little and Rubin (2019, 23). Again consider the bivariate case with \\(y_{i1}, y_{i2}\\). There are 4 possible missing data patterns: \\[\n(m_{i1}, m_{i2}) \\in \\{(0,0), (0,1), (1,0), (1,1)\\}\n\\] We’ll need to define \\(f_{M \\mid Y}(m_{i1} = r, m_{i2} = s \\mid y_{i1}, y_{i2}, \\phi)\\). To simplify the notation, let \\[\ng_{rs}(y_{i1}, y_{i2}, \\phi) = f_{M \\mid Y}(m_{i1} = r, m_{i2} = s \\mid y_{i1}, y_{i2}, \\phi)\n\\] The MAR assumption implies the following: \\[\n\\begin{aligned}\ng_{11}(y_{i1}, y_{i2}, \\phi) & = g_{11}(\\phi) \\\\\ng_{01}(y_{i1}, y_{i2}, \\phi) & = g_{01}(y_{i1}, \\phi) \\\\\ng_{10}(y_{i1}, y_{i2}, \\phi) & = g_{10}(y_{i2}, \\phi) \\\\\ng_{00}(y_{i1}, y_{i2}, \\phi) & = 1 - g_{10}(y_{i2}, \\phi)  - g_{01}(y_{i1}, \\phi) -  g_{11}(\\phi)\n\\end{aligned}\n\\] Thus the probability that \\(y_{ij}\\) is missing can depend only on \\(y_{i(-j)}\\), which is a bit odd.\nLittle and Rubin (2019) proposes the following modification:\n\\[\n\\begin{aligned}\ng_{11}(y_{i1}, y_{i2}, \\phi) & = g_{1+}(y_{i1}, \\phi)g_{+1}(y_{i2}, \\phi) \\\\\ng_{01}(y_{i1}, y_{i2}, \\phi) & = (1 - g_{1+}(y_{i1}, \\phi))g_{+1}(y_{i2}, \\phi) \\\\\ng_{10}(y_{i1}, y_{i2}, \\phi) & = g_{1+}(y_{i1}, \\phi)(1 - g_{+1}(y_{i2}, \\phi)) \\\\\ng_{00}(y_{i1}, y_{i2}, \\phi) & = (1 - g_{1+}(y_{i1}, \\phi))(1 - g_{+1}(y_{i2}, \\phi))\n\\end{aligned}\n\\] While this is maybe more realistic, though it does make an assumption that \\(m_{i1}\\) and \\(m_{i2}\\) are conditionally independent given \\(y_{i1}, y_{i2}\\), it is also hard to estimate, because we won’t observe missing values of \\(y_{i1}\\) and \\(y_{i2}\\).\nThis is a scenario called missing-not-at-random, or MNAR. This is defined in the next subsection."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#univariate-mar-data",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#univariate-mar-data",
    "title": "Missing data lecture 2",
    "section": "Univariate MAR data",
    "text": "Univariate MAR data\nUnivariate MAR data is useful to consider because it elucidates a key assumption that can be used in MAR analyses.\nSuppose we have a single measurement on \\(n\\) individuals, \\(Y_i\\) and \\(r\\) of the individuals in our sample have observations \\(\\tilde{y}_i\\) while \\(n-r\\) individuals have missing values for \\(Y_i\\) (i.e. \\(\\tilde{y}_{(1)i} = \\tilde{y}_{i}\\), \\(\\tilde{y}_{(0)i} = \\emptyset\\)). Suppose we also have iid measurements and an iid missingness process.\nWriting out the missingness mechanism for this scenario gives us: \\[\n\\begin{aligned}\nf_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y_{(1)i}, \\phi) & = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{(0)i} = \\tilde{y}_{(0)i}, Y_{(1)i} = y^\\star_{(1)i}, \\phi) \\\\\n& = f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid \\phi)  \n\\end{aligned}\n\\] This implies something about the relationship between the units the have observed data and those that do not: \\[\n\\begin{aligned}\nf_{Y_i \\mid M_i}(Y_{i} = y_{i} \\mid M_i = \\tilde{m}_i, \\theta) & = \\frac{f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid Y_{i} = y_i, \\phi) f_{Y_i}(Y_i = y_i \\mid \\theta)}{f_{M_i\\mid Y_i}(M_i = \\tilde{m}_i \\mid \\phi)} \\\\\n& = f_{Y_i}(Y_i = y_i \\mid \\theta)\n\\end{aligned}\n\\] This implies that \\[\n\\begin{aligned}\nf_{Y_i \\mid M_i}(Y_{i} = y_{i} \\mid M_i = 1, \\theta) & = f_{Y_i \\mid M_i}(Y_{i} = y_{i} \\mid M_i = 0, \\theta)\n\\end{aligned}\n\\] Thus, we can use the distribution we learn from the complete cases as the distribution for the cases that are missing. This idea holds for more general missingness patterns with more measurements. We’ll see this later in the course."
  },
  {
    "objectID": "survival-material/lecture-3.html#type-i-censoring",
    "href": "survival-material/lecture-3.html#type-i-censoring",
    "title": "Lecture 3",
    "section": "2.1 Type I censoring",
    "text": "2.1 Type I censoring\nThe simplest censoring scenario is one in which all individuals have the same, nonrandom censoring time. Imagine a study is designed to follow \\(5\\) startups that are spun out of a tech incubator to study how long it takes a company to land its first contract. This information will be used for designing investments \\(2\\) years from the study date, so the study has a length of \\(1.75\\) years. We can say that all observations will have to have occurred, or not, by \\(1.75\\) years.\nFigure 1 shows a potential result of the study, where \\(2\\) out of the \\(5\\) companies have not landed a contract.\n\n\n\n\n\n\n\n\nFigure 1: Type I censoring\n\n\n\n\n\nIn this case, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3.html#generalized-type-i-censoring",
    "href": "survival-material/lecture-3.html#generalized-type-i-censoring",
    "title": "Lecture 3",
    "section": "2.2 Generalized type I censoring",
    "text": "2.2 Generalized type I censoring\nA more general scenario, which is closer to most examples in clinical trials, is when each individual has a different study entry time and the investigator has a preset study end time. This is called generalized Type I censoring. These study entry times are typically assumed to be independent of the survival time. This is shown in Figure 2.\n\n\n\n\n\n\n\n\nFigure 2: Example of generalized Type I censoring, where each individual has a separate study entry time.\n\n\n\n\n\nWhen study entry is independent from survival time, the analysis proceeds as shown in Figure 3.\n\n\n\n\n\n\n\n\nFigure 3: Example of generalized Type I censoring, viewed in patient time.\n\n\n\n\n\nFor generalized type I censoring, - For all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\n\\(\\delta_i = 1 \\implies T_i = X_i\\).\n\nThis is different from Type I censoring in that each individual has a different censoring time."
  },
  {
    "objectID": "survival-material/lecture-3.html#type-ii-censoring",
    "href": "survival-material/lecture-3.html#type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.3 Type II censoring",
    "text": "2.3 Type II censoring\nType II censoring occurs when all units have the same study entry time, but researchers design the study to end when \\(r &lt; n\\) units fail out of \\(n\\) total units under observation.\n\nFor the first \\(r\\), lucky or unlucky participants, \\(\\delta_i = 1 \\implies T_i = X_{(i)}\\) or the \\(i^\\mathrm{th}\\) order statistic.\nFor the remaining \\(n - r\\) individuals, \\(\\delta_i = 0 \\implies X_i &gt; X_{(r)}\\)."
  },
  {
    "objectID": "survival-material/lecture-3.html#generalized-type-ii-censoring",
    "href": "survival-material/lecture-3.html#generalized-type-ii-censoring",
    "title": "Lecture 3",
    "section": "2.4 Generalized Type II censoring",
    "text": "2.4 Generalized Type II censoring\nYou may be wondering, what happens when units have differing start times but we want to end the trial after the \\(r\\)-th failure? It turns out that this was not a solved problem until , which was quite surprising to me."
  },
  {
    "objectID": "survival-material/lecture-3.html#independent-censoring",
    "href": "survival-material/lecture-3.html#independent-censoring",
    "title": "Lecture 3",
    "section": "2.5 Independent censoring",
    "text": "2.5 Independent censoring\nA third type of censoring, helpfully called independent censoring, takes \\(X_i \\indy C_i\\), and thus conclusions similar to those of generalized type I censoring can be drawn:\n\nFor all individuals such that \\(\\delta_i = 0 \\implies X_i &gt; C_i\\)\n\\(\\delta_i = 1 \\implies T_i = X_i\\)."
  },
  {
    "objectID": "survival-material/lecture-3.html#reasons-for-informative-censoring",
    "href": "survival-material/lecture-3.html#reasons-for-informative-censoring",
    "title": "Lecture 3",
    "section": "3.1 Reasons for informative censoring",
    "text": "3.1 Reasons for informative censoring\nA simple hypothetical situation with informative censoring would be one in which sick patients are lost to follow-up."
  },
  {
    "objectID": "missing-data-material-W-26/notes/lecture-2-notes.html#bias-of-cca",
    "href": "missing-data-material-W-26/notes/lecture-2-notes.html#bias-of-cca",
    "title": "Missing data lecture 2",
    "section": "Bias of CCA",
    "text": "Bias of CCA\nWe showed earlier that the bias of the complete case analysis was dependent on the covariance between the missingness indicator and the response; another way to decompose the bias is via the following representation of the sample mean. Recall that \\(\\mathcal{R} = \\{i \\mid \\sum_{j=1}^K \\tilde{m}_{ij} = 0, i = 1, \\dots, n\\}\\); we can define the complement of this set \\(\\mathcal{R}^\\comp\\). Then we can represent the sample mean as: \\[\n\\bar{y} = \\frac{r}{n}\\bar{y}^\\mathrm{CC} + (1 - \\frac{r}{n})\\bar{y}^{\\mathcal{R}^\\comp}\n\\] Note that the sample mean may be unobservable because some units may be missing \\(y_i\\). Then, conditional on the missingness pattern that was observed, the bias of the\n\\[\n\\Exp{\\bar{y}^\\mathrm{CC} - \\bar{y} \\mid M = \\tilde{m}} = \\lp 1 - \\frac{r}{n}\\rp(\\Exp{\\bar{y}^\\mathrm{CC} \\mid M = \\tilde{m}} - \\Exp{\\bar{y}^{\\mathcal{R}^\\comp} \\mid M = \\tilde{m}})\n\\]"
  },
  {
    "objectID": "survival-material/lecture-3.html#likelihood-construction",
    "href": "survival-material/lecture-3.html#likelihood-construction",
    "title": "Lecture 3",
    "section": "4.1 Likelihood construction",
    "text": "4.1 Likelihood construction\nWe now turn to how to construct likelihoods in each of the prior scenarios, under censored or truncated data. As a reminder:\n\nLet \\(X_i\\) be the time to failure, or time to event for individual \\(i\\).\nLet \\(C_i\\) be the time to censoring. It may be helpful to think about \\(C_i\\) as the time to investigator measurement.\nLet \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\).\nLet \\(T_i = \\min(X_i, C_i)\\).\n\nWhen \\(\\delta_i = 1\\), we observe \\(T_i = X_i\\); this is the event that \\(\\{X_i = T_i, C_i \\geq X_i\\}\\). When \\(\\delta_i = 0\\), we observe \\(T_i = C_i\\); this is the event that \\(\\{C_i = T_i, C_i &lt; X_i\\}\\). Let the joint distribution of \\(X_i, C_i\\) be written as \\(P_\\theta(X &gt; x, C &gt; c)\\), and further let \\(\\theta = (\\eta, \\phi)\\) such that \\(P_\\theta(X &gt; x, C &gt; c) = P_\\eta(X &gt; x) P_\\phi(C &gt; c \\mid X &gt; x)\\). We showed that the likelihood corresponding to the random variables \\(T_i, \\delta_i\\), \\(f_{T_i, \\delta_i}(t, \\delta;\\,\\theta)\\), can be written in terms of partial derivatives of the joint density function when \\(X_i\\) and \\(C_i\\) are absolutely continuous random variables. \\[\\begin{align*}\n  f_{T_i, \\delta_i}(t, \\delta;\\,\\theta)  = \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq u, C \\geq t) \\mid_{u = t}  \\right)^\\delta \\left(-\\frac{\\partial}{\\partial u} P_\\theta(X \\geq t, C \\geq u) \\mid_{u = t}  \\right)^{1 - \\delta}\n\\end{align*}\\] Let’s rewrite the partial derivatives in terms of their limits: \\[\\begin{align*}\n   f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t, C \\geq t) \\right)^\\delta\n    \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(X \\geq t, t \\leq C &lt; t + \\Delta t)   \\right)^{1 - \\delta}\n\\end{align*}\\] We can factorize the distribution function: \\[\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(C \\geq t \\mid X \\geq t) P_\\eta(X \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(C \\geq t \\mid X \\geq t)P_\\eta(X \\geq t)   \\right)^{1 - \\delta}\n\\end{align*}\\]. Assuming noninformative censoring: \\[\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) & = \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\eta(t \\leq X &lt; t + \\Delta t \\mid X \\geq t)P_\\theta(C \\geq t \\mid X \\geq t) P_\\eta(X \\geq t) \\right)^\\delta  \\\\\n    &\\quad  \\times \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)P_\\theta(C \\geq t \\mid X \\geq t)P_\\eta(X \\geq t)   \\right)^{1 - \\delta}\n\\end{align*}\\].\nand rearranging and subbing in \\(\\lambda_\\eta(t)\\) for the hazard function: \\[\n\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta)  & = \\left(\\lambda_\\eta(t)\\right)^\\delta P_\\eta(X \\geq t)  P_\\theta(C \\geq t \\mid X \\geq t)  \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\right)^{1 - \\delta}\n\\end{align*}\\] Assuming that \\[\n\\begin{aligned}\nP_\\theta(C_i \\geq t \\mid X_i \\geq t) & = P_\\phi(C_i \\geq t \\mid X_i \\geq t) \\\\\nP_\\theta(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t) & = P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\n\\end{aligned}\n\\] and leads to the following \\[\\begin{align*}\n    f_{T_i, \\delta_i}(t, \\delta;\\,\\theta)  & = \\left(\\lambda_\\eta(t)\\right)^\\delta P_\\eta(X \\geq t)  P_\\phi(C \\geq t \\mid X \\geq t)^\\delta  \\left(\\lim_{\\Delta t \\searrow 0} \\frac{1}{\\Delta t} P_\\phi(t \\leq C &lt; t + \\Delta t \\mid X \\geq t, C \\geq t)\\right)^{1 - \\delta}\n\\end{align*}\\] This means that we can factorize the joint density: \\[f_{T_i, \\delta_i}(t, \\delta;\\,\\theta) = f_{X_i, \\delta_i}(t, \\delta;\\,\\eta) f_{C_i, \\delta_i}(t, \\delta;\\,\\phi).\\] Thus, noninformative censoring and parameter separability yield a separable joint density. This means that when we want to do maximum likelihood for survival data, we can ignore the model for the censoring times, \\(f_{C_i, \\delta_i}(t, \\delta;\\,\\phi)\\), and focus on only the model for the failure times: \\[f_{X_i, \\delta_i}(t, \\delta;\\,\\eta) = \\lambda_{X_i}(t)^\\delta P_\\eta(X \\geq t).\\] We can write this expression fully in terms of the hazard function by recalling [eq:exp-hazard]: \\[\\begin{align}\nf_{X_i, \\delta_i}(t, \\delta;\\,\\eta) = \\lambda_{X_i}(t)^\\delta \\exp\\left(-\\int_0^t \\lambda_{X_i}(u) du \\right).\n\\end{align}\n\\tag{4}\\]\n\nExample 3.3. MLE for exponential survival timeLet \\(X_i \\overset{\\text{iid}}{\\sim} \\text{Exp}(\\alpha)\\) and assume we have independent censoring (\\(X_i \\perp\\!\\!\\!\\perp C_i\\)), the parameters for the censoring process are separable from \\(\\alpha\\), and that \\(C_i\\) are iid such that \\(\\Exp{C_i} &lt; \\infty\\). Then our observed data are \\(T_i = \\min(X_i, C_i)\\) and \\(\\delta_i = \\mathbbm{1}\\left(X_i \\leq C_i\\right)\\). According to Equation 4 we can write the likelihood as \\[\\begin{align*}\nf_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) & = \\prod_{i=1}^n \\alpha^{\\delta_i} \\exp(-\\textstyle{\\sum_{i=1}^n \\int_0^{t_i} }\\alpha du)\\\\\n& = \\alpha^{\\sum_{i=1}^n \\delta_i} \\exp(-\\alpha \\textstyle\\sum_{i=1}^n t_i)\n\\end{align*}\\] The log-likelihood is \\[\\log(f_\\alpha(t_1, \\dots, t_n, \\delta_1, \\dots, \\delta_n) ) = \\log(\\alpha)\\sum_{i=1}^n \\delta_i -\\alpha \\sum_{i=1}^n t_i\\] which has the maximizer \\[\\hat{\\alpha} = \\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}.\\] Let’s show that this converges a.s. to \\(\\alpha\\) as \\(n\\to\\infty\\). We can rewrite \\(\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i}\\) as \\[\\begin{align*}\n    \\frac{\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right)}{\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}\n\\end{align*}\\] The top and bottom expressions converge a.s. by Kolmogorov’s Strong Law of Large Numbers to \\[\\begin{align*}\n\\frac{1}{n}\\sum_{i=1}^n \\mathbbm{1}\\left(X_i \\leq C_i\\right) & \\overset{\\text{a.s.}}{\\to} \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} \\\\\n\\frac{1}{n}\\sum_{i=1}^n X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right) & \\overset{\\text{a.s.}}{\\to}  \\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)}\n\\end{align*}\\] We can evaluate the top expression using the tower property of expectation: \\[\\begin{align*}\n    \\ExpA{\\mathbbm{1}\\left(X_i \\leq C_i\\right)}{(X_i, C_i)} & =  \\ExpA{\\ExpA{\\mathbbm{1}\\left(X_i \\leq c\\right)\\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n    & = \\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] where the second line follows from the independent censoring condition. The bottom expression becomes: \\[\\begin{align*}\n\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq C_i\\right) + C_i \\mathbbm{1}\\left(X_i &gt; C_i\\right)}{(X_i, C_i)} & = \\ExpA{\\ExpA{X_i \\mathbbm{1}\\left(X_i \\leq c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& + \\ExpA{\\ExpA{c \\mathbbm{1}\\left(X_i &gt; c\\right) \\mid C_i = c}{X_i \\mid C_i}}{C_i} \\\\\n& = \\ExpA{\\frac{1}{\\alpha}(1 - (1 + \\alpha C_i) e^{-\\alpha C_i})}{C_i} + \\ExpA{C_i e^{-\\alpha C_i}}{C_i} \\\\\n& = \\frac{1}{\\alpha}\\ExpA{1 - e^{-\\alpha C_i}}{C_i}\n\\end{align*}\\] Thus \\[\\frac{\\sum_{i=1}^n \\delta_i}{\\sum_{i=1}^n t_i} \\overset{\\text{a.s.}}{\\to} \\alpha\\] To show that \\(\\int_0^c x \\alpha e^{-\\alpha x} dx = \\frac{1}{\\alpha}(1 - (1 + \\alpha c) e^{-\\alpha c})\\), we can use the trick of differentiating under the integral sign. \\[\\begin{align*}\n  \\alpha \\int_0^c xe^{-\\alpha x} dx & = \\alpha \\int_0^c -\\frac{d}{d \\alpha} e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\int_0^c e^{-\\alpha x} dx \\\\\n  & = \\alpha \\left(-\\frac{d}{d \\alpha}\\right)\\frac{1}{\\alpha} (1 - e^{-\\alpha c}) \\\\\n  & = \\alpha \\left(\\frac{1 - (1 + \\alpha c) e^{-\\alpha c}}{\\alpha^2}\\right)\n\\end{align*}\\]"
  }
]