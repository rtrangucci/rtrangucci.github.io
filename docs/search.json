[
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "My papers",
    "section": "",
    "text": "Here’s a selection of my published papers:\n\nModeling Racial/ethnic Differences in COVID-19 Incidence with Covariates Subject to Non-random Missingness (Trangucci, Chen, and Zelner 2023)\nRacial Disparities in Coronavirus Disease 2019 (COVID-19) Mortality Are Driven by Unequal Infection Risks (Zelner et al. 2021)\nQuantifying Observed Prior Impact (Jones, Trangucci, and Chen 2021)\nModeling Spatial Risk of Diarrheal Disease Associated with Household Proximity to Untreated Wastewater Used for Irrigation in the Mezquital Valley, Mexico (Contreras Jesse D. et al. 2020)\nBayesian Hierarchical Weighting Adjustment and Survey Inference. (Si 2020)\nEffects of Sequential Influenza A(H1N1)Pdm09 Vaccination on Antibody Waning (Zelner et al. 2019)"
  },
  {
    "objectID": "papers.html#preprints",
    "href": "papers.html#preprints",
    "title": "My papers",
    "section": "Preprints",
    "text": "Preprints\n\nIdentified vaccine efficacy for binary post-infection outcomes under misclassification without monotonicity (Trangucci, Chen, and Zelner 2022)\nBayesian Methods for Modeling Cumulative Exposure to Extensive Environmental Health Hazards (Submitted) (Trangucci et al. 2024)"
  },
  {
    "objectID": "papers.html#conference-papers",
    "href": "papers.html#conference-papers",
    "title": "My papers",
    "section": "Conference papers",
    "text": "Conference papers\n\nHierarchical Gaussian Processes in Stan (Trangucci 2017)\nPrior formulation for Gaussian process hyperparameters. (Trangucci, Betancourt, and Vehtari 2016)"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses",
    "section": "",
    "text": "STAT 625 - Winter 2024\n\n\n\n\n\nSTAT 625 - Winter 2025\nSTAT 599 - Winter 2025"
  },
  {
    "objectID": "posts/heckman-in-stan.html",
    "href": "posts/heckman-in-stan.html",
    "title": "Heckman models in Stan",
    "section": "",
    "text": "This is cmdstanr version 0.8.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /Users/robertntrangucci/.cmdstan/cmdstan-2.35.0\n\n\n- CmdStan version: 2.35.0\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable cmdstanr_no_ver_check=TRUE.\n\n\nLoading required package: stats4\n\n\nLoading required package: splines\n\n\nI took a class in the first year of my Master’s program about missing data from Ben Goodrich. The class was all about what to do when you encountered some sort of missing observations when analyzing a dataset. Missingness can arise for many reasons when analyzing data: it can arise from people declining to participate in a survey, survey respondents refusing to answer a question, from patients who drop out of a randomized study, or from the fact that an outcome of interest is only observable for a subset of patients who experience (or don’t experience) an intermediate event.\nWhen dealing with missing data, it is crucial that we understand why an observation, be it an outcome, a covariate, or a unit, is missing. If the\nThe Heckman selection model was one of the first examples we were shown in class of a missing data problem where the probability that an individual had a missing observation depended on the value of the missing observation (Heckman 1979). These are the hardest problems to address in missing data because\nA standard survey example would be that people with very high and those with very low incomes may decline to report income on a survey more than those with moderate incomes. In the vaccine example, the selection effect we are concerned with is that people with weaker immune systems may become infected even with a vaccine and thus have higher viral loads. A naive comparison of viral loads between vaccinated and unvaccinated people might show that the vaccine increases viral loads in those who are vaccinated (Hudgens, Hoering, and Self 2003).\nTo make things a little more concrete, suppose we’re interested in learning about the how vaccines moderate post-infection viral loads. The precise quantity we’re interested in is the change in post-infection viral load for people who would be infected with the pathogen no matter their vaccination status. These unlucky people are referred to as the ``Always-Infected’’ group. We might be tempted to get a rough estimate of this quantity by comparing viral loads in infected people in the vaccinated group vs. those in the unvaccinated group. The problem with this analysis is that if the vaccine has any causal effect on infection the individual characteristics of the two groups, infected-vaccinated people and infected-unvaccinated people, may differ. In this problem missingness arises in two forms. The first is that we observe only one outcome for each participant in our trial: the outcome corresponding to the treatment arm assignment. This is what Holland called the fundamental problem of causal inference. The second is more unique to this setting: we observe viral loads only in those who are infected.\nLet \\(Z_i\\) be the vaccine treatment, with \\(Z_i = 1\\) indicating treatment and \\(Z_i = 0\\) indicating placebo. For the ease of exposition, let’s suppose that treatment is randomized. We don’t need more selection bias (though selection into treatment could very well be a source of additional selection bias). Let \\(S_i(z)\\) be the infection status of individual \\(i\\) with vaccination status \\(z\\), and let \\(Y_i(z)\\) be the viral load of individual \\(i\\). If \\(S_i(z) = 0\\), we set \\(Y_i(z) = *\\).\nSuppose we also observe the study site, denoted as \\(R_i\\). This is the hospital or health clinic that enrolled patient \\(i\\) into the study. This study site is assumed to impact that probability of infection, but it is assumed to be independent of post-infection viral load. The standard term for a variable like \\(R_i\\) is an instrumental variable.\nThe model we’ll simulate data from is the following: \\[\n\\begin{aligned}\n\\log Y_i(z) & = \\mu_Y(z) + \\sigma_z\\, U_i(z) \\\\\n\\tilde{S}_i(z) \\mid R_i & = \\mu_S(R_i) + z \\,\\gamma_S + W_i \\\\\nS_i(z) & = 1(\\tilde{S}_i(z) > 0) \\\\\n(U_i(0),U_i(1),W_i) & \\sim \\text{Normal}(\\mathbf{0}, \\boldsymbol{\\Omega})\n\\end{aligned}\n\\] The individuals who are always infected are identified as those with \\(\\{S_i(1) = 1,S_i(0) = 1\\}\\). One key assumption of this model is that these individuals can be identified with certainty. This is also known as a assumption. To see why, let’s see what our model implies about the always-infected event: \\[\n\\begin{aligned}\n\\{S_i(1) = 1, S_i(0) = 1\\} & = \\{\\tilde{S}_i(1) > 0, \\tilde{S}_i(0) > 0\\} \\\\\n& = \\{\\mu_S(R_i) + \\gamma_S > -W_i, \\mu_S(R_i) > -W_i\\} \\\\\n& = \\{\\min(\\mu_S(R_i) + \\gamma_S,\\mu_S(R_i))  > -W_i\\}\n\\end{aligned}\n\\] The event that \\(S_i(z) = 1\\) is equivalent to \\[\n\\{\\mu_S(R_i) + \\gamma_S > -W_i\\}\n\\] Thus, if \\(\\gamma_S\\) is less than zero, as we would hope in a vaccine efficacy trial, then those who are infected in the vaccine arm are also those who would be infected in the placebo arm. The monotonicity assumption is a consequence of two assumptions of the model: - there is one error term for the probit regression - there is no individual heterogeneity in the coefficient \\(\\gamma_S\\).\nThe likelihood for the model is fairly straightforward: For individuals who remain uninfected, the likelihood term corresponds to \\(P(\\tilde{S}_i(z) \\leq 0)\\), which corresponds to the univariate standard normal CDF evaluated at \\(-\\mu_S(z,r)\\), or: \\[\nP(\\mu_S(z,r) + W_i \\leq 0) = \\Phi(-\\mu_S(z,r))\n\\]\nThe likelihood for an individual \\(i\\) who is infected corresponds to observing \\(\\log Y_i(z) = y_i\\) and \\(\\tilde{S}_i(z) > 0 \\mid \\log Y_i(z) = y_i, R_i = r_i\\): \\[\n\\begin{aligned}\nL_i(\\theta) & = \\frac{1}{\\sqrt{2\\pi} \\sigma_z}\\exp\\left(-\\frac{1}{2\\sigma_z^2}(y_i - \\mu_Y(z_i))^2\\right) \\\\\n& \\Phi\\left(\\frac{\\mu_S(z_i) + \\frac{\\rho_{z_i}}{\\sigma_z}(y_i - \\mu_Y(z_i))}{\\sqrt{1 - \\rho_{z_i}^2}}\\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(\\rho_z = \\text{Cor}(W_i,U_i(z))\\).\nTo get a sense for what this model implies about the treatment effect, let’s examine the conditional expectation of \\(Y_i(z)\\) given \\(\\tilde{S}_i(z) > 0\\) when a patient is vaccinated vs. when the patient is unvaccinated:\n\\[\n\\tau^\\text{naive} = \\mathbb{E} \\left[Y_i(1) \\mid \\tilde{S}_i(1) > 0 \\right] - \\mathbb{E} \\left[Y_i(0) \\mid \\tilde{S}_i(0) > 0 \\right]\n\\] This is what we’ll call the naive estimand, because it conditions on a post-treatment outcome, namely infection, which can induce selection bias, as we’ll see.\nThe expectation \\(\\mathbb{E} \\left[Y_i(z) \\mid \\tilde{S}_i(z) > 0 \\right]\\) is the following:\n\\[\n\\begin{aligned}\n\\frac{(2 \\pi\\sigma_z^2)^{-1/2}}{\\Phi(\\mu_S(z_i))}\\int_{-\\infty}^{\\infty}  \\exp\\left(y-\\frac{1}{2\\sigma_z^2}(y - \\mu_Y(z_i))^2\\right)\n\\Phi\\left(\\frac{\\mu_S(z_i) + \\frac{\\rho_{z_i}}{\\sigma_z}(y - \\mu_Y(z_i))}{\\sqrt{1 - \\rho_{z_i}^2}}\\right) dy\n\\end{aligned}\n\\] To my knowledge, this doesn’t have a closed form expression because of the exponentiation of \\(y\\), though there might be a clever way to evaluate this integral. We can, however, use R’s 1-d numerical integration routine to approximate this expectation.\nThe code below evaluates \\(\\tau^\\text{naive}\\), as well as\n\ncond_exp <- function(y, mu_y, mu_s, sd_y, rho) {\n  exp(y + dnorm(y, mu_y, sd_y, log = TRUE) + pnorm((mu_s + rho / sd_y * (y - mu_y))/sqrt(1 - rho^2),log.p = TRUE))\n}\ntau_naive <- function(rho_0, rho_1, sd_y, mu_y, mu_s, s_eff, y_eff) {\n  I1 <- integrate(cond_exp, -Inf, Inf, \n                  mu_y = mu_y + y_eff, \n                  mu_s = mu_s + s_eff, \n                  sd_y = sd_y, \n                  rho = rho_1, \n                  abs.tol = 0L, \n                  rel.tol = .Machine$double.eps^0.85\n  )\n  stopifnot(I1$subdivisions > 3)\n  D1 <- pnorm(mu_s + s_eff)\n  I2 <- integrate(cond_exp, -Inf, Inf, \n                  mu_y = mu_y, \n                  mu_s = mu_s, \n                  sd_y = sd_y, \n                  rho = rho_0, \n                  abs.tol = 0L, \n                  rel.tol = .Machine$double.eps^0.85)\n  stopifnot(I2$subdivisions > 3)\n  D2 <- pnorm(mu_s)\n  return(I1$value / D1 - I2$value / D2)\n}\ntrue_eff <- function(sd_y, mu_y, y_eff) {\n  mu_0 <- exp(mu_y + sd_y^2 / 2)\n  mu_1 <- exp(mu_y + y_eff + sd_y^2 / 2)\n  return(mu_1 - mu_0)\n}\ndelta_s <- function(mu_z, s_eff) {\n  p_0 <- pnorm(mu_z)\n  p_1 <- pnorm(mu_z + s_eff)\n  return(p_1 - p_0)\n}\n\nFirst we’ll explore a scenario where there is high positive correlation between the errors in the viral load equation and the latent variable related to infection. We set \\(\\rho_1 = \\rho_0 = 0.9\\), \\(\\sigma_0 = \\sigma_1 = 0.5\\), \\(\\mu_Y(0) = 10\\) and \\(\\mu_S(0) = -1.5\\). Let the true effect of the vaccine on mean log-viral load be \\(\\delta_y = -0.25\\).\nWe will plot how the naive estimand of viral load effect changes with the change in infection probability.\n\neffs <- seq(-3, 3, by = 0.01)\np_s <- sapply(effs, \\(x) delta_s(-0.5, x)) \nestimand <- sapply(effs, \\(x) f_naive_est(rho = 0.9, sd_y = 0.5, mu_y = 10, mu_s = -1.5, s_eff = x, y_eff = -0.25))\np_change_in_sign <- p_s[which.min(abs(estimand))]\nplot(p_s, \n     estimand,\n     main = \"Naive effect estimand vs. infection effect\",\n     ylab = bquote(E(y ~ \"|\" ~ S == 1, Z == 1) - E(y ~ \"|\" ~ S == 1, Z == 0)),\n     xlab = bquote(P(S == 1 ~ \"|\" ~ Z == 1) - P(S == 1 ~ \"|\" ~ Z == 0)),\n     type=\"l\")\nabline(v = p_change_in_sign, col = \"red\")\nabline(h = 0, col = \"red\")\n\n\n\n\nThis graph shows that as the vaccine becomes more effective at preventing infection, the apparent benefit of the vaccine on viral load decreases. In fact, at around a -0.18 decrease in the probability of infection, the naive estimand shows that the vaccine increases viral load, which is not the case.\nIf we fix a scenario and\n\neffs <- seq(-3, 3, by = 0.01)\np_s <- sapply(effs, \\(x) delta_s(-0.5, x)) \nestimand <- sapply(effs, \\(x) f_naive_est(rho = 0.9, sd_y = 0.5, mu_y = 10, mu_s = -1.5, s_eff = x, y_eff = -0.25))\np_change_in_sign <- p_s[which.min(abs(estimand))]\nplot(p_s, \n     estimand,\n     main = \"Naive effect estimand vs. infection effect\",\n     ylab = bquote(E(y ~ \"|\" ~ S == 1, Z == 1) - E(y ~ \"|\" ~ S == 1, Z == 0)),\n     xlab = bquote(P(S == 1 ~ \"|\" ~ Z == 1) - P(S == 1 ~ \"|\" ~ Z == 0)),\n     type=\"l\")\nabline(v = p_change_in_sign, col = \"red\")\nabline(h = 0, col = \"red\")\n\n\n\n\nNote that \\(\\rho_{01} = \\text{Cor}(U_i(0),U_i(1))\\) does not enter into the likelihood, which makes sense because we’ll never observe both \\(S_i(0)\\) and \\(S_i(1)\\) for the same individual. This does not mean that we won’t have any information about \\(\\rho_{01}\\); because \\(\\boldsymbol{\\Omega}\\) must be positive definite, the \\(\\det \\boldsymbol{\\Omega} > 0\\). This means that what we do learn about \\(\\rho_0\\) and \\(\\rho_1\\) will constrain the domain of \\(\\rho_{01}\\) so that \\(\\det \\boldsymbol{\\Omega} > 0\\). This translates to the following bounds on \\(\\rho_{01}\\) \\[\n\\rho_{01} \\in \\left(\\rho_0\\rho_1 - \\sqrt{1 - \\rho_0^2 - \\rho_1^2 + \\rho_0^2\\rho_1^2},\\, \\rho_0\\rho_1 + \\sqrt{1 - \\rho_0^2 - \\rho_1^2 + \\rho_0^2\\rho_1^2}\\right)\n\\] If we incorporate prior information, the asymptotic posterior for \\(\\rho_{01}\\) will take on the shape of the conditional prior for \\(\\rho_{01}\\) given the values for \\(\\rho_0\\) and \\(\\rho_1\\).\nGiven our focus on comparing the impact of vaccination on viral load, the target causal estimand is a measure of the average decrease (hopefully) in viral load caused by vaccination in individuals who are always infected. Mathematically, this is: \\[\\mathbb{E}[Y_i(1) - Y_i(0) \\mid S_i(1) = S_i(0) = 1].\\] This estimand is somewhat odd in that we can never observe both infection outcomes, so we can never calculate this estimand exactly from observed data without extra assumptions. I explained above why we can’t equate this target estimand with the ``naive’’ version, the difference in viral load between vaccinated and unvaccinated people: \\[\n\\mathbb{E}[Y_i(1) \\mid S_i(1) = 1] - \\mathbb{E}[Y_i(0) \\mid S_i(0) = 1].\n\\] Logically, it makes sense, because the naive estimand is a mixture over several groups, instead of just the always-infected group, but we’ll simulate some data to get a better sense of how different the two estimands can be.\nIn our simulated example, we’ll suppose we have a randomized vaccine trial with 20,000 participants spread evenly across 10 study sites. We’ll set \\(\\rho_0\\) and \\(\\rho_1\\) to \\(0.75\\), which equates to strong residual correlation between infection risk and viral load; that might be reasonable if there is an unobserved factor impacting both infection and viral load. Maybe this is prior exposure to the virus plus genetic factors related to immune system health.\nThe covariates we’re including in the infection risk equation, \\(\\mu_S(z)\\) are age and study site, while the only covariates we’re including in the viral load model are categorical age predictors.\n\nset.seed(12222)\nn <- 20000\n\nsigma_0 <- 0.7\nsigma_1 <- 0.7\nsigma_W <- 1\nrho_0 <- 0.9\nrho_1 <- 0.9\nlb <- rho_0*rho_1 - sqrt(1 - rho_0^2 - rho_1^2 + rho_0^2*rho_1^2)\nub <- rho_0*rho_1 + sqrt(1 - rho_0^2 - rho_1^2 + rho_0^2*rho_1^2)\nrho_01 <- 0.7\ndiag_sigs <- diag(c(sigma_0,sigma_1,sigma_W))\nOmega <- matrix(c(1,rho_01,rho_0,rho_01,1,rho_1,rho_0,rho_1,1),3,3)\nSigma <- diag_sigs %*% Omega  %*% diag_sigs\nU0_U1_W <- mvrnorm(n = n, mu = c(0, 0, 0), Sigma = Sigma)\n\nThe plot of \\(U_0\\) vs. \\(W\\) gives a sense of how related the two error terms are:\n\n\n\n\n\nThe covariates we will include in the model are age, which we treat as a categorical variable, the study site, which is also treated as a categorical variable, and the binary treatment variable, which is assumed to be balanced within study sites.\n\nN_age <- 4\nN_sites <- 10\nage_vec <- sample(N_age, n, TRUE) |> factor()\nsite_vec <- rep(1:N_sites,each=n / N_sites) |> factor()\ntreat <- rep(c(rep(0,n/20),rep(1,n/20)),10)\n\nWe’ll create the model matrices from these variables, along with the coefficients for each model equation:\n\nX_S <- model.matrix(~ age_vec + site_vec)\nX_Y <- model.matrix(~ age_vec)\nd_S <- ncol(X_S)\nd_Y <- ncol(X_Y)\ngamma_Y <- -0.25\ngamma_S <- -0.85\n\nbeta_S <- c(-1.5, 0.1, 0.2, 0.3, rnorm(N_sites - 1) * 0.1)\nbeta_Y <- c(10.3, 0.5, 0.5, 0.5)\n\nS_0  <- (X_S %*% beta_S + U0_U1_W[,3]) >= 0\nS_1  <- (X_S %*% beta_S + gamma_S + U0_U1_W[,3]) >= 0\nS <- cbind(as.integer(S_0), as.integer(S_1))\nY_0  <- X_Y %*% beta_Y + U0_U1_W[,1]\nY_1  <- X_Y %*% beta_Y + gamma_Y + U0_U1_W[,2]\nY <- cbind(ifelse(S_0 == 1, Y_0, NA),\n           ifelse(S_1 == 1, Y_1, NA))\n\nsel <- as.vector(rbind(1-treat, treat))\nS_o <- as.vector(t(S))[sel == 1]\nY_o <- as.vector(t(Y))[sel == 1]\n\nidx_S_o <- which(S_o == 1)\nidx_AI <- which(S_0 == 1 & S_1 == 1)\n\nY_o_sel <- Y_o[idx_S_o]\nX_Y_sel <- X_Y[idx_S_o, ]\n\n\nnaive_estimand <- mean(exp(Y_o_sel[treat[idx_S_o] == 1])) -\n                  mean(exp(Y_o_sel[treat[idx_S_o] == 0]))\n\nY_estimand <- mean(exp(Y_1[idx_AI])) -\n               mean(exp(Y_0[idx_AI]))\n\nn_s_0 <- sum(treat[idx_S_o] == 1)\nn_s_1 <- sum(treat[idx_S_o] == 0)\n\nn_AI <- length(idx_AI)\n\nAfter all is said and done, the finite-sample estimator for the population estimand is -63,883, with a standard error of 6888.1207534 while the naive estimator for the population estimand is 41,600 with a standard error of 8335.141496. Note that these standard errors are understated a bit because the selection indicators are treated as fixed. In reality, both the selection and the outcome are random, so a more complete standard error calculation would account for the randomness in both sets of outcomesboth sets of outcomes.\nThis means that the naive estimator understates the benefit of the vaccine for people who are always infected by about 165%.\nThe approximate standard error for the naive estimator is 6888.1207534, while the approximate standard error .\nLet’s build the Stan model block by block. First, the data block needs to take in the sizes and types of all the matrices and vectors that we’ll need to fit the model.\n\nData block\n\n\ndata {\n  int<lower=1> N;     \n  int<lower=1> N_neg; \n  int<lower=1> N_pos; \n  int<lower=1> D_S;   \n  int<lower=1> D_Y;   \n  vector[N_pos] Y;    \n  array[N] int<lower=0,upper=1> S;  \n  array[N] int<lower=0,upper=1> Z;  \n  matrix[N_pos,D_Y] X_Y; \n  matrix[N,D_S] X_S;  \n}\n\nWe need the number of observations, N, the number of negative and positive cases (N_neg and N_pos). We also need the dimensions of the predictors for the \\(S\\) equation and the \\(Y\\) equation, D_S and D_Y. The observed viral loads are collected into a length-N_pos vector Y, while the observed infection status and treatment assignment are collected into length-N binary integer arrays S and Z. Finally, we have the predictors for the \\(Y\\) equation, X_Y, which is an N_pos by D_Y matrix, and the predictors for the \\(S\\) equation, which is an N by D_S matrix.\n\nTransformed data block\n\nThe transformed data block is needed to collect the indices, measured from \\(1,\\dots,N\\), of the negative cases (\\(S_i = 0\\)) and positive cases (\\(S_i = 1\\)). The reason we need this information is because the likelihood contribution to the infection status probit model, shown in Equation 1, requires the value of the log-viral load, so we’ll need to match the infection status outcomes to the viral load outcomes. We could do this matching in the R code outside the Stan model, but I prefer to do it within the Stan code so everything is in one place. We’ll collect the indices of the positive cases in n_pos, which is an array of length N_pos and holds integers between 1 and N, while negative case indices are collected in n_neg, defined similarly.\nThe other piece of information the infection status likelihood will need for positive cases is the value \\(\\rho_z\\), which depends on the individual treatment assignment. This information is collected in the vector Z_idx.\n\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\n\n\nParameters block\n\nThe parameter block is where we define which unknown parameters we need to estimate with our model. Our unknown parameters is the set: - \\(\\boldsymbol{\\Omega}\\): correlation matrix between the error terms for the regressions - \\(\\sigma_0\\): error standard deviation for log-viral load regression in placebo group - \\(\\sigma_1\\): error standard deviation for log-viral load regression in treatment group - \\(\\beta_y\\): regression coefficients for the log-viral load regression - \\(\\beta_s\\) - \\(\\gamma_s\\) _y)$, the correlation matrix for the errors. In our case, we have a\n\nparameters {\n  cholesky_factor_corr[3] L_Omega;\n  vector<lower=0>[2] sd_Y;\n  vector[D_Y] b_Y;\n  vector[D_S] b_S;\n  real gamma_S;\n  real gamma_Y;\n}\n\nThe Stan model is written like so:\n\ndata {\n  int<lower=1> N;     \n  int<lower=1> N_neg; \n  int<lower=1> N_pos; \n  int<lower=1> D_S;   \n  int<lower=1> D_Y;   \n  vector[N_pos] Y;    \n  array[N] int<lower=0,upper=1> S;  \n  array[N] int<lower=0,upper=1> Z;  \n  matrix[N_pos,D_Y] X_Y; \n  matrix[N,D_S] X_S;  \n}\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\nparameters {\n  cholesky_factor_corr[3] L_Omega;\n  vector<lower=0>[2] sd_Y;\n  vector[D_Y] b_Y;\n  vector[D_S] b_S;\n  real gamma_S;\n  real gamma_Y;\n}\ntransformed parameters {\n  matrix[3,3] Omega = L_Omega * L_Omega';\n}\nmodel {\n  vector[N] mu_S = X_S * b_S + to_vector(Z) * gamma_S;\n  vector[N_pos] mu_Y = X_Y * b_Y + to_vector(Z[n_pos]) * gamma_Y;\n  array[2] real rho = {Omega[1,3], Omega[2,3]};\n\n  b_Y ~ normal(0, 1);\n  b_S ~ normal(0, 1);\n  sd_Y ~ normal(0, 2);\n  gamma_S ~ normal(0, 5);\n  gamma_Y ~ normal(0, 5);\n\n  for (n in 1:N_neg) {\n    target += log(Phi(-mu_S[n_neg[n]]));\n  }\n  for (n in 1:N_pos) {\n    int treat_idx = Z_idx[n_pos[n]];\n    target += log(Phi(sqrt((1 - rho[treat_idx]) * (1 + rho[treat_idx]))^(-1)*(mu_S[n_pos[n]]\n                               + rho[treat_idx] / sd_Y[treat_idx]\n                               * (Y[n] - mu_Y[n]))));\n    Y[n] ~ normal(mu_Y[n], sd_Y[treat_idx]);\n  }\n}\ngenerated quantities {\n  array[2] real rho = {Omega[1,3], Omega[2,3]};\n  real Y_estimand = 0;\n  {\n    vector[N] mu_S = X_S * b_S;\n    matrix[3,3] Sigma = quad_form_diag(Omega, append_row(sd_Y, [1]'));\n    int tot_11 = 0;\n    for (n in 1:N) {\n      vector[3] errors = multi_normal_rng(rep_vector(0,3), Sigma);\n      if (errors[3] > -mu_S[n] - gamma_S && errors[3] > -mu_S[n]) {\n        real mu_Y_cf = X_S[n,1:D_Y] * b_Y;\n        real Y_0 = errors[1] + mu_Y_cf;\n        real Y_1 = errors[2] + mu_Y_cf + gamma_Y;\n        Y_estimand += Y_1 -  Y_0;\n        tot_11 += 1;\n      }\n    }\n    Y_estimand /= tot_11;\n  }\n}\n\n\ntransformed data {\n  array[N_pos] int<lower=1,upper=N> n_pos;\n  array[N_neg] int<lower=0,upper=N> n_neg;\n  array[N] int Z_idx;\n  {\n    int i;\n    int j;\n    i = 1;\n    j = 1;\n    for (n in 1:N) {\n      if (S[n] == 1) {\n        n_pos[i] = n;\n        i = i + 1;\n      } else {\n        n_neg[j] = n;\n        j = j + 1;\n      }\n      Z_idx[n] = Z[n] + 1;\n    }\n  }\n}\n\nSome comments on the Stan code are in order. The data block is pretty straightforward, though it’s worth keeping in mind that we’ll need to keep track of three dimensions: the total number of data points, the number of infected and uninfected patients. This is because we’ll only have access to viral loads in the infected patients.\nLet’s generate some data to see how the errors change how the .\nFirst, let’s see whether the\n\n\n\n\nReferences\n\nHeckman, James J. 1979. “Sample Selection Bias as a Specification Error.” Econometrica 47 (1): 153. https://doi.org/10.2307/1912352.\n\n\nHudgens, Michael G., Antje Hoering, and Steven G. Self. 2003. “On the Analysis of Viral Load Endpoints in HIV Vaccine Trials.” Statistics in Medicine 22 (14): 2281–98. https://doi.org/10.1002/sim.1394."
  },
  {
    "objectID": "posts/beta-processes-in-stan.html",
    "href": "posts/beta-processes-in-stan.html",
    "title": "Gamma and Beta Processes in Stan",
    "section": "",
    "text": "rdirichlet <- function(n, alpha) {\n  d <- length(alpha)\n  rates <- rep(1, d)\n  gammas <- replicate(n = n,\n                      rgamma(d, shape = alpha,\n                             rate = rates)\n                      )\n  draws <- sweep(gammas, MARGIN = 2, STATS = colSums(gammas), FUN = \"/\")\n  return(t(draws))\n}\n\nI taught the PhD-level survival analysis course in the 2024 Winter quarter at Oregon State (website). The prior years’ courses in survival analysis were exclusively focused on Frequentist nonparametric and semiparametrci methods for inference: Kaplan-Meier, Cox proportional hazards model, etc. I wanted to see if I could bring a bit more Bayesian inference into the course. Ultimately, I didn’t include any Bayesian nonparametrics, but it gave me the opportunity to try some simple nonparametric methods in Stan (Stan is always my first choice to develop bespoke models, it’s second nature at this point).\nMy starting point to learn more about Bayesian nonparametric survival analysis modeling was our textbook for the course Klein and Moeschberger (Klein and Moeschberger 2003). They review several nonparameteric Bayesian models for survival analysis in 6.4: Dirichlet processes, and beta processes. Both methods put a nonparametric prior over the survival function, or \\(S(t)\\), but they do so in different ways.\nThese methods are used to model time-to-event data. Let’s call the time-to-event random variable \\(T_i\\) for an individual \\(i\\). The survival function for \\(T_i\\) is the complement of the cumulative distribution function, \\(P(T_i \\leq t)\\):\n\\[\nS(t) = P(T_i > t) = 1 - P(T_i \\leq t)\n\\]\nTime-to-event data is complicated by the fact that we often can’t observe all times to failure, but instead we observe only a subset of these failures corresponding to those times that occur prior to a censoring time. In the simplest case, called Type I censoring, we observe only \\(T_i\\) that are less than a constant \\(C\\).\nThe standard set-up for survival analysis data is to define two new random variables for each observation \\(i\\):\n\\[\nX_i = 1(T_i \\leq C) T_i + C 1(T_i > C), \\Delta_i = 1(T_i \\leq C).\n\\]\nNow we have a single time-to-event random variable \\(X_i\\) which is equal to \\(T_i\\) if \\(T_i\\) occurs prior to the censoring time \\(C\\). In this case, \\(\\Delta_i = 1\\). If \\(T_i\\) occurs after the censoring time, we don’t observe \\(T_i\\) but rather we observe only the censoring time. This sort of censoring set up occurs if we enroll patients in a randomized controlled trial, administer a treatment, and follow each patient for, say, 10 months. If the event of interest occurs within the 10-month window, we will have observed \\(T_i\\), otherwise, our only knowledge of the event time is that it is greater than 10-months.\nBack to our modeling strategy to learn about \\(S(t)\\) from a set of data, \\(\\{(X_i, \\Delta_i), i.= 1, \\dots, n\\}\\). One strategy to model the survival function nonparametrically is to define a partition of the positive real line, \\(\\{t_j, j = 0, \\dots, J\\}\\) with \\(t_0 = 0, t_J = \\infty\\)to model increments of the survival function \\(S(t_j) - S(t_{j-1})\\) over this partition. These increments will be between \\((0,1)\\) and they will sum to \\(1\\). A natural distribution for these random variables is a Dirichlet distribution, which respects the natural constraints of the increments process:\n\\[\n(S(t_0) - S(t_1), \\dots, S(t_j) - S(t_{j-1}), S(t_{J-1}) - S(t_{J})) \\sim \\text{Dirichlet}(c\\,\\boldsymbol{\\alpha}).\n\\]\nTypically, \\(\\boldsymbol{\\alpha}\\) is chosen so that it too is a function of time \\(t\\) and that it corresponds to a known parametric survival function, like say the exponential function: \\(S(t) = \\exp(-\\lambda t)\\).\nWhen we combine the prior over the survival function with counts of failures within an interval\n, but the function that we’re modeling is different between the models.\nThe Dirichlet process directly models the unknown survival function, while the beta process models the unknown cumulative hazard function.\nAnother popular choice for Bayesian nonparametric survival analysis, not covered by KM for some reason, is a gamma process. My first choice was a gamma process.\nOne of the proposed methods is the Beta process, which is rigorously developed in (Hjort 1990).\n\n\n\n\nReferences\n\nHjort, Nils Lid. 1990. “Nonparametric Bayes Estimators Based on Beta Processes in Models for Life History Data.” The Annals of Statistics 18 (3). https://doi.org/10.1214/aos/1176347749.\n\n\nKlein, John P., and Melvin L. Moeschberger. 2003. Survival Analysis: Techniques for Censored and Truncated Data. 2nd ed. Statistics for Biology and Health. New York: Springer."
  },
  {
    "objectID": "st_625_w_24.html",
    "href": "st_625_w_24.html",
    "title": "ST 625 - W 24",
    "section": "",
    "text": "Required: Survival Analysis: Techniques for censored and truncated data, Klein and Moeschberger\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nNotes"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rob Trangucci",
    "section": "",
    "text": "I am an assistant professor of Statistics at Oregon State University. My research focuses on novel statistical methodology in missing data analysis and causal inference for problems in epidemiology, designing Bayesian methods for survey inference, and creating tools to quantify how priors impact posterior inferences. Before I returned to academia to pursue a doctorate in statistics, I worked as a data scientist for a fintech startup, a statistical consultant for a Big Five publisher, and a core developer for the Stan statistical modeling and inference platform (https://mc-stan.org/). I earned a PhD in Statistics from the University of Michigan, an M.A. in Quantitative Methods in the Social Sciences from Columbia University, and a B.A. in Physics from Bucknell University."
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Rob Trangucci",
    "section": "Interests",
    "text": "Interests\n\nCausal inference for vaccine efficacy\nMissing data\nPrincipal stratification\nPrior influence\nMultilevel regression and poststratification (MRP)\nBayesian inference"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Rob Trangucci",
    "section": "Education",
    "text": "Education\n\nPhD in Statistics, 2023\n\nUniversity of Michigan\n\nMA in Quantitative Methods in the Social Science, 2014\n\nColumbia University\n\nBA in Physics, 2009\n\nBucknell University"
  },
  {
    "objectID": "st_625_w_25.html",
    "href": "st_625_w_25.html",
    "title": "ST 625 - W 25",
    "section": "",
    "text": "Required: Modelling Survival Data in Medical Research, 4th Edition, Collett\nOptional: Survival and Event History Analysis, Aalen, Borgan, and Gjessing\nOptional: Counting Processes and Survival Analysis, Fleming and Harrington\n\n\n\n\nTo highlight the unique challenges posed by the analysis of failure/survival data. To allow you to analyze survival data using parametric and nonparametric techniques in the face of these challenges. To apply these techniques to real data using R code and R packages for survival analysis. To understand the theory and methodology through math, practice and code.\nCourse content\nConcepts to be discussed include: hazard function (failure rate function); nonparametric likeli- hood; counting processes; empirical distribution function; censoring and truncation; Kaplan-Meier estimator; Bias of the KM estimator; Cox proportional hazards model; Accelerated Failure Time Model; Partial Likelihood; log-rank test; martingales. R will be the programming language used in the course.\n\n\n\n\nSyllabus\n\n\n\n\n\nNotes"
  },
  {
    "objectID": "st_599_w_25.html",
    "href": "st_599_w_25.html",
    "title": "ST 599 - W 25: Missing data and causal inference",
    "section": "",
    "text": "Required: Statistical Analysis with Missing Data, 3rd edition Little and Rubin\nRequired: Causal Inference for Statistics, Social, and Biomedical Sciences, Imbens and Rubin\nOptional: Bayesian Inference for Partially Identified Models, Gustafson\n\n\n\n\nUpon completion of this course, students should be able to critically evaluate how published literature handles (or does not handle) missing data, and analyze datasets that have missing values by designing models that account for missingness. Students should also be able to read published literature using randomized study designs, and assess whether researchers’ causal conclusions are reasonable.\n\n\n\n\nDifferentiate between missing-completely-at-random, missing-at-random (MAR), and missing-not-at-random (MNAR) processes via assumptions about the joint distribution of missingness indicators, outcomes, and covariates.\nEvaluate whether estimands of interest are identifiable for a given data generating process.\nDerive the identification region and limiting posterior density for partially-identified models.\nDerive a principal causal effect using the Neyman-Rubin causal model.\nConstruct and fit maximum likelihood (in R)/Bayesian models (in Stan) for MAR, MNAR, and causal models.\n\n\n\n\n\nSyllabus\n\n\n\n\n\nLecture 1 notes\nLecture 2 notes"
  }
]