<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Lecture 8</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
window.MathJax = {
  startup: {
    ready() {
      MathJax.startup.defaultReady();
      const {STATE} = MathJax._.core.MathItem;
      MathJax.tex2mml(String.raw`
        \newcommand{\lp}{\left(}
        \newcommand{\rp}{\right)}
        \newcommand{\R}{\mathbb{R}}
        \newcommand{\lb}{\left[}
        \newcommand{\rb}{\right]}
        \newcommand{\ind}[1]{\mathbbm{1}\lp#1\rp}
        \newcommand{\Exp}[1]{\mathbb{E} \lb #1 \rb}
        \newcommand{\ExpA}[2]{\mathbb{E}_{#2} \lb #1 \rb}
        \newcommand{\Prob}[2]{\mathbb{P}_{#2} \lp #1 \rp}
        \newcommand{\indy}{\mathrel{\unicode{x2AEB}}}
        \newcommand{\comp}{^\mathsf{c}}
        \newcommand{\widebar}[1]{\overline{\! #1}}
        \newcommand{\Var}[1]{\mathrm{Var}\lp #1 \rp}
        \newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
      `);
    }
  },
  loader: {load: ['[tex]/bbm']},
  tex: {packages: {'[+]': ['bbm']}}
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@4.1/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../papers.html"> 
<span class="menu-text">My papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html"> 
<span class="menu-text">Random thoughts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../courses.html"> 
<span class="menu-text">Courses</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#example-continued" id="toc-example-continued" class="nav-link active" data-scroll-target="#example-continued"><span class="header-section-number">1</span> Example continued</a>
  <ul class="collapse">
  <li><a href="#fisher-information" id="toc-fisher-information" class="nav-link" data-scroll-target="#fisher-information"><span class="header-section-number">1.1</span> Fisher information</a></li>
  </ul></li>
  <li><a href="#sec:asymptotics" id="toc-sec:asymptotics" class="nav-link" data-scroll-target="#sec\:asymptotics"><span class="header-section-number">2</span> Asymptotic interlude</a>
  <ul class="collapse">
  <li><a href="#estimators-of-variance-covariance-matrix" id="toc-estimators-of-variance-covariance-matrix" class="nav-link" data-scroll-target="#estimators-of-variance-covariance-matrix"><span class="header-section-number">2.0.1</span> Estimators of variance-covariance matrix</a></li>
  <li><a href="#asymptotic-confidence-intervals" id="toc-asymptotic-confidence-intervals" class="nav-link" data-scroll-target="#asymptotic-confidence-intervals"><span class="header-section-number">2.1</span> Asymptotic confidence intervals</a></li>
  <li><a href="#asymptotic-tests" id="toc-asymptotic-tests" class="nav-link" data-scroll-target="#asymptotic-tests"><span class="header-section-number">2.2</span> Asymptotic tests</a>
  <ul class="collapse">
  <li><a href="#wald-test" id="toc-wald-test" class="nav-link" data-scroll-target="#wald-test"><span class="header-section-number">2.2.1</span> Wald test</a></li>
  <li><a href="#raos-score-test" id="toc-raos-score-test" class="nav-link" data-scroll-target="#raos-score-test"><span class="header-section-number">2.2.2</span> Rao’s score test</a></li>
  <li><a href="#likelihood-ratio-test" id="toc-likelihood-ratio-test" class="nav-link" data-scroll-target="#likelihood-ratio-test"><span class="header-section-number">2.2.3</span> Likelihood ratio test</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lecture 8</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="example-continued" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Example continued</h1>
<p>In the preceding example, we shied away from using the Fisher information because <span class="math inline">\(T_2\)</span> was not easily accessible. But we can use the results from [[exmp:mle-exp]] to derive an exact expression for the asymptotic sampling variance for the MLE.</p>
<div id="exmp-fisher">
<section id="fisher-information" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="fisher-information"><span class="header-section-number">1.1</span> Fisher information</h2>
<p>Continued example This is an expansion of the example in <span class="citation" data-cites="collett1994modelling">(<a href="#ref-collett1994modelling" role="doc-biblioref">Collett 1994</a>)</span>. <span class="math display">\[\begin{align}
\frac{\partial}{\partial \lambda}\left(\frac{\partial}{\partial \lambda}\ell(\lambda, \psi)\right)&amp; = -\frac{r_1 + r_2}{\lambda^2} \\
\frac{\partial}{\partial \beta}\left(\frac{\partial}{\partial \lambda}\ell(\lambda, \psi)\right)&amp; = -e^\beta T_2 \\
\frac{\partial}{\partial \beta}\left(\frac{\partial}{\partial \beta}\ell(\lambda, \psi)\right)&amp; = -\lambda e^\beta T_2
\end{align}\]</span><br>
We know that <span class="math display">\[\Exp{r_1} = n_1\Exp{1 - e^{-\lambda C_i}}{C_i}, \, \Exp{r_2} = n_2\Exp{1 - e^{-\lambda e^\beta C_i}}{C_i}, \textrm{and} \, \Exp{T_2} = n_2 \frac{1}{\lambda e^\beta} \Exp{1 - e^{-\lambda e^\beta C_i}}{C_i}\]</span> Then the Fisher information is <span class="math display">\[\begin{align}
    \begin{bmatrix}
        \frac{n_1\Exp{1 - e^{-\lambda C_i}}{C_i} + n_2\Exp{1 - e^{-\lambda e^\beta C_i}}{C_i}}{\lambda^2} &amp; \frac{1}{\lambda} n_2\Exp{1 - e^{-\lambda e^\beta C_i}}{C_i} \\
        \frac{1}{\lambda} n_2\Exp{1 - e^{-\lambda e^\beta C_i}}{C_i} &amp; n_2\Exp{1 - e^{-\lambda e^\beta C_i}}{C_i}
    \end{bmatrix}
\end{align}\]</span> Let <span class="math inline">\(\Exp{r_{i1}} = \Exp{1 - e^{-\lambda C_i}}{C_i}\)</span> and <span class="math inline">\(\Exp{r_{i2}} = \Exp{1 - e^{-\lambda e^\beta C_i}}{C_i}\)</span>. We know the asymptotic variance of the MLE is the inverse of the Fisher information matrix. The inverse is: <span class="math display">\[\begin{align}
    \frac{\lambda^2}{n_{1} n_{2} \Exp{r_{i1}}\Exp{r_{i2}}}
    \begin{bmatrix}
      n_2 \Exp{r_{i2}}  &amp; -n_2\Exp{r_{i2}}/\lambda \\
        -n_2\Exp{r_{i2}}/\lambda &amp; \frac{n_1\Exp{r_{i1}} + n_2\Exp{r_{i2}}}{\lambda^2}
    \end{bmatrix} =
    \begin{bmatrix}
      \frac{\lambda^2}{n_1\Exp{r_{i1}}}  &amp; -\frac{\lambda}{n_1 \Exp{r_{i1}}} \\
       -\frac{\lambda}{n_1 \Exp{r_{i1}}} &amp;
\frac{n_1\Exp{r_{i1}} + n_2\Exp{r_{i2}}}{n_1 n_2 \Exp{r_{i1}}\Exp{r_{i2}}}
    \end{bmatrix}
\end{align}\]</span> So the asymptotic standard error for <span class="math inline">\(\beta\)</span> is <span class="math display">\[\sqrt{\frac{n_1\Exp{1 - e^{-\lambda C_i}}{C_i} + n_2\Exp{1 - e^{-\lambda e^\beta C_i}}{C_i}}{n_1 n_2\Exp{1 - e^{-\lambda C_i}}{C_i}\Exp{1 - e^{-\lambda e^\beta C_i}}{C_i}}}\]</span></p>
</section>
</div>
</section>
<section id="sec:asymptotics" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Asymptotic interlude</h1>
<p>As you’ve already no doubt gathered, many of the results for inference and hypothesis testing in survival analysis rely on asymptotic normality of the MLE. Before we get too much further into the quarter, I thought it would be a good idea to review the asymptotic results for maximum likelihood. This outline of results is from <span class="citation" data-cites="keener_theoretical_2010">(<a href="#ref-keener_theoretical_2010" role="doc-biblioref">Keener 2010</a>)</span>.</p>
<p>Let <span class="math inline">\(X_i, i = 1, 2, \dots\)</span> be distributed <span class="math inline">\(i.i.d.\)</span> with density <span class="math inline">\(f_\theta\)</span> where <span class="math inline">\(\theta \in \R^p\)</span>. We suppose that the support of <span class="math inline">\(X_i\)</span> does not depend on <span class="math inline">\(\theta\)</span>, and that our MLE’s are consistent for <span class="math inline">\(\theta\)</span>. This is pretty mild, and only requires that likelihood ratios are integrable and our model is identifiable.</p>
<p>Let the log-likelihood be denoted <span class="math inline">\(\ell(\theta)\)</span>, in which we suppress the dependence of the likelihood on the data. If we need to indicate the dependence on data, we’ll write it as <span class="math inline">\(\ell(\theta; x)\)</span>. Finally, denote the gradient of the log-likelihood with respect to <span class="math inline">\(\theta\)</span> evaluated at <span class="math inline">\(\theta^\prime\)</span> as: <span class="math display">\[
\ell_{\theta}(\theta^\prime; x) \equiv \nabla_\theta \log f_\theta(x)\mid_{\theta = \theta^\prime}
\]</span> and the Hessian of the log-likelihood (i.e.&nbsp;the matrix of second derivatives of the log-likelihood) be: <span class="math display">\[
\ell_{\theta\theta}(\theta^\prime; x) \equiv \nabla^2_\theta \log f_\theta(x)\mid_{\theta = \theta^\prime}
\]</span> Given that <span class="math inline">\(\theta \in \R^p\)</span>, we’ll denote an element of the vector <span class="math inline">\(\ell_\theta(\theta^\prime)\)</span> as <span class="math inline">\((\ell_\theta(\theta^\prime))_j\)</span> and a row of <span class="math inline">\(\ell_{\theta\theta}(\theta^\prime; x)\)</span> as <span class="math inline">\((\ell_{\theta\theta}(\theta^\prime; x))_j\)</span>. Finally, let <span class="math inline">\((\ell_{\theta\theta\theta}(\theta^\prime))_j = \nabla_\theta (\ell_{\theta\theta}(\theta^\prime; x))_j\)</span> which is a <span class="math inline">\(p \times p\)</span> matrix.</p>
<p>Further assumptions will be needed:</p>
<ol type="1">
<li><p>Continuity of <span class="math inline">\(\ell_{\theta\theta}(\cdot)\)</span></p></li>
<li><p>For <span class="math inline">\(\theta \in N(\theta^\dagger)\)</span>, <span class="math inline">\(\sup_{\theta}\ell_{\theta\theta\theta}(\theta) \leq g(x)\)</span> where <span class="math inline">\(g(x)\)</span> is integrable with respect to <span class="math inline">\(f_\theta(x) dx\)</span>.</p></li>
<li><p><span class="math inline">\(\Exp{-\ell_{\theta\theta}(\theta; X_i)}\)</span> is invertible</p></li>
</ol>
<p>We can expand each dimension of the gradient of the log-likelihood evaluated at the MLE <span class="math inline">\(\ell(\hat{\theta})\)</span> around the true parameter value <span class="math inline">\(\theta^\dagger\)</span> in a two-term Taylor expansion: <span class="math display">\[
\begin{align*}
    (\ell_\theta(\hat{\theta}_n))_j = (\ell_\theta(\theta^\dagger))_j + (\ell_{\theta\theta}(\theta^\dagger))_j (\hat{\theta}_n - \theta^\dagger) + \frac{1}{2}(\hat{\theta}_n - \theta^\dagger)^T (\ell_{\theta\theta\theta}(\tilde{\theta}_{n,j}))_j(\hat{\theta}_n - \theta^\dagger)
\end{align*}
\]</span> where <span class="math inline">\(\tilde{\theta}_{n,j}\)</span> is a point on the chord between <span class="math inline">\(\hat{\theta}_n\)</span> and <span class="math inline">\(\theta^\dagger\)</span> and may depend on the coordinate <span class="math inline">\(j\)</span>.</p>
<p>Noting that <span class="math inline">\(\ell_\theta(\hat{\theta}_n)_j = 0\)</span> for all <span class="math inline">\(j\)</span>, we get the set of <span class="math inline">\(p\)</span> linear equations: <span class="math display">\[\begin{align*}
(\ell_\theta(\theta^\dagger))_j =  -(\ell_{\theta\theta}(\theta^\dagger))_j (\hat{\theta}_n - \theta^\dagger) - \frac{1}{2}(\hat{\theta}_n - \theta^\dagger)^T (\ell_{\theta\theta\theta}(\tilde{\theta}_{n,j}))_j(\hat{\theta}_n - \theta^\dagger)
\end{align*}\]</span> Multiplying both sides by <span class="math inline">\(\frac{\sqrt{n}}{n}\)</span> gives: <span class="math display">\[
\begin{align*}
\sqrt{n}\lp\frac{1}{n} (\ell_\theta(\theta^\dagger))_j\rp &amp; =  -\left(\frac{1}{n}(\ell_{\theta\theta}(\theta^\dagger))_j\right) \sqrt{n}(\hat{\theta}_n - \theta^\dagger) - \frac{1}{2}(\hat{\theta}_n - \theta^\dagger)^T \lp \frac{1}{n}(\ell_{\theta\theta\theta}(\tilde{\theta}_{n,j}))_j\rp\sqrt{n}(\hat{\theta}_n - \theta^\dagger) \\
&amp; = \lp -\left(\frac{1}{n}(\ell_{\theta\theta}(\theta^\dagger))_j\right) - \frac{1}{2}(\hat{\theta}_n - \theta^\dagger)^T \lp \frac{1}{n}(\ell_{\theta\theta\theta}(\tilde{\theta}_{n,j}))_j\rp\rp\sqrt{n}(\hat{\theta}_n - \theta^\dagger)
\end{align*}
\]</span> The term <span class="math inline">\(\frac{1}{n}(\ell_{\theta\theta}(\theta^\dagger))_j \overset{p}{\to} \Exp{(\ell_{\theta\theta}(\theta^\dagger))_j}\)</span> by the WLLN, while <span class="math display">\[
\frac{1}{2}(\hat{\theta}_n - \theta^\dagger)^T \lp \frac{1}{n}(\ell_{\theta\theta\theta}(\tilde{\theta}_{n,j}))_j\rp \overset{p}{\to} 0
\]</span> by the WLLN, the boundedness condition of the third-order derivatives and the consistency of our estimator.</p>
<p>To be precise, we want to show that <span class="math display">\[
\lim_{n\to\infty}\Prob{\norm{(\hat{\theta}_n - \theta^\dagger)^T \lp \frac{1}{n}(\ell_{\theta\theta\theta}(\tilde{\theta}_{n,j}))_j\rp} &gt; \epsilon}{} = 0
\]</span> <span class="math display">\[
\begin{aligned}
\Prob{\norm{(\hat{\theta}_n - \theta^\dagger)^T \lp \frac{1}{n}(\ell_{\theta\theta\theta}(\tilde{\theta}_{n,j}))_j\rp} &gt; \epsilon}{} &amp; \leq \Prob{\norm{(\hat{\theta}_n - \theta^\dagger)^T} \norm{ \frac{1}{n}(\ell_{\theta\theta\theta}(\tilde{\theta}_{n,j}))_j} &gt; \epsilon}{} \\
&amp; \leq \Prob{\norm{(\hat{\theta}_n - \theta^\dagger)^T} \norm{\frac{1}{n}\sum_{i=1}^n g(Y_i)} &gt; \epsilon}{} \\
&amp; \leq \Prob{\norm{(\hat{\theta}_n - \theta^\dagger)^T} \norm{\frac{1}{n}\sum_{i=1}^n g(Y_i) - \Exp{g(Y_1)}} &gt; \frac{\epsilon}{2}}{} \\
&amp; \quad + \Prob{\norm{(\hat{\theta}_n - \theta^\dagger)^T} \norm{\Exp{g(Y_1)}} &gt; \frac{\epsilon}{2}}{}
\end{aligned}
\]</span> The last line follows from a trick used in a proof in § 6.3 in <span class="citation" data-cites="resnick2019probability">Resnick (<a href="#ref-resnick2019probability" role="doc-biblioref">2019</a>)</span>. If the following holds</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \left\{\norm{(\hat{\theta}_n - \theta^\dagger)^T} \norm{\frac{1}{n}\sum_{i=1}^n g(Y_i) - \Exp{g(Y_1)}} \leq \frac{\epsilon}{2}\right\}  \bigcap\\
&amp; \left\{\norm{(\hat{\theta}_n - \theta^\dagger)^T} \norm{\Exp{g(Y_1)}} \leq \frac{\epsilon}{2}\right\}
\end{aligned}
\]</span></p>
<p>Then the triangle inequality implies that <span class="math display">\[
\norm{(\hat{\theta}_n - \theta^\dagger)^T} \norm{\frac{1}{n}\sum_{i=1}^n g(Y_i)} \leq \epsilon.
\]</span> Taking complements and using Boole’s inequality (<span class="math inline">\(P(A \cup B) \leq P(A) + P(B)\)</span>) yields the final inequality. Both terms converge to zero due to the convergence in probability of the MLE, and the WLLN applied to the empirical average of <span class="math inline">\(g(Y_i)\)</span>.</p>
<p>By Slutsky’s theorem, <span class="math display">\[
-\left(\frac{1}{n}(\ell_{\theta\theta}(\theta^\dagger))_j\right) - \frac{1}{2}(\hat{\theta}_n - \theta^\dagger)^T \lp \frac{1}{n}(\ell_{\theta\theta\theta}(\tilde{\theta}_{n,j}))_j\rp \overset{p}{\to}-\Exp{(\ell_{\theta\theta}(\theta^\dagger))_j}
\]</span> Collecting our <span class="math inline">\(p\)</span> equations into one set of equations yields: <span id="eq-lin-eqs"><span class="math display">\[
\sqrt{n}\lp\frac{1}{n} \ell_\theta(\theta^\dagger)\rp = (-\Exp{\ell_{\theta\theta}(\theta^\dagger))} + o_p(1)) \sqrt{n}(\hat{\theta}_n - \theta^\dagger)
\tag{1}\]</span></span></p>
<p>Writing out the expressions as explicit sums: <span id="eq-grad-sum"><span class="math display">\[\begin{align*}
\frac{\sqrt{n}}{n}  \sum_{i=1}^n \ell_\theta(\theta^\dagger; x_i) = (-\Exp{\ell_{\theta\theta}(\theta^\dagger))} + o_p(1)) \sqrt{n}(\hat{\theta}_n - \theta^\dagger)
\end{align*} \tag{2}\]</span></span></p>
<p>The left-hand side of <a href="#eq-grad-sum" class="quarto-xref">Equation&nbsp;2</a> will be amenable to a multivariate version of the CLT. We’ll take the following multivariate CLT as given:</p>
<div class="theorem">
<p><strong>Theorem 1</strong>. Multivariate CLT, <span class="citation" data-cites="keener_theoretical_2010">(<a href="#ref-keener_theoretical_2010" role="doc-biblioref">Keener 2010</a>)</span> Let <span class="math inline">\(X_1, X_2, \dots\)</span> be i.i.d random vectors in <span class="math inline">\(\R^k\)</span> with a common mean <span class="math inline">\(\Exp{X_i} = \mu\)</span> and common covariance matrix <span class="math inline">\(\Sigma = \Exp{(X_i - \mu)(X_i - \mu)^T}\)</span>. If <span class="math inline">\(\bar{X} = \frac{\sum_{i=1}^n X_i}{n}\)</span>, then <span class="math display">\[\sqrt{n}(\bar{X} - \mu) \overset{d}{\to} \text{Normal}(0, \Sigma)\]</span></p>
</div>
<p>Recall that <span class="math display">\[
\Exp{\ell_\theta(\theta; X_i)} = 0
\]</span> By the multivariate central limit (MCLT) theorem, <a href="#eq-grad-sum" class="quarto-xref">Equation&nbsp;2</a> converges in distribution to a multivariate normal distribution with mean zero and covariance matrix <span class="math inline">\(\Exp{\ell_\theta(\theta;X_i)\ell_\theta(\theta;X_i)^T}\)</span>.</p>
<p>We’ll also need a lemma about the solutions to random linear equations:</p>
<div id="lem-eqs" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1 (Lemma 5.2 in <span class="citation" data-cites="lehmann-casella">(<a href="#ref-lehmann-casella" role="doc-biblioref">Lehmann and Casella 1998</a>)</span>)</strong></span> Suppose there are a set of <span class="math inline">\(p\)</span> equations, <span class="math inline">\(j = 1, \dots, p\)</span>: <span class="math display">\[\sum_{k=1}^p A_{jkn} Y_{kn} = T_{jn}.\]</span> Let <span class="math inline">\(T_{1n}, \dots, T_{pn}\)</span> converge in distribution to <span class="math inline">\(T_1, \dots, T_p\)</span>. Furthermore, suppose that for each <span class="math inline">\(j,k\)</span>, <span class="math inline">\(A_{jkn} \overset{p}{\to} a_{jk}\)</span> such that the matrix <span class="math inline">\(A\)</span> with <span class="math inline">\((j,k)^{\mathrm{th}}\)</span> element <span class="math inline">\(a_{jk}\)</span> is nonsingular. Then if the distribution of <span class="math inline">\(T_1, \dots, T_p\)</span> has a disitribution with repsect to the Lebesgue measure over <span class="math inline">\(\R^p\)</span>, <span class="math inline">\(Y_{1n}, \dots, Y_{pn}\)</span> tend in probability to <span class="math inline">\(A^{-1} T\)</span>.</p>
<p>Written in matrix form and using the fact that convergence in probability implies convergence in distribution: <span class="math display">\[
T_n = A_n Y_n  \implies Y_n \overset{d}{\to} A^{-1}T  
\]</span></p>
</div>
<p>We have that the left-hand side of <a href="#eq-lin-eqs" class="quarto-xref">Equation&nbsp;1</a> converges in distribtuion to a multivariate normal distribution, and we have that the matrix on the RHS of <a href="#eq-lin-eqs" class="quarto-xref">Equation&nbsp;1</a> convegens in probability to <span class="math inline">\(-\Exp{\ell_{\theta\theta}(\theta^\dagger))}\)</span>, which by assumption is invertble. Thus by <a href="#lem-eqs" class="quarto-xref">Lemma&nbsp;1</a> <span class="math inline">\(\sqrt{n}(\hat{\theta}_n - \theta^\dagger)\)</span> converges in probability to <span class="math display">\[\begin{align*}
\sqrt{n}(\hat{\theta}_n - \theta^\dagger) \overset{p}{\to} (-\Exp{\ell_{\theta\theta}(\theta^\dagger; X_i)})^{-1} \Exp{\ell_\theta(\theta;X_i)\ell_\theta(\theta;X_i)^T}^{1/2}\mathcal{Z}
\end{align*}\]</span> where <span class="math inline">\(\mathcal{Z} \sim \text{Normal}(0, I_p)\)</span>, or <span class="math display">\[\begin{align*}
&amp;\sqrt{n}(\hat{\theta}_n - \theta^\dagger)  \overset{d}{\to} \mathcal{N}\left(0, (-\Exp{\ell_{\theta\theta}(\theta^\dagger; X_i)})^{-1} \Exp{\ell_\theta(\theta;X_i)\ell_\theta(\theta;X_i)^T}(-\Exp{\ell_{\theta\theta}(\theta^\dagger; X_i)})^{-1}\right)
\end{align*}\]</span></p>
<p>Assuming further that <span class="math display">\[
\Exp{\ell_{\theta\theta}(\theta^\dagger; X_i)} + \Exp{\ell_\theta(\theta;X_i)\ell_\theta(\theta;X_i)^T}=0 \implies (-\Exp{\ell_{\theta\theta}(\theta^\dagger; X_i)})^{-1}\Exp{\ell_\theta(\theta;X_i)\ell_\theta(\theta;X_i)^T} = I_p
\]</span> Putting this all together shows that <span class="math display">\[\begin{align*}
     \sqrt{n}(\hat{\theta}_n - \theta^\dagger) \overset{d}{\to} \mathcal{N}(0, \mathcal{I}(\theta^\dagger)^{-1})
\end{align*}\]</span> where <span class="math inline">\(\mathcal{I}(\theta^\dagger) = -\Exp{\ell_{\theta\theta}(\theta^\dagger; X_i)}\)</span></p>
<section id="estimators-of-variance-covariance-matrix" class="level3" data-number="2.0.1">
<h3 data-number="2.0.1" class="anchored" data-anchor-id="estimators-of-variance-covariance-matrix"><span class="header-section-number">2.0.1</span> Estimators of variance-covariance matrix</h3>
<p>In the previous section, we encountered several consistent estimators of the variance covariance matrix: <span class="math display">\[\begin{align*}
  -\frac{1}{n} \nabla^2_\theta \ell(\theta)\mid_{\theta = \theta^\dagger} &amp; \overset{p}{\to} \mathcal{I}(\theta^\dagger) \\
  \frac{1}{n} \sum_{i=1}^n (\nabla_\theta \log f_\theta(X_i))\mid_{\theta = \theta^\dagger}(\nabla_\theta \log f_\theta(X_i))\mid_{\theta = \theta^\dagger}^T  &amp; \overset{p}{\to} \mathcal{I}(\theta^\dagger)
\end{align*}\]</span> These expressions assume that our inferential model matches the data generating model. In the event our inferential model is different than the true data generating model, it can be shown that the scaled MLE converges asymptotically to <span class="math display">\[\begin{align*}
     \sqrt{n}(\hat{\theta}_n - \theta^\dagger) \overset{d}{\to} \mathcal{N}\left(0, \Exp{-(\nabla^2_\theta \log f_\theta(X_1)) \mid_{\theta = \theta^\dagger}}^{-1}\Exp{(\nabla_\theta \log f_\theta(X_i))\mid_{\theta = \theta^\dagger}(\nabla_\theta \log f_\theta(X_i))\mid_{\theta = \theta^\dagger}^T}\Exp{-(\nabla^2_\theta \log f_\theta(X_1)) \mid_{\theta = \theta^\dagger}}^{-1}\right)
\end{align*}\]</span> where the key difference is that <span class="math inline">\(\theta^\dagger\)</span> is no longer the parameter for the true data generating process, but is instead the parameter the minimizes the KL divergence between the assumed inferential model and the true distribution generating the data.</p>
<p>Thus, the following sandwich estimator for the variance covariance matrix is often preferred over either of the above expressions: <span class="math display">\[\begin{align}
\hat{\Sigma}_{R} &amp; = \left(-\frac{1}{n} \nabla^2_\theta \ell(\theta)\mid_{\theta = \hat{\theta}}\right)^{-1}\frac{1}{n} \sum_{i=1}^n (\nabla_\theta \log f_\theta(X_i))\mid_{\theta = \hat{\theta}}(\nabla_\theta \log f_\theta(X_i))\mid_{\theta = \hat{\theta}}^T \left(-\frac{1}{n} \nabla^2_\theta \ell(\theta)\mid_{\theta = \hat{\theta}}\right)^{-1} \label{eq:sandwich}\\
    &amp; \overset{p}{\to} \text{Var}\left(\sqrt{n}(\hat{\theta}_n - \theta^\dagger)\right)
\end{align}\]</span> where <span class="math inline">\(\hat{\theta}\)</span> is the MLE.</p>
</section>
<section id="asymptotic-confidence-intervals" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="asymptotic-confidence-intervals"><span class="header-section-number">2.1</span> Asymptotic confidence intervals</h2>
<p>For the most part, we’ll be concerned with univariate confidence intervals, but in multivariate models like the Weibull distribution we’ll need to compute the full inverse of the Fisher information. WLOG, let the index of the parameter of interest be <span class="math inline">\(1\)</span>, so the asymptotic variance of our MLE for the parameter of interest is <span class="math inline">\(\sigma_1^2(\theta^\dagger) = \mathcal{I}(\theta^\dagger)^{-1}_{1,1}\)</span>. We can also define <span class="math display">\[\sigma_1^2(\hat{\theta}) = \mathcal{I}(\hat{\theta})^{-1}_{1,1}.\]</span> I’ll also ditch the <span class="math inline">\(n\)</span> subscript and just let <span class="math inline">\(\hat{\theta}\)</span> be our MLE based on <span class="math inline">\(n\)</span> observations. By <a href="#eq:cont-map" data-reference-type="ref+Label" data-reference="eq:cont-map">[eq:cont-map]</a>, <span class="math display">\[\frac{\sigma_1^2(\hat{\theta})}{\sigma_1^2(\theta^\dagger)} \overset{p}{\to} 1.\]</span> This allows us to use a plug-in estimator for <span class="math inline">\(\mathcal{I}(\theta^\dagger)^{-1}\)</span>, <span class="math inline">\(\mathcal{I}(\hat{\theta})^{-1}\)</span>. <span class="math display">\[\begin{align*}
     \frac{\sqrt{n}(\hat{\theta}_1 - \theta_1^\dagger)}{\sigma_1(\hat{\theta})} &amp; = \frac{\sigma_1(\theta^\dagger)}{\sigma_1(\hat{\theta})}\frac{\sqrt{n}(\hat{\theta}_1 - \theta_1^\dagger)}{\sigma_1(\theta^\dagger)} \\
     &amp; \overset{d}{\to} \mathcal{N}(0, 1)
\end{align*}\]</span> Using <a href="#eq:prod-slutsky" data-reference-type="ref+Label" data-reference="eq:prod-slutsky">[eq:prod-slutsky]</a>, we can create an asymptotic confidence interval by noting that: <span class="math display">\[P\left(\frac{\sqrt{n}(\hat{\theta}_1 - \theta^\dagger_1)}{\sigma_1(\hat{\theta})} \leq x\right) = \Phi(x),\]</span> where <span class="math inline">\(\Phi(x)\)</span> is the CDF a normal distribution with zero mean and unit variance.</p>
<p>Then <span class="math display">\[\begin{align*}
P\left(\frac{\sqrt{n}(\hat{\theta}_1 - \theta^\dagger_1)}{\sigma_1(\hat{\theta}_1)} \in (-z_{1-\alpha/2},z_{1-\alpha/2})\right) &amp; = P\left(\theta_1^\dagger \in \left(\hat{\theta}_1 - z_{1-\alpha/2} \frac{\sigma_1(\hat{\theta}_1)}{\sqrt{n}}, \hat{\theta}_1 + z_{1-\alpha/2} \frac{\sigma_1(\hat{\theta}_1)}{\sqrt{n}}\right)\right)
\end{align*}\]</span></p>
</section>
<section id="asymptotic-tests" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="asymptotic-tests"><span class="header-section-number">2.2</span> Asymptotic tests</h2>
<section id="wald-test" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="wald-test"><span class="header-section-number">2.2.1</span> Wald test</h3>
<p>The Wald test is derived directly from the asymptotic distribution of the MLE. Under the null hypothesis <span class="math inline">\(\theta^\dagger = \theta_0\)</span>, the test statistic: <span class="math display">\[\begin{align*}
     \sqrt{n}(\hat{\theta}_n - \theta_0) \overset{d}{\to} \mathcal{N}(0, \mathcal{I}(\theta_0)^{-1})
\end{align*}\]</span> so <span class="math display">\[n (\hat{\theta}_n - \theta_0)^T \mathcal{I}(\theta_0) (\hat{\theta}_n - \theta_0) \sim \chi^2(p)\]</span> This follows from the simple fact that if a random vector in <span class="math inline">\(\R^n\)</span>, <span class="math inline">\(Z\)</span>, is distributed multivariate normal, or <span class="math inline">\(Z \sim \mathcal{N}(0, \Sigma)\)</span>, then <span class="math inline">\(\Sigma^{-1/2} Z \sim \mathcal{N}(0, I)\)</span>, so <span class="math inline">\(Z^T\Sigma^{-1/2}\Sigma^{-1/2}Z = \sum_{i=1}^n X_i^2\)</span> where <span class="math inline">\(X_i \sim \mathcal{N}(0,1)\)</span>.</p>
</section>
<section id="raos-score-test" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="raos-score-test"><span class="header-section-number">2.2.2</span> Rao’s score test</h3>
<p>In our proof of the asymptotic distribution of the MLE, we used the fact that <span class="math display">\[\sqrt{n}\frac{1}{n} \sum_{i=1}^n (\nabla_\theta \log f_\theta(X_i))\mid_{\theta = \theta^\dagger} \overset{d}{\to} \mathcal{N}(0, \mathcal{I}(\theta^\dagger)).\]</span> This idea can be used to derive the Rao’s Score test, which uses the fact that under <span class="math inline">\(H_0: \theta \in \Theta_0\)</span>, the gradient evaluated at the restricted MLE (i.e.&nbsp;the MLE restricted to the parameter space <span class="math inline">\(\Theta_0\)</span>) is nearly zero, and we can recover a similar limiting distribution. As above let <span class="math display">\[\frac{1}{\sqrt{n}} \nabla_\theta \ell(\theta) \mid_{\theta = \theta^\dagger} = \sqrt{n}\frac{1}{n} \sum_{i=1}^n (\nabla_\theta \log f_\theta(X_i))\mid_{\theta = \theta^\dagger}\]</span> Assuming that under the null distribution the restrited MLE <span class="math inline">\(\hat{\theta}_0\)</span> is consistent for <span class="math inline">\(\theta^\dagger \in \Theta_0\)</span>, then <span class="math display">\[\frac{1}{\sqrt{n}} \nabla_\theta \ell(\theta) \mid_{\theta = \hat{\theta}_0} \overset{d}{\to} \mathcal{N}(0, \mathcal{I}(\theta^\dagger))\]</span> The Score test statistic is: <span class="math display">\[T_S = \left(\frac{1}{\sqrt{n}} \nabla_\theta \ell(\theta) \mid_{\theta = \hat{\theta}_0} \right)^T \mathcal{I}(\hat{\theta}_0)^{-1}\frac{1}{\sqrt{n}} \nabla_\theta \ell(\theta) \mid_{\theta = \hat{\theta}_0}\]</span> This test statistic is distribution <span class="math inline">\(\chi^2(p)\)</span> under <span class="math inline">\(H_0\)</span>.</p>
</section>
<section id="likelihood-ratio-test" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="likelihood-ratio-test"><span class="header-section-number">2.2.3</span> Likelihood ratio test</h3>
<p>The LRT comes from a two-term asymptotic expansion of the log-likelihood, as opposed to the one term expansion: <span class="math display">\[\begin{align*}
    -\ell(\theta_0) &amp; = -\ell(\hat{\theta}) - \nabla_\theta \ell(\theta)\mid_{\theta=\hat{\theta}}(\hat{\theta}-\theta_0) - \frac{1}{2}(\hat{\theta}-\theta_0)^T\nabla^2_\theta \ell(\theta)\mid_{\theta=\tilde{\theta}}(\hat{\theta}-\theta_0) \\
    \ell(\hat{\theta}) - \ell(\theta_0)  &amp; = -\frac{1}{2}(\hat{\theta}-\theta_0)^T\nabla^2_\theta \ell(\theta)\mid_{\theta=\tilde{\theta}}(\hat{\theta}-\theta_0) \\
    &amp; = \frac{1}{2}(\sqrt{n}(\hat{\theta}-\theta_0))^T\frac{-\nabla^2_\theta \ell(\theta)\mid_{\theta=\tilde{\theta}}}{n}(\sqrt{n}(\hat{\theta}-\theta_0)) \\
\end{align*}\]</span> As before, <span class="math display">\[\sqrt{n}(\hat{\theta}-\theta_0) \overset{d}{\to} \mathcal{N}(0, \mathcal{I}(\theta_0)^{-1})\]</span> and <span class="math display">\[-\frac{\nabla^2_\theta \ell(\theta)\mid_{\theta=\tilde{\theta}}}{n} \overset{p}{\to} \mathcal{I}(\theta_0)\]</span> so <span class="math display">\[2 (\ell(\hat{\theta}) - \ell(\theta_0)) \overset{d}{\to} \chi^2(p)\]</span></p>



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-collett1994modelling" class="csl-entry" role="listitem">
Collett, David. 1994. <em>Modelling Survival Data in Medical Research</em>. Chapman &amp; Hall.
</div>
<div id="ref-keener_theoretical_2010" class="csl-entry" role="listitem">
Keener, Robert W. 2010. <em>Theoretical <span>Statistics</span></em>. Springer <span>Texts</span> in <span>Statistics</span>. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-0-387-93839-4">https://doi.org/10.1007/978-0-387-93839-4</a>.
</div>
<div id="ref-lehmann-casella" class="csl-entry" role="listitem">
Lehmann, E. L., and George Casella. 1998. <em>Theory of Point Estimation</em>. 2nd ed. Springer Texts in Statistics. New York: Springer.
</div>
<div id="ref-resnick2019probability" class="csl-entry" role="listitem">
Resnick, Sidney. 2019. <em>A Probability Path</em>. Springer.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>