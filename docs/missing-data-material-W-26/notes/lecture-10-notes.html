<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Missing data lecture 10: Flawed approach to missing data and EM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-fd68462179fdb1eb8400a3e2e38edf1e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
window.MathJax = {
  startup: {
    ready() {
      MathJax.startup.defaultReady();
      const {STATE} = MathJax._.core.MathItem;
      MathJax.tex2mml(String.raw`
      \newcommand{\R}{\mathbb{R}}
      \newcommand{\comp}{\mathsf{c}}
        \newcommand{\lp}{\left(}
        \newcommand{\rp}{\right)}
        \newcommand{\lb}{\left[}
        \newcommand{\rb}{\right]}
        \newcommand{\ind}[1]{\mathbbm{1}\lp#1\rp}
        \newcommand{\Exp}[1]{\mathbb{E} \lb #1 \rb}
        \newcommand{\ExpA}[2]{\mathbb{E}_{#2} \lb #1 \rb}
        \newcommand{\Var}[1]{\text{Var} \lp #1 \rp}
        \newcommand{\VarA}[2]{\text{Var}_{#2} \lp #1 \rp}
        \newcommand{\indy}{\mathrel{\unicode{x2AEB}}}
        \newcommand{\Prob}[2]{\mathbb{P}_{#2} \lp #1 \rp}
        \newcommand{\abs}[1]{\left| #1 \right|}
        \newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
      `);
    }
  },
  loader: {load: ['[tex]/bbm']},
  tex: {packages: {'[+]': ['bbm']}}
};
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@4.1/tex-mml-chtml.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../papers.html"> 
<span class="menu-text">My papers</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Random thoughts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../courses.html"> 
<span class="menu-text">Courses</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#flawed-approach-to-missing-data" id="toc-flawed-approach-to-missing-data" class="nav-link active" data-scroll-target="#flawed-approach-to-missing-data">Flawed approach to missing data</a></li>
  <li><a href="#em-algorithm" id="toc-em-algorithm" class="nav-link" data-scroll-target="#em-algorithm">EM algorithm</a>
  <ul class="collapse">
  <li><a href="#nontrivial-example" id="toc-nontrivial-example" class="nav-link" data-scroll-target="#nontrivial-example">Nontrivial example</a></li>
  </ul></li>
  <li><a href="#why-does-em-work" id="toc-why-does-em-work" class="nav-link" data-scroll-target="#why-does-em-work">Why does EM work?</a>
  <ul class="collapse">
  <li><a href="#multivariate-normal-example-again" id="toc-multivariate-normal-example-again" class="nav-link" data-scroll-target="#multivariate-normal-example-again">Multivariate normal example again</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Missing data lecture 10: Flawed approach to missing data and EM</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="flawed-approach-to-missing-data" class="level2">
<h2 class="anchored" data-anchor-id="flawed-approach-to-missing-data">Flawed approach to missing data</h2>
<p>One generally flawed approach to inference in missing data problems is to treat the missing values as unknown parameters and to maximize the following function of parameters <em>and</em> missing values: <span class="math display">\[
L_{\text{mispar}}(\theta, y_{(1)} \mid y_{(0)}) = f_{Y}(y_{(1)}, y_{(0)} \mid \theta)
\]</span> The MLE using this distribution would require you to jointly maximize the likelihood with respect to <span class="math inline">\(\theta\)</span> <em>and</em> <span class="math inline">\(y_{(1)}\)</span>. Let’s take this approach with the censored exponential samples and see what we get. We had that the likelihood was</p>
<p><span class="math display">\[
\prod_{i=1}^r \theta^{-1} e^{-y_i / \theta} \ind{y_i &lt; c} \prod_{i={r+1}}^n \theta^{-1} e^{-y_i / \theta} \ind{y_i \geq c} f(y \mid \theta)
\]</span> Because <span class="math inline">\(e^{-y_i/\theta}\)</span> is monotonically decreasing in <span class="math inline">\(y_i\)</span> for any <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat{y}_i\)</span> is <span class="math inline">\(c\)</span> for the missing observations.</p>
<p>Plugging this into the log-likelihood gives <span class="math display">\[
-r \log\theta -(n-r) \log \theta -\frac{\sum_{i=1}^r y_i + (n - r)c}{\theta}
\]</span> Taking derivatives and setting this equal to zero gives: <span class="math display">\[
\hat{\theta}_{\text{mispar}} = \frac{\sum_{i=1}^r y_i + (n - r)c}{n}  = \frac{r}{n}\hat{\theta}
\]</span> This understates the true value <span class="math inline">\(\theta\)</span>, and one can show that this estimator isn’t consistent for <span class="math inline">\(\theta\)</span>. The only way this estimator is consistent for <span class="math inline">\(\theta\)</span> is if <span class="math inline">\(r / n \to 1\)</span>.</p>
<p>This example shows that the goal in missing data analysis isn’t to predict missing values, it is to account for the uncertainty in missing values by integrating over the distribution of missing values.</p>
<p>Here is another example.</p>
<p>Let <span class="math inline">\(y_i\)</span> be normally distributed with unknown mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Suppose there are <span class="math inline">\(r\)</span> observed values and <span class="math inline">\(n - r\)</span> missing values. We assume that the data are MAR and the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are distinct from the parameters of the missingness distribution.</p>
<p>The MLE from the ignorable likelihood is just the MLE on the complete cases: <span class="math display">\[
\hat{\mu} = \sum_{i=1}^r \frac{y_i}{r}, \hat{\sigma}^2 = \sum_{i=1}^r \frac{(y_i - \hat{\mu}^2)}{r}
\]</span> If we write down the mistaken likelihood for the data we get:</p>
<p><span class="math display">\[
\ell_{\text{mispar}}(\mu, \sigma^2, y_{r+1},\dots, y_n \mid y_{(0)}) =
-\frac{n}{2} \log \sigma^2 - \frac{1}{2\sigma^2} \sum_{i=1}^r (y_i - \mu)^2 - \frac{1}{2\sigma^2} \sum_{i=r+1}^n (y_i - \mu)^2
\]</span> We can maximize this by setting <span class="math inline">\(y_{r+1}, \dots, y_n\)</span> to <span class="math inline">\(\mu\)</span>, thereby eliminating the second sum. This leads to an MLE for <span class="math inline">\(\mu\)</span> that is equal to <span class="math inline">\(\hat{\mu}\)</span> above. The variance, however, is incorrectly estimated. Taking gradients and solving for <span class="math inline">\(\sigma^2\)</span> gives <span class="math display">\[
\hat{\sigma}^2 = \sum_{i=1}^r \frac{(y_i - \hat{\mu})^2}{n}
\]</span> Why is this wrong? Intuitively, we can see that the expected value of the second term isn’t zero, and thus we’ll understate the variance Thus, the MLE in the misparametrized model yields a variance estimate that is too low.</p>
<p>Again, we want to integrate over our uncertainty in the missing values, rather than predict missing values.</p>
</section>
<section id="em-algorithm" class="level2">
<h2 class="anchored" data-anchor-id="em-algorithm">EM algorithm</h2>
<p>The last section discussed how maximizing the likelihood as a function of the missing data points was an incorrect way to go about doing likelihood inference when there is missing data.</p>
<p>The ``right” way to do so is to integrate over the uncertainty in your likelihood stemming from the missing observations, and then maximize this object.</p>
<p><span class="math display">\[
L_\text{ign}(\theta \mid Y_{(0)} = \tilde{y}_{(0)}) \propto \int_{\mathcal{Y}_{(1)}} f_Y(Y_{(0)} = \tilde{y}_{(0)}, Y_{(0)} = y_{(1)} \mid \theta) dy_{(1)}
\]</span> We’ll call the log-likelihood above: <span class="math display">\[
\ell(\theta \mid  Y_{(0)} = \tilde{y}_{(0)})
\]</span></p>
<p>Sometimes we can’t do this maximization directly in one step because it is tricky. Instead, of doing the maximization directly we do so iteratively, and this is the motivation for the Expectation-Maximization algorithm: Let <span class="math inline">\(\ell_Y(\theta \mid Y)\)</span> be the complete data log-likelihood, and <span class="math inline">\(f(Y_{(1)} \mid Y_{(0)}, \theta^{(t)})\)</span> be the distribution of missing values given the observed values and the current best guess at the parameters <span class="math inline">\(\theta^{(t)}\)</span>. The object, which we’ll call <span class="math inline">\(Q(\theta \mid \theta^{(t)})\)</span>, we want to maximize is the expected log-likelihood, where we take the expectation over the conditional distribution of the missing values</p>
<p><span class="math display">\[
Q(\theta \mid \theta^{(t)}) = \int_{\mathcal{Y}_{(1)}}\ell_Y(\theta \mid Y_{(1)}, Y_{(0)} = \tilde{y}_{(0)})f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta^{(t)})dY_{(1)}
\]</span> ### Normal example, again</p>
<p>Continuing the normal example from above, we have that our complete data log-likelihood is: <span class="math display">\[
\ell_Y(\mu, \sigma^2 \mid Y_{(1)}, Y_{(0)}) = -\frac{n}{2} \log \sigma^2 -\frac{1}{2\sigma^2} \sum_{i=1}^r (y_i - \mu)^2 -\frac{1}{2\sigma^2} \sum_{i=r+1}^n (y_i - \mu)^2
\]</span></p>
<p>Because we’ve assumed that the data are MAR (which in univariate settings equals MCAR), we have that <span class="math display">\[
y_i \sim \text{Normal}(\mu^{(t)}, (\sigma^2)^{(t)})
\]</span> Then for each <span class="math inline">\(y_i\)</span>, <span class="math display">\[
\Exp{y_i^2 - 2\mu y_i + \mu^2} = (\mu^{(t)})^2 + (\sigma^{t})^2  - 2 \mu \mu^{(t)} + \mu^2
\]</span> Then the expected log-likelihood is: <span class="math display">\[
Q(\theta \mid \theta^{(t)}) = -\frac{n}{2} \log \sigma^2 -\frac{1}{2\sigma^2} \sum_{i=1}^r (y_i - \mu)^2 -\frac{1}{2\sigma^2} (n-r) ((\mu^{(t)})^2 + (\sigma^{t})^2  - 2 \mu \mu^{(t)} + \mu^2)
\]</span> Taking partial derivatives with respect to <span class="math inline">\(\mu\)</span> gives: <span class="math display">\[
\begin{aligned}
0 &amp; = \frac{1}{\sigma^2} \sum_{i=1}^r (y_i - \mu) -\frac{1}{\sigma^2} (n-r) (-2 \mu^{(t)} + 2 \mu)  \\
&amp; = \frac{1}{\sigma^2} (\sum_{i=1}^r y_i  + (n - r) \mu^{(t)}) - n\frac{\mu}{\sigma^2}
\end{aligned}
\]</span> Solving for <span class="math inline">\(\mu\)</span> gives the update rule for <span class="math inline">\(\mu^{(t)}\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\mu^{(t+1)} = \frac{\sum_{i=1}^r y_i  + (n - r) \mu^{(t)}}{n}
\end{aligned}
\]</span> Taking partials with respect to <span class="math inline">\(\sigma^2\)</span> <span class="math display">\[
\frac{\partial Q(\theta \mid \theta^{(t)})}{\partial \sigma^2} = -\frac{n}{2 \sigma^2} +\frac{1}{2\sigma^4} (\sum_{i=1}^r (y_i - \mu)^2 + (n-r) ((\mu^{(t)})^2 + (\sigma^{(t)})^2  - 2 \mu \mu^{(t)} + \mu^2))
\]</span> Setting this equal to zero and simplifying gives: <span class="math display">\[
\begin{aligned}
0 &amp; = -\frac{n}{2 \sigma^2} +\frac{1}{2\sigma^4} (\sum_{i=1}^r (y_i - \mu^{(t+1)})^2 + (n-r) ((\mu^{(t)})^2 + (\sigma^{(t)})^2  - 2 \mu^{(t+1)} \mu^{(t)} + (\mu^{(t+1)})^2))
\end{aligned}
\]</span> This yields: <span class="math display">\[
\begin{aligned}
(\sigma^2)^{(t+1)} &amp; = \frac{(\sum_{i=1}^r (y_i - \mu^{(t+1)})^2 + (n-r) ((\mu^{(t)})^2 + (\sigma^{(t)})^2  - 2 \mu^{(t+1)} \mu^{(t)} + (\mu^{(t+1)})^2))}{n}
\end{aligned}
\]</span> which simplifies to <span class="math display">\[
\begin{aligned}
(\sigma^2)^{(t+1)} &amp; = \frac{(\sum_{i=1}^r y_i^2 + (n-r) ((\mu^{(t)})^2 + (\sigma^{(t)})^2)}{n} - (\mu^{(t+1)})^2
\end{aligned}
\]</span> This shows why the prior procedure didn’t work; namely it ignores the extra variability in imputations for the missing <span class="math inline">\(y_i^2\)</span> terms.</p>
<p>To see why this simplifies, expand out <span class="math display">\[
\sum_{i=1}^r (y_i - \mu^{(t+1)})^2 = \sum_{i=1}^r y_i^2 - 2 \mu^{(t+1)} \sum_{i=1}^r y_i +  r (\mu^{(t+1)})^2
\]</span> and sub in the following expression for <span class="math inline">\(\sum_i y_i\)</span> <span class="math display">\[
\begin{aligned}
\sum_{i=1}^r y_i = n \mu^{(t+1)}  - (n - r)\mu^{(t)}
\end{aligned}
\]</span></p>
<p>Plugging in our estimate for <span class="math inline">\(\mu^{(t+1)}\)</span> gives the solution above.</p>
<p>One can show that the EM solution converges to the complete data result.</p>
<section id="nontrivial-example" class="level3">
<h3 class="anchored" data-anchor-id="nontrivial-example">Nontrivial example</h3>
<p>Here’s an example: Let’s say we have two outcomes, so <span class="math inline">\(Y\)</span> is an <span class="math inline">\(n \times 2\)</span> matrix. For simplicity’s sake, we assume the missingness is ignorable, and assume we have a general missingness pattern, i.e.&nbsp;there are 3 patterns of missingness, assuming all participants had at least one measurement. The parameters of interest are <span class="math inline">\(\mu_1, \mu_2, \Sigma\)</span>, and we’d like to use all the available data to do inference. If we had a complete dataset, we know from an earlier lecture the ML solutions for these quantities: <span class="math display">\[
\hat{\mu}_1 = \bar{y}_1,\,\hat{\mu}_2 = \bar{y}_2,\, \hat{\Sigma} = 1/n\sum_i y_i y_i^T - \hat{\mu}\hat{\mu}^T
\]</span> Let <span class="math display">\[
r_1 = \# (y_{1i} \text{ obs, }, y_{2i} \text{ missing }), r_2 = \# (y_{2i} \text{ obs, }, y_{1i} \text{ missing }), n - r_1 - r_2 = \# (y_{2i} \text{ obs, }, y_{1i} \text{ obs })
\]</span>, and suppose we have arranged our indices <span class="math inline">\(i\)</span> so <span class="math inline">\(i \in \{1, \dots, r_1\}\)</span> have <span class="math inline">\(y_1\)</span> observed, <span class="math inline">\(i \in \{r_1 + 1, \dots, r_1 + r_2\}\)</span> have <span class="math inline">\(y_2\)</span> observed and <span class="math inline">\(i \in \{r_1+r_2+1, \dots, n\}\)</span> have all data observed. Let <span class="math inline">\(f_{Y_j}(y_{ji} \mid \mu_j, \Sigma_{j,j}), j = 1, 2\)</span> be the univariate normal density, while <span class="math inline">\(f_{Y}(y_{i} \mid \mu_1, \mu_2, \Sigma)\)</span> is the bivariate normal density. The observed data likelihood is <span class="math display">\[
L(\mu_1, \mu_2, \Sigma \mid Y_{(0)}) = \prod_{i=1}^{r_1} f_{Y_1}(y_{1i} \mid \mu_1, \Sigma_{1,1})\prod_{i=r_1+1}^{r_1+r_2} f_{Y_2}(y_{2i} \mid \mu_2, \Sigma_{2,2})\prod_{i=r_1+r_2+1}^{n} f_{Y_2}(y_{i} \mid \mu_1, \mu_2, \Sigma)
\]</span> We can find the MLEs for this expression, but it isn’t standard, and it’ll involve some thinking, whereas if we had a complete dataset we could just do the maximization very easily. The two variable dataset seems pretty tractable, but imagine we had many variables with lots of missingness patterns, and then the maximization would be hard.</p>
</section>
</section>
<section id="why-does-em-work" class="level2">
<h2 class="anchored" data-anchor-id="why-does-em-work">Why does EM work?</h2>
<p>Why does this work? We want to show that maximizing <span class="math inline">\(Q\)</span> is equivalent to maximizing <span class="math inline">\(L_\text{ign}\)</span>.</p>
<p>We can write the complete data distribution as the product of two factors: <span class="math display">\[
f_Y(Y_{(1)}, Y_{(0)} \mid \theta) = f(Y_{(0)} \mid \theta)f(Y_{(1)}\mid Y_{(0)}, \theta)
\]</span> Taking logs gives us the complete data likelihood in terms of the observe data likelihood and the log likelihood of the conditional distribution of the missing data given the observed data and a parameter value. <span class="math display">\[
\ell_Y(Y_{(1)}, Y_{(0)} \mid \theta) = \log(f(Y_{(0)} \mid \theta)) + \log(f(Y_{(1)}\mid Y_{(0)}, \theta))
\]</span> The first term is the observed data likelihood, which is what we want to maximize. Rearranging gives: <span class="math display">\[
\log(f(Y_{(0)} \mid \theta)) = \ell_Y(Y_{(1)}, Y_{(0)} \mid \theta) - \log(f(Y_{(1)}\mid Y_{(0)}, \theta))
\]</span> Taking expectations for a current iterate <span class="math inline">\(\theta^r\)</span> over <span class="math inline">\(f(Y_{(1)}\mid Y_{(0)}, \theta^t)\)</span>, gives <span class="math display">\[
\Exp{\log(f(Y_{(0)} \mid \theta))}{f(Y_{(1)}\mid Y_{(0)}, \theta^t)} = Q(\theta \mid \theta^t) - H(\theta \mid \theta^t)
\]</span> where <span class="math inline">\(Q\)</span> is as above and <span class="math inline">\(H(\theta \mid \theta^t)\)</span> is: <span class="math display">\[
H(\theta \mid \theta^{(t)}) = \int_{\mathcal{Y}_{(1)}} \log f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta)f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta^t)dY_{(1)}
\]</span> We can show that <span class="math inline">\(H(\theta \mid \theta^t) \leq H(\theta^t \mid \theta^t)\)</span> for all <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
    H(\theta \mid \theta^t)  - H(\theta^t \mid \theta^t) &amp; = \int_{\mathcal{Y}_{(1)}} \log \frac{f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta)}{f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta^t)}f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta^t)dY_{(1)}  \\
&amp; = \Exp{\log \frac{f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta)}{f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}}}{f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta^t)} \\
&amp; \leq
\log \Exp{\frac{f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta)}{f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}}}{f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta^t)} \\
&amp; = \log 1 = 0
\end{aligned}
\]</span> This is the same proof that shows that the KL divergence is always positive! <span class="math display">\[
\begin{aligned}
\text{KL}(f \mid g) = \int_{\mathcal{Y}} \log \frac{f(y)}{g(y)} f(y) dy
\end{aligned}
\]</span></p>
<p>Now we look at the value of the maximized oberved data likelihood: <span class="math display">\[
\Exp{\log(f(Y_{(0)} \mid \theta^{t}))}{f(Y_{(1)} \mid Y_{(0)}, \theta^t)}
\]</span> and how this changes between steps <span class="math inline">\(t\)</span> and <span class="math inline">\(t+1\)</span> so that <span class="math inline">\(\theta^{t+1}\)</span> as <span class="math inline">\(\theta^{t+1} = \text{argmax}_\theta Q(\theta \mid \theta^{t})\)</span>.</p>
<p>The difference between the observed likelihoods</p>
<p><span class="math display">\[
\begin{aligned}
\Exp{\log(f(Y_{(0)} \mid \theta^{t+1}))}{f(Y_{(1)} \mid Y_{(0)}, \theta^{t+1})} - &amp; \Exp{\log(f(Y_{(0)} \mid \theta^{t}))}{{f(Y_{(1)} \mid Y_{(0)}, \theta^{t})}} \\ &amp; =  Q(\theta^{t+1} \mid \theta^t) - Q(\theta^{t} \mid \theta^t) - (H(\theta^{t+1} \mid \theta^t) - H(\theta^{t} \mid \theta^t))
\end{aligned}
\]</span> Thus, by maximizing the expected complete data likelihood, we in turn maximize the function we really want, namely the observed likelihood.</p>
<p>Another result is that if <span class="math inline">\(\theta^{t+1}\)</span> is chosen such that: 1. <span class="math inline">\(\frac{\partial}{\partial \theta} Q(\theta \mid \theta^t) \mid_{\theta = \theta^{t+1}} = 0\)</span></p>
<ol start="2" type="1">
<li><p><span class="math inline">\(\theta^{t+1} \to \theta^\star\)</span></p></li>
<li><p><span class="math inline">\(f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta)\)</span> is sufficiently smooth in <span class="math inline">\(\theta\)</span></p></li>
</ol>
<p>Then</p>
<p><span class="math inline">\(\frac{\partial}{\partial \theta} \ell(\theta \mid Y_{(0)} = \tilde{y}_{(0)}) \mid_{\theta = \theta^\star} = 0\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial}{\partial \theta} \ell(\theta \mid Y_{(0)} = \tilde{y}_{(0)}) \mid_{\theta = \theta^\star} = \frac{\partial}{\partial \theta} Q(\theta \mid \theta^{t}) \mid_{\theta = \theta^\star} - \frac{\partial}{\partial \theta} H(\theta \mid \theta^{t}) \mid_{\theta = \theta^\star}
\end{aligned}
\]</span> The first quantity on the RHS is zero by the condition that we pick <span class="math inline">\(\theta^{t+1}\)</span> as that which leads to <span class="math inline">\(\frac{\partial}{\partial \theta} Q(\theta \mid \theta^{t}) \mid_{\theta = \theta^\star}=0\)</span>, so if <span class="math inline">\(\theta^t \to \theta^\star\)</span>, we must have gotten there by setting these gradients equal to zero. The second quantity on the RHS is <span class="math display">\[
\begin{aligned}
\frac{\partial}{\partial \theta} H(\theta \mid \theta^{t}) \mid_{\theta = \theta^\star} &amp; = \frac{\partial}{\partial \theta} \int_{\mathcal{Y}_{(1)}} \log f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta) f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta^t) dY_{(1)} \mid_{\theta=\theta^\star} \\
&amp; = \int_{\mathcal{Y}_{(1)}} \frac{\frac{\partial}{\partial \theta}f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta)\mid_{\theta=\theta^\star}}{f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta^\star)}  f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta^t) dY_{(1)} \mid_{\theta=\theta^\star} \\
\end{aligned}
\]</span> as <span class="math inline">\(\theta^t \to \theta^\star\)</span> the denominators cancel, leaving</p>
<p><span class="math display">\[
\int_{\mathcal{Y}_{(1)}} \frac{\partial}{\partial \theta}f(Y_{(1)} \mid Y_{(0)} = \tilde{y}_{(0)}, \theta)\mid_{\theta=\theta^\star}dY_{(1)}
\]</span> which, if we pull the derivative out of the integral again, equals zero because we’re differentiating a constant.</p>
<section id="multivariate-normal-example-again" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-normal-example-again">Multivariate normal example again</h3>
<p>In the multivariate normal example we know how to maximize the likelihood, but how do we do the conditional expectation?</p>
<p><span class="math display">\[
\Exp{\ell_{Y}(\mu, \Sigma \mid Y) \mid Y_{(0)}, \mu^t, \Sigma^t} = \frac{1}{2} \log (\det\Sigma^{-1}) -\frac{1}{2}\text{tr}\sum_i\Exp{\lp(y_i - \mu) (y_i - \mu)^T  \mid Y_{(0)}, \mu^t, \Sigma^t}\Sigma^{-1}\rp
\]</span> If we expand out the cross product, we see we need <span class="math display">\[
\Exp{(y_i - \mu) (y_i - \mu)^T  \mid Y_{(0)}, \mu^t, \Sigma^t} = \Exp{y_i y_i^T \mid Y_{(0)}, \mu^t, \Sigma^t} - \Exp{\mu y_i^T - y_i^T \mu \mid Y_{(0)}, \mu^t, \Sigma^t}  - \mu\mu^T
\]</span> The first term is : <span class="math display">\[
\Exp{y_i y_i^T \mid Y_{(0)}, \mu^t, \Sigma^t} = \text{Cov}(y_i \mid Y_{(0)}, \mu^t, \Sigma^t) + \Exp{y_i \mid Y_{(0)}, \mu^t, \Sigma^t}\Exp{y_i \mid Y_{(0)}, \mu^t, \Sigma^t}^T
\]</span> Plugging this back in above gives:</p>
<p><span class="math display">\[
\text{Cov}(y_i \mid Y_{(0)}, \mu^t, \Sigma^t) + \Exp{y_i \mid Y_{(0)}, \mu^t, \Sigma^t}\Exp{y_i \mid Y_{(0)}, \mu^t, \Sigma^t}^T - \Exp{\mu y_i^T - y_i^T \mu \mid Y_{(0)}, \mu^t, \Sigma^t}  - \mu\mu^T
\]</span> simplifying to <span class="math display">\[
\text{Cov}(y_i \mid Y_{(0)}, \mu^t, \Sigma^t) + (\Exp{y_i \mid Y_{(0)}, \mu^t, \Sigma^t} - \mu )(\Exp{y_i \mid Y_{(0)}, \mu^t, \Sigma^t} - \mu )^T
\]</span> Pluggig this back in above, we get the expected log-likelihood <span class="math display">\[
\begin{aligned}
&amp; \Exp{\ell_{Y}(\mu, \Sigma \mid Y) \mid Y_{(0)}, \mu^t, \Sigma^t} = \frac{1}{2} \log (\det\Sigma^{-1}) \\
&amp; -\frac{1}{2}\text{tr}\sum_i\lp\text{Cov}(y_i \mid Y_{(0)}, \mu^t, \Sigma^t) + (\Exp{y_i \mid Y_{(0)}, \mu^t, \Sigma^t} - \mu )(\Exp{y_i \mid Y_{(0)}, \mu^t, \Sigma^t} - \mu )^T\rp\Sigma^{-1}
\end{aligned}
\]</span> This leads to the M step estimates:</p>
<p><span class="math display">\[
\mu^{t+1} = \frac{1}{n} \sum_i \Exp{y_i \mid Y_{(0)}, \mu^t, \Sigma^t},
\]</span></p>
<p>and for</p>
<p><span class="math display">\[
\Sigma^{t+1} = \frac{1}{n}\sum_i\text{Cov}(y_i \mid y_{i(0)}, \mu^t, \Sigma^t) + (\Exp{y_i \mid y_{i(0)}, \mu^t, \Sigma^t} - \mu^{t+1} )(\Exp{y_i \mid y_{i(0)}, \mu^t, \Sigma^t} - \mu^{t+1} )^T
\]</span></p>
<p>The key is that the conditional expectation and covariances for <span class="math inline">\(y_i\)</span> are informed by the observed data.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>